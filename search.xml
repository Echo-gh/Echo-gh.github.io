<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>论文记录 Co-fusion_Real-time segmentation, tracking and fusion of multiple objects</title>
    <url>/2024/01/27/Co-fusion/</url>
    <content><![CDATA[<p>Runz, Martin, and Lourdes Agapito. “Co-Fusion: Real-Time Segmentation, Tracking and Fusion of Multiple Objects.” In <em>2017 IEEE International Conference on Robotics and Automation (ICRA)</em>, 4471–78. Singapore, Singapore: IEEE, 2017. <a href="https://doi.org/10.1109/ICRA.2017.7989518">https://doi.org/10.1109/ICRA.2017.7989518</a>.</p>
<h1 id="3-Overview-of-our-Method"><a href="#3-Overview-of-our-Method" class="headerlink" title="3 Overview of our Method"></a>3 Overview of our Method</h1><p>Co-Fusion 是一个可以<strong>实时处理</strong>每一帧输入图片的RGB-D SLAM 系统，本系统为场景中每一个分割的物体<strong>存储模型</strong>，而且可以<strong>独立跟踪</strong>它们的运动，每个模型是由<strong>一组3D 点</strong>构成的。本系统维护两组物体模型：当前在视野中可见的<strong>active 模型</strong>，以及曾经观测到的模型，但是目前不在视野中，记为<strong>inactive 模型</strong>。本系统的框架如Fig. 2所示，在初始化阶段，场景只包含一个active 模型——<strong>背景</strong>，初始化完成后，按照Fig .2的流程处理每一帧图片。</p>
<span id="more"></span>
<p><img src="/2024/01/27/Co-fusion/overview.png" alt="overview" title="Overview"></p>
<p><strong>tracking</strong> 与 <strong>fusion</strong> 步骤是在GPU 上完成的，而<strong>segmentation</strong> 步骤是在CPU 上完成的。</p>
<h2 id="3-1-Tracking"><a href="#3-1-Tracking" class="headerlink" title="3.1 Tracking"></a><strong>3.1 Tracking</strong></h2><p>在当前帧中跟踪每一个active 模型的6DOF 位姿，通过最小化每个模型独立的目标方程来实现，该目标方程包含：</p>
<ul>
<li><strong>几何误差</strong>：基于稠密的iterative closest point (ICP) 对齐；</li>
<li><strong>光度误差</strong>：基于当前帧和存储的3D 模型之间的颜色差异。</li>
</ul>
<h2 id="3-2-Segmentation"><a href="#3-2-Segmentation" class="headerlink" title="3.2 Segmentation"></a><strong>3.2 Segmentation</strong></h2><p>该阶段将当前帧中的每一个像素与某个active 模型/物体联系起来，有两种手段来实现该过程：motion 以及 semantic labels。</p>
<h3 id="3-2-1-Motion-segmentation"><a href="#3-2-1-Motion-segmentation" class="headerlink" title="3.2.1 Motion segmentation"></a>3.2.1 Motion segmentation</h3><p>将运动分割构建为一个使用全连接的Conditional Random Field (CRF) 解决的<strong>分类问题</strong>，可在CPU 上实时处理。当将一个像素与一个运动模型联系起来时，unary potentials 编码一个几何 <strong>ICP 损失函数</strong>。</p>
<h3 id="3-2-2-Multi-class-image-segmentation"><a href="#3-2-2-Multi-class-image-segmentation" class="headerlink" title="3.2.2 Multi-class image segmentation"></a>3.2.2 Multi-class image segmentation</h3><p>利用基于<strong>深度学习</strong>的方法实现像素级的语义分割，作为动作分割的备选方案。</p>
<h2 id="3-3-Fusion"><a href="#3-3-Fusion" class="headerlink" title="3.3 Fusion"></a>3.3 Fusion</h2><p>本系统使用surfel-based 融合方法，利用新估计的6DOF 位姿将属于某个模型的点更新至其active 模型。其中，每个模型是由一个sufel 列表构成的，$\mathcal{M}_m^s \in (\mathbf{p}\in \mathbb{R}^3, \mathbf{n}\in \mathbb{R}^3, \mathbf{c}\in \mathbb{N}^3, w \in \mathbb{R}, r \in \mathbb{R}, \mathbf{t}\in \mathbb{R}^2)$ ，分别表示位置、法向量、颜色、权重、半径以及两个时间戳。</p>
<p>为了解决<strong>动态物体</strong>的跟踪问题，本系统使用 $\mathcal{T}_t = \{\mathbf{T}_{tm}()\}$ 来表示每个active 模型 $\mathcal{M}_m$ 在时间 t 相对于全局参考坐标系的位姿转换几何，即 $\mathbf{T}_{tm}$ 表示时间 t 时模型 $\mathcal{M}_m$ 的全局位姿。特别地，作者使用 $\mathbf{T}_{tb}$ 来表示<strong>背景模型</strong>的位姿转换。</p>
<h1 id="5-Tracking-Active-Models"><a href="#5-Tracking-Active-Models" class="headerlink" title="5 Tracking Active Models"></a>5 Tracking Active Models</h1><p>对于时间t 的图像帧中的每一个active 模型  $\mathcal{M}_m$ ，系统通过配准<strong>当前的深度图</strong>和<strong>前一帧的深度图</strong>（通过将存储的3D 模型利用t-1 的估计位姿进行投影而获取）来跟踪其全局位姿 $\mathbf{T}_{tm}$ ，对每一个active 模型进行<strong>独立优化和跟踪</strong>。</p>
<h2 id="5-1-Energy"><a href="#5-1-Energy" class="headerlink" title="5.1 Energy"></a>5.1 Energy</h2><p>误差项包含<strong>ICP 几何误差</strong>和<strong>光度误差</strong>，其中光度误差是由预测的图片（将之前帧中存储的3D 模型投影而获取）与当前图片的<strong>颜色差异</strong>构成，</p>
<p><img src="/2024/01/27/Co-fusion/formula1.png" alt="formula1"></p>
<h2 id="5-2-Geometry-Term"><a href="#5-2-Geometry-Term" class="headerlink" title="5.2 Geometry Term"></a>5.2 Geometry Term</h2><p>ICP 几何误差定义为以下两者之间的误差：</p>
<ul>
<li>当前帧深度图的<strong>逆向投影</strong>3D 点；</li>
<li>前一帧t-1 <strong>预测</strong>的深度图。</li>
</ul>
<p><img src="/2024/01/27/Co-fusion/formula2.png" alt="formula2"></p>
<p>其中，$\mathbf{v}_t^i$ 是当前帧深度图 $\mathcal{D}_t$ 中第i 个点的反向映射3D 点；$\mathbf{v}^i, \mathbf{n}^i$ 分别是t-1 时刻<strong>预测的深度图</strong>中模型m 第i 个点的反向映射点以及其法向量；$\mathbf{T}_m$ 是将前一帧与当前帧的模型m 对齐的<strong>位姿转换</strong>。</p>
<h2 id="5-3-Photometric-Color-Term"><a href="#5-3-Photometric-Color-Term" class="headerlink" title="5.3 Photometric Color Term"></a>5.3 Photometric Color Term</h2><p>在给定(1)当前深度图、(2)每个active 模型的3D 几何估计，以及(3)将每个模型与前一帧对齐的位姿转换关系，即可将当前场景合成为一个与前一帧对齐的虚拟投影，进而，跟踪问题就变为了当前帧与合成的虚拟投影之间的<strong>光度配准</strong>问题：</p>
<p><img src="/2024/01/27/Co-fusion/formula3.png" alt="formula3"></p>
<p>其中，$\mathbf{T}_m$ 是将前一帧与当前帧的模型m 对齐的<strong>位姿转换</strong>；$\mathbf{I}_{t-1}()$ 表示提供模型在前一帧上顶点的颜色参数。</p>
<p>为了鲁棒性和效率，本优化使用一个<strong>4层的空间金字塔</strong>来集成到一个<strong>由粗到细</strong>的方法中，并在GPU 中完成运算。</p>
<h1 id="6-Motion-Segmentation"><a href="#6-Motion-Segmentation" class="headerlink" title="6 Motion Segmentation"></a>6 Motion Segmentation</h1><p>在跟踪步骤之后，在t 时刻有 $M_t$ 个新的位姿转换 $\{\mathbf{T}_{tm}\}$ ，来描述每个active 模型相对于全局坐标系的绝对位姿；接下来作者将帧t 的运动分割问题构建为一个<strong>分类标记问题</strong>，而标签为 $M_t$ 个位姿转换 $\{\mathbf{T}_{tm}\}$ ，作者将 $M_t+1$ 种可能分配到<strong>每一个像素</strong>中，即 $\mathcal{l} \in \mathcal{L}_t = \{1, …, |M_t|+1\}$ ，除了 $M_t$ 个位姿转换 $\{\mathbf{T}_{tm}\}$外还包含一个<strong>外点标签</strong> $\mathcal{l}_{|M_t|+1}$ 。</p>
<p>为了可以在CPU上实施完成分割步骤，系统首先将当前帧分割为SLIC 超像素，并在超像素级别上进行分类标记，超像素的位置、颜色与深度由从属的所有像素均值得到。代价函数如下所示：</p>
<p><img src="/2024/01/27/Co-fusion/formula4.png" alt="formula4"></p>
<p>其中，i 和 j 图片中超像素的索引（从1到S）。</p>
<h2 id="6-1-The-Unary-Potentials"><a href="#6-1-The-Unary-Potentials" class="headerlink" title="6.1 The Unary Potentials"></a>6.1 The Unary Potentials</h2><p>对于 $\psi_u(x_i)$ 表示为超像素 $s_i$ 分配标签为 $x_i$ 的代价，对于运动分割模式，该代价为<strong>ICP 几何对齐损失函数</strong>（式 2）。</p>
<h2 id="6-2-The-Pairwise-Potentials"><a href="#6-2-The-Pairwise-Potentials" class="headerlink" title="6.2 The Pairwise Potentials"></a>6.2 The Pairwise Potentials</h2><p>对于 $\psi_p(x_i, x_j)$ 可表示为：</p>
<p><img src="/2024/01/27/Co-fusion/formula5.png" alt="formula5"></p>
<p>其中，$\mu(x_i, x_j)$ 惩罚<strong>临近像素标签不同</strong>的情况；$k_m(f_i, f_j)$ 测量像素外观之间的<strong>相似度</strong>，代表的含义是：两个超像素的<strong>特征向量</strong>之间的距离较小时应具有相同的标签，所谓的特征向量 $f_i$  包含2D 位置、RGB 颜色以及深度值。</p>
<h2 id="6-3-Post-processing"><a href="#6-3-Post-processing" class="headerlink" title="6.3 Post-processing"></a>6.3 Post-processing</h2><p>在分割之后，采用一系列<strong>后处理步骤</strong>来获取更为鲁棒的结果：</p>
<ol>
<li>对具有相似位姿转换关系的模型进行<strong>融合操作</strong>；</li>
<li>抑制相同标签中除最大之外的所有区域来保证不连接区域的<strong>独立建模</strong>；</li>
<li>小于一定阈值的区域被<strong>舍弃</strong>。</li>
</ol>
<h2 id="6-4-Addition-of-New-Models"><a href="#6-4-Addition-of-New-Models" class="headerlink" title="6.4 Addition of New Models"></a>6.4 Addition of New Models</h2><p>一个区域内的外点数量若大于总像素数的3%，则判定该物体为一个新物体；若该新物体的部分几何结构已存在于地图中，会对重复的构建进行剔除。</p>
<p>如果一个物体消失在视野中，并在一定帧内不再出现，则将该模型添加进inactive 列表中。</p>
<h1 id="7-Object-Instance-Segmentation"><a href="#7-Object-Instance-Segmentation" class="headerlink" title="7 Object Instance Segmentation"></a>7 Object Instance Segmentation</h1><p>使用实例分割网络SharpMask 进行物体语义信息的获取。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu系统安装</title>
    <url>/2024/01/26/Ubuntu-setup/</url>
    <content><![CDATA[<h1 id="1-系统盘制作"><a href="#1-系统盘制作" class="headerlink" title="1 系统盘制作"></a>1 系统盘制作</h1><p>推荐使用 Rufus 这个软件进行制作：</p>
<span id="more"></span>
<p><img src="/2024/01/26/Ubuntu-setup/rufus.png" alt="rufus" title="rufus设置界面"></p>
<h1 id="2-进入BIOS更改启动顺序"><a href="#2-进入BIOS更改启动顺序" class="headerlink" title="2 进入BIOS更改启动顺序"></a>2 进入BIOS更改启动顺序</h1><p>将制作好的U盘插入电脑，启动电脑进入BIOS，设置第一启动项为U盘启动，保存BIOS设置并重新启动电脑，启动后选择”Try or install Ubuntu”选项，注意，此时可能会出现电脑黑屏的情况，但是屏幕是点亮的、鼠标有灯效，证明确实进入了安装系统，但是由于Ubuntu显卡驱动的问题，此时需要进行额外设置：</p>
<ol>
<li><p>重新启动电脑，在光标选择”Try or install Ubuntu”选项后，不要点击Enter键，而是点击”e”键进入命令行编辑模型；</p>
</li>
<li><p>删除”quite splash”后的”—-“，并添加”nomodeset”（依照不同显卡进行不同显卡驱动选项的添加，对于Nvidia显卡，添加nomodeset）；</p>
</li>
<li><p>然后，点击”F10”开始安装，此时电脑屏幕会正常。</p>
</li>
</ol>
<p>值得注意的是，装机成功重启后可能也会出现黑屏的情况（本人没有遇到），此时应在开机后点击”e”键，同样找到”quite splash” 并在后面添加”nomodeset”，按”F10”启动系统 ，进去系统之后编辑”/etc/default/grub”这个文件，具体操作在此不做赘述，可参考<a href="https://blog.csdn.net/qq_32285693/article/details/118900765">该文章</a>。</p>
<h1 id="3-设置硬盘分区"><a href="#3-设置硬盘分区" class="headerlink" title="3 设置硬盘分区"></a>3 设置硬盘分区</h1><p>本人想将之前的双系统进行清空，但又不想使用默认设置，故选择”something else”：</p>
<ol>
<li>清空之前双系统的所有磁盘空间，将硬盘全置为free；</li>
<li>根据个人需求设置硬盘分区，以下是重点部分，本人设置情况如下所示（<strong>注意分区类型</strong>）：</li>
</ol>
<p><img src="/2024/01/26/Ubuntu-setup/disk_set.png" alt="磁盘分区设置" title="磁盘分区设置"></p>
<p>具体空间分配可根据个人使用习惯和硬盘大小进行设置，值得注意的是，<strong>EFI 分区一定是第一个设置的！！！</strong></p>
<h1 id="4-Ubuntu设置"><a href="#4-Ubuntu设置" class="headerlink" title="4 Ubuntu设置"></a>4 Ubuntu设置</h1><p>进入Ubuntu系统后的设置不再赘述。</p>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu系统安装显卡驱动、CUDA、cuDNN</title>
    <url>/2024/01/26/Ubuntu-dl-setup/</url>
    <content><![CDATA[<p><strong>本文主要参考</strong><a href="https://zhuanlan.zhihu.com/p/643954422">该文章</a>。</p>
<h2 id="1-安装显卡驱动"><a href="#1-安装显卡驱动" class="headerlink" title="1 安装显卡驱动"></a>1 安装显卡驱动</h2><h3 id="1-1-前期准备"><a href="#1-1-前期准备" class="headerlink" title="1.1 前期准备"></a>1.1 前期准备</h3><p>根据显卡型号在<a href="https://www.nvidia.com/Download/Find.aspx?lang=en-us#">Nvidia 官网</a>下载相应的驱动程序，然后安装必备软件：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 更新源</span></span><br><span class="line">sudo apt-get update</span><br><span class="line"><span class="comment">## 安装必备软件</span></span><br><span class="line">sudo apt-get install g++</span><br><span class="line">sudo apt-get install gcc</span><br><span class="line">sudo apt-get install make</span><br></pre></td></tr></table></figure>
<h3 id="1-2-禁用默认驱动"><a href="#1-2-禁用默认驱动" class="headerlink" title="1.2 禁用默认驱动"></a>1.2 禁用默认驱动</h3><p>在安装NVIDIA驱动前需要禁止系统自带的显卡驱动 nouveau：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 编辑blacklist.conf 文件</span></span><br><span class="line">sudo vi /etc/modprobe.d/blacklist.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在文件末尾添加以下内容并保存</span></span><br><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新initramfs，然后重启电脑</span></span><br><span class="line">sudo update-initramfs -u</span><br><span class="line">sudo reboot now</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查是否关闭nouveau，若没有输出，则说明已成功关闭</span></span><br><span class="line">lsmod | grep nouveau</span><br></pre></td></tr></table></figure>
<h3 id="1-3-安装驱动"><a href="#1-3-安装驱动" class="headerlink" title="1.3 安装驱动"></a>1.3 安装驱动</h3><p>进入tty 模式，并关闭图形进程：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo service gdm3 stop</span><br></pre></td></tr></table></figure>
<p>开始安装驱动：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 赋予执行权限</span></span><br><span class="line">sudo <span class="built_in">chmod</span> 777 NVIDIA-Linux-x86_64-535.54.03.run</span><br><span class="line"><span class="comment">## 安装：不安装OpenGL,安装时关闭X服务  </span></span><br><span class="line">sudo ./NVIDIA-Linux-x86_64-535.54.03.run –no-opengl-files -no-x-check</span><br><span class="line"><span class="comment">## Install Nvidia&#x27;s 32-bit compatibility libraries?</span></span><br><span class="line"><span class="comment">## 选择 &quot;No&quot;</span></span><br><span class="line"><span class="comment">## Would you like to run the nvidia-xconfig utility to automatically update your X configuration file so that the NVIDIA X driver dill be used dhen you restart X? Any pre-existing X configuration file will be backed up.</span></span><br><span class="line"><span class="comment">## 选择 &quot;Yes&quot;</span></span><br></pre></td></tr></table></figure>
<p>成功安装之后，会进入图形界面，此时使用命令nvidia-smi 检查驱动是否安装成功，若出现下图界面，证明驱动安装成功。</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/nvidia-smi.png" alt="nvidia-smi" title="nvidia-smi显示界面"></p>
<h3 id="1-4-显卡驱动失效问题记录"><a href="#1-4-显卡驱动失效问题记录" class="headerlink" title="1.4 显卡驱动失效问题记录"></a>1.4 显卡驱动失效问题记录</h3><p>一次重启电脑后，发现Ubuntu提示某个文件发生错误，是否需要反馈给Ubuntu，本人当时没在意点了“否”，在后续操作过程中发现nvidia-smi命令报错，显示找不到显卡驱动：</p>
<blockquote>
<p>NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</p>
</blockquote>
<p>但使用nvcc -V 命令可以正常显示CUDA 版本。由于之前也遇到过类似驱动失效的问题，所以选择重装显卡驱动，按照之前的步骤检查nouveau、关闭图形界面、安装驱动……然后就一路报错了……</p>
<blockquote>
<p>NVIDIA-SMI has failed because it couldn‘t communicate with the NVIDIA driver.</p>
</blockquote>
<p>网上找解决方案，这篇<a href="https://blog.csdn.net/wjinjie/article/details/108997692">文章</a>提到一个解决方案，使用dkms：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 首先，查看显卡驱动版本</span></span><br><span class="line"><span class="built_in">ls</span> /usr/src | grep nvidia</span><br><span class="line">nvidia-535.146.02</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 然后，安装dkms，并修复显卡驱动</span></span><br><span class="line">sudo apt-get install dkms</span><br><span class="line">sudo dkms install -m nvidia -v 535.146.02</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 输入nvidia-smi命令，一切恢复正常</span></span><br></pre></td></tr></table></figure>
<p>然后搜了一下dkms的作用，wikipedia 中介绍如下：</p>
<blockquote>
<p>动态内核模块支持 （Dynamic Kernel Module Support，DKMS）是用来生成Linux的内核模块的一个框架，其源代码一般不在Linux内核源代码树。 当新的内核安装时，DKMS 支持的内核设备驱动程序 到时会自动重建。 DKMS 可以用在两个方向：如果一个新的内核版本安装，自动编译所有的模块，或安装新的模块（驱动程序）在现有的系统版本上，而不需要任何的手动编译或预编译软件包需要。例如，这使得新的显卡可以使用在旧的Linux系统上。</p>
</blockquote>
<p>唔……很奇怪，我应该没有更新Ubuntu的内核，不晓得为什么会出现这个情况，先记录一下吧，当前本人的Ubuntu内核版本为：</p>
<blockquote>
<p>Linux echo-dell 6.5.0-14-generic #14~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Nov 20 18:15:30 UTC 2 x86_64 x86_64 x86_64 GNU/Linux</p>
</blockquote>
<h2 id="2-安装CUDA"><a href="#2-安装CUDA" class="headerlink" title="2 安装CUDA"></a>2 安装CUDA</h2><h3 id="2-1-下载与安装"><a href="#2-1-下载与安装" class="headerlink" title="2.1 下载与安装"></a>2.1 下载与安装</h3><p>可同时安装不同的CUDA 版本，根据不同环境需求选择使用不同版本，本处以CUDA-11.8为例进行安装说明。</p>
<p>在<a href="https://developer.nvidia.com/cuda-toolkit-archive">NVIDIA官网</a>下载对应版本，推荐使用runfile (local) 进行安装：</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/CUDA.png" alt="cuda" title="CUDA安装命令"></p>
<p>根据官网给出的下载和安装命令执行即可：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run</span><br><span class="line">sudo sh cuda_11.8.0_520.61.05_linux.run</span><br></pre></td></tr></table></figure>
<h3 id="2-2-环境变量配置"><a href="#2-2-环境变量配置" class="headerlink" title="2.2 环境变量配置"></a>2.2 环境变量配置</h3><p>安装完成后，打开账户的配置文件，进行以下修改：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## CUDA ENV</span></span><br><span class="line"><span class="built_in">export</span> CUDA_HOME=/usr/local/cuda</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>:<span class="variable">$&#123;CUDA_HOME&#125;</span>/lib64</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;CUDA_HOME&#125;</span>/bin:<span class="variable">$&#123;PATH&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新配置文件设置</span></span><br><span class="line"><span class="built_in">source</span> ~/.zshrc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证安装是否成功</span></span><br><span class="line">nvcc -V</span><br></pre></td></tr></table></figure>
<p>若出现下图界面，则证明CUDA 安装成功:</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/nvcc.png" alt="cuda" title="检验CUDA安装成功"></p>
<h3 id="2-3-多版本切换"><a href="#2-3-多版本切换" class="headerlink" title="2.3 多版本切换"></a>2.3 多版本切换</h3><p>CUDA 安装位置的文件如下图所示：</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/CUDA-version.png" alt="cuda-version" title="CUDA版本切换"></p>
<p>根据所需版本，切换cuda 的软链接即可：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo <span class="built_in">ln</span> -snf /usr/local/cuda-11.8 /usr/local/cuda</span><br></pre></td></tr></table></figure>
<h2 id="3-安装cuDNN"><a href="#3-安装cuDNN" class="headerlink" title="3 安装cuDNN"></a>3 安装cuDNN</h2><p>注：此处可参考<a href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#verify">NVIDIA官网教程</a>进行安装。</p>
<h3 id="3-1-前期准备与下载"><a href="#3-1-前期准备与下载" class="headerlink" title="3.1 前期准备与下载"></a>3.1 前期准备与下载</h3><p>首先安装依赖包：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install zlib1g</span><br></pre></td></tr></table></figure>
<p>然后在<a href="https://developer.nvidia.com/rdp/cudnn-download">NVIDIA官网</a>下载相应版本的安装包，注意，下载cuDNN 需要注册NVIDIA账号。选取合适的版本进行下载：</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/cudnn.png" alt="cudnn" title="cuDNN安装包"></p>
<h3 id="3-2-安装"><a href="#3-2-安装" class="headerlink" title="3.2 安装"></a>3.2 安装</h3><p>解压并安装：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i cudnn-local-repo-ubuntu2204-8.9.3.28_1.0-1_amd64.deb</span><br></pre></td></tr></table></figure>
<p>按照提示导入CUDA GPG key：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo <span class="built_in">cp</span> /var/cudnn-local-repo-ubuntu2204-8.9.3.28/cudnn-local-*-keyring.gpg /usr/share/keyrings/</span><br></pre></td></tr></table></figure>
<p>更新源并安装：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line"><span class="comment">## 1. Install the runtime library.</span></span><br><span class="line"><span class="comment">## sudo apt-get install libcudnn8=8.x.x.x-1+cudaX.Y</span></span><br><span class="line">sudo apt-get install libcudnn8=8.9.3.28-1+cuda11.8</span><br><span class="line"><span class="comment">## 2. Install the developer library.</span></span><br><span class="line"><span class="comment">## sudo apt-get install libcudnn8-dev=8.x.x.x-1+cudaX.Y</span></span><br><span class="line">sudo apt-get install libcudnn8-dev=8.9.3.28-1+cuda11.8</span><br><span class="line"><span class="comment">## 3. Install the code samples.</span></span><br><span class="line"><span class="comment">## sudo apt-get install libcudnn8-samples=8.x.x.x-1+cudaX.Y</span></span><br><span class="line">sudo apt-get install libcudnn8-samples=8.9.3.28-1+cuda11.8</span><br></pre></td></tr></table></figure>
<h3 id="3-3-测试"><a href="#3-3-测试" class="headerlink" title="3.3 测试"></a>3.3 测试</h3><p>输入检查命令，出现下图证明安装初步成功。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo dpkg -l | grep cudnn</span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/Ubuntu-dl-setup/cudnn-dpkg.png" alt="cudnn-version" title="cuDNN安装成功"></p>
<p>按照官网教程，进行代码测试：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将cuDNN samples 拷贝至主目录</span></span><br><span class="line"><span class="built_in">cp</span> -r /usr/src/cudnn_samples_v8/ <span class="variable">$HOME</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入samples 文件夹，并编译</span></span><br><span class="line"><span class="built_in">cd</span>  <span class="variable">$HOME</span>/cudnn_samples_v8/mnistCUDNN</span><br><span class="line">make clean &amp;&amp; make</span><br></pre></td></tr></table></figure>
<p>此时可能出现如下报错：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">fatal error: FreeImage.h: No such file <span class="keyword">or</span> directory</span><br><span class="line">    <span class="number">1</span> | <span class="meta">#<span class="keyword">include</span> <span class="string">&quot;FreeImage.h&quot;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/Ubuntu-dl-setup/cudnn-error.png" alt="cudnn-error" title="cuDNN报错"></p>
<p>可安装相应包进行解决：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libfreeimage3 libfreeimage-dev</span><br></pre></td></tr></table></figure>
<p>安装之后重新编译，并运行生成文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 重新编译</span></span><br><span class="line">make clean &amp;&amp; make</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行生成文件</span></span><br><span class="line">./mnistCUDNN</span><br></pre></td></tr></table></figure>
<p>若出现如下结果，则证明cuDNN 安装成功：</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/cudnn-success.png" alt="cudnn-success" title="cuDNN安装成功"></p>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Deep Learning</tag>
        <tag>Nvidia</tag>
        <tag>CUDA</tag>
        <tag>cuDNN</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Probabilistic data association for semantic SLAM</title>
    <url>/2024/02/18/bowman2017/</url>
    <content><![CDATA[<p>Bowman, Sean L., Nikolay Atanasov, Kostas Daniilidis, and George J. Pappas. “Probabilistic Data Association for Semantic SLAM.” In <em>2017 IEEE International Conference on Robotics and Automation (ICRA)</em>, 1722–29. Singapore, Singapore: IEEE, 2017. <a href="https://doi.org/10.1109/ICRA.2017.7989203">https://doi.org/10.1109/ICRA.2017.7989203</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文做出的贡献：</p>
<ul>
<li>第一个将惯性、几何以及语义观测信息<strong>紧耦合至一个优化框架</strong>的定位算法；</li>
<li>将联合metric-semantic SLAM 问题分解为<strong>连续（位姿）</strong>和<strong>离散（数据关联DA和语义标签）</strong>优化子问题；</li>
<li>在包含<strong>光照变化的杂乱场景中</strong>利用里程计和视觉测量信息进行<strong>室内外真实场景</strong>的长轨迹测试实验。</li>
</ul>
<span id="more"></span>
<h1 id="2-Probabilistic-Data-Association-in-SLAM"><a href="#2-Probabilistic-Data-Association-in-SLAM" class="headerlink" title="2 Probabilistic Data Association in SLAM"></a>2 Probabilistic Data Association in SLAM</h1><p>经典的SLAM问题可表述如下：移动传感器穿越一个未知环境，环境中包含M 个静态路标集合 $\mathcal{L} = \{l_m\}^M_{m=1}$ ，给定一组传感器测量数据 $\mathcal{Z} = \{z_k\}^K_{k=1}$ ，任务是估计地标位置 $\mathcal{L}$ 以及传感器的位姿序列 $\mathcal{X} = \{x_t\}^T_{t=1}$ 。</p>
<p>大部分现有的工作都聚焦于估计  $\mathcal{L}$ 和  $\mathcal{X}$ ，很少强调数据关联 $\mathcal{D}$ <strong>实际上是未知的</strong>。这里的数据关联 $\mathcal{D} = \{(\alpha_k, \beta_k)\}^K_{k=1}$ 指的是在传感器位姿 $x_{\alpha_k}$ 情况下对路标 $l_{\beta_k}$ 的观测数据为 $z_k$ ，即在相机位姿 $x_{\alpha_k}$ 下，观测数据 $z_k$ 和路标 $l_{\beta_k}$ 之间的关联。</p>
<p>SLAM问题的求解可表述为：在给定观测数据 $\mathcal{Z}$ 时，求解  $\mathcal{L}$ ，$\mathcal{X}$ 和 $\mathcal{D}$ 的最大似然估计：</p>
<p><img src="/2024/02/18/bowman2017/f1.png" alt="f1" title="f1"></p>
<p><strong>求解方法一：</strong></p>
<p>给定先验估计 $\mathcal{X}^0,\mathcal{L}^0$ ，求得数据关联的最大似然估计 $\hat{\mathcal{D}}$ ，然后利用 $\hat{\mathcal{D}}$ 计算路标与传感器状态  $\mathcal{L}$ ，$\mathcal{X}$ 的最大似然估计。</p>
<p><img src="/2024/02/18/bowman2017/f2.png" alt="f2" title="f2"></p>
<p>上述方法的缺点在于，错误的数据匹配会严重影响估计结果；而如果为了避免错误匹配而舍弃模糊测量，在后续位姿优化后也无法利用这些模糊测量。</p>
<p><strong>求解方法二：坐标下降法coordinate descent</strong></p>
<p>为了避免方法一简单的一步处理，本方法使用迭代计算：</p>
<p><img src="/2024/02/18/bowman2017/f3.png" alt="f3" title="f3"></p>
<p>本方法可以在状态估计得到优化后返回来重新估计数据关联；然而，本方法不能解决模糊测量的问题，因为仍然需要一个<strong>硬数据关联</strong>。</p>
<p>为解决模糊测量导致的数据关联问题，当估计数据关联时，作者不是简单地选取物体属于某个特定的类别 $\hat{\mathcal{D}}=p(\mathcal{D}|\mathcal{X}, \mathcal{L}, \mathcal{Z})$ ，而是<strong>考虑观测数据与路标之间所有可能的数据关联</strong> $D$ 。给定一个初始估计 $\mathcal{X}^i, \mathcal{L}^i$ ,使用 $\mathcal{D}$ 中的全部可能可得到一个更优的估计值，作者采用了expectation maximization (EM) 方法来最大化 expected measurement likelihood 进行计算：</p>
<p><img src="/2024/02/18/bowman2017/f4.png" alt="f4" title="f4"></p>
<p>其中， $\mathbb{D}$ 是 $\mathcal{D}$ 的所有取值空间。本EM 方程优势在于不需要硬数据关联 hard decisions on data association，而是平均所有可能的数据关联情况。</p>
<p>为了与坐标下降法进行比较，上式可重写为：</p>
<p><img src="/2024/02/18/bowman2017/f5.png" alt="f5" title="f5"></p>
<p>其中，$w_{kj}^i$ 是一个量化了<strong>软数据关联</strong>的权重参数，表示K 个观测和M 个路标之间所有关联对应的权重参数，$w_{kj}^i = \sum_{\mathcal{D}\in\mathbb{D}(k,j)}p(\mathcal{D}|\mathcal{X}^i, \mathcal{L}^i, \mathcal{Z})$ ，其中，$\mathbb{D}(k,j) = \{\mathcal{D}\in\mathbb{D} | \beta_k = j\}\subseteq \mathbb{D}$ 是观测 $k$ 分配给地标 $j$ 的所有数据关联组合。</p>
<p>上式的求解同样采用<strong>EM 方法</strong>：</p>
<ul>
<li>E 步骤：以权重参数 $w_{kj}^i$ 的形式来估计<strong>数据关联分布</strong> $p(\mathcal{D}|\mathcal{X}^i, \mathcal{L}^i, \mathcal{Z})$ ；</li>
<li>M 步骤：根据E 步骤计算得到的分布来最大化 expected measurement log likelihood。</li>
</ul>
<h1 id="3-Semantic-SLAM"><a href="#3-Semantic-SLAM" class="headerlink" title="3 Semantic SLAM"></a>3 Semantic SLAM</h1><p>作者设定地标的状态 $l$ 包含位置信息 $l^p \in \mathbb{R}^3$ 和类别标签 $l^c$ 。为了估计<strong>地标状态</strong> $\mathcal{L}$ <strong>（位姿与种类）和传感器运动轨迹</strong> $\mathcal{X}$ ，作者使用三种信息源：惯性、几何点特征和物体语义观测。</p>
<p><img src="/2024/02/18/bowman2017/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-1-Inertial-information"><a href="#3-1-Inertial-information" class="headerlink" title="3.1 Inertial information"></a>3.1 Inertial information</h2><p>作者每隔n 帧选取一张单目相机采集到的图像作为关键帧，第t 帧对应的传感器状态表示为 $x_t$  ，包含6自由度的位姿、速度以及IMU 偏差值。在相邻两个关键帧之间还有一组IMU 测量数据 $\mathcal{I}$ ，包含线性加速度和旋转角速度。</p>
<h2 id="3-2-Geometric-information"><a href="#3-2-Geometric-information" class="headerlink" title="3.2 Geometric information"></a>3.2 Geometric information</h2><p>作者从每一个关键帧中提取ORB 特征点，并与后续的关键帧进行匹配。</p>
<h2 id="3-3-Semantic-information"><a href="#3-3-Semantic-information" class="headerlink" title="3.3 Semantic information"></a>3.3 Semantic information</h2><p>从第t 个关键帧中检测到一组物体 $\mathcal{S}_t$ ，其中每个物体表示为 $\mathbf{s}_k = (s_k^c, s_k^s, s_k^b) \in \mathcal{S}_t$ ，三个参数分别表示所属的物体种类、量化的检测置信度得分，以及bounding box。本文的物体检测使用deformable parts model (DPM) 算法完成，该算法可以在CPU上实时运行。</p>
<p>如果语义观测 $\mathbf{s}_k$ 的数据关联 $\mathcal{D} = \{(\alpha_k, \beta_k)\}$ 已知，<strong>语义观测似然</strong>可以分解为：</p>
<script type="math/tex; mode=display">p(\mathbf{s}_k|x_{\alpha_k},l_{\beta_k})=p(s_k^c|l_{\beta_k}^c)p(s_k^s|l_{\beta_k}^c,s_k^c)p(s_k^b|x_{\alpha_k},l_{\beta_k}^p)</script><p>其中，类别估计概率分布 $p(s_k^c|l_{\beta_k}^c)$ 对应于物体检测的<strong>混淆矩阵</strong>，其与得分分布（置信度）$p(s_k^s|l_{\beta_k}^c,s_k^c)$ 可以经过离线学习获取。而bounding-box 似然 $p(s_k^b|x_{\alpha_k},l_{\beta_k}^p)$ 被假设服从正态分布，其均值为物体中心在成像平面上的透视投影 (???)，协方差与探测的bounding-box 尺寸成比例关系。</p>
<blockquote>
<p>作者将<strong>语义SLAM 问题</strong>表述如下：</p>
<p>给定惯性测量 $\mathcal{I} = \{\mathcal{I}_t\}^T_{t=1}$ ，几何测量 $\mathcal{Y} = \{\mathcal{Y}_t\}^T_{t=1}$ 和语义测量 $\mathcal{S} = \{\mathcal{S}_t\}^T_{t=1}$ ，估计传感器的状态轨迹 $\mathcal{X}$ 以及环境中物体的位置和类别 $\mathcal{L}$ 。</p>
</blockquote>
<p>作者使用惯性和视觉几何测量信息来跟踪<strong>传感器的局部运动轨迹</strong>，不恢复几何结构。而语义测量信息用来构建包含物体信息的地图用以<strong>回环检测</strong>，不仅鲁棒性更好、精确度更高，而且相较于保留全部几何结构信息的SLAM 方法更为高效。</p>
<h1 id="4-Semantic-SLAM-using-EM"><a href="#4-Semantic-SLAM-using-EM" class="headerlink" title="4 Semantic SLAM using EM"></a>4 Semantic SLAM using EM</h1><blockquote>
<p>在优化过程中，作者除了将<strong>数据关联</strong>视为一个隐变量外，也将离散的<strong>路标类别</strong>视为一个隐变量，从而对离散变量和连续变量完成干净且高效的区分。</p>
</blockquote>
<p>几何特征点的数据关联由现有的特征跟踪算法提供，因此，本文提到的隐变量为语义观测信息的数据关联 $\mathcal{D}$  和地标所属类别 $l_{1:M}^c$  。</p>
<p>利用EM 求解语义SLAM 过程如下：</p>
<p><img src="/2024/02/18/bowman2017/f6.png" alt="f6" title="f6"></p>
<p>其中，$\mathbb{D}_t$ 是在时间戳 t 时刻观测数据中所有可能数据关联的组合，$\mathbb{D}_t(i,j)\subseteq \mathbb{D}_t$ 是所有可能的数据关联中观测 $i$ 分配给地标 $j$ 的组合。</p>
<h2 id="Object-classes-and-data-association-E-step"><a href="#Object-classes-and-data-association-E-step" class="headerlink" title="Object classes and data association (E step)"></a>Object classes and data association (E step)</h2><p>一旦每一个<strong>测量-地标匹配对</strong>的权重参数 $w_{kj}^{t,(i)}$ 得到之后，即可用于传感器状态和地标位置的连续优化中。此外，物体类别的最大似然 $l^c$ 也可以通过计算得到的 k 值进行恢复：</p>
<p><img src="/2024/02/18/bowman2017/f7.png" alt="f7" title="f7"></p>
<h2 id="Pose-graph-optimization-M-step"><a href="#Pose-graph-optimization-M-step" class="headerlink" title="Pose graph optimization (M step)"></a>Pose graph optimization (M step)</h2><p>作者使用位姿图进行优化。图包含一组顶点 $\mathcal{V}$ ，每个顶点对应一个优化变量；顶点之间还包含一组因子 $\mathcal{F}$ ，对应损失函数。优化方程如下所示：</p>
<p><img src="/2024/02/18/bowman2017/f8.png" alt="f8" title="f8"></p>
<p>前人的工作使用硬数据关联 a hard data association，在传感器位姿节点和地标节点之间定义一个单独的因子factor；而本文考虑使用soft semantic data association multiple factors，在节点间使用多个因子进行关联：Semantic Factors, Geometric Factors and Inertial Factors。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Semantic Monocular SLAM for Highly Dynamic Environments</title>
    <url>/2024/02/02/brasch2018/</url>
    <content><![CDATA[<p>Brasch, Nikolas, Aljaz Bozic, Joe Lallemand, and Federico Tombari. “Semantic Monocular SLAM for Highly Dynamic Environments.” In <em>2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 393–400. Madrid: IEEE, 2018. <a href="https://doi.org/10.1109/IROS.2018.8593828">https://doi.org/10.1109/IROS.2018.8593828</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者通过结合<strong>基于特征法</strong>与<strong>直接法</strong>，来实现动态环境中的鲁棒性。</p>
<p>本文的贡献：</p>
<ul>
<li>利用语义分割获取场景的语义信息，可检测<strong>潜在动态物体</strong>，并在后续跟踪中避免使用潜在动态物体上的特征点；</li>
<li>提出一个<strong>概率模型</strong>，考虑观测到某个地图点的<strong>所有帧的语义信息</strong>来更新该地图点的语义类别；</li>
<li>除了使用语义信息，还是用<strong>临时动作信息temporal dynamic information</strong> 来判断地图点是否是静态的；</li>
<li>为了实现实时性能，作者设计了一个<strong>高效的在线概率更新方法</strong>；</li>
<li>作者在合成和实际场景中测试了本算法，证明了本算法在高动态场景中可以获取更稳定的结果。</li>
</ul>
<span id="more"></span>
<p><img src="/2024/02/02/brasch2018/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-Probabilistic-Semantic-SLAM"><a href="#3-Probabilistic-Semantic-SLAM" class="headerlink" title="3 Probabilistic Semantic SLAM"></a>3 Probabilistic Semantic SLAM</h1><p>本系统是基于ORB-SLAM 开发的，整体架构如Fig. 2所示，</p>
<p><img src="/2024/02/02/brasch2018/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-Pose-estimation-and-mapping"><a href="#3-1-Pose-estimation-and-mapping" class="headerlink" title="3.1 Pose estimation and mapping"></a>3.1 Pose estimation and mapping</h2><p>鉴于ORB 特征点法和直接法的特点，作者会<strong>优先选择使用ORB 特征点法</strong>；而在特征点不充足的情况下（低纹理特征场景），使用直接法。包含重投影误差 $E_R$ 和光度误差 $E_P$ 的<strong>综合误差项</strong>如下所示：</p>
<p><img src="/2024/02/02/brasch2018/f1.png" alt="f1" title="formula 1"></p>
<p>作者使用带有鲁棒Huber 核的高斯牛顿法来求解非线性最小二乘问题，并使用逆协方差来代表<strong>观测的不确定度</strong>；对于每个新的观测，使用下式进行<strong>协方差传递更新</strong>：</p>
<p><img src="/2024/02/02/brasch2018/f5.png" alt="f5" title="formula 5"></p>
<h2 id="3-2-Probabilistic-outlier-rejection"><a href="#3-2-Probabilistic-outlier-rejection" class="headerlink" title="3.2 Probabilistic outlier rejection"></a>3.2 Probabilistic outlier rejection</h2><p>在更新地图点的位置参数时，由于一些观测较其他观测更为可靠，因此，使用一个概率模型将<strong>观测的方差</strong>作为权重的方法要比所有观测使用相同权重的方法表现更好。作者使用参数 $\phi$ 表示地图点的<strong>内点概率</strong>（静态点的可能），这样，地图点除了位置参数外，还包含深度 d，内点概率 $\phi$ ，以及语义类别 c。</p>
<p>当地图点被观测到时，利用三角化计算深度估计值 $d_i$ ，以及估计的方差 $\tau_i^2$ 。作者用 $CNN(c_k | I_i) \in [0, 1]$ 表示CNN 网络输出结果，表示当前图片 $I_i$ 中该特征点属于类别 $c_k$ 的概率。特征点的<strong>深度观测似然概率</strong>如下所示：</p>
<p><img src="/2024/02/02/brasch2018/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$\alpha_i$ 表示<strong>匹配准确度</strong>；$\bar{x} = (1 - x)$ 。上式的内在机理为：若当前特征点正确匹配，且地图点是静态的，那么 $\alpha_i, \phi$ 接近于1，则深度估计 $d_i$ 近似服从<strong>高斯分布</strong> $\mathcal{N}(\mu, \sigma^2)$ ；若当前匹配错误，或者特征点是动态的，则当前观测被认为服从均匀分布 $\mathcal{U}(a, b)$ ，在平均深度的估计中不提供任何有用的信息。与深度估计类似，作者将<strong>地图点的语义信息</strong>建模为网络输出 $CNN(c_k | I_i)$ 和错误匹配的均匀分布的混合：</p>
<p><img src="/2024/02/02/brasch2018/f7.png" alt="f7" title="formula 7"></p>
<p>这使得地图点可以高效地在线更新，以及在动态和静态之间进行<strong>平滑转换</strong>。</p>
<p>对于特征点的<strong>内点概率与语义类别的关系</strong>，可用Beta 分布来建模：</p>
<p><img src="/2024/02/02/brasch2018/f8.png" alt="f8" title="formula 8"></p>
<p>其中，$A_k, B_k$ 是针对每种类别 k 的固定常数，表示种类 k 属于静态或动态的概率，如，对于汽车，$A_k$ 较低、$B_k$ 较高，意味着汽车具有较高的动态概率；此外，$A_k, B_k$ 还可以作为调整语义观测与深度观测之间的权重参数：较高的$A_k, B_k$ 会给予语义先验较运动先验在内点概率估计中更高的权重。</p>
<p>Fig. 3给出了深度信息、内点概率和语义类别联合概率模型的依赖图：</p>
<p><img src="/2024/02/02/brasch2018/fig3.png" alt="fig3" title="figure 3"></p>
<p>近似推导可得到一个<strong>结合三项的后验概率</strong>：</p>
<p><img src="/2024/02/02/brasch2018/f9.png" alt="f9" title="formula 9"></p>
<p>其中，第一项将深度视为高斯分布；第二项为内点概率服从关于深度观测信息的Beta 分布；第三项为内点概率服从关于语义类别的Beta 分布；$D = \{d_1, …, d_N\}$ 表示深度观测信息；$S = \{s_1, …, s_N\}$ 表示语义观测信息，其中，$s_i = (CNN(c_1|I_i), …, CNN(c_K|I_i))$ 为CNN 网络在K 个类别上的<strong>概率密度</strong>。</p>
<p>当前<strong>内点概率</strong>可通过下式计算得到：</p>
<p><img src="/2024/02/02/brasch2018/f15.png" alt="f15" title="formula 15"></p>
<h2 id="3-3-Real-time-semantic-segmentation"><a href="#3-3-Real-time-semantic-segmentation" class="headerlink" title="3.3 Real-time semantic segmentation"></a>3.3 Real-time semantic segmentation</h2><p>高动态场景中，图片内容会快速发生变化，因此需要在每一帧中提取新的关键点来保持足够多的数量进行跟踪，为了让每个新的地图点得到一致的语义观测，作者对<strong>所有帧</strong>进行语义分割。作者使用<strong>ICNet</strong> 进行语义分割。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 CubeSLAM_Monocular 3-D Object SLAM</title>
    <url>/2024/01/31/cubeslam/</url>
    <content><![CDATA[<p>Yang, Shichao, and Sebastian Scherer. “CubeSLAM: Monocular 3-D Object SLAM.” <em>IEEE Transactions on Robotics</em> 35, no. 4 (August 2019): 925–38. <a href="https://doi.org/10.1109/TRO.2019.2909168">https://doi.org/10.1109/TRO.2019.2909168</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文作者提出了一个可以同时应用于<strong>静态与动态场景</strong>中，将2D、3D 物体检测与SLAM 位姿估计相结合的系统，如Fig. 1所示。前提假设：物体立方体经投影后会与2D bbox 相吻合；在此假设下，当给定检测到的2D 物体时，作者利用vanishing point (VP) 产生3D 立方体 proposals，然后通过<strong>多视角BA</strong> ，利用点和相机对立方体进行进一步的优化。</p>
<span id="more"></span>
<p><img src="/2024/01/31/cubeslam/fig1.png" alt="fig1" title="figure 1"></p>
<p>物体在本系统中的作用有两个方面：</p>
<ol>
<li>在BA 过程中提供<strong>几何和尺度约束</strong>；</li>
<li>为难以进行三角测量的点提供<strong>深度初始化信息</strong>。</li>
</ol>
<p>在动态场景中，本系统不是简单地将动态观测物体视为外点，而是基于<strong>动态点观测</strong>与<strong>运动模型约束</strong>，联合优化相机与物体的轨迹。</p>
<p>本文做出的贡献总结如下：</p>
<ul>
<li>不使用先验物体模型的高效、准确以及鲁棒的<strong>单张图片3D 物体检测方法</strong>；</li>
<li>提出了一种在相机、物体与点之间使用新颖的观测手段的<strong>object SLAM 方法</strong>，在许多数据集上实现了更好的位姿估计；</li>
<li>实验结果证明<strong>物体检测</strong>与<strong>SLAM</strong> 两者相互促进；</li>
<li>一个在动态场景中<strong>利用移动物体来提高位姿估计</strong>的方法。</li>
</ul>
<h1 id="3-Single-Image-3-D-Object-Detection"><a href="#3-Single-Image-3-D-Object-Detection" class="headerlink" title="3 Single Image 3-D Object Detection"></a>3 Single Image 3-D Object Detection</h1><h2 id="3-1-3D-Box-Proposal-Generation"><a href="#3-1-3D-Box-Proposal-Generation" class="headerlink" title="3.1 3D Box Proposal Generation"></a>3.1 3D Box Proposal Generation</h2><h3 id="3-1-1-3D-Box-Proposal-Generation"><a href="#3-1-1-3D-Box-Proposal-Generation" class="headerlink" title="3.1.1 3D Box Proposal Generation"></a>3.1.1 3D Box Proposal Generation</h3><p>一个通用的3D 立方体有9自由度：位置 $\mathbf{t} = [t_x, t_y, t_z]$ ，旋转矩阵 R，以及三维尺度 $\mathbf{d} = [d_x, d_y, d_z]$ 。前提假设：立方体的角在图像上的投影与2D bbox 紧密重合。在该假设下，2D bbox只能提供<strong>4个约束</strong>（4条边），无法构建9自由度的立方体。已有文献利用预先提供或预测的尺度信息和物体朝向信息作为额外的约束，作者选择使用<strong>消失点 VP</strong> 来减少回归参数，以适用于通用物体。</p>
<p>消失点 VP 是平行线在<strong>透视图像</strong>上的交点，3D 立方体有3个正交轴，因此经过投影可形成3个VP，投影过程依赖于相对于相机坐标系的<strong>旋转矩阵 R</strong> 和<strong>相机内参 K</strong>：</p>
<p><img src="/2024/01/31/cubeslam/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$R_{col(i)}$ 表示R 的第 i 列。</p>
<h3 id="3-1-2-Get-2D-Corners-From-the-VP"><a href="#3-1-2-Get-2D-Corners-From-the-VP" class="headerlink" title="3.1.2 Get 2D Corners From the VP"></a>3.1.2 Get 2D Corners From the VP</h3><p>对于一个立方体，单一视角下最多可同时观测到其3个面，基于观测到的面数量，作者分为三组情况，如Fig. 2所示。</p>
<p><img src="/2024/01/31/cubeslam/fig2.png" alt="fig2" title="figure 2"></p>
<p>作者以Fig. 2（a）的情况说明，如何基于VP 获取立方体的8个2D 角点：假设VP 和角点 $p_1$ 已知或已被估计，则有 $p_2 = (VP_1, p_1) \times (B, C)$ ， $p_4 = (VP_2, p_1) \times (A, D)$ ， $p_3 = (VP_1, p_4) \times (VP_2, p_2)$ ，其余2D 角点均可通过类似方法获取，其中 $\times$ 表示两直线的交点。</p>
<h3 id="3-1-3-Get-3D-Box-Pose-From-2D-Corners"><a href="#3-1-3-Get-3D-Box-Pose-From-2D-Corners" class="headerlink" title="3.1.3 Get 3D Box Pose From 2D Corners"></a>3.1.3 Get 3D Box Pose From 2D Corners</h3><p>在获取了2D 角点之后，需要进一步获取立方体的3D 角点，作者分两种情况进行讨论：</p>
<p><strong>情况一 任意位姿的物体：</strong>作者使用PnP solver 来求解通用立方体的3D 位置和维度（缺少尺度参数，因为是单目相机）。在物体坐标系（原点设在物体中心）中，3D 立方体的8个顶点坐标可表示为 $[\pm d_x, \pm d_y, \pm d_z]/2$ ，则2D 角点 $p_1$ 可由相应的3D 角点通过下式获取：</p>
<p><img src="/2024/01/31/cubeslam/f2.png" alt="f2" title="formula 2"></p>
<p>其中，$\pi$ 表示相机投影方程。则式2可提供两个约束（2D 坐标分别代表两个约束方程），那么，选取4个邻接角点（如1，2，4，7）即可得到全约束方程（除了尺度参数）。</p>
<p><strong>情况二 位于地面上的物体：</strong>对于位于地面上的物体，可以大幅简化情况一的求解过程并获取<strong>尺度参数</strong>。在地平面上构建世界坐标系，则物体的roll/pitch 角均为0，这里不再使用复杂的PnP solver，而是直接将8个2D 角点逆投影至3D 地平面，然后计算其他垂直角，从而组成一个3D 立方体；该方法<strong>高效且有解析表达式</strong>，如位于3D 地平面上的角点5，使用 $[\mathbf{n}, m]$ 来表示（相机坐标系的法向量和距离），则3D 角点 $P_5$ 可通过求<strong>逆投影线</strong> $K^{-1}p_5$ 与地平面的交点获取：</p>
<p><img src="/2024/01/31/cubeslam/f3.png" alt="f3" title="formula 3"></p>
<p>尺度参数由投影过程中<strong>相机的高度</strong>来决定。</p>
<h3 id="3-1-4-Sample-VP-and-Summary"><a href="#3-1-4-Sample-VP-and-Summary" class="headerlink" title="3.1.4 Sample VP and Summary"></a>3.1.4 Sample VP and Summary</h3><p>经过上述分析，3D 立方体的估计问题变为了如何获取三个VPs 以及一个 2D 角点。从式1可得，VPs 通过物体旋转矩阵获取，尽管可通过深度学习来预测物体的旋转矩阵，但是为了<strong>更好的泛化性</strong>，作者选择<strong>手动采样旋转矩阵</strong>，并对它们进行<strong>评分</strong>。本文只考虑位于地面上的物体，因此只需要相机的roll/pitch 角和物体的yaw 角，对于多视角视频数据，作者使用SLAM 来估计相机位姿，因此使得采样空间大幅减小且更为准确。</p>
<h2 id="3-2-Proposal-Scoring"><a href="#3-2-Proposal-Scoring" class="headerlink" title="3.2 Proposal Scoring"></a>3.2 Proposal Scoring</h2><p>在获取了许多立方体proposals 后，作者定义损失函数来进行评分，如Fig. 3所示。</p>
<p><img src="/2024/01/31/cubeslam/fig3.png" alt="fig3" title="figure 3"></p>
<p>作者提出了不同的损失函数，如语义分割、edge distance、HOG 特征等，最终，作者使用了将立方体和图片边特征进行对齐的快速有效的损失函数，该方法对于具有清晰边缘的物体效果较好，但是由于VP 和鲁棒边滤波的约束，使得其对于自行车、马桶等效果也不差。</p>
<p>将图片记为 $I$ ，立方体表示为 $O = \{R, \mathbf{t}, \mathbf{d}\}$ ，损失函数表示为：</p>
<p><img src="/2024/01/31/cubeslam/f4.png" alt="f4" title="formula 4"></p>
<p>其中，$\phi_{dist},\phi_{angle},\phi_{shape}$ 代表三个损失项，$w_1=0.8, w_2=1.5$ 表示权重参数。</p>
<p><strong>3.2.1 Distance Error</strong> $\phi_{dist} = (O, I)$</p>
<p>作者认为，立方体的2D 边应该与实际图片中的边相吻合；首先检测Canny 边，并基于此构建一个distance transform map；然后对立方体的每个可见边（Fig. 2（a）中的蓝色实线边）均匀采样10个点，归纳所有的距离地图值并除以2D bbox 的对角线。</p>
<p><strong>3.2.2 Angle Alignment Error</strong> $\phi_{angle}(O, I)$</p>
<p>distance error 容易受到噪声影响，如物体表面的纹理会被识别成边，因此，作者也检测长线段（Fig. 3 中的绿线），并测量它们的角度是否与VP 对齐；首先基于点线关系将这些线与三个VPs 中的一个相关联，对于每个VP，可以找到最外侧的两个线段（分别具有最大、最小的坡度slope），分别记为 $<l_{i-ms}, l_{i-mt}>,<l_{i-ns}, l_{i-nt}>$。其中，$<a, b>$ 表示两顶点 a，b 组成的线段的坡度角。最终，角度对齐误差表示为：</a,></l_{i-ns},></l_{i-ms},></p>
<p><img src="/2024/01/31/cubeslam/f5.png" alt="f5" title="formula 5"></p>
<p><strong>3.2.3 Shape Error</strong> $\phi_{shape}(O)$</p>
<p>同一组2D 角点也许会生成相当不同的3D 立方体，因此，作者对具有较大长宽比的立方体增加了一个惩罚项：</p>
<p><img src="/2024/01/31/cubeslam/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$s = max(d_x/d_y, d_y/d_x)$ 表示物体长宽比；阈值 $\sigma$ 设为1；上式表示若物体长宽比为1，则该惩罚项为0。</p>
<h1 id="4-Object-SLAM"><a href="#4-Object-SLAM" class="headerlink" title="4 Object SLAM"></a>4 Object SLAM</h1><p>作者将单图片3D 物体检测扩展至object SLAM中对物体和相机位姿进行联合优化，系统基于ORB-SLAM2 搭建的，包括前端相机跟踪和后端BA，作者主要对BA 部分进行修改以包含物体、特征点和相机位姿；值得注意的是，本节内容只使用静态物体。</p>
<h2 id="4-1-BA-Formulation"><a href="#4-1-BA-Formulation" class="headerlink" title="4.1 BA Formulation"></a>4.1 BA Formulation</h2><p>作者提到，除了使用物体进行约束外，还是用了特征点，这是因为<strong>仅靠物体难以完整约束相机位姿</strong>。相机位姿、3D 立方体和特征点集合分别表示为：$C = \{C_i\}, O = \{O_j\}, P = \{P_k\}$ ，则BA 可表示为以下的非线性最小二乘优化问题：</p>
<p><img src="/2024/01/31/cubeslam/f7.png" alt="f7" title="formula 7"></p>
<h2 id="4-2-Measurement-Errors"><a href="#4-2-Measurement-Errors" class="headerlink" title="4.2 Measurement Errors"></a>4.2 Measurement Errors</h2><h3 id="4-2-1-Camera-Object-Measurement"><a href="#4-2-1-Camera-Object-Measurement" class="headerlink" title="4.2.1 Camera-Object Measurement"></a>4.2.1 Camera-Object Measurement</h3><p>作者提供两种观测误差：</p>
<p><strong>3D Measurements</strong></p>
<p>当3D 物体检测较准确时（如使用RGB-D 相机时）使用3D 观测误差。从单张图片检测到的<strong>物体位姿</strong> $O_m = (T_{om}, \mathbf{d}_m)$ 作为<strong>相机坐标系下</strong>的物体观测信息，然后将物体地标转换至相机坐标系以计算观测误差：</p>
<p><img src="/2024/01/31/cubeslam/f8.png" alt="f8" title="formula 8"></p>
<p>其中，log 将SE3 误差映射至 6 自由度的切向量空间，因此 $e_{co-3D} \in \mathbb{R}^9$ 。</p>
<p>值得注意的是，由于没有先验物体模型，本文使用的基于图片的立方体检测不能区分物体的前后方向，因此，对于式8，需要沿着物体z 轴旋转 $0, \pm90^o, 180^o$ 寻找最小的误差值。</p>
<p><strong>2D Measurements</strong></p>
<p>对于2D 观测，作者将物体地标投影至图片坐标系中以获取2D bbox，如Fig. 4（b）中的红色长方形表示，然后与2D 物体检测bbox （蓝色长方形）进行比较。具体来讲，将3D 立方体的8个角点投影至图像坐标系中，寻找xy 坐标轴的极值点组成一个长方形。</p>
<p><img src="/2024/01/31/cubeslam/f9.png" alt="f9" title="formula 9"></p>
<p>其中，$c, s$ 分别表示2D bbox 的中心坐标和长宽尺寸；因此，4维矩形误差表示为：</p>
<p><img src="/2024/01/31/cubeslam/f10.png" alt="f10" title="formula 10"></p>
<p>该观测误差较式8表示的3D 观测误差有着小得多的不确定性，因为2D 物体检测相较于3D 检测<strong>通常更准确</strong>；但同时也丢失了不少信息，因为许多不同的3D 立方体可以投影为相同的2D 矩形，因此需要更多观测信息来约束相机位姿和立方体。</p>
<p>作者表示，由于<strong>复杂的检测过程</strong>，对误差协方差矩阵 $\sum$ 或者Hessian 矩阵 W 的建模和估计相较于特征点来说要更难，因此，作者对高语义置信度和几何相似物体赋予<strong>更高的权重</strong>，假设立方体与相机之间距离为 d，物体检测概率为 p，则对于KITTI 数据集设定以下权重参数：$w = p \times max(70-d, 0)/50$ ，其中，70m是截断距离，该参数对不同的数据集会有不同的取值。</p>
<h3 id="4-2-2-Object-Point-Measurement"><a href="#4-2-2-Object-Point-Measurement" class="headerlink" title="4.2.2 Object-Point Measurement"></a>4.2.2 Object-Point Measurement</h3><p>特征点与物体可以相互提供约束，如果点 P 属于某个物体，如Fig. 4（b）所示，则其应该位于3D 立方体内部。作者首先将特征点转换至立方体坐标系中，然后与立方体尺寸进行比较来获取3D 误差：</p>
<p><img src="/2024/01/31/cubeslam/f11.png" alt="f11" title="formula 11"></p>
<h3 id="4-2-3-Camera-Point-Measurement"><a href="#4-2-3-Camera-Point-Measurement" class="headerlink" title="4.2.3 Camera-Point Measurement"></a>4.2.3 Camera-Point Measurement</h3><p>使用标准的3D 点重投影误差作为相机-特征点观测误差：</p>
<p><img src="/2024/01/31/cubeslam/f12.png" alt="f12" title="formula 12"></p>
<p>其中，$z_m$ 是3D 点P 的观测像素坐标。</p>
<p><img src="/2024/01/31/cubeslam/fig4.png" alt="fig4" title="figure 4"></p>
<h2 id="4-3-Data-Association"><a href="#4-3-Data-Association" class="headerlink" title="4.3 Data Association"></a>4.3 Data Association</h2><p>作者提出了一种<strong>基于特征点匹配的物体关联</strong>，数据关联的步骤如下：</p>
<ol>
<li>首先，如果特征点在至少两帧图片中被观测到位于2D 物体检测框中，且它们距离3D 的立方体中心小于1m 时，将这些<strong>特征点与对应的物体关联起来</strong>；</li>
<li>如果两个物体在不同帧中拥有最多数量的共同特征点，且点的数量超过一定阈值（本文设定为10）时，判定两个物体为同一物体并进行数据关联。</li>
<li>需要注意的是，当物体几乎没有与其他帧相关联的特征点时，将该物体<strong>判定为动态物体</strong>，如Fig. 5中的青色框汽车。</li>
</ol>
<p><img src="/2024/01/31/cubeslam/fig5.png" alt="fig5" title="figure 5"></p>
<h1 id="5-Dynamic-SLAM"><a href="#5-Dynamic-SLAM" class="headerlink" title="5 Dynamic SLAM"></a>5 Dynamic SLAM</h1><p>为了减少未知参量让问题可解，作者使用了两个假设：<strong>刚体假设</strong>与<strong>物理运动模型假设</strong>。</p>
<ul>
<li>刚体假设是为了保证特征点在关联物体上的位置保持不变，这使得作者可以使用标准的3D <strong>地图点重投影误差</strong>来优化位置参数；</li>
<li>最简单的运动模型就是<strong>匀速运动模型</strong>，至于一些特定物体，如自行车，会额外受限于nonholonomic wheel model。</li>
</ul>
<h2 id="5-1-Notations"><a href="#5-1-Notations" class="headerlink" title="5.1 Notations"></a>5.1 Notations</h2><p>作者使用 $^jO^i$ 表示动态物体 $O^i$ 在观测帧 j 中的位姿；对于动态物体 $O^i$ 上的动态特征点 $P^k$ 表示为 $^iP^k$ （物体坐标系），基于刚体假设，其在物体上的位置是固定的。</p>
<h2 id="5-2-SLAM-Optimization"><a href="#5-2-SLAM-Optimization" class="headerlink" title="5.2 SLAM Optimization"></a>5.2 SLAM Optimization</h2><p>动态物体估计的因子图如Fig. 6所示，其中绿色方框是观测量，包含：camera-object 因子（式10），object-velocity 因子（式14），以及point-camera-object 因子（式15）。</p>
<p><img src="/2024/01/31/cubeslam/fig6.png" alt="fig6" title="figure 6"></p>
<h3 id="5-2-1-Object-Motion-Model"><a href="#5-2-1-Object-Motion-Model" class="headerlink" title="5.2.1 Object Motion Model"></a>5.2.1 Object Motion Model</h3><p>3D 物体的通用运动方式可表示为位姿转换矩阵 $T \in SE(3)$ ，作者采用一个更严格的非完整轮模型 nonholonomic wheel model，将汽车运动表示为线速度 $v$ 和转向角 $\phi$ 。假设汽车运动在平面上，则三维向量即可表示完整状态 $T_0 = [R(\theta), [t_x, t_y, o]’]$ ，则根据速度预测的状态表示为：</p>
<p><img src="/2024/01/31/cubeslam/f13.png" alt="f13" title="formula 13"></p>
<p>其中，L 表示汽车前后轮中心的距离。最终的<strong>运动模型误差</strong>表示为：</p>
<p><img src="/2024/01/31/cubeslam/f14.png" alt="f14" title="formula 14"></p>
<h3 id="5-2-2-Dynamic-Point-Observation"><a href="#5-2-2-Dynamic-Point-Observation" class="headerlink" title="5.2.2 Dynamic Point Observation"></a>5.2.2 Dynamic Point Observation</h3><p>对于动态物体上的特征点，首先将其从物体坐标系转换至世界坐标系，再根据相机位姿投影至像素平面，重投影误差表示为：</p>
<p><img src="/2024/01/31/cubeslam/f15.png" alt="f15" title="formula 15"></p>
<p>其中，$T^j_c$ 表示相机位姿；$z_{kj}$ 表示观测到的像素坐标。</p>
<h2 id="5-3-Dynamic-Data-Association"><a href="#5-3-Dynamic-Data-Association" class="headerlink" title="5.3 Dynamic Data Association"></a>5.3 Dynamic Data Association</h2><p>4.3节使用的静态物体数据关联不适用于动态物体，跟踪特征点的<strong>经典方法</strong>是预测投影位置，然后在附近搜索<strong>描述子匹配</strong>的特征点，然后检查<strong>对极几何约束</strong>验证是否匹配正确；但单眼相机动态场景中，很难准确预测物体与特征点的运动，且当物体运动不准确时难以应用对极几何进行计算。因此，作者使用2D <strong>KLT 稀疏光流算法</strong>进行动态物体特征点的跟踪，该算法并不需要3D 点位置信息。在使用像素追踪之后，考虑物体的运动利用三角测量计算动态特征的3D 位置。</p>
<p>假设两帧图片的投影矩阵为 $M_1, M_2$ ；同一3D 点在两帧对应的位置分别是 $P_1, P_2$ ；对应像素位置分别为 $z_1, z_2$ ；物体在两帧间的运动转换矩阵为 $\Delta T$ 。则有 $P_2 = \Delta T P_1$ ，基于此，有：</p>
<p><img src="/2024/01/31/cubeslam/f16.png" alt="f16" title="formula 16"></p>
<p>若将 $M_2 \Delta T$ 视为<strong>补偿物体运动的相机位姿</strong>，则式16可视为<strong>标准的两视角三角化问题</strong>，可使用SVD 方法进行求解。</p>
<p>值得注意的是，作者提到当<strong>像素位移过大</strong>时KLT 方法会跟踪失败，因此，对于动态物体跟踪作者直接使用<strong>视觉物体跟踪算法</strong>：跟踪物体2D bbox，从前一帧预测其在当前帧的大概位置，然后从当前帧的2D bbox 中选取具有最大重叠比例的bbox 实现跟踪。</p>
<h1 id="8-Experiments-Object-SLAM"><a href="#8-Experiments-Object-SLAM" class="headerlink" title="8 Experiments-Object SLAM"></a>8 Experiments-Object SLAM</h1><h2 id="8-1-TUM-RGBD-and-ICL-NUIM-Dataset"><a href="#8-1-TUM-RGBD-and-ICL-NUIM-Dataset" class="headerlink" title="8.1 TUM RGBD and ICL-NUIM Dataset"></a>8.1 TUM RGBD and ICL-NUIM Dataset</h2><p>为了更好地评估单目位姿偏移，作者在使用及结果对比时<strong>关掉了ORB SLAM 的回环检测模块</strong>。</p>
<p>Fig. 9所示的是在TUM fr3_cabinet 数据集上的实验结果，由于该场景特征点太少，导致现有的单目SLAM 算法均无法完成实验。而本文提出的算法使用object-camera 观测作为额外约束，实现了较好的定位效果。Fig. 9左边下图中估计的立方体存在较大的误差，但经过多视角优化后，3D立方体可以与真实点云实现良好的契合。实验结果如Table 2所示。</p>
<p><img src="/2024/01/31/cubeslam/fig9.png" alt="fig9" title="figure 9"></p>
<p><img src="/2024/01/31/cubeslam/t2.png" alt="t2" title="table 2"></p>
<h2 id="8-2-Collected-Chair-Dataset"><a href="#8-2-Collected-Chair-Dataset" class="headerlink" title="8.2 Collected Chair Dataset"></a>8.2 Collected Chair Dataset</h2><p><img src="/2024/01/31/cubeslam/fig10.png" alt="fig10" title="figure 10"></p>
<h2 id="8-4-Dynamic-Object"><a href="#8-4-Dynamic-Object" class="headerlink" title="8.4 Dynamic Object"></a>8.4 Dynamic Object</h2><p><img src="/2024/01/31/cubeslam/fig12.png" alt="fig12" title="figure 12"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Pose Optimization</tag>
        <tag>Object SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Towards semantic SLAM using a monocular camera</title>
    <url>/2024/01/30/civera2011/</url>
    <content><![CDATA[<p>Civera, J., D. Galvez-Lopez, L. Riazuelo, J. D. Tardos, and J. M. M. Montiel. “Towards Semantic SLAM Using a Monocular Camera.” In <em>2011 IEEE/RSJ International Conference on Intelligent Robots and Systems</em>, 1277–84. San Francisco, CA: IEEE, 2011. <a href="https://doi.org/10.1109/IROS.2011.6094648">https://doi.org/10.1109/IROS.2011.6094648</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文提出了一种语义SLAM 算法，本算法将传统<strong>无意义的特征点</strong>和<strong>物体</strong>融合进到地图中。</p>
<p>本算法融合了三个不同领域的SOTA：</p>
<ol>
<li>一个<strong>EKF 单目SLAM</strong> 提供相机位姿的在线实时估计，以及包含点特征的稀疏地图；</li>
<li>利用 <strong>Structure from Motion (SfM)</strong> 从稀疏图片中计算一个物体模型数据库；</li>
<li><strong>视觉识别</strong>来检测图片流中物体是否存在。</li>
</ol>
<span id="more"></span>
<h1 id="3-Notation-and-General-Overview"><a href="#3-Notation-and-General-Overview" class="headerlink" title="3 Notation and General Overview"></a>3 Notation and General Overview</h1><p>本算法分为两个支流：</p>
<ol>
<li>单目SLAM；</li>
<li>物体识别（通常要慢于SLAM支流）。</li>
</ol>
<p>本算法的整体视图如Fig. 1所示：</p>
<p><img src="/2024/01/30/civera2011/fig1.png" alt="fig1" title="figure 1"></p>
<p>单目SLAM 利用图片从<strong>k-m-1 到 k-m</strong>更新几何状态向量 x，而视觉识别结果会在<strong>步骤k</strong> 处得到，但是物体插入应当是相对于输入图片 $I_{k-m}$ 而言的。</p>
<p>作者提前对每一个想要识别的物体进行建模，这些模型包含<strong>外观和几何信息</strong>：</p>
<ul>
<li>外观信息是由<strong>SURF 描述子</strong>组成的；</li>
<li>几何信息是这些<strong>SURF 特征点的3D 位置</strong>信息。</li>
</ul>
<p>物体识别支流将数据库中的物体插入到SLAM 地图的步骤如下：</p>
<ol>
<li>算法搜寻图片 $I_{k-m}$ 中SURF 特征点与数据库中每一个物体之间的关联；</li>
<li>利用RANSAC 来计算每个物体的<strong>一致几何模型</strong>，来最大化关联数量；</li>
<li>如果存在足够的一致关联，那么这个特征点就被认为属于该物体，并插入SLAM 地图中；</li>
<li>被插入地图中的特征点会被持续跟踪与位置优化。</li>
</ol>
<h1 id="4-Object-Model"><a href="#4-Object-Model" class="headerlink" title="4 Object Model"></a>4 Object Model</h1><p>如上文所述，物体模型包含由SURF 特征描述子构成的<strong>外观信息</strong>和特征点位置<strong>几何信息</strong>，其构建示例如Fig. 2所示：</p>
<p><img src="/2024/01/30/civera2011/fig2.png" alt="fig2" title="figure 2"></p>
<p>图中的黄色圆圈代表用来进行识别的SURF 特征。</p>
<p>物体的构建是通过不同的faces 完成的，而构建模型的<strong>每一幅图片</strong>是一个face 的<strong>基础</strong>。作者用一个tuple F 来表示face，该tuple中包含了SURF 特征点的位置坐标和描述子，以及形成该face 的图片的位置信息和朝向信息。</p>
<h1 id="5-Object-Recognition"><a href="#5-Object-Recognition" class="headerlink" title="5 Object Recognition"></a>5 Object Recognition</h1><p>物体识别是通过计算图片 $I_{k-m}$ 与物体数据库中每个物体的关联来实现的：</p>
<ol>
<li>对于每个物体而言，计算图片 $I_{k-m}$ 与属于该物体的faces F之间的关联；</li>
<li>然后使用RANSAC 进行外点剔除；</li>
<li>最终至少包含5对关系才能确认为图片 $I_{k-m}$ 与物体 $I_F$ 之间建立了联系；</li>
<li>利用PnP 来估计相对位姿转换关系。</li>
</ol>
<h1 id="6-Monocular-SLAM"><a href="#6-Monocular-SLAM" class="headerlink" title="6 Monocular SLAM"></a>6 Monocular SLAM</h1><h2 id="6-1-Standard-Mode-EKF"><a href="#6-1-Standard-Mode-EKF" class="headerlink" title="6.1 Standard Mode EKF"></a>6.1 Standard Mode EKF</h2><p>估计参数建模为多维高斯变量x，包含相机的运动参数以及n 个特征点：</p>
<p><img src="/2024/01/30/civera2011/f1.png" alt="f1" title="formula 1"></p>
<p>其中，运动参数 $\mathbf{x}_{C_k}$ 包含相机的位置参数 $\mathbf{t}_{C_{k-m}}$ 和方向参数 $\mathbf{q}_{C_{k-m}}$ ，以及线速度和角速度。</p>
<h2 id="6-2-State-Augmentation-with-Past-Camera-Pose"><a href="#6-2-State-Augmentation-with-Past-Camera-Pose" class="headerlink" title="6.2 State Augmentation with Past Camera Pose"></a>6.2 State Augmentation with Past Camera Pose</h2><p>当<strong>物体识别支流</strong>在步骤k-m 开始时，SLAM的状态向量需要使用当前步骤的<strong>相机位姿进行增强</strong>：将当前步骤的位姿参数复制到状态向量中去，并传播相应的协方差矩阵。假设 k 步完成了物体识别与插入操作，那么k-1 步骤的状态向量表示如下：</p>
<p><img src="/2024/01/30/civera2011/f2.png" alt="f2" title="formula 2"></p>
<p>如果物体识别成功，则过去的相机位姿（本例中为步骤 k-m）被用来进行物体的<strong>延迟初始化</strong>。在此完成之后，用来增强的相机位姿不再需要，即可从状态向量中进行剔除。</p>
<h2 id="6-3-Object-Insertion"><a href="#6-3-Object-Insertion" class="headerlink" title="6.3 Object Insertion"></a>6.3 Object Insertion</h2><p><strong>物体识别支流的输出</strong>是一组物体参考坐标系中的特征点 $\mathbf{y}_F^O$ ，并由此计算face F 与k-m 时刻相机之间的位姿转换关系  $\mathbf{t}^{C_{k-m}}_F, \mathbf{q}^{C_{k-m}}_F$  ；基于此并利用6.2节中的k-m 步骤的增强相机位姿，实现物体特征点到SLAM 地图的插入操作：</p>
<p><img src="/2024/01/30/civera2011/f5.png" alt="f5" title="formula 5"></p>
<p>因为物体模型中点的3D 位置信息是已知的，因此该模型的所有点可以根据 $\mathbf{y}_F^W$ 结合欧式坐标添加进SLAM 地图中。</p>
<p>在完成插入操作后，物体点被跟踪并利用单目SLAM 算法进行位姿优化。</p>
<h1 id="8-Conclusions-and-Future-Works"><a href="#8-Conclusions-and-Future-Works" class="headerlink" title="8 Conclusions and Future Works"></a>8 Conclusions and Future Works</h1><p>本文提出的算法只使用相机作为传感器，是第一个将普通3D 物体<strong>实时加入几何SLAM 地图</strong>中的算法。</p>
<p>作者提到本算法将<strong>相机运动和3D 场景理解</strong>结合了起来，提供了部分标注的地图和相机位姿，方便后期机器人的任务开发（如抓住某个物体等）；此外，基于提前构建的物体3D 模型库，可以实现在观察到物体某一面之后即可将整个3D 物体添加进地图中去，完成对于未观测信息的补充。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Detect-SLAM_Making Object Detection and SLAM Mutually Beneficial</title>
    <url>/2024/01/30/detect-slam/</url>
    <content><![CDATA[<p>Zhong, Fangwei, Sheng Wang, Ziqi Zhang, China Chen, and Yizhou Wang. “Detect-SLAM: Making Object Detection and SLAM Mutually Beneficial.” In <em>2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, 1001–10, 2018. <a href="https://doi.org/10.1109/WACV.2018.00115">https://doi.org/10.1109/WACV.2018.00115</a>.</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文提出的Detect-SLAM 较其他SOTA 方法的优势：</p>
<ol>
<li>通过利用物体检测器完成对移动物体上的<strong>不可靠特征点</strong>进行剔除，极大地提高了SLAM 算法在动态环境中的<strong>准确性与鲁棒性</strong>；</li>
<li>在线构建一个<strong>实例级语义地图</strong>；</li>
<li>通过利用物体语义地图来<strong>提高物体检测器性能</strong>，使得其在挑战性环境中可以更有效地识别出物体。</li>
</ol>
<span id="more"></span>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>Detect-SLAM 是第一个将SLAM 和 基于DNN 的检测器结合起来同时完成三种视觉任务的工作：提高SLAM 在动态环境中的鲁棒性，构建语义地图，以及增强物体检测性能。</p>
<p><img src="/2024/01/30/detect-slam/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-Detect-SLAM"><a href="#3-Detect-SLAM" class="headerlink" title="3 Detect-SLAM"></a>3 Detect-SLAM</h1><p>本架构是基于ORB-SLAM2 算法的，在其基础上包含三个新步骤：</p>
<ul>
<li>Moving objects removal：将动态物体上的特征点进行<strong>剔除</strong>；</li>
<li>Object Mapping：对关键帧中的静态物体进行<strong>重建</strong>，物体语义地图包含带有物体ID 的稠密点云；</li>
<li>SLAM-enhanced Detector：利用物体语义地图作为先验知识来<strong>增强</strong>探测器在挑战环境中的性能。</li>
</ul>
<h2 id="3-1-Moving-Objects-Removal"><a href="#3-1-Moving-Objects-Removal" class="headerlink" title="3.1 Moving Objects Removal"></a>3.1 Moving Objects Removal</h2><p>动态物体剔除如Fig. 3所示，值得注意的是，此处的动态物体指的是属于可移动种类中的物体，并不能保证其一定是移动的，即<strong>潜在动态物体</strong>。然而，利用DNN 进行动态物体检测速度较慢，SSD 只能实现3 FPS的速度。为解决该问题，作者采用两个策略进行应对：</p>
<ol>
<li>只在<strong>关键帧</strong>中进行动态物体检测，然后在local map 中更新特征点的<strong>移动概率</strong>来加速tracking 支流；</li>
<li>在tracking 支流中利用<strong>特征匹配</strong>和<strong>匹配点扩张</strong>来传递移动概率，从而在相机位姿估计前有效移除动态物体上的特征点。</li>
</ol>
<p>作者将物体的移动概率分为4个层次，如Fig. 2所示，使用<strong>高置信度点</strong>在<strong>匹配点扩张</strong>中将移动概率传递给周围<strong>未匹配</strong>的特征点，在每个点都得到相应的移动概率后，去除动态特征点并使用RANSAC 滤除其他外点，然后进行位姿估计。</p>
<p><img src="/2024/01/30/detect-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h3 id="3-1-1-Updating-Moving-Probability"><a href="#3-1-1-Updating-Moving-Probability" class="headerlink" title="3.1.1 Updating Moving Probability"></a>3.1.1 Updating Moving Probability</h3><p><img src="/2024/01/30/detect-slam/fig3.png" alt="fig3" title="figure 3"></p>
<p>关键帧的物体检测完成后会被插入到local map 中并更新地图中特征点的移动概率：</p>
<p><img src="/2024/01/30/detect-slam/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$P_{t-1}(X^i)$ 表示3D 点 $X^i$ 在上个关键帧更新后的移动概率，若点 $X^i$ 是新添加的点，则令 $P_{t-1}(X^i)=0.5$ ；与点 $X^i$ 匹配的关键帧中的点 $x_i$ 若位于动态物体的bbox 内，则令 $S_t(x^i) = 1$ ，其他情况下令其为0。</p>
<h3 id="3-1-2-Moving-Probability-Propagation"><a href="#3-1-2-Moving-Probability-Propagation" class="headerlink" title="3.1.2 Moving Probability Propagation"></a>3.1.2 Moving Probability Propagation</h3><p>移动概率传递在帧间通过两种方式确定：<strong>特征匹配</strong>和<strong>匹配点扩张</strong>，如Fig. 4所示。</p>
<p><img src="/2024/01/30/detect-slam/fig4.png" alt="fig4" title="figure 4"></p>
<p>对于<strong>特征匹配</strong>的情况，若一个特征点同时与上一帧特征点和地图中的3D 点匹配上时，应该<strong>以地图中3D 点的移动概率为准</strong>。将未匹配的特征点移动概率初始化为0.5。特征匹配的情况总结为下式：</p>
<p><img src="/2024/01/30/detect-slam/f2.png" alt="f2" title="formula 2"></p>
<p>对于匹配点扩张的情况，是利用<strong>高置信度的特征点</strong>（包括高静态可能和高动态可能特征点）对周围未匹配特征点的移动概率进行确认，确认过程如下所示：</p>
<p><img src="/2024/01/30/detect-slam/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$\chi_t$ 为高置信度特征点；$P_{init}$ 为初始化概率值，为0.5；$\lambda(d)$ 为距离因子，若d 小于半径阈值，则令 $\lambda(d) = Ce^{-d/r}$ 。</p>
<h2 id="3-2-Mapping-Objects"><a href="#3-2-Mapping-Objects" class="headerlink" title="3.2 Mapping Objects"></a>3.2 Mapping Objects</h2><h3 id="3-2-1-Predicting-Region-ID"><a href="#3-2-1-Predicting-Region-ID" class="headerlink" title="3.2.1 Predicting Region ID"></a>3.2.1 Predicting Region ID</h3><p>物体ID 的预测是基于一个<strong>几何假设</strong>：地图中物体投影到关键帧中的区域与帧中物体检测区域<strong>存在重叠</strong>，且区域属于同一个物体。利用IOU 来表示两个区域的重叠程度：</p>
<p><img src="/2024/01/30/detect-slam/f4.png" alt="f4" title="formula 4"></p>
<p>若IOU 大于0.5，则进一步估计<strong>深度似然</strong>：</p>
<p><img src="/2024/01/30/detect-slam/f5.png" alt="f5" title="formula 5"></p>
<p>其中，Err 是重叠区域内观测与投影深度的MSE：</p>
<p><img src="/2024/01/30/detect-slam/f6.png" alt="f6" title="formula 6"></p>
<p>若深度似然大于设定阈值，则将地图物体的ID 赋予给检测区域，否则，分配一个新的ID 给该检测区域。</p>
<h3 id="3-2-2-Cutting-Background"><a href="#3-2-2-Cutting-Background" class="headerlink" title="3.2.2 Cutting Background"></a>3.2.2 Cutting Background</h3><p>将投影与观测重叠区域的点作为前景seed，将bbox 外的点作为背景seed，利用Grab-Cut 算法得到物体的分割掩码。</p>
<p><img src="/2024/01/30/detect-slam/fig5.png" alt="fig5" title="figure 5"></p>
<h3 id="3-2-3-Recobstruction"><a href="#3-2-3-Recobstruction" class="headerlink" title="3.2.3 Recobstruction"></a>3.2.3 Recobstruction</h3><p>利用物体掩码，创建物体点云并剔除噪声点，最终将物体点云转换至世界坐标系中并添加至物体语义地图。</p>
<h2 id="3-3-SLAM-enhanced-Detector"><a href="#3-3-SLAM-enhanced-Detector" class="headerlink" title="3.3 SLAM-enhanced Detector"></a>3.3 SLAM-enhanced Detector</h2><p>利用物体语义地图和相机位姿来增强物体检测器的性能。</p>
<h3 id="3-3-1-Region-Proposal"><a href="#3-3-1-Region-Proposal" class="headerlink" title="3.3.1 Region Proposal"></a>3.3.1 Region Proposal</h3><p>利用当前估计的相机位姿将语义地图中的3D 物体投影至2D 平面，将具有相同物体ID 的<strong>像素进行聚类</strong>来确定可能含有物体的区域。</p>
<h3 id="3-3-2-Region-Filter"><a href="#3-3-2-Region-Filter" class="headerlink" title="3.3.2 Region Filter"></a>3.3.2 Region Filter</h3><p>去除掉较小尺寸（20<em>20像素）的候选区域，并估计观测深度与投影深度之间的<em>*似然</em></em>来检测遮挡的候选目标。</p>
<h3 id="3-3-3-Hard-Example-Mining"><a href="#3-3-3-Hard-Example-Mining" class="headerlink" title="3.3.3 Hard Example Mining"></a>3.3.3 Hard Example Mining</h3><p>前人的工作证明了选择使用<strong>困难的数据</strong>来训练或者细调DNN 网络可以极大促进检测效果，于是，作者使用SLAM-enhanced 物体检测器来挖掘困难的数据（Fig. 7所示）来<strong>强化训练集</strong>，然后用这些强化的数据集来细调SSD 网络。</p>
<p><img src="/2024/01/30/detect-slam/fig7.png" alt="fig7" title="figure 7"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 DS-SLAM_A Semantic Visual SLAM towards Dynamic Environments</title>
    <url>/2024/02/04/ds-slam/</url>
    <content><![CDATA[<p>Yu, Chao, Zuxin Liu, Xin-Jun Liu, Fugui Xie, Yi Yang, Qi Wei, and Qiao Fei. “DS-SLAM: A Semantic Visual SLAM towards Dynamic Environments.” In <em>2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 1168–74. Madrid: IEEE, 2018. <a href="https://doi.org/10.1109/IROS.2018.8593691">https://doi.org/10.1109/IROS.2018.8593691</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ol>
<li>提出一个基于ORB-SLAM2 应用于动态环境中的语义SLAM 系统——DS-SLAM，在公开数据集上进行测试，证明了本系统在动态环境中的<strong>精度与鲁棒性</strong>；</li>
<li>添加一个<strong>实时语义分割网络线程</strong>，结合<strong>语义分割和运动一致性检验</strong>来滤除场景中的动态区域；</li>
<li>创建了一个构建稠密3D 语义八叉树地图的线程。</li>
</ol>
<span id="more"></span>
<h1 id="3-System-Introduction"><a href="#3-System-Introduction" class="headerlink" title="3 System Introduction"></a>3 System Introduction</h1><h2 id="3-1-Framework-of-DS-SLAM"><a href="#3-1-Framework-of-DS-SLAM" class="headerlink" title="3.1 Framework of DS-SLAM"></a>3.1 Framework of DS-SLAM</h2><p>系统架构如Fig. 1所示，DS-SLAM 运行5个线程：tracking, semantic segmentation, local mapping, loop closing and dense map creation。</p>
<p><img src="/2024/02/04/ds-slam/fig1.png" alt="fig1" title="figure 1"></p>
<p>系统框架如Fig. 2所示，RGB 图片同时被跟踪线程和语义分割线程处理，跟踪线程提取ORB 特征点，<strong>粗略检验特征点的运动一致性，并保存潜在的外点；</strong>待像素级语义分割结果生成后，位于<strong>移动物体中的ORB 外点</strong>会被舍弃，然后利用剩余匹配的稳定特征点来计算转换矩阵。</p>
<p><img src="/2024/02/04/ds-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-3-Moving-Consistency-Check"><a href="#3-3-Moving-Consistency-Check" class="headerlink" title="3.3 Moving Consistency Check"></a>3.3 Moving Consistency Check</h2><p>本系统认为，<strong>如果一个分割物体内的一些点被识别为动态的，那么就判定该物体是动态物体</strong>。</p>
<p><strong>运动一致性检验</strong>步骤如下：</p>
<ol>
<li>首先，计算<strong>光流金字塔</strong>来获取当前帧中的匹配特征点；</li>
<li>然后，如果特征点匹配对太靠近图片边缘，或者特征点匹配对中心的3*3 图像块的像素差异过大，那么该匹配对会被舍弃；</li>
<li>接着，利用<strong>RANSAC</strong> 策略计算<strong>基础矩阵</strong>，在此基础上在当前帧中计算<strong>极线</strong>；</li>
<li>最后，根据匹配点对与极线之间的距离来判断该匹配点对是否是动态的。</li>
</ol>
<p><img src="/2024/02/04/ds-slam/a1.png" alt="a1" title="algorithm1"></p>
<h2 id="3-4-Outliers-Rejection"><a href="#3-4-Outliers-Rejection" class="headerlink" title="3.4 Outliers Rejection"></a>3.4 Outliers Rejection</h2><p>利用几何方法确定物体的轮廓太难，因此DS-SLAM 结合语义分割结果得到的物体轮廓，利用几何模块的运动一致性检验得到的<strong>动态特征点</strong>，判断物体轮廓内动态特征点的数量<strong>是否超过阈值</strong>，若超过阈值，则判定该物体属于<strong>动态物体</strong>，然后剔除掉属于该物体的<strong>所有特征点</strong>。</p>
<p>作者的实验证明，本系统ORB 特征提取时间+移动一致性检验的时间，与语义分割线程所需的时间基本一致。</p>
<p><img src="/2024/02/04/ds-slam/t1.png" alt="t1" title="table 4"></p>
<p>由于人类活动会在大部分真实场景中影响机器人的定位，所以作者以人类作为一个典型示例，展示DS-SLAM 的效果，<strong>但理论上DS-SLAM 可以应用于多种不同的动态物体检测。</strong>如果语义分割结果中没有人类被检测到，则使用所有的ORB 特征点进行匹配与位姿解算；如果有人类被检测到，则结合运动一致性来判断是否属于动态物体。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 DynaSLAM_Tracking, Mapping, and Inpainting in Dynamic Scenes</title>
    <url>/2024/01/30/dynaslam/</url>
    <content><![CDATA[<p>Bescos, Berta, Jose M. Facil, Javier Civera, and Jose Neira. “DynaSLAM: Tracking, Mapping, and Inpainting in Dynamic Scenes.” <em>IEEE Robotics and Automation Letters</em> 3, no. 4 (October 2018): 4076–83. <a href="https://doi.org/10.1109/LRA.2018.2860039">https://doi.org/10.1109/LRA.2018.2860039</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文提出的DynaSLAM 是在ORB-SLAM2 基础上增加一个前端模块，来处理环境中的动态物体。对于单目和双目相机，使用CNN 产生物体的像素级语义分割结果，剔除掉先验动态物体中的特征点；对于RGB-D 相机，结合多视角几何模型和CNN 来检测动态物体，从图片中移除动态物体并进行场景恢复。</p>
<span id="more"></span>
<p><img src="/2024/01/30/dynaslam/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-System-Description"><a href="#3-System-Description" class="headerlink" title="3 System Description"></a>3 System Description</h1><p>本系统架构如Fig. 2所示，在处理RGB-D 图像时，本系统使用<strong>多视角几何</strong>在两方面提高动态物体分割的效果：</p>
<ol>
<li><strong>细调</strong>Mask R-CNN 获取的先验动态物体分割区域；</li>
<li>利用多视角几何判断<strong>先验静态物体</strong>是否是动态的。</li>
</ol>
<p>在处理单目或双目图像时，CNN 获取的语义分割中，属于先验动态物体的特征点<strong>直接被舍弃</strong>，不参与跟踪或制图。</p>
<p><img src="/2024/01/30/dynaslam/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-Segmentation-of-Potential-Dynamic-Content-Using-a-CNN"><a href="#3-1-Segmentation-of-Potential-Dynamic-Content-Using-a-CNN" class="headerlink" title="3.1 Segmentation of Potential Dynamic Content Using a CNN"></a>3.1 Segmentation of Potential Dynamic Content Using a CNN</h2><p>本系统使用Mask R-CNN 实现像素级语义分割，利用MS COCO 数据集进行训练，识别出图片中的先验动态物体。</p>
<h2 id="3-2-Low-Cost-Tracking"><a href="#3-2-Low-Cost-Tracking" class="headerlink" title="3.2 Low-Cost Tracking"></a>3.2 Low-Cost Tracking</h2><p>利用语义分割获取先验动态物体后，利用图片中的静态区域进行相机位姿估计。</p>
<h2 id="3-3-Segmentation-of-Dynamic-Content-Using-Mask-R-CNN-and-Multi-View-Geometry"><a href="#3-3-Segmentation-of-Dynamic-Content-Using-Mask-R-CNN-and-Multi-View-Geometry" class="headerlink" title="3.3 Segmentation of Dynamic Content Using Mask R-CNN and Multi-View Geometry"></a>3.3 Segmentation of Dynamic Content Using Mask R-CNN and Multi-View Geometry</h2><p>由于<strong>先验静态物体</strong>也可能是移动的，所以需要结合<strong>多视角几何</strong>实现更精确的动态物体检测。多视角几何检验过程如Fig. 3所示，将之前关键帧中的每个关键点 $x$ 投影到当前帧，得到关键点 $x’$ 以及投影深度 $z_{proj}$ ，然后计算 $x, x’$ 之间的视差角 $\alpha$ ，如果该角度大于30°，则该点可能被遮挡，应当忽略该点；然后从当前帧的深度图中获取 $x’$ 的深度 $z’$ ，比较 $\Delta z = z_{proj} - z’$ 是否超过阈值，若超过阈值则判定该点为<strong>动态特征点</strong>。</p>
<p><img src="/2024/01/30/dynaslam/fig3.png" alt="fig3" title="figure 3"></p>
<p>对于位于动态物体<strong>边缘</strong>上的特征点，在深度图中如果该点周边块的<strong>方差较高</strong>，则判定该点为<strong>静态点</strong>。为了区分所有属于动态物体的像素，本系统在深度图中动态特征点周围的区域进行<strong>生成操作</strong>，得到相应的动态区域掩码，如Fig. 4（a）所示。Fig. 4展示了结合语义信息与多视角几何信息（<strong>两种信息互补</strong>），得到最终的动态物体检测效果。</p>
<p><img src="/2024/01/30/dynaslam/fig4.png" alt="fig4" title="figure 4"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>gcc、g++版本管理</title>
    <url>/2024/01/26/gcc-g/</url>
    <content><![CDATA[<p>本文主要参考<a href="https://zhuanlan.zhihu.com/p/261001751">文章</a>。</p>
<h2 id="1-版本查看"><a href="#1-版本查看" class="headerlink" title="1 版本查看"></a>1 版本查看</h2><p>对系统中的现有gcc、g++版本进行查看，安装所需版本：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 查看当前版本</span></span><br><span class="line">gcc -v</span><br><span class="line">g++ -v</span><br><span class="line"></span><br><span class="line"><span class="comment">## 查看系统已安装版本</span></span><br><span class="line"><span class="built_in">ls</span> /usr/bin/gcc*</span><br><span class="line"><span class="built_in">ls</span> /usr/bin/g++*</span><br><span class="line"></span><br><span class="line"><span class="comment">## 安装新版本</span></span><br><span class="line">sudo apt install gcc-11</span><br><span class="line">sudo apt install g++-11</span><br></pre></td></tr></table></figure>
<h2 id="2-版本切换"><a href="#2-版本切换" class="headerlink" title="2 版本切换"></a>2 版本切换</h2><p>首先，将已有版本添加到update-alternatives中：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 最后的数值代表该版本的权重参数，越大优先级越高</span></span><br><span class="line"><span class="comment"># gcc</span></span><br><span class="line">sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 100</span><br><span class="line">sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 70</span><br><span class="line"></span><br><span class="line"><span class="comment"># g++</span></span><br><span class="line">sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 100</span><br><span class="line">sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-11 70</span><br></pre></td></tr></table></figure>
<p>若想删除某个版本的管理：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo update-alternatives --remove gcc /usr/bin/gcc-11</span><br></pre></td></tr></table></figure>
<p>手动切换版本：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo update-alternatives --config gcc</span><br><span class="line">sudo update-alternatives --config g++</span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/gcc-g/g++.png" alt="g++" title="g++ version"></p>
<p>如上图所示，输入相应的id 即可实现不同版本之间的切换。</p>
<p>切换完之后，查看版本是否切换成功：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 查看当前版本</span></span><br><span class="line">gcc -v</span><br><span class="line">g++ -v</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>g++</tag>
        <tag>gcc</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Semantic Mapping for View-Invariant Relocalization</title>
    <url>/2024/02/02/li2019/</url>
    <content><![CDATA[<p>Li, Jimmy, David Meger, and Gregory Dudek. “Semantic Mapping for View-Invariant Relocalization.” In <em>2019 International Conference on Robotics and Automation (ICRA)</em>, 7108–15. Montreal, QC, Canada: IEEE, 2019. <a href="https://doi.org/10.1109/ICRA.2019.8793624">https://doi.org/10.1109/ICRA.2019.8793624</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文使用了<strong>基于外观的几何特征</strong>与<strong>物体级语义特征</strong>的混合策略，主要贡献就是将传统视觉SLAM 和语义地标进行<strong>协同集成</strong>。</p>
<span id="more"></span>
<h1 id="3-Method"><a href="#3-Method" class="headerlink" title="3 Method"></a>3 Method</h1><h2 id="3-1-Problem-Statement"><a href="#3-1-Problem-Statement" class="headerlink" title="3.1 Problem Statement"></a>3.1 Problem Statement</h2><p>本文方法包含两个组件：</p>
<ol>
<li><strong>语义制图算法</strong>：跟踪物体帧间的3D 位姿变换，并产生包含物体的度量地图；</li>
<li><strong>重定位算法</strong>：给定同一场景的两个语义地图，进行对齐并产生位姿变换。</li>
</ol>
<h2 id="3-2-Semantic-Mapping"><a href="#3-2-Semantic-Mapping" class="headerlink" title="3.2 Semantic Mapping"></a>3.2 Semantic Mapping</h2><p>使用<strong>立方体</strong>来表示物体，包含9自由度：位置、朝向以及尺寸。</p>
<p>利用ORB-SLAM2 获取相机位姿，对物体检测算法处理过的图片中的物体进行三角化，通过<strong>基于采样的推理程序</strong>得到<strong>3D 立体框</strong>。为了简化3D 物体的几何推理，作者假定物体与场景布局是<strong>对齐的</strong>，场景布局包含三个正交轴。</p>
<p><img src="/2024/02/02/li2019/a1.png" alt="a1" title="algorithm 1"></p>
<h3 id="3-2-2-Data-Association"><a href="#3-2-2-Data-Association" class="headerlink" title="3.2.2 Data Association"></a>3.2.2 Data Association</h3><p>利用ORB-SLAM 获取关键帧的位姿后，将3D 物体地标<strong>投影至关键帧</strong>中得到相应 p 的bbox，关键帧中检测到的物体的bbox 记为 d，则定义p 与 d 之间的<strong>损失函数</strong>：</p>
<p><img src="/2024/02/02/li2019/f1.png" alt="f1" title="formula 1"></p>
<p>下标 $l, t, r, b$ 分别表示bbox 在像素坐标系中的左边、上边、右边、下边四条边。分母作用是<strong>归一化</strong>，防止较大物体的bbox 主导损失函数。</p>
<h3 id="3-2-3-Object-Pose-Update"><a href="#3-2-3-Object-Pose-Update" class="headerlink" title="3.2.3 Object Pose Update"></a>3.2.3 Object Pose Update</h3><p>物体3D 框的<strong>复杂几何性质</strong>以及<strong>模糊的投影过程</strong>，会造成强烈的<strong>非凸性搜索空间</strong>；作者提出一种高效的搜索策略：</p>
<ul>
<li>首先，在一个<strong>缩小的</strong>搜索空间中高效产生多个物体假设；</li>
<li>然后，利用它们在<strong>完整的</strong>搜索空间中快速探索多个局部最小值。</li>
</ul>
<p>将位于物体<strong>上表面的中心点</strong>X 投影至图像平面，构建服从高斯分布的概率模型：</p>
<p><img src="/2024/02/02/li2019/f2.png" alt="f2" title="formula 2"></p>
<p>其中，$f_k$ 表示将3D 点X 投影至图片关键帧k 中。计算后验分布：</p>
<p><img src="/2024/02/02/li2019/f3.png" alt="f3" title="formula 3"></p>
<p>得到一组3D 采样点，然后将其视为<strong>上表面的中心点</strong>来形成相应的3D bbox，将方向与场景布局进行<strong>对齐</strong>，尺寸选择该物体的<strong>平均尺寸</strong>，由此形成完整的3D bbox 估计集合 L，假设 $o\in L$ 对应的假设为 $H_o$ ，路标 $o$ 对应的物体假设 $h$ 的<strong>得分</strong>为：</p>
<p><img src="/2024/02/02/li2019/f4.png" alt="f4" title="formula 4"></p>
<p>其中，$K_o$ 表示有 o 关联检测的所有关键帧集合；$\delta_k$ 表示在关键帧 $k$ 中的关联检测；$f_k$ 将物体假设 h 投影至关键帧 $k$ 中；损失函数 $c$ 由式1描述；$\Gamma$ 表示两个物体 $h, h’$ <strong>在上下文环境中的相关性</strong>（如键盘和鼠标更有可能同时出现在一表面上）。</p>
<p>得到所有假设的得分之后，利用最高得分的假设（且必须比现有假设的得分更高）来更新每个路标。</p>
<h3 id="3-2-4-Contextual-Coherence"><a href="#3-2-4-Contextual-Coherence" class="headerlink" title="3.2.4 Contextual Coherence"></a>3.2.4 Contextual Coherence</h3><p>作者使用上下文约束来对物体的位姿估计进行<strong>正则化</strong>，以鼓励物体地标更符合典型的空间关系，作者之前的研究也证明了物体的<strong>共面性coplanarity</strong> 可视为对物体位姿估计的可靠约束。作者定义相关性函数 $\Gamma$ ：</p>
<p><img src="/2024/02/02/li2019/f5.png" alt="f5" title="formula 5"></p>
<p>其中，$BOTTOMDIST(h, h’)$ 给定两个3D bbox 的底面距离，$COPLANAR$ 指的是位于相同的表面上的两个物体。</p>
<h2 id="3-3-Relocalization"><a href="#3-3-Relocalization" class="headerlink" title="3.3 Relocalization"></a>3.3 Relocalization</h2><p>给定两个包含一组物体地标的语义地图 $L_1, L_2$ ，重定位过程可通过下述公式解决：</p>
<p><img src="/2024/02/02/li2019/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$\theta(o, L)$ 返回一组地标，地标位姿在坐标系 $o$ 中来表示；$\psi(s, L)$ 返回一组尺寸放大 $s$ 倍的地标；函数 $\Omega(L_1, L_2)$ 包含两个操作：</p>
<ol>
<li>利用Hungarian 算法计算两组地标的<strong>最佳匹配</strong>；</li>
<li>对匹配结果进行内点识别，最终返回<strong>内点数量</strong>。</li>
</ol>
<p>完成地图匹配之后即可得到两个相机轨迹之间的相对转换关系。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Topology</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Topology</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 LaneLoc_Lane marking based localization using highly accurate maps</title>
    <url>/2024/01/31/laneloc/</url>
    <content><![CDATA[<p>Schreiber, Markus, Carsten Knoppel, and Uwe Franke. “LaneLoc: Lane Marking Based Localization Using Highly Accurate Maps.” In <em>2013 IEEE Intelligent Vehicles Symposium (IV)</em>, 449–54. Gold Coast City, Australia: IEEE, 2013. <a href="https://doi.org/10.1109/IVS.2013.6629509">https://doi.org/10.1109/IVS.2013.6629509</a>.</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>作者利用双目立体相机系统和包含路沿与道路标志的先验高精度地图实现车辆定位。定位过程中，GNSS 位置只是用来进行初始化，后续的定位解算不需要GNSS。作者在长约50 km的郊区道路进行测试，最终的定位精度在分米级。</p>
<span id="more"></span>
<h1 id="2-Mapping"><a href="#2-Mapping" class="headerlink" title="2 Mapping"></a>2 Mapping</h1><p>制图过程如下所示：</p>
<p><img src="/2024/01/31/laneloc/fig2.png" alt="fig2" title="figure 2"></p>
<p>利用各个传感器采集到的数据制作鸟瞰视角地图，如下图所示：</p>
<p><img src="/2024/01/31/laneloc/fig4.png" alt="fig4" title="figure 4"></p>
<h1 id="3-Online-Localization"><a href="#3-Online-Localization" class="headerlink" title="3 Online Localization"></a>3 Online Localization</h1><p>作者的定位系统选择一个前向的立体相机系统以及一个IMU，如下图所示，其中GNSS 模块进行初始化。</p>
<p><img src="/2024/01/31/laneloc/fig6.png" alt="fig6" title="figure 6"></p>
<p>作者使用基于Kalman 滤波器的定位模型。</p>
<h2 id="3-1-Localization-Model"><a href="#3-1-Localization-Model" class="headerlink" title="3.1 Localization Model"></a>3.1 Localization Model</h2><p><strong>观测模型</strong>表示：所有测量点 $\vec{P}_{e,i}’$ 的<strong>预期位置</strong>（依赖于状态向量的方程${h}(\vec{x}_{veh})$ ）和所有<strong>观测值</strong> $\vec{P}_{m,i}$ （表示为 $\vec{y}$ ）在汽车坐标系下的<strong>残差</strong>：</p>
<p><img src="/2024/01/31/laneloc/fig7.png" alt="fig7" title="figure 7"></p>
<p><img src="/2024/01/31/laneloc/f6.png" alt="f6" title="formula 6"></p>
<p><strong>观测噪声</strong>的方差包含：</p>
<ul>
<li>地图数据噪声 $\sigma_{map}^2$ ;</li>
<li>back-projection 噪声 $\sigma_{cam}^2$ ，取决于相机高度和路的朝向。</li>
</ul>
<p><img src="/2024/01/31/laneloc/f7.png" alt="f7" title="formula 7"></p>
<p>作者在实验过程中分别假设：$\sigma_{map}^2 = 10 cm$ ，$\sigma_{cam}^2 = 3 px$ 。观测噪声表示为一个<strong>取决于残差的概率函数</strong>。</p>
<h2 id="3-2-Map-Matching"><a href="#3-2-Map-Matching" class="headerlink" title="3.2 Map Matching"></a>3.2 Map Matching</h2><p>用来定位的高精度地图包含表示<strong>路标和路沿的线段</strong>，而观测数据包含<strong>点</strong>；地图匹配的目标是实现<strong>测量点与线段之间的最佳可能匹配</strong>，必须满足横向和纵向匹配残差的最小化方可实现最佳匹配，因此，作者选择对地图上的线段<strong>采样为地图点进行匹配</strong>，如Fig. 8所示。</p>
<p><img src="/2024/01/31/laneloc/fig8.png" alt="fig8" title="figure 8"></p>
<h2 id="3-3-Measurement-Extraction"><a href="#3-3-Measurement-Extraction" class="headerlink" title="3.3 Measurement Extraction"></a>3.3 Measurement Extraction</h2><h3 id="3-3-1-Lane-Marking-Measurement"><a href="#3-3-1-Lane-Marking-Measurement" class="headerlink" title="3.3.1 Lane Marking Measurement"></a>3.3.1 Lane Marking Measurement</h3><p>作者使用一个<strong>定向匹配滤波器 oriented matched filter</strong>（在<strong>普通车道线检测系统</strong>中得到了成功应用）来检测车道线。</p>
<p>为了检测车道线，利用当前位姿估计将地图投影到图片中，并将<strong>搜索线</strong>置于预期的车道线周围，定向匹配滤波器会在这些搜索线中根据图片中测量的路标识别出一个<strong>low-high-low灰度值模式</strong>，利用立体相机提供的深度信息，这些被探测到的位置会投影至道路平面上来决定在汽车坐标系中所需的观测值（即残差 $\vec{r}’$ ）。</p>
<p>由于该投影过程对于汽车的俯仰角非常敏感，所以作者使用一个基于V-Diaparity 方法的<strong>俯仰角估计器</strong>来解决该问题。</p>
<p><img src="/2024/01/31/laneloc/fig10.png" alt="fig10" title="figure 10"></p>
<h3 id="3-3-2-Curb-Measurement"><a href="#3-3-2-Curb-Measurement" class="headerlink" title="3.3.2 Curb Measurement"></a>3.3.2 Curb Measurement</h3><p>作者使用一个基于强度图片 intensity image 和高度信息的<strong>基于分类器的识别</strong>来进行路沿探测，得到路沿在图片中的位置以及存在的概率参数。</p>
<p><img src="/2024/01/31/laneloc/fig11.png" alt="fig11" title="figure 11"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>Object SLAM部署过程</title>
    <url>/2024/01/26/object-slam/</url>
    <content><![CDATA[<p>系统代码包下载地址为<a href="https://github.com/yangliu9527/Object_SLAM.git">github地址</a>，论文为(Liu 等, 2023).</p>
<h2 id="1-ORB-SLAM2基础问题"><a href="#1-ORB-SLAM2基础问题" class="headerlink" title="1 ORB-SLAM2基础问题"></a>1 ORB-SLAM2基础问题</h2><p>该算法是在ORB-SLAM2 的基础上进行改进的，编译过程可参考ORB-SLAM2的部署教程，此处不再赘述。除此之外，本人在部署过程中还遇到了其他问题，这里记录一下。</p>
<span id="more"></span>
<h2 id="2-其他问题"><a href="#2-其他问题" class="headerlink" title="2 其他问题"></a>2 其他问题</h2><ul>
<li>PCL 报错：</li>
</ul>
<blockquote>
<p>error: #error PCL requires C++14 or above</p>
</blockquote>
<p>参考这篇<a href="https://blog.csdn.net/handily_1/article/details/122421305">文章</a>解释，使用C++14编译器，更改主目录下的CMakeLists.txt：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">### 修改前</span><br><span class="line"># Check C++<span class="number">11</span> <span class="keyword">or</span> C++<span class="number">0</span><span class="function">x support</span></span><br><span class="line"><span class="function"><span class="title">include</span><span class="params">(CheckCXXCompilerFlag)</span></span></span><br><span class="line"><span class="function"><span class="title">CHECK_CXX_COMPILER_FLAG</span><span class="params">(<span class="string">&quot;-std=c++11&quot;</span> COMPILER_SUPPORTS_CXX11)</span></span></span><br><span class="line"><span class="function"><span class="title">CHECK_CXX_COMPILER_FLAG</span><span class="params">(<span class="string">&quot;-std=c++0x&quot;</span> COMPILER_SUPPORTS_CXX0X)</span></span></span><br><span class="line"><span class="function"><span class="title">if</span><span class="params">(COMPILER_SUPPORTS_CXX11)</span></span></span><br><span class="line"><span class="function">   <span class="title">set</span><span class="params">(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++11&quot;</span>)</span></span></span><br><span class="line"><span class="function">   <span class="title">add_definitions</span><span class="params">(-DCOMPILEDWITHC11)</span></span></span><br><span class="line"><span class="function">   <span class="title">message</span><span class="params">(STATUS <span class="string">&quot;Using flag -std=c++11.&quot;</span>)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">### 修改后</span></span><br><span class="line"><span class="function"># Check C++11 <span class="keyword">or</span> C++0x support</span></span><br><span class="line"><span class="function"><span class="title">include</span><span class="params">(CheckCXXCompilerFlag)</span></span></span><br><span class="line"><span class="function"><span class="title">CHECK_CXX_COMPILER_FLAG</span><span class="params">(<span class="string">&quot;-std=c++11&quot;</span> COMPILER_SUPPORTS_CXX11)</span></span></span><br><span class="line"><span class="function"><span class="title">CHECK_CXX_COMPILER_FLAG</span><span class="params">(<span class="string">&quot;-std=c++0x&quot;</span> COMPILER_SUPPORTS_CXX0X)</span></span></span><br><span class="line"><span class="function"><span class="title">if</span><span class="params">(COMPILER_SUPPORTS_CXX11)</span></span></span><br><span class="line"><span class="function">   <span class="title">set</span><span class="params">(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++14&quot;</span>)</span></span></span><br><span class="line"><span class="function">   <span class="title">add_definitions</span><span class="params">(-DCOMPILEDWITHC11)</span></span></span><br><span class="line"><span class="function">   <span class="title">message</span><span class="params">(STATUS <span class="string">&quot;Using flag -std=c++14.&quot;</span>)</span></span></span><br></pre></td></tr></table></figure>
<p>修改完之后不再报错，可正常编译。</p>
<ul>
<li>针对不同的输入图片，需要设置不同的通道变换方式，否则会报与数据通道相关的错误，如下图所示；修改方法为修改frame.cc 代码 392行附近的内容：</li>
</ul>
<p><img src="/2024/01/26/object-slam/err1.png" alt="err1" title="err1"></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//* for gray pictures (e.g. KITTI-odometry dataset)</span></span><br><span class="line">cv::Mat imRGB_1;</span><br><span class="line">cv::<span class="built_in">cvtColor</span>(imRGB.<span class="built_in">clone</span>(), imRGB_1, CV_GRAY2BGR);</span><br><span class="line">cv::<span class="built_in">cvtColor</span>(imRGB_1.<span class="built_in">clone</span>(), Img_HSV, CV_BGR2HSV);</span><br><span class="line"></span><br><span class="line"><span class="comment">//* for RGB pictures (e.g. TUM dataset)</span></span><br><span class="line"><span class="comment">// cv::cvtColor(imRGB.clone(), Img_HSV, CV_BGR2HSV);</span></span><br></pre></td></tr></table></figure>
<ul>
<li>在VS Code 里对程序进行调试运行时，可能会报错：</li>
</ul>
<p><img src="/2024/01/26/object-slam/err2.png" alt="err2"></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 只有在VS Code 中调试才会出现该错误，解决办法是在VS Code 中unset GTK_PATH即可：</span></span><br><span class="line"><span class="built_in">unset</span> GTK_PATH</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Pose Optimization</tag>
        <tag>Object SLAM</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 ORB-SLAM_A Versatile and Accurate Monocular SLAM System</title>
    <url>/2024/01/29/orb-slam/</url>
    <content><![CDATA[<p>Mur-Artal, Raul, J. M. M. Montiel, and Juan D. Tardos. “ORB-SLAM: A Versatile and Accurate Monocular SLAM System.” <em>IEEE Transactions on Robotics</em> 31, no. 5 (October 2015): 1147–63. <a href="https://doi.org/10.1109/TRO.2015.2463671">https://doi.org/10.1109/TRO.2015.2463671</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献如下：</p>
<ol>
<li>对所有任务使用相同的特性：tracking, mapping, relocalization and loop closing, 这使得我们的系统更加<strong>高效</strong>、<strong>简单</strong>且<strong>可靠</strong>；使用ORB 特征，可在CPU 上实现实时运行，且具有较好的<strong>视角、光照不变性</strong>；</li>
<li>利用<strong>共视图</strong>使得跟踪与制图都聚焦于一个局部共视区域，从而实现在大规模环境中地<strong>实时</strong>操作，可不受全局地图尺寸地影响；</li>
<li>基于<strong>位姿图优化</strong>的实时回环检测（作者称其为 Essential Graph），其构建于系统维护的spanning tree、回环连接以及共视图中的线；</li>
<li>基于良好的视角和光照不变性实现的<strong>实时相机重定位</strong>，可在跟踪失败时进行<strong>重初始化</strong>，并增强了<strong>地图的重用性</strong>；</li>
<li>提出一个基于模型选择的自动鲁棒的<strong>初始化程序</strong>，可为平面和非平面场景创建一个<strong>初始化地图</strong>；</li>
<li>对于地图点和关键帧采取“适者生存” <strong>survival of the fittest</strong> 策略，在生成时非常宽松，而在剔除时非常严格，该策略提高了跟踪的鲁棒性，且由于冗余的关键帧被舍弃了，从而增强了 lifelong operation。</li>
</ol>
<span id="more"></span>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3 System Overview"></a>3 System Overview</h1><h2 id="3-1-Feature-Choice"><a href="#3-1-Feature-Choice" class="headerlink" title="3.1 Feature Choice"></a>3.1 Feature Choice</h2><p>本系统的一个重要设计是：制图与跟踪所使用的<strong>同样特性</strong>会用于<strong>地点重识别</strong>，来进行帧级的重定位和回环检测。</p>
<h2 id="3-2-Three-Threads-Tracking-Local-Mapping-and-Loop-Closing"><a href="#3-2-Three-Threads-Tracking-Local-Mapping-and-Loop-Closing" class="headerlink" title="3.2 Three Threads: Tracking, Local Mapping, and Loop Closing"></a>3.2 Three Threads: Tracking, Local Mapping, and Loop Closing</h2><p>系统整体架构如Fig. 1所示：</p>
<p><img src="/2024/01/29/orb-slam/overview.png" alt="overview" title="overview"></p>
<h2 id="3-3-Map-Points-Keyframes-and-Their-Selection"><a href="#3-3-Map-Points-Keyframes-and-Their-Selection" class="headerlink" title="3.3 Map Points, Keyframes, and Their Selection"></a>3.3 Map Points, Keyframes, and Their Selection</h2><p>每个地图点 $p_i$ 存储以下信息：</p>
<ol>
<li>世界坐标系中的3D 位置信息 $\mathbf{X}_{w, i}$ ；</li>
<li>视角朝向 $\mathbf{n}_i$，是所有视角方向（观测到该点的关键帧的相机光心与该点的连线）的平均单位向量；</li>
<li>一个代表性的 ORB 描述子 $\mathbf{D}_i$ ，利用所有观测到该点的关键帧中的描述子计算一个最小汉明距离的ORB 描述子；</li>
<li>根据ORB 特征的尺度不变约束，计算该点可被观测到的最大距离和最小距离 $d_{max}, d_{min}$ 。</li>
</ol>
<p>每个关键帧 $K_i$ 存储以下信息：</p>
<ol>
<li>相机<strong>位姿</strong> $\mathbf{T}_{iw}$ ，是从世界坐标系到相机坐标系的刚体转换关系；</li>
<li>相机<strong>内参</strong>，包含焦距和光心；</li>
<li>该帧图片中提取的<strong>所有ORB 特征</strong>，是否与地图点关联。</li>
</ol>
<h2 id="3-4-Covisibility-Graph-and-Essential-Graph"><a href="#3-4-Covisibility-Graph-and-Essential-Graph" class="headerlink" title="3.4 Covisibility Graph and Essential Graph"></a>3.4 Covisibility Graph and Essential Graph</h2><p>关键帧之间的<strong>共视信息</strong>对于本系统的许多任务而言至关重要，该共视信息使用<strong>无向加权图</strong>来表示，图中每个节点代表一个关键帧，如果两个关键帧之间的共视地图点超过15个，则进行节点间的连线，并使用共视地图点的数量来表示权重参数 $\theta$ 。</p>
<p>作者使用<strong>位姿图优化</strong>对回环检测到的位姿进行整体优化，为了避免包含共视图中所有的边（过于稠密），作者提出 <strong>Essential Graph</strong>只保留<strong>所有的节点与部分边</strong>，仍然可以保留强壮的网络结构来产生精确的结果。</p>
<p>系统会从初始帧开始构建一个<strong>增量式spanning tree</strong>，提供一个具有最小边数量的共视图的<strong>连接子图</strong>，当一个新的关键帧被插入时，被包含在该树中，并和与其<strong>有最多共视点</strong>的关键帧连接；当某个关键帧被剔除后，会更新相应的受影响的连线。</p>
<p>而Essential Graph 包含<strong>spanning tree</strong>、<strong>共视图</strong>中共视点数超过100个的边，以及<strong>回环检测边</strong>，从而形成一个强壮的相机轨迹网络。</p>
<p><img src="/2024/01/29/orb-slam/reconstruction.png" alt="reconstruction" title="reconstruciton and graphs"></p>
<h2 id="3-5-Bags-of-Words-Place-Recognition"><a href="#3-5-Bags-of-Words-Place-Recognition" class="headerlink" title="3.5 Bags of Words Place Recognition"></a>3.5 Bags of Words Place Recognition</h2><p>系统集成了一个基于DBoW2 的词袋库地点重识别模块来进行回环检测和重定位，本系统创建一个增量式的数据集以进行查询和更新。</p>
<h1 id="4-Automatic-Map-Initialization"><a href="#4-Automatic-Map-Initialization" class="headerlink" title="4 Automatic Map Initialization"></a>4 Automatic Map Initialization</h1><p>地图初始化的目标是计算两帧图片之间的<strong>相对位姿</strong>来<strong>三角化</strong>一组地图点，作者提出并行计算两种几何模型：平面场景下的<strong>单应矩阵（homography）</strong>，及非平面场景下的<strong>基础矩阵</strong>。本系统的地图初始化步骤如下所示：</p>
<ol>
<li><strong>寻找初始关系：</strong>提取当前帧中的ORB 特征，并寻找与参考帧之间的匹配，如果没有找到足够的匹配，重置参考帧；</li>
<li><strong>并行计算两个模型：</strong>分别计算单应矩阵和基础矩阵，并在迭代中计算相应的得分，最后保留得分最高的矩阵；</li>
<li><strong>模型选择：</strong>若场景属于平面、近似平面或者存在较小的视差，可选用单应矩阵；如果是有足够视差的非平面场景，应当选用基础矩阵；</li>
<li><strong>从移动恢复（motion recovery）中得到动作和结构：</strong>对于单应矩阵，作者直接对8组解进行三角化，并检查是否存在一个解使得大部分点位于相机前部且有着较低的重投影误差，如果不存在一个具有明显优势的解，则返回第一步重新开始初始化，该方法被认为是本系统鲁棒性的关键所在；对于基础矩阵，利用内参计算出本质矩阵，利用奇异值分解恢复出四个运动假设，然后采用与单应矩阵相同的方法进行求解；</li>
<li><strong>BA：</strong>最后，利用<strong>full BA</strong> 来优化初始重建。</li>
</ol>
<p>一个初始化的例子如下图所示，PTAM 和 LSD-SLAM 初始化一个平面上的所有点，而本系统会等到有足够的视差后利用基础矩阵才进行正确的初始化。</p>
<p><img src="/2024/01/29/orb-slam/initialization.png" alt="initiallization" title="initialization"></p>
<h1 id="5-Tracking"><a href="#5-Tracking" class="headerlink" title="5 Tracking"></a>5 Tracking</h1><h2 id="5-2-Initial-Pose-Estimation-From-Previous-Frame"><a href="#5-2-Initial-Pose-Estimation-From-Previous-Frame" class="headerlink" title="5.2 Initial Pose Estimation From Previous Frame"></a>5.2 Initial Pose Estimation From Previous Frame</h2><p>如果上一帧跟踪成功，本系统使用一个<strong>固定速度运动模型</strong>来预测相机位姿，并对上一帧观测到的地图点进行一个<strong>引导式搜索</strong>，然后利用寻找到的关联对位姿进行优化。</p>
<h2 id="5-3-Initial-Pose-Estimation-via-Global-Relocalization"><a href="#5-3-Initial-Pose-Estimation-via-Global-Relocalization" class="headerlink" title="5.3 Initial Pose Estimation via Global Relocalization"></a>5.3 Initial Pose Estimation via Global Relocalization</h2><p>如果跟踪失败，则将该帧图片转换为<strong>词袋</strong>并进行词袋库搜索以实现<strong>全局重定位</strong>，对每个候选关键帧进行RANSAC 迭代并使用PnP 计算相机的位姿；若找到具有足够内点的相机位姿，则根据匹配关键帧的地图点搜寻更多的匹配以进行位姿优化。</p>
<h2 id="5-4-Track-Local-Map"><a href="#5-4-Track-Local-Map" class="headerlink" title="5.4 Track Local Map"></a>5.4 Track Local Map</h2><p>获取相机位姿的估计和一组初始化特征匹配后，将<strong>局部地图</strong>投影至当前帧中来搜寻更多的匹配。该局部地图包含与当前帧有匹配特征点的关键帧 $\mathcal{K}_1$ ，以及在共视图中 $\mathcal{K}_1$ 的邻接关键帧 $\mathcal{K}_2$ ，关键帧 $\mathcal{K}_1$ 、$\mathcal{K}_2$ 中的所有点进行以下搜索策略：</p>
<ol>
<li>将点投影至当前帧中，舍弃掉超出图片界限的点；</li>
<li>比较当前帧中点和地图中点的视角朝向，舍弃小于60读夹角的点；</li>
<li>计算地图点到相机光心的距离，舍弃掉超范围的点 $d \notin [d_{min}, d_{max}]$ ；</li>
<li>计算尺度信息 $d/d_{min}$ ；</li>
<li>将地图点的代表性描述子与当前帧中未匹配的ORB 特征进行比较，寻找地图点的最佳匹配。</li>
</ol>
<p>最终利用所有匹配的点对相机位姿进行优化。</p>
<h2 id="5-5-New-Frame-Decision"><a href="#5-5-New-Frame-Decision" class="headerlink" title="5.5 New Frame Decision"></a>5.5 New Frame Decision</h2><p><strong>新的关键帧</strong>需满足以下所有的要求：</p>
<ol>
<li>距离上一次全局重定位不少于20帧（为了更好地重定位）；</li>
<li>局部制图线程空闲，或者距离上次插入关键帧已超过20帧；</li>
<li>当前帧至少跟踪了50个地图点（为了更好地跟踪）；</li>
<li>当前帧比参考帧少跟踪90%的点（限制最小的视角变化）。</li>
</ol>
<h1 id="6-Local-Mapping"><a href="#6-Local-Mapping" class="headerlink" title="6 Local Mapping"></a>6 Local Mapping</h1><h2 id="6-1-Keyframe-Insertion"><a href="#6-1-Keyframe-Insertion" class="headerlink" title="6.1 Keyframe Insertion"></a>6.1 Keyframe Insertion</h2><p>每当插入一个新的关键帧，进行一下操作：</p>
<ul>
<li>更新<strong>共视图</strong>，增加新的节点和边；</li>
<li>更新<strong>spanning tree</strong>；</li>
<li>计算该关键帧的<strong>词袋表示</strong>。</li>
</ul>
<h2 id="6-2-Recent-Map-Points-Culling"><a href="#6-2-Recent-Map-Points-Culling" class="headerlink" title="6.2 Recent Map Points Culling"></a>6.2 Recent Map Points Culling</h2><p>地图点要想保留在地图中，需要在创建后的前三个关键帧中经过严格的测试：</p>
<ol>
<li>跟踪线程需要在其被预测可见的关键帧中至少有25%的比例被观测到；</li>
<li>若该点创建后已经过了一个关键帧，那么至少需要被三个关键帧观测到。</li>
</ol>
<h2 id="6-3-New-Map-Point-Creation"><a href="#6-3-New-Map-Point-Creation" class="headerlink" title="6.3 New Map Point Creation"></a>6.3 New Map Point Creation</h2><p>新的地图点通过共视图中的相连关键帧对ORB 特征点进行三角化来创建。</p>
<h2 id="6-4-Local-Bundle-Adjustment"><a href="#6-4-Local-Bundle-Adjustment" class="headerlink" title="6.4 Local Bundle Adjustment"></a>6.4 Local Bundle Adjustment</h2><p>局部BA 的优化对象为：当前关键帧，共视图中与当前关键帧相连的所有关键帧，以及这些关键帧中的所有地图点。至于其余可观测到这些地图点但是未与当前关键帧相连的那些关键帧也会参与到优化过程中，但是其自身保持固定。</p>
<h2 id="6-5-Local-Keyframe-Culling"><a href="#6-5-Local-Keyframe-Culling" class="headerlink" title="6.5 Local Keyframe Culling"></a>6.5 Local Keyframe Culling</h2><p>之所以要控制关键帧的数量，是因为BA 的计算复杂度与该数量正相关，而且在lifelong operation 中，同一场景中的关键帧数量不能无限制增长。</p>
<p>本系统根据以下准则舍弃掉关键帧：其90%的地图点在其他至少三个关键帧中以相同或更好的尺度被观测到。这个尺度条件确保了地图点可以保留那些自身被最好精度观测到的关键帧。</p>
<h1 id="7-Loop-Closing"><a href="#7-Loop-Closing" class="headerlink" title="7 Loop Closing"></a>7 Loop Closing</h1><h2 id="7-1-Loop-Candidates-Detection"><a href="#7-1-Loop-Candidates-Detection" class="headerlink" title="7.1 Loop Candidates Detection"></a>7.1 Loop Candidates Detection</h2><p>首先，将当前帧和共视图中与其相连的所有帧（共视点数量不少于30个）计算BoW 向量的相似性，得到一个最低的阈值 $s_{min}$ ；然后再词袋库中进行匹配，舍弃掉低于该阈值的关键帧，此外，还需舍弃掉与当前帧直接相连的关键帧；只有当检测到连续三个一致的回环候选帧（在共视图中是相连的），才判定存在回环。</p>
<h2 id="7-2-Compute-the-Similarity-Transformation"><a href="#7-2-Compute-the-Similarity-Transformation" class="headerlink" title="7.2 Compute the Similarity Transformation"></a>7.2 Compute the Similarity Transformation</h2><p>在单目SLAM 中，地图有<strong>7自由度</strong>：旋转、平移，以及一个尺度因子。因此，为了闭合一个回环，需要计算一个从当前帧到回环帧之间的<strong>相似转换</strong>，以获取回环的累积误差；同时，该相似度计算也作为该回环的<strong>几何验证</strong>。</p>
<p>首先计算当前帧与回环候选帧之间的<strong>地图点关联</strong>，然后为每个候选回环帧进行RANSAC ，若获取足够多的内点，则进行优化并进一步获取更多的关联，并进一步进行优化，此时若有足够的内点数量支持该相似度，则确定回环。</p>
<h2 id="7-3-Loop-Fusion"><a href="#7-3-Loop-Fusion" class="headerlink" title="7.3 Loop Fusion"></a>7.3 Loop Fusion</h2><p>回环矫正的第一步是将<strong>重复的地图点</strong>进行融合，并在<strong>共视图</strong>中插入新的边以完成回环闭合。使用计算得到的<strong>相似转换</strong>来矫正当前关键帧的位姿，并将该矫正传递至与其邻接的关键帧，使得回环两侧进行<strong>对齐</strong>。</p>
<h2 id="7-4-Essential-Graph-Optimization"><a href="#7-4-Essential-Graph-Optimization" class="headerlink" title="7.4 Essential Graph Optimization"></a>7.4 Essential Graph Optimization</h2><p>利用essential graph 将回环闭合误差分布至整个图上以进行优化；利用<strong>相似转换</strong>进行优化来矫正尺度偏移。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>ORB-SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>C++</tag>
        <tag>ORB-SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>寒武纪MLU220 开发环境Docker搭建</title>
    <url>/2024/01/26/mlu220/</url>
    <content><![CDATA[<p>首先进入<a href="https://cair.cambricon.com/#/home/catalog">寒武纪开发主页</a>并登录寒武纪账号，进入cambricon_pytorch docker 页面。由于本人只使用MLU220 进行边缘端推理，所以不需要在主机上安装MLU 驱动，因此可跳过第一步直接按照提示安装docker。本人在尝试按照页面说明时遇到了一些问题，在此记录一下。</p>
<span id="more"></span>
<p>若是初次使用docker，则建议将Ubuntu个人账户添加进docker组里，这样就避免每次使用时都要输入sudo了，该部分参考<a href="https://www.cnblogs.com/jzcn/p/16591083.html">文章</a>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">## 将当前用户添加进docker组，并更新</span><br><span class="line">sudo gpasswd -a user docker</span><br><span class="line">newgrp docker</span><br><span class="line"></span><br><span class="line">## 然后重启电脑才可永久生效</span><br></pre></td></tr></table></figure>
<p>然后按照寒武纪提示，依次进行如下操作：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">## 登录harbor</span><br><span class="line">docker login cair.cambricon.com</span><br><span class="line"></span><br><span class="line">## 输入用户名、API密钥（网页用户名下拉框中有API密钥选项）</span><br><span class="line">Username: (username)</span><br><span class="line">Password: (API密钥)</span><br></pre></td></tr></table></figure>
<p>注意，这里网页提示使用docker pull命令下载相应的镜像文件，但经过本人尝试之后发现，在后续的docker run命令中会重复下载，因此<strong>跳过使用docker pull，直接使用docker run命令创建容器</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -it --name mlu220 -v /home/***/work:/work cair.cambricon.com/cambricon/cambricon_pytorch:ubuntu18.04_sdk_v1.7.0_pytorch_v0.15.0-2 /bin/bash</span><br></pre></td></tr></table></figure>
<p>上述命令会自动下载镜像文件，并改名为“mlu220”，且将主机的“/home/<em>*</em>/work”映射至docker端的“/work”。</p>
<p>此时，可以查看新建的容器：</p>
<p><img src="/2024/01/26/mlu220/docker1.png" alt="docker" title="容器查看"></p>
<p>启动mlu220容器并查看：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">❯ docker start mlu220</span><br><span class="line">mlu220</span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/mlu220/docker2.png" alt="docker" title="启动容器"></p>
<p>最后进入docker并激活开发环境：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it mlu220 /bin/bash</span><br><span class="line"><span class="built_in">source</span> torch/venv3/pytorch/bin/activate</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Cambricon</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 ORB-SLAM2_An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras</title>
    <url>/2024/01/29/orb-slam2/</url>
    <content><![CDATA[<p>Mur-Artal, Raul, and Juan D. Tardos. “ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras.” <em>IEEE Transactions on Robotics</em> 33, no. 5 (October 2017): 1255–62. <a href="https://doi.org/10.1109/TRO.2017.2705103">https://doi.org/10.1109/TRO.2017.2705103</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ol>
<li>第一个适用于<strong>单目</strong>、<strong>双目</strong>以及<strong>RGB-D 相机</strong>的开源SLAM 系统，该系统包含回环检测、重定位以及地图重用；</li>
<li>本系统运行RGB-D 的结果证明：使用<strong>BA</strong> 可以实现比基于ICP 或者光度深度误差最小化的SOTA 方法更高的精度；</li>
<li>通过使用<strong>近远立体点和单目观测</strong>，本系统运行双目的结果要比直接双目SLAM 的 SOTA 算法精度更高；</li>
<li>提出了一种关闭制图功能情况下，有效<strong>重使用地图</strong>的轻量级定位模式。</li>
</ol>
<span id="more"></span>
<p><img src="/2024/01/29/orb-slam2/fig1.png" alt="fig1" title="fig 1"></p>
<h1 id="3-ORB-SLAM2"><a href="#3-ORB-SLAM2" class="headerlink" title="3 ORB-SLAM2"></a>3 ORB-SLAM2</h1><p>ORB-SLAM2 的整体架构如下所示：</p>
<p><img src="/2024/01/29/orb-slam2/fig2.png" alt="fig2" title="fig 2"></p>
<p>系统包含三个主要的并行线程：</p>
<ol>
<li><strong>跟踪线程：</strong>寻找与<strong>局部地图</strong>相匹配的特征点，利用<strong>motion-only BA</strong> <strong>最小化重投影误差</strong>，来解算<strong>每帧图片</strong>对应的相机位姿；</li>
<li><strong>局部制图：</strong>管理<strong>局部地图</strong>，并使用<strong>局部BA</strong> 对其进行优化；</li>
<li><strong>回环检测：</strong>检测大回环，并使用<strong>位姿图优化</strong>来消除累积漂移；然后开启第四个线程，进行<strong>全局BA</strong> 优化解算地图与位姿的最优解。</li>
</ol>
<p>本系统还嵌入了一个基于<strong>DBoW2</strong> 的地点重识别模块进行<strong>重定位</strong>，在跟踪失败或利用现有地图进行重初始化时使用；本系统维护一个<strong>共视图</strong>，来关联任意两个具有共同观测特征点的关键帧，并使用一个<strong>最小化spanning tree</strong> 来连接所有的关键帧；这些图结构方便恢复关键帧的<strong>局部窗口</strong>以进行局部的跟踪与制图，并为回环检测中的位姿图优化提供结构。</p>
<h2 id="3-1-Monocular-Close-Stereo-and-Far-Stereo-Keypoints"><a href="#3-1-Monocular-Close-Stereo-and-Far-Stereo-Keypoints" class="headerlink" title="3.1 Monocular, Close Stereo, and Far Stereo Keypoints"></a>3.1 Monocular, Close Stereo, and Far Stereo Keypoints</h2><p>本系统经过如图Fig. 2（b）的<strong>图片预处理操作</strong>，提取出关键点的特征，系统后续的操作均是基于这些<strong>特征点（立体关键点和单目关键点）</strong>的，实现独立于所使用的传感器类型。后续的操作均基于立体关键点和单目关键点。</p>
<p><strong>立体关键点</strong>使用三个坐标进行定义：$\mathbf{x}_s = (u_L, v_L, u_R)$ ，其中 $(u_L, v_L)$ 是特征点在左边图片中的坐标，$u_R$ 是特征点在右边图片的水平座标。对于RGB-D 相机，作者将深度值 d 转化为一个<strong>虚拟的右图坐标</strong>：</p>
<p><img src="/2024/01/29/orb-slam2/f1.png" alt="f1" title="formula 1"></p>
<p>作者定义，如果一个关键点的<strong>深度小于基线长度的40倍</strong>，则被视为近点，否则视为远点。对于近点，可以使用一帧图片进行安全的三角化，因为其深度信息得到了准确估计，可以提供相应的<strong>尺度、平移和旋转信息</strong>；而对于远点，可以提供<strong>准确的旋转信息</strong>，但是尺度与平移信息较不可靠，只对多视角观测的远点进行三角化。</p>
<p><strong>单目关键点</strong>使用左图的两个坐标进行定义 $\mathbf{x}_m = (u_L, v_L)$ ，是针对那些立体匹配失败或RGB-D 深度参数不可靠的点；这些点只通过多视角观测进行三角化，且不提供尺度信息，但会参与旋转与平移估计的解算。</p>
<h2 id="3-2-System-Bootstrapping"><a href="#3-2-System-Bootstrapping" class="headerlink" title="3.2 System Bootstrapping"></a>3.2 System Bootstrapping</h2><p>使用立体相机或RGB-D 相机的一个主要优势在于：可仅使用一帧图片获取<strong>深度信息</strong>，而不需要单目相机的动作初始化操作。系统启动后，使用第一帧作为关键帧，将其位姿定为<strong>原点</strong>，并利用所有的<strong>立体关键点</strong>创建一个初始地图。</p>
<h2 id="3-3-Bundle-Adjustment-with-Monocular-and-Stereo-Constraints"><a href="#3-3-Bundle-Adjustment-with-Monocular-and-Stereo-Constraints" class="headerlink" title="3.3 Bundle Adjustment with Monocular and Stereo Constraints"></a>3.3 Bundle Adjustment with Monocular and Stereo Constraints</h2><p>BA 在本系统中的应用：</p>
<ul>
<li>在跟踪线程中优化相机位姿（motion-only BA）</li>
<li>在局部制图线程中优化局部窗口内的关键帧和特征点（local BA）</li>
<li>回环检测之后优化所有的关键帧与特征点（full BA）</li>
</ul>
<h3 id="Motion-only-BA"><a href="#Motion-only-BA" class="headerlink" title="Motion-only BA"></a>Motion-only BA</h3><p>对关键点 $\mathbf{x}_{(.)}^i$ （包括单目点 $\mathbf{x}_m^i \in \mathbb{R}^2$ 和立体点 $\mathbf{x}_s^i \in \mathbb{R}^3$ ）进行最小化重投影误差，其中 $i\in \mathcal{X}$ 为所有匹配点集合：</p>
<p><img src="/2024/01/29/orb-slam2/f2.png" alt="f2" title="formula 2"></p>
<p>其中，$\rho$ 是<strong>鲁棒Huber 损失函数</strong>，$\sum$ 是关键点尺度参数对应的协方差矩阵。单目点和立体点的投影矩阵如下所示：</p>
<p><img src="/2024/01/29/orb-slam2/f3.png" alt="f3" title="formula 3"></p>
<h3 id="Local-BA"><a href="#Local-BA" class="headerlink" title="Local BA"></a>Local BA</h3><p><img src="/2024/01/29/orb-slam2/f4.png" alt="f4" title="formula 4"></p>
<p>其中，$\mathcal{K}_L$ 为一组共视关键帧；$\mathcal{P}_L$ 为这些共视关键帧中的所有点；至于其他观测到 $\mathcal{P}_L$ 中的点且不属于 $\mathcal{K}_L$ 的关键帧 $\mathcal{K}_F$ ，会参与损失函数的构建，但是在优化中<strong>保持固定</strong>；$\mathcal{X}_k$ 表示 $\mathcal{P}_L$ 中与关键帧 k 匹配的点列表。</p>
<h3 id="Full-BA"><a href="#Full-BA" class="headerlink" title="Full BA"></a>Full BA</h3><p>是一种局部BA 的特殊情况，除了<strong>初始关键帧</strong>是固定的，地图中其余的所有关键帧和点都参与优化过程。</p>
<h2 id="3-4-Loop-Closing-and-Full-BA"><a href="#3-4-Loop-Closing-and-Full-BA" class="headerlink" title="3.4 Loop Closing and Full BA"></a>3.4 Loop Closing and Full BA</h2><p>回环检测包含两步：</p>
<ol>
<li>回环的检测与验证；</li>
<li>通过位姿图优化来矫正回环。</li>
</ol>
<h2 id="3-5-Keyframe-Insertion"><a href="#3-5-Keyframe-Insertion" class="headerlink" title="3.5 Keyframe Insertion"></a>3.5 Keyframe Insertion</h2><p>本系统遵循ORB-SLAM 的关键帧插入策略，此外，基于立体远近点创建了一个新的关键帧挑选策略：如果跟踪的近点数量低于 $\tau_t = 100$，且可新增近点数量大于 $\tau_c = 70$ 时，将其作为新的关键帧进行插入。</p>
<p><img src="/2024/01/29/orb-slam2/fig3.png" alt="fig3" title="fig 3"></p>
<h2 id="3-6-Localization-Mode"><a href="#3-6-Localization-Mode" class="headerlink" title="3.6 Localization Mode"></a>3.6 Localization Mode</h2><p>本系统引入了一个<strong>定位模式</strong>，可在已经制图的区域进行<strong>轻量级长期定位</strong>。该过程利用<strong>视觉里程计匹配点</strong>和<strong>地图匹配点</strong>进行定位，其中视觉里程计匹配是基于当前帧的ORB 和历史帧创建的3D 点之间进行的，这些匹配可在无地图区域进行定位，但会存在累积漂移；而地图点匹配会得到与地图无偏的定位结果。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>ORB-SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>C++</tag>
        <tag>ORB-SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 SOF-SLAM_A Semantic Visual SLAM for Dynamic Environments</title>
    <url>/2024/02/18/sof-slam/</url>
    <content><![CDATA[<p>Cui, Linyan, and Chaowei Ma. “SOF-SLAM: A Semantic Visual SLAM for Dynamic Environments.” <em>IEEE Access</em> 7 (2019): 166528–39. <a href="https://doi.org/10.1109/ACCESS.2019.2952161">https://doi.org/10.1109/ACCESS.2019.2952161</a>.</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>无论是DynaSLAM还是DS-SLAM，它们都是简单地将先验语义信息和几何一致性信息进行<strong>“松组合”</strong>，即利用语义信息和几何信息分别确定出各自认定的动态特征点，然后采用<strong>投票机制</strong>对最终的动态特征点进行确认；其中，DynaSLAM 认为只要有一种方法判定一个点属于动态点就认为该点是动态点（<strong>OR操作</strong>），而DS-SLAM 认为只有两种方法同时认定该点属于动态点才将该点视为动态点（<strong>AND操作</strong>）。由于动态语义先验信息的不确定性，如人或汽车存在静止的情况，而DynaSLAM所代表的方法会剔除掉大量静态特征点，导致位姿估计精度下降，甚至在部分场景中由于特征点的缺少导致解算失败；同时，语义分割存在准确度不够的情况，如物体边缘部分的分割效果较差，DS-SLAM 代表的方法可能会保留实际的动态特征点，从而导致位姿估计精度下降。</p>
<p>针对以上存在的问题，Cui 等人提出了 SOF-SLAM——Semantic Optical Flow SLAM，一种将语义信息和几何信息进行<strong>“紧组合”</strong>来实现动态目标检测和剔除的鲁棒SLAM方法；所谓的紧组合指的是<strong>利用语义信息来辅助几何信息进行动态物体的探测</strong>：作者首先利用语义分割结果去除掉场景中动态物体和潜在动态物体上的特征点，仅利用<strong>高可能性静态物体</strong>上的特征点进行基础矩阵的求解，进而利用该可靠性更高的基础矩阵应用<strong>对极几何理论</strong>实现对动态特征点的剔除，从而实现更为鲁棒可靠的动态特征点剔除方法；最终，作者在公开数据集和真实场景中进行了测试，证明了本方法较ORB-SLAM2以及其他语义SLAM算法的优势。</p>
<span id="more"></span>
<p><img src="/2024/02/18/sof-slam/image-20240218103807414.png" alt="image-20240218103807414" title="figure 2"></p>
<p><img src="/2024/02/18/sof-slam/image-20240218104037049.png" alt="image-20240218104037049" title="figure 3"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>ORB-SLAM2环境搭建与运行</title>
    <url>/2024/01/26/orbslam2-env/</url>
    <content><![CDATA[<p>本文主要参考<a href="https://blog.csdn.net/meng_152634/article/details/127570220">该文章</a>。</p>
<h2 id="1-Eigen3安装与卸载"><a href="#1-Eigen3安装与卸载" class="headerlink" title="1 Eigen3安装与卸载"></a>1 Eigen3安装与卸载</h2><h3 id="1-1-安装"><a href="#1-1-安装" class="headerlink" title="1.1 安装"></a>1.1 安装</h3><p>可通过apt命令安装，由于使用源码安装的方式在后续编译ORB-SLAM2 过程中可能会遇到一些问题，因此，本人<strong>建议使用apt命令进行安装</strong>：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install libeigen3-dev</span><br></pre></td></tr></table></figure>
<p>也可在<a href="https://eigen.tuxfamily.org/index.php?title=Main_Page">官网</a>下载eigen3 源码，然后编译安装（本人不推荐该方法，后续会出现程序找不到eigen3的问题）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> eigen-xxx</span><br><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>版本查看命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">❯ pkg-config --modversion eigen3</span><br><span class="line">3.4.0</span><br></pre></td></tr></table></figure>
<h3 id="1-2-卸载"><a href="#1-2-卸载" class="headerlink" title="1.2 卸载"></a>1.2 卸载</h3><p>通过apt 方式安装的采用以下方式进行卸载：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 通过remove 卸载</span></span><br><span class="line">sudo apt remove libeigen3-dev</span><br><span class="line"></span><br><span class="line"><span class="comment">## 可通过locate 进一步定位残余文件进行手动删除，并手动删除/usr/local/和/usr/include/目录下的eigen目录</span></span><br><span class="line">sudo updatedb</span><br><span class="line">locate eigen</span><br></pre></td></tr></table></figure>
<p>通过源码安装的，需要手动删除/usr/local/include/等目录下的eigen3目录：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo updatedb</span><br><span class="line">locate eigen3  <span class="comment"># 查看eigen3的位置</span></span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/include/eigen3</span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/lib/cmake/eigen3</span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/local//include/eigen3</span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/share/doc/libeigen3-dev</span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/local/share/pkgconfig/eigen3.pc /usr/share/pkgconfig/eigen3.pc /var/lib/dpkg/info/libeigen3-dev.list /var/lib/dpkg/info/libeigen3-dev.md5sums</span><br></pre></td></tr></table></figure>
<h2 id="2-Pangolin-安装与卸载"><a href="#2-Pangolin-安装与卸载" class="headerlink" title="2 Pangolin 安装与卸载"></a>2 Pangolin 安装与卸载</h2><h3 id="2-1-安装"><a href="#2-1-安装" class="headerlink" title="2.1 安装"></a>2.1 安装</h3><p>首先安装依赖项：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install libglew-dev</span><br><span class="line">sudo apt install libboost-dev libboost-thread-dev libboost-filesystem-dev</span><br><span class="line">sudo apt install libboost-all-dev</span><br><span class="line">sudo apt install libwayland-dev wayland-protocols</span><br><span class="line">sudo apt install libxkbcommon-dev</span><br><span class="line">sudo apt install libegl1-mesa-dev</span><br><span class="line">sudo apt install ninja-build</span><br></pre></td></tr></table></figure>
<p>然后下载源码，编译安装</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/stevenlovegrove/Pangolin.git</span><br><span class="line"></span><br><span class="line"><span class="comment">## 编译安装</span></span><br><span class="line"><span class="built_in">cd</span> Pangolin</span><br><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>然后可在/usr/local/include/和/usr/local/lib/目录下找到pangolin相关的目录及库文件：</p>
<p><img src="/2024/01/26/orbslam2-env/pangolin.png" alt="pangolin" title="Pangolin目录及库文件"></p>
<p>安装完成后进行测试：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> Pangolin/build/examples/HelloPangolin/</span><br><span class="line">./HelloPangolin</span><br></pre></td></tr></table></figure>
<p>出现以下界面说明安装成功：</p>
<p><img src="/2024/01/26/orbslam2-env/pangolin-success.png" alt="pangolin success" title="Pangolin安装成功"></p>
<h3 id="2-2-卸载"><a href="#2-2-卸载" class="headerlink" title="2.2 卸载"></a>2.2 卸载</h3><p>Pangolin 的卸载需要手动删除相关文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo updatedb</span><br><span class="line">locate pangolin</span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/local/include/pangolin</span><br><span class="line">sudo <span class="built_in">rm</span> /usr/local/lib/libpango_*.so</span><br></pre></td></tr></table></figure>
<h2 id="3-opencv-安装与卸载"><a href="#3-opencv-安装与卸载" class="headerlink" title="3 opencv 安装与卸载"></a>3 opencv 安装与卸载</h2><p>分别到<a href="https://opencv.org/releases/">opencv</a>、<a href="https://github.com/opencv/opencv_contrib/tags">contrib</a> 开发库下载对应版本，注意，两个文件的版本需要保持一致。</p>
<h3 id="3-0-pkg-config"><a href="#3-0-pkg-config" class="headerlink" title="3.0 pkg-config"></a>3.0 pkg-config</h3><h4 id="3-0-1-介绍"><a href="#3-0-1-介绍" class="headerlink" title="3.0.1 介绍"></a>3.0.1 介绍</h4><p>opencv 多版本管理主要参考<a href="https://www.cntofu.com/book/46/opencv/ubuntuxia_duo_ban_ben_opencv_qie_huan.md">文章1</a>和<a href="https://ivanzz1001.github.io/records/post/linux/2017/09/08/linux-pkg-config#1-pkg-config简单介绍">文章2</a>，该文章主要利用pkg-config 包管理工具来管理多版本的opencv。</p>
<p>一般用第三方库的时候，就少不了要使用到<strong>第三方的头文件</strong>和<strong>库文件</strong>。我们在编译、链接的时候必须要指定这些头文件和库文件的位置。对于一个比较大的第三方库，其头文件和库文件的数量是比较多的，如果我们一个个手动地写，那将是相当的麻烦的。因此，pkg-config就应运而生了。pkg-config能够把这些头文件和库文件的位置指出来，给编译器使用。pkg-config主要提供了下面几个功能：</p>
<ul>
<li>检查库的版本号。 如果所需要的库的版本不满足要求，它会打印出错误信息，避免链接错误版本的库文件</li>
<li>获得编译预处理参数，如宏定义、头文件的位置</li>
<li>获得链接参数，如库及依赖的其他库的位置，文件名及其他一些链接参数</li>
<li>自动加入所依赖的其他库的设置</li>
</ul>
<p>如以下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">gcc -o <span class="built_in">test</span> test.c `pkg-config --libs --cflags glib-2.0`</span><br></pre></td></tr></table></figure>
<p>其中，—libs 用于指定<strong>库文件</strong>，—cflags 用于指定<strong>头文件</strong>。</p>
<p>pkg-config 命令基本用法如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pkg-config &lt;options&gt; &lt;library-name&gt;</span><br></pre></td></tr></table></figure>
<h4 id="3-0-2-配置环境变量"><a href="#3-0-2-配置环境变量" class="headerlink" title="3.0.2 配置环境变量"></a>3.0.2 配置环境变量</h4><p>事实上，pkg-config只是一个工具，所以不是你安装了一个第三方库，pkg-config就能知道第三方库的头文件和库文件的位置的。为了让pkg-config可以得到一个库的信息，就要求库的提供者提供一个<strong>.pc 文件</strong>。例如，本人安装的opencv-4.6.0 中包含了对应的opencv4.pc 文件：</p>
<p><img src="/2024/01/26/orbslam2-env/opencv4.png" alt="opencv4" title="opencv4.pc"></p>
<p>首先，将.pc 文件拷贝至pkgconfig 路径下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo <span class="built_in">cp</span> ~/Softwares/opencv/opencv-4.6.0/lib/pkgconfig/opencv4.pc /usr/lib/pkgconfig/opencv4.pc</span><br></pre></td></tr></table></figure>
<p>然后，添加链接库路径：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 新建文件并编辑</span></span><br><span class="line">sudo vi /etc/ld.so.conf.d/opencv4.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">## 添加链接库路径</span></span><br><span class="line">/home/echo/Softwares/opencv/opencv-4.6.0/lib/</span><br><span class="line"></span><br><span class="line"><span class="comment">## 刷新</span></span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure>
<p>刷新之后，即可检验相应版本是否添加成功：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; pkg-config --modversion opencv4</span><br><span class="line">4.6.0</span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/orbslam2-env/opencv4-1.png" alt="opencv4" title="opencv4库文件与头文件"></p>
<p>相应地，编译工程时即可使用如下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">g++ -o cv_test cv_test.cpp `pkg-config --libs --cflags opencv4`</span><br></pre></td></tr></table></figure>
<h3 id="3-1-opencv-4-6-0"><a href="#3-1-opencv-4-6-0" class="headerlink" title="3.1 opencv-4.6.0"></a>3.1 opencv-4.6.0</h3><h4 id="3-1-1-编译与安装"><a href="#3-1-1-编译与安装" class="headerlink" title="3.1.1 编译与安装"></a>3.1.1 编译与安装</h4><p>编译命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cmake -D CMAKE_BUILD_TYPE=RELEASE \</span><br><span class="line">	-D CMAKE_INSTALL_PREFIX=/home/***/Softwares/opencv/opencv-4.6.0 \ <span class="comment">## 更换为相应位置</span></span><br><span class="line">	-D INSTALL_C_EXAMPLES=OFF \</span><br><span class="line">	-D OPENCV_ENABLE_NONFREE=ON \</span><br><span class="line">	-D WITH_CUDA=ON \</span><br><span class="line">	-D WITH_CUDNN=ON \</span><br><span class="line">	-D OPENCV_DNN_CUDA=ON \</span><br><span class="line">	-D ENABLE_FAST_MATH=1 \</span><br><span class="line">	-D CUDA_FAST_MATH=1 \</span><br><span class="line">	-D CUDA_ARCH_BIN=8.6 \  <span class="comment">## 一定更改为显卡对应的算力版本</span></span><br><span class="line">	-D WITH_CUBLAS=1 \</span><br><span class="line">	-D OPENCV_EXTRA_MODULES_PATH=/home/***/Softwares/opencv/opencv-4.6.0/opencv_contrib/modules \</span><br><span class="line">	-D BUILD_EXAMPLES=ON ..</span><br></pre></td></tr></table></figure>
<p>因为要使用pkg-config 对不同版本的opencv 进行管理，但是<strong>opencv4默认将opencv.pc 的产生选项关闭了</strong>，查看CMakelist.txt 相关语句如下所示：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">OCV_OPTION(OPENCV_GENERATE_PKGCONFIG <span class="string">&quot;Generate .pc file for pkg-config build tool (deprecated)&quot;</span> OFF)</span><br></pre></td></tr></table></figure>
<p>所以需将CMakelist.txt 中对应语句的<strong>参数改为ON</strong>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 将对应命令更改如下：</span></span><br><span class="line">OCV_OPTION(OPENCV_GENERATE_PKGCONFIG <span class="string">&quot;Generate .pc file for pkg-config build tool (deprecated)&quot;</span> ON)</span><br></pre></td></tr></table></figure>
<p>编译成功之后，make、安装：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">make -j13 -w</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>值得注意的是，由于是安装到了自定义目录下，在利用CMakeLists.txt 进行编译前，需要进行以下操作：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 将build文件夹中的OpenCVConfig.cmake、OpenCVModules.cmake移至share/opencv4中</span></span><br><span class="line">sudo <span class="built_in">cp</span> OpenCVConfig.cmake OpenCVModules.cmake ../../share/opencv4/</span><br><span class="line"></span><br><span class="line"><span class="comment">## 修改CMakeLists.txt 文件，设置搜寻路径</span></span><br><span class="line"><span class="built_in">set</span>(OpenCV_DIR /home/echo/Softwares/opencv/opencv-4.6.0/share/opencv4)</span><br><span class="line">find_package(OpenCV)</span><br></pre></td></tr></table></figure>
<h4 id="3-1-2-多版本控制"><a href="#3-1-2-多版本控制" class="headerlink" title="3.1.2 多版本控制"></a>3.1.2 多版本控制</h4><p>按照pkgconfig  版本控制，使用opencv-4.6.0 对自带的示例进行测试，修改CMakeLists.txt 文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 设置搜寻路径</span></span><br><span class="line"><span class="built_in">set</span>(OpenCV_DIR /home/echo/Softwares/opencv/opencv-4.6.0/share/opencv4)</span><br><span class="line">find_package(OpenCV)</span><br></pre></td></tr></table></figure>
<p>进行cmake编译并执行：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cmake 过程中会出现相应的版本号、库文件等信息</span></span><br><span class="line">cmake .</span><br><span class="line">make</span><br><span class="line">./opencv_example</span><br></pre></td></tr></table></figure>
<p>得到如下输出结果，证明安装成功：</p>
<p><img src="/2024/01/26/orbslam2-env/opencv4-success.png" alt="opencv-success" title="opencv安装成功"></p>
<p>同样地，使用g++ 命令进行编译：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">g++ -o opencv_example example.cpp `pkg-config --libs --cflags opencv4`</span><br></pre></td></tr></table></figure>
<p>执行生成文件，得到与上图相同的结果证明安装成功。</p>
<h3 id="3-2-opencv-3-4-11"><a href="#3-2-opencv-3-4-11" class="headerlink" title="3.2 opencv-3.4.11"></a>3.2 opencv-3.4.11</h3><p>编译命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cmake -D CMAKE_BUILD_TYPE=RELEASE \</span><br><span class="line">	-D CMAKE_INSTALL_PREFIX=/home/***/Softwares/opencv/opencv-3.4.11 \</span><br><span class="line">	-D INSTALL_C_EXAMPLES=OFF \</span><br><span class="line">	-D OPENCV_ENABLE_NONFREE=ON \</span><br><span class="line">	-D WITH_CUDA=ON \</span><br><span class="line">	-D WITH_CUDNN=ON \</span><br><span class="line">	-D OPENCV_DNN_CUDA=ON \</span><br><span class="line">	-D ENABLE_FAST_MATH=1 \</span><br><span class="line">	-D CUDA_FAST_MATH=1 \</span><br><span class="line">	-D CUDA_ARCH_BIN=8.6 \</span><br><span class="line">	-D WITH_CUBLAS=1 \</span><br><span class="line">	-D OPENCV_EXTRA_MODULES_PATH=/home/***/Softwares/opencv/opencv-3.4.11/opencv_contrib/modules \</span><br><span class="line">	-D BUILD_EXAMPLES=ON ..</span><br></pre></td></tr></table></figure>
<p>该版本的opencv 会自动在share/OpenCV 文件夹下生成OpenCVConfig.cmake、OpenCVModules.cmake 等文件，所以无需从build 文件夹中进行复制。</p>
<p>其他使用pkg-config 进行多版本控制方法与opencv-4.6.0 一致，在此不做赘述。</p>
<h3 id="3-3-opencv-卸载"><a href="#3-3-opencv-卸载" class="headerlink" title="3.3 opencv 卸载"></a>3.3 opencv 卸载</h3><p>首先，到编译目录build 下执行卸载命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo make uninstall</span><br></pre></td></tr></table></figure>
<p>然后，到opencv 的安装目录下，将bin、lib、share、include 等文件删除即可。若不再安装本版本，则将pkg-config 的相关配置清除掉。</p>
<h2 id="4-ORB-SLAM2-编译与运行"><a href="#4-ORB-SLAM2-编译与运行" class="headerlink" title="4 ORB-SLAM2 编译与运行"></a>4 ORB-SLAM2 编译与运行</h2><h3 id="4-1-前期准备"><a href="#4-1-前期准备" class="headerlink" title="4.1 前期准备"></a>4.1 前期准备</h3><p>在此注明一下本人各个软件包的<strong>最终版本</strong>如下所示：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">opencv: 3.4.11</span><br><span class="line">eigen3: 3.4.0</span><br><span class="line">pabgolin: 0.6</span><br></pre></td></tr></table></figure>
<p>明确完软件包版本后，首先到<a href="https://github.com/raulmur/ORB_SLAM2">ORB-SLAM2仓库</a>下载，进入下载的文件夹后对以下文件进行更改：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 对主目录及DBoW2下的CMakeLists.txt 文件中的opencv版本及搜寻路径进行相应修改</span></span><br><span class="line"><span class="built_in">set</span>(OpenCV_DIR /home/echo/Softwares/opencv/opencv-3.4.11/share/OpenCV)</span><br><span class="line">find_package(OpenCV 3.4.11 QUIET)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 对主目录下的CMakeLists.txt 文件中的eigen3 版本进行修改</span></span><br><span class="line">find_package(Eigen3 3.4.0 REQUIRED)</span><br></pre></td></tr></table></figure>
<h3 id="4-2-编译"><a href="#4-2-编译" class="headerlink" title="4.2 编译"></a>4.2 编译</h3><p>按照官方介绍进行编译：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +x build.sh</span><br><span class="line">./build.sh</span><br></pre></td></tr></table></figure>
<p>正常编译成功的话会在<em>lib</em> 文件夹内生成<strong>libORB_SLAM2.so</strong> 函数库，在<em>Examples</em> 文件夹内生成对应的可执行文件：<strong>mono_tum, mono_kitti, rgbd_tum, stereo_kitti, mono_euroc</strong> 和 <strong>stereo_euroc</strong>。但一般都会有各种报错，这里记录一下本人的报错及相应处理。</p>
<ul>
<li>首先是可能因为软件包版本问题报错：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 报错内容：</span></span><br><span class="line">Pangolin could not be found because dependency Eigen3 could not be found</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 解决方案：</span></span><br><span class="line">根据网上找的资料，可能是Pangolin和Eigen3的版本问题；按照网上教程，卸载了Pangolin和Eigen3，由于要手动选取版本，所以这里Eigen3选择了源文件安装，但后续又出现了其他问题，故又卸载新安装的Eigen3，重新使用apt安装了Eigen3，所以最终情况是：Pangolin降级为v0.6，Eigen3仍是原来的3.4.0。至此，该问题得到解决</span><br></pre></td></tr></table></figure>
<ul>
<li>再次编译可能会出现<strong>无法找到Eigen3 函数库</strong>的报错，此时根据报错内容，将Eigen3 的安装位置：/usr/include/目录下的eigen目录软链接到相应位置即可。</li>
<li>过程中没有其他报错内容，DBoW2、g2o等均编译成功，但最终编译ORB-SLAM2 各个运行程序时会提示：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 输出提示</span></span><br><span class="line">make: *** No targets specified and no makefile found.  Stop.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 在ORB-SLAM2的issues中找到解决方案：</span></span><br><span class="line"><span class="comment">## 删掉build文件，直接执行build.sh中的最后一步</span></span><br><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br></pre></td></tr></table></figure>
<ul>
<li>出现usleep() 未声明的报错：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">## 报错内容：</span><br><span class="line">error: ‘usleep’ was <span class="keyword">not</span> declared in <span class="keyword">this</span> scope</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 解决方案：在system.h文件中加入</span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li>LoopClosing.h中的报错：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">## 报错内容：</span><br><span class="line">/usr/include/c++/<span class="number">9</span>/bits/stl_map.h: In instantiation of ‘<span class="keyword">class</span> <span class="title class_">std</span>::map&lt;ORB_SLAM2::KeyFrame*, g2o::Sim3, std::less&lt;ORB_SLAM2::KeyFrame*&gt;, Eigen::aligned_allocator&lt;std::pair&lt;<span class="type">const</span> ORB_SLAM2::KeyFrame*, g2o::Sim3&gt; &gt; &gt;’:</span><br><span class="line">ORB_SLAM2/src/LoopClosing.cc:<span class="number">438</span>:<span class="number">21</span>: required from here</span><br><span class="line">/usr/include/c++/<span class="number">9</span>/bits/stl_map.h:<span class="number">122</span>:<span class="number">71</span>: error: <span class="type">static</span> assertion failed: std::map must have the same value_type as its allocator</span><br><span class="line"><span class="number">122</span> | <span class="built_in">static_assert</span>(is_same&lt;<span class="keyword">typename</span> _Alloc::value_type, value_type&gt;::value,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 解决方案：</span><br><span class="line">修改LoopClosing.h文件中的<span class="number">49</span>和<span class="number">50</span>行：</span><br><span class="line">修改前：</span><br><span class="line"><span class="keyword">typedef</span> map&lt;KeyFrame*,g2o::Sim3,std::less&lt;KeyFrame*&gt;,</span><br><span class="line">        Eigen::aligned_allocator&lt;std::pair&lt;<span class="type">const</span> KeyFrame*, g2o::Sim3&gt; &gt; &gt; KeyFrameAndPose;</span><br><span class="line">修改后：</span><br><span class="line"><span class="keyword">typedef</span> map&lt;KeyFrame*,g2o::Sim3,std::less&lt;KeyFrame*&gt;,</span><br><span class="line">       Eigen::aligned_allocator&lt;std::pair&lt;KeyFrame *<span class="type">const</span>, g2o::Sim3&gt; &gt; &gt; KeyFrameAndPose;</span><br></pre></td></tr></table></figure>
<p>至此，安装成功，生成了相应的可执行文件。</p>
<h3 id="4-3-运行测试"><a href="#4-3-运行测试" class="headerlink" title="4.3 运行测试"></a>4.3 运行测试</h3><p>本处使用了TUM RGB-D 数据集进行测试，测试命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt Examples/RGB-D/TUM1.yaml ~/datasets/TUM/rgbd_dataset_freiburg1_desk ~/datasets/TUM/rgbd_dataset_freiburg1_desk/associate.txt </span><br></pre></td></tr></table></figure>
<p>运行界面如下图所示：</p>
<p><img src="/2024/01/26/orbslam2-env/orbslam2-success.png" alt="orbslam2" title="ORB-SLAM2成功运行"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>ORB-SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Ubuntu</tag>
        <tag>C++</tag>
        <tag>ORB-SLAM2</tag>
        <tag>OpenCV</tag>
      </tags>
  </entry>
  <entry>
    <title>SSH免密登录设置</title>
    <url>/2024/01/26/ssh-setup/</url>
    <content><![CDATA[<h1 id="1-ssh服务"><a href="#1-ssh服务" class="headerlink" title="1 ssh服务"></a>1 ssh服务</h1><p>Ubuntu开启ssh服务需要下载openssh-server，命令为：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br></pre></td></tr></table></figure>
<h1 id="2-ssh-key生成"><a href="#2-ssh-key生成" class="headerlink" title="2 ssh key生成"></a>2 ssh key生成</h1><p>生成ssh key的命令是：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure>
<p>后续一路默认设置即可，该命令会在主目录默认生成.ssh文件，内包含以下文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">id_rsa: 私钥</span><br><span class="line">id_rsa.pub: 公钥</span><br></pre></td></tr></table></figure>
<h1 id="3-无密码远程登陆服务器"><a href="#3-无密码远程登陆服务器" class="headerlink" title="3 无密码远程登陆服务器"></a>3 无密码远程登陆服务器</h1><p>将本地产生的公钥上传至服务器：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-copy-id -p <span class="comment">#port master@ubuntu</span></span><br></pre></td></tr></table></figure>
<h1 id="4-Windows免密登录Ubuntu"><a href="#4-Windows免密登录Ubuntu" class="headerlink" title="4 Windows免密登录Ubuntu"></a>4 Windows免密登录Ubuntu</h1><p>Windows在终端中执行以下命令生成公私密钥：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>
<p>会在C:\Users\xxx.ssh 文件夹中生成以下三个文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">id_rsa 本地私钥</span><br><span class="line">id_rsa.pub 公钥</span><br><span class="line">known_hosts 已知的ip</span><br></pre></td></tr></table></figure>
<p>将公钥上传至Ubuntu，可使用scp命令，然后执行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将公钥拷贝至Ubuntu的authorized_keys文件中</span></span><br><span class="line"><span class="built_in">cat</span> ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改sshd_config文件</span></span><br><span class="line">sudo vim /etc/ssh/sshd_config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将一下三行命令添加至sshd_config文件</span></span><br><span class="line">RSAAuthentication <span class="built_in">yes</span></span><br><span class="line">PubkeyAuthentication <span class="built_in">yes</span></span><br><span class="line">PasswordAuthentication no</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启ssh</span></span><br><span class="line">sudo service sshd restart</span><br></pre></td></tr></table></figure>
<p>然后即可实现Windows免密登录Ubuntu。</p>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 VSO_Visual Semantic Odometry</title>
    <url>/2024/01/30/vso/</url>
    <content><![CDATA[<p>Lianos, Konstantinos-Nektarios, Johannes L. Schönberger, Marc Pollefeys, and Torsten Sattler. “VSO: Visual Semantic Odometry.” In <em>Computer Vision – ECCV 2018</em>, edited by Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, 11208:246–63. Lecture Notes in Computer Science. Cham: Springer International Publishing, 2018. <a href="https://doi.org/10.1007/978-3-030-01225-0_15">https://doi.org/10.1007/978-3-030-01225-0_15</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者认为，VO 的核心问题在于<strong>数据关联DA</strong>。</p>
<p>通常来讲，有两种方法来减少VO 中的漂移：</p>
<ol>
<li>方法一：利用连续图片之间的<strong>短期关联</strong>来进行偏移矫正；</li>
<li>方法二：利用回环检测实现<strong>长期关联</strong>。</li>
</ol>
<p>传统的几何特征（点、线、面）在光照、视角变化下鲁棒性较差，不能在长距离上保持持续跟踪，而语义特征作为更高级的特征，语义特征在光照、视角、尺寸发生较大变化时仍然可以保持不变，使得<strong>中期关联</strong>成为可能，如Fig. 1所示。</p>
<span id="more"></span>
<p><img src="/2024/01/30/vso/fig1.png" alt="fig1" title="figure 1"></p>
<p>本文的贡献如下：</p>
<ul>
<li>构建一个新的<strong>损失函数</strong>来最小化<strong>语义特征的重投影误差</strong>，并使用<strong>EM 算法</strong>进行最小化，且可以适用于任何语义分割算法；</li>
<li>作者证明了将语义损失考虑进VO 中可以大幅减小平移漂移，且本算法可以<strong>直接集成</strong>到现有的VO 算法中，无论是直接法还是间接法；</li>
<li>作者进行了实验验证，表明了该算法在特定条件下的效果，并讨论了当前的限制。</li>
</ul>
<h1 id="3-Visual-Semantic-Odometry"><a href="#3-Visual-Semantic-Odometry" class="headerlink" title="3 Visual Semantic Odometry"></a>3 Visual Semantic Odometry</h1><p>与文章(Bowman 等, 2017)采用物体的离散DA不同，本文考虑与物体边界的<strong>连续距离</strong>来定义损失函数。</p>
<h2 id="3-1-Visual-Semantic-Odometry-Framework"><a href="#3-1-Visual-Semantic-Odometry-Framework" class="headerlink" title="3.1 Visual Semantic Odometry Framework"></a>3.1 Visual Semantic Odometry Framework</h2><p>传统里程计的目标函数是：</p>
<p><img src="/2024/01/30/vso/f1.png" alt="f1" title="formula 1"></p>
<p>作者定义的语义损失函数是：</p>
<p><img src="/2024/01/30/vso/f2.png" alt="f2" title="formula 2"></p>
<p>将传统的损失函数和语义损失函数结合起来便形成了本文算法的目标函数：</p>
<p><img src="/2024/01/30/vso/f3.png" alt="f3" title="formula 3"></p>
<h2 id="3-2-Semantic-Cost-Function"><a href="#3-2-Semantic-Cost-Function" class="headerlink" title="3.2 Semantic Cost Function"></a>3.2 Semantic Cost Function</h2><p>联系语义观测 $S_k$ ，相机位姿 $T_k$ 和3D 点 $P_i$ （包含标签 $Z_i$ 和位置 $X_i$ ）来定义<strong>观测似然模型</strong>：$p(S_k|T_k, X_i, Z_i=c)$ ，该似然函数应该随着 3D 点 $P_i$ 在图片中的投影位置 $\pi(T_k, X_i)$ 与标注为类别 c 的区域的最近<strong>距离成反比</strong>；基于此，作者利用了distance transform，如Fig. 2所示，作者首先由语义分割图片（a）为每一个分类 c 生成相应的二分值灰度图片（b），然后基于该灰度图片定义一个distance transform：</p>
<p><img src="/2024/01/30/vso/f4.png" alt="DT" title="DT"></p>
<p><img src="/2024/01/30/vso/fig2.png" alt="fig2" title="figure 2"></p>
<p>利用 $DT^{(c)}_k(p)$ 定义观测似然函数：</p>
<p><img src="/2024/01/30/vso/f4-1.png" alt="f4" title="formula 4"></p>
<p>其中，$\pi$ 表示投影函数；$\sigma$ 表示语义图片分类的<strong>不确定性</strong>。为了简明起见，作者省略了归一化因子。根据上式，可通过调整相机位姿与点的位置，使得投影点移动到正确标签的区域内，以此最大化该似然函数。</p>
<p>根据式（4）作者定义了语义损失项：</p>
<p><img src="/2024/01/30/vso/f5.png" alt="f5" title="formula 5"></p>
<p>其中，$w_i^{(c)}$ 表示点 $P_i$ 属于种类 c 的概率。从上式可以看出，$e_{sem}(k,i)$ 表示<strong>2D 距离的加权平均</strong>，每个投影点 $\pi(T_k, X_i)$ 到最近属于种类 c 的距离 $DT^{(c)}_k(\pi(T_k, X_i))$ 被相应的类别概率 $w_i^{(c)}$ 加权。</p>
<p>值得注意的是，对于点 $P_i$ 的标签概率向量 $w_i$ 是通过所有对到该点的观测计算得到的：</p>
<p><img src="/2024/01/30/vso/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$\mathcal{T}_i$ 表示观测到点 $P_i$ 的相机位姿集合；参数 $\alpha$ 为归一化参数。利用该准则可以通过累积语义观测来实现对标签概率向量 $w_i$ 的<strong>增量式细调</strong>。</p>
<h2 id="3-3-Optimization"><a href="#3-3-Optimization" class="headerlink" title="3.3 Optimization"></a>3.3 Optimization</h2><p>式（5）、（6）表明优化参数涉及3D 点位置坐标，相机位姿以及种类关联参数，作者使用EM 算法进行求解，步骤如下：</p>
<ul>
<li>E step：基于式（6）计算每个点的权重向量 $w_i$ ，此步骤点坐标和相机位姿保持固定；</li>
<li>M step：优化点坐标和相机位姿，此步骤权重参数保持固定。</li>
</ul>
<p>该优化框架中，将点 $P_i$ 的标签 $Z_i$ 视为<strong>隐变量</strong>。</p>
<p>作者提到，使用本文提出的语义约束，可以实现不变观测，但是<strong>缺乏结构信息</strong>；仅使用语义项对地图点和相机位姿进行优化会导致<strong>欠约束</strong>，因为等式（4）表示的似然函数在物体边界内部服从<strong>均匀分布</strong>，为解决该问题，对于 $E_{sem}$ 的优化操作如下：</p>
<ol>
<li>如式（3）所示，与传统的VO 进行<strong>联合优化</strong>；</li>
<li>利用<strong>多重点和语义约束</strong>来优化相机位姿；</li>
<li>仅提供语义约束的点得到固定，然后只优化与它们相关的相机位姿来减小漂移，该方法不仅可以限制优化参数的数量，而且会在点之间引入<strong>结构相关</strong>，从而约束位姿求解（如Fig. 3所示）；</li>
<li><strong>频繁</strong>使用语义优化可以减少一个点错误关联的概率，因为基于distance transform 梯度的优化可以保证该点足够靠近正确标签的区域，并不断拉向该区域。</li>
</ol>
<p><img src="/2024/01/30/vso/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="3-4-Obtaining-Semantic-Constraints-amp-System-Integration"><a href="#3-4-Obtaining-Semantic-Constraints-amp-System-Integration" class="headerlink" title="3.4 Obtaining Semantic Constraints &amp; System Integration"></a>3.4 Obtaining Semantic Constraints &amp; System Integration</h2><p>类似于传统VO 使用的关键帧active window (AW)，作者也定义了一个关键帧active semantic window (ASW)，一个关键帧从AW 中剔除后会被添加进ASW，本算法在尽可能覆盖更多的轨迹的同时，也会限制ASW 中关键帧的数量。且，ASW 中关键帧的位姿<strong>不再进行优化</strong>，因为这些关键帧通常与目前的帧缺少光度/几何约束。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Pose Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows远程桌面控制ubuntu</title>
    <url>/2024/01/26/win-remote-ubuntu/</url>
    <content><![CDATA[<h2 id="1-Ubuntu设置与软件安装"><a href="#1-Ubuntu设置与软件安装" class="headerlink" title="1 Ubuntu设置与软件安装"></a>1 Ubuntu设置与软件安装</h2><p>首先打开Ubuntu设置，将Sharing-Remote Desktop打开，如下图所示：</p>
<span id="more"></span>
<p><img src="/2024/01/26/win-remote-ubuntu/ubuntu-setting.png" alt="ubuntu-setting" title="Ubuntu远程桌面设置"></p>
<p>然后安装xrdp：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install xrdp</span><br></pre></td></tr></table></figure>
<p>接下来的步骤网上有很多不同的做法，其中<a href="https://zhuanlan.zhihu.com/p/145614559">这篇文章</a>提到需要修改startwm.sh文件:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo vi /etc/xrdp/startwm.sh</span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/win-remote-ubuntu/startwm.png" alt="startwm.sh" title="修改startwm.sh文件"></p>
<p>将最后两行注释掉，即：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 注释掉这两行</span></span><br><span class="line"><span class="comment"># test -x /etc/X11/Xsession &amp;&amp; exec /etc/X11/Xsession</span></span><br><span class="line"><span class="comment"># exec /bin/sh /etc/X11/Xsession</span></span><br></pre></td></tr></table></figure>
<p>然后即可通过win10自带的远程桌面（或在Microsoft Store安装的远程桌面）进行连接：</p>
<p><img src="/2024/01/26/win-remote-ubuntu/remote-access.png" alt="remote" title="win10远程桌面"></p>
<p><img src="/2024/01/26/win-remote-ubuntu/image-20240129103319493.png" alt="远程桌面" title="远程桌面"></p>
<p>到目前为止，本人可以在Windows中正常远程控制Ubuntu，且可实现Ubuntu显示和远程控制<strong>同时在线</strong>，即通过远程控制操作Ubuntu，同时另一个屏幕可正常显示Ubuntu界面，且实时显示远程控制的操作。</p>
<h2 id="2-黑屏问题"><a href="#2-黑屏问题" class="headerlink" title="2 黑屏问题"></a>2 黑屏问题</h2><p>但是，在这之后我重新安装了NVIDIA显卡驱动、CUDA、cuDNN，然后就发现远程控制无法进入Ubuntu系统了，点击“连接”后出现一段时间的黑屏后会自动退出。在网上查询了很多案例，有各种五花八门的解决方案，如换一个桌面程序、使用dconf-editor更改配置文件、远程控制与Ubuntu本地无法同时登陆（必须有一方log out）等等，以下是个人尝试的结果：</p>
<ul>
<li>换桌面程序：由于本人安装的是桌面版Ubuntu，故系统自带桌面程序gnome，之前配置远程控制时就受到网上各种五花八门意见的影响，使用了xfce4桌面程序，后来发现完全没必要，而且就在不久前还可以正常远程连接，说明应该不是桌面程序的原因，因此就没有尝试更换桌面程序；</li>
<li>使用dconf-editor更改配置文件：在<a href="https://zhuanlan.zhihu.com/p/345738274">文章</a>中提到，进入dconf-editor后，依次进入“org-&gt;gnome-&gt;desktop-&gt;remote-access”，将 requre-encryption 设为 False，这也是本人在上个系统（Ubuntu20.04）中的操作，但是在本系统（Ubuntu22.04）中没有发现有“remote-access”，故该方法也没有成功；</li>
<li>本地端与远程端无法共存：这个比较简单，本人试着将Ubuntu本地端账户log out，或者使用不同账户进行远程控制，发现仍然失败；但<strong>需要注意的是</strong>，在本人解决远程黑屏问题之后，发现该问题确实是存在的，若本地端或远程端没有log out，那么另一端就无法正常登陆显示界面，这边记录一下；</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 修改startwm.sh文件，添加下面几行，仍旧没有解决问题</span></span><br><span class="line"><span class="built_in">unset</span> DBUS_SESSION_BUS_ADDRESS</span><br><span class="line"><span class="built_in">unset</span> XDG_RUNTIME_DIR</span><br><span class="line">. <span class="variable">$HOME</span>/.profile</span><br></pre></td></tr></table></figure>
<p>最后，不知道怎么想起来，尝试把startwm.sh文件中注释掉的两行取消注释，峰回路转、柳暗花明，竟然成了！！！于是，本人最终的startwm.sh文件内容如下所示：</p>
<p><img src="/2024/01/26/win-remote-ubuntu/startwm-1.png" alt="startwm" title="startwm.sh文件最终版"></p>
<h2 id="3-后记"><a href="#3-后记" class="headerlink" title="3 后记"></a>3 后记</h2><p>虽然不知道原因是什么，只知道目前这样子是可行的，但同时也发现无法像安装显卡驱动前那样本地端和远程端同时登陆了，必须要有一端log out，另一端才可正常工作，现做以下记录：</p>
<ul>
<li>使用远程连接桌面后，不能简单地关掉远程桌面：若只是简单地关掉远程桌面后，在本地端可以正常进入账号选择、密码输入界面，输完密码后会显示黑屏；此时，只有重新进入远程桌面，选择log out，本地端方可正常登录。</li>
<li>对于本地端Todesk等远程控制软件：经过本人测试，Todesk 软件若想正常工作需要本地端正常登录，即要求远程端log out，然后本地端log in；那么也就意味着本地端电脑需要连接显示器。</li>
</ul>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>remote access</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 VINS-Mono_A Robust and Versatile Monocular Visual-Inertial State Estimator</title>
    <url>/2024/01/31/vins/</url>
    <content><![CDATA[<p>Qin, Tong, Peiliang Li, and Shaojie Shen. “VINS-Mono: A Robust and Versatile Monocular Visual-Inertial State Estimator.” <em>IEEE Transactions on Robotics</em> 34, no. 4 (August 2018): 1004–20. <a href="https://doi.org/10.1109/TRO.2018.2853729">https://doi.org/10.1109/TRO.2018.2853729</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>集成IMU 观测可通过减少由于光照变化、纹理稀少区域或运动模糊造成的视觉跟踪精度损失，来大幅提高运动跟踪的表现。但是，单目VINS (Visual-Inertial System) 在使用中也有一些问题需要解决：</p>
<ol>
<li>第一个问题是初始化困难：由于缺失直接的距离观测，很难直接将单目视觉结构与惯性测量进行融合；</li>
<li>其次是VINS 严重的非线性问题：这会在估计器初始化过程中带来巨大的挑战，再大部分场景中系统需要放置在一个位置已知的静态区域，然后缓慢小心地移动，这会极大限制系统的应用场景；</li>
<li>另一个问题是VIO 的长期漂移问题：为了消除累积漂移，会使用回环检测、重定位及全局优化技术；</li>
<li>此外还有对于地图保存与重使用的需求正在不断增长。</li>
</ol>
<span id="more"></span>
<p>为解决上述问题，作者提出了VINS-Mono 系统——一个鲁棒且多功能的单目视觉-惯性状态估计器，该系统包含以下特点：</p>
<ol>
<li><strong>鲁棒的初始化程序</strong>使得系统可以在未知状态进行启动；</li>
<li>紧耦合、基于优化的单目VIO 带有相机-IMU <strong>外参标定</strong>和IMU <strong>偏差校正</strong>；</li>
<li><strong>在线重定位</strong>以及4自由度的<strong>全局位姿图优化</strong>；</li>
<li>位姿图重用可<strong>保存、载入</strong>并<strong>融合</strong>多个局部位姿图。</li>
</ol>
<p>该系统已经成功应用于小规模的AR 场景、中规模的无人机导航，以及大规模的状态估计任务，如Fig. 1所示：</p>
<p><img src="/2024/01/31/vins/image-20240131090730704.png" alt="image-20240131090730704" title="figure 1"></p>
<h1 id="3-Overview"><a href="#3-Overview" class="headerlink" title="3 Overview"></a>3 Overview</h1><p>本文提出的单目视觉-惯性状态估计器的架构如Fig. 2所示，</p>
<p><img src="/2024/01/31/vins/image-20240131094458729.png" alt="image-20240131094458729" title="figure 2"></p>
<p>相较于适用于双目相机的SOTA 算法OKVIS，本算法是专为单目相机设计的，作者针对性设计了初始化程序、关键帧选取标准，并使用具有大视角的相机进行更好地跟踪；此外，本算法作为一个完整的系统还包含了回环检测以及位姿图重用模块。</p>
<p>本文的标注规则：</p>
<ul>
<li>$(.)^w$ 表示世界坐标系，重力方向与世界坐标系z 轴对齐；</li>
<li>$(.)^b$ 表示物体坐标系，与IMU 坐标系相同；</li>
<li>$(.)^c$ 表示相机坐标系；</li>
<li>使用旋转矩阵 $\mathbf{R}$ 和四元数 $\mathbf{q}$ 来表示旋转；</li>
<li>$\mathbf{q}^w_b$ 和 $\mathbf{p}^w_b$ 分别表示从物体坐标系到世界坐标系的旋转和平移；</li>
<li>$b_k$ 表示第<em>k</em> 张图片时刻的物体坐标系，$c_k$ 表示相机坐标系；</li>
<li>$\mathbf{g}^w = [0, 0, g]^T$ 是世界坐标系下的重力向量；</li>
<li>使用 $\hat{(.)}$ 表示某个参数的噪声观测或估计值。</li>
</ul>
<h1 id="4-Measurement-Preprocessing"><a href="#4-Measurement-Preprocessing" class="headerlink" title="4 Measurement Preprocessing"></a>4 Measurement Preprocessing</h1><p>观测预处理步骤：</p>
<ul>
<li>对于视觉观测：跟踪连续帧之间的特征，并在最新帧中检测新的特征；</li>
<li>对于IMU 观测：对连续帧之间的IMU 进行预积分。</li>
</ul>
<h2 id="4-1-Vision-Processing-Front-End"><a href="#4-1-Vision-Processing-Front-End" class="headerlink" title="4.1 Vision Processing Front End"></a>4.1 Vision Processing Front End</h2><p>对每张新图片，使用KLT 稀疏光流法来跟踪已有的特征；此外，检测新的角点特征来维持图片的最小特征数量（100-300）。探测器通过设定一个特征间最小像素间隔来执行特征均匀提取，2D 特征首先经过去畸变、外点剔除，然后投影至一个单位球上，外点剔除是通过RANSAC 方法实现的。</p>
<p>关键帧选取标准有两个：</p>
<ol>
<li><strong>较前一个关键帧的平均视差</strong>：若当前帧与上一个关键帧之间的平均视差超过一定阈值，则将当前帧设为关键帧；值得注意的是，平移与旋转均可以造成视差，但是纯旋转运动无法对特征进行三角化，为解决该问题，作者在计算视差时使用短期的<strong>陀螺仪观测来补偿旋转</strong>，需要说明的是，旋转补偿只用于关键帧选取，并不用于VINS 的旋转计算。</li>
<li><strong>跟踪质量</strong>：如果当前帧跟踪到的特征数量低于一个阈值，则将当前帧设为关键帧。</li>
</ol>
<h2 id="4-2-IMU-Preintegration"><a href="#4-2-IMU-Preintegration" class="headerlink" title="4.2 IMU Preintegration"></a>4.2 IMU Preintegration</h2><p>作者使用之前工作中用的连续时间基于四元数的IMU 预积分推导方法。</p>
<h3 id="4-2-1-IMU-Noise-and-Bias"><a href="#4-2-1-IMU-Noise-and-Bias" class="headerlink" title="4.2.1 IMU Noise and Bias"></a>4.2.1 IMU Noise and Bias</h3><p>IMU 的观测信息是在物体坐标系下的，结合了 the force for countering gravity and the platform dynamics，观测还包含加速度偏差 $\mathbf{b}_a$ 、陀螺仪偏差 $\mathbf{b}_w$ 以及额外噪声，陀螺仪和加速度计的原始观测量 $\hat{\mathcal{w}}, \hat{\mathcal{a}}$ 如下所示：</p>
<p><img src="/2024/01/31/vins/image-20240131103735319.png" alt="image-20240131103735319" title="formula 1"></p>
<p>作者假设加速度计和陀螺仪的观测噪声服从高斯分布：$\mathbf{n}_a\sim\mathcal{N}(0, \sigma_a^2), \mathbf{n}_w\sim\mathcal{N}(0, \sigma_w^2)$ ；偏差 $\mathbf{b}_a$ 、 $\mathbf{b}_w$ 被建模为随机游走，其对应导数为高斯白噪声，$\mathbf{n}_{b_a}\sim\mathcal{N}(0, \sigma_{b_a}^2), \mathbf{n}_{b_w}\sim\mathcal{N}(0, \sigma_{b_w}^2)$ 。</p>
<p><img src="/2024/01/31/vins/image-20240131104508327.png" alt="image-20240131104508327" title="formula 2"></p>
<h3 id="4-2-2-Preintegration"><a href="#4-2-2-Preintegration" class="headerlink" title="4.2.2 Preintegration"></a>4.2.2 Preintegration</h3><p>连续两帧图片 $b_k, b_{k+1}$ 之间存在多组惯性观测数据，给定偏差估计，在局部坐标系 $b_k$ 中进行积分：</p>
<p><img src="/2024/01/31/vins/image-20240131104847991.png" alt="image-20240131104847991" title="formula 3"></p>
<p>其中：</p>
<p><img src="/2024/01/31/vins/image-20240131105001484.png" alt="image-20240131105001484" title="formula 4"></p>
<p>$\alpha, \beta, \gamma$ 的协方差 $\mathbf{P}_{b_{k+1}}^{b_k}$ 也进行相应传递；可以看出预积分项（式3）可通过给定 $b_k$ 作为偏差的参考坐标系，仅用IMU 观测即可获取。</p>
<h3 id="4-2-3-Bias-Correction"><a href="#4-2-3-Bias-Correction" class="headerlink" title="4.2.3 Bias Correction"></a>4.2.3 Bias Correction</h3><p>如果偏差估计改变量较小，利用一阶导数近似进行调整：</p>
<p><img src="/2024/01/31/vins/image-20240131105701602.png" alt="image-20240131105701602" title="formula 5"></p>
<p>如果偏差估计改变量较大，则在新的偏差估计下进行传递。该策略可为基于优化的算法节省大量计算资源，因为不需要重复进行IMU 观测传递。</p>
<h1 id="5-Estimator-Initialization"><a href="#5-Estimator-Initialization" class="headerlink" title="5 Estimator Initialization"></a>5 Estimator Initialization</h1><p>单目相机紧耦合的VIO 是一个高非线性系统，需要在开始时刻进行准确的初始化估计。作者通过将IMU 预积分和视觉观测进行松对齐来获取必要的初始估计值。</p>
<h2 id="5-1-Vision-Only-SfM-in-Sliding-Window"><a href="#5-1-Vision-Only-SfM-in-Sliding-Window" class="headerlink" title="5.1 Vision-Only SfM in Sliding Window"></a>5.1 Vision-Only SfM in Sliding Window</h2><p>作者控制滑动窗口内的图像帧数来限制计算复杂度，SfM 过程采取以下步骤：</p>
<ol>
<li>首先，确定当前帧与窗口内所有历史帧之间的特征关联，如果可以找到稳定的特征跟踪（超过30个跟踪特征）以及充分的视差（超过20像素），则可利用five-point 算法获取两帧之间的相对旋转和位移；</li>
<li>然后，随意设定尺度参数，对两帧中的所有跟踪特征进行三角化，基于三角化特征，使用PnP 算法来估计窗口内其他所有帧的位姿；</li>
<li>最终，利用全局BA 来最小化所有观测特征的重投影误差。</li>
</ol>
<p>由于尚未确定世界坐标系，所以将第一帧图片的位姿 $(.)^{c_0}$ 作为SfM 的参考坐标系，所有图片代表的相机位姿 $(\overline{\mathbf{p}}_{c_k}^{c_0}, \mathbf{q}_{c_k}^{c_0})$ 和特征位置都表示为 $(.)^{c_0}$ 的相对量，给定相机和IMU 之间的外参 $(\mathbf{p}_c^b, \mathbf{q}_c^b)$ ，可将相机位姿从相机坐标系转换至物体坐标系：</p>
<p><img src="/2024/01/31/vins/image-20240131111947126.png" alt="image-20240131111947126" title="formula 6"></p>
<p>其中，<em>s</em> 是未知的尺度参数。</p>
<h2 id="5-2-Visual-Inertial-Alignment"><a href="#5-2-Visual-Inertial-Alignment" class="headerlink" title="5.2 Visual-Inertial Alignment"></a>5.2 Visual-Inertial Alignment</h2><p>视觉-惯性的对齐过程如Fig. 3所示，基本想法是将视觉SfM 和IMU 预积分匹配起来。</p>
<p><img src="/2024/01/31/vins/image-20240131112327470.png" alt="image-20240131112327470" title="figure 3"></p>
<h3 id="5-2-1-Gyroscope-Bias-Calibration"><a href="#5-2-1-Gyroscope-Bias-Calibration" class="headerlink" title="5.2.1 Gyroscope Bias Calibration"></a>5.2.1 Gyroscope Bias Calibration</h3><p>假设从SfM 获取窗口内连续两帧 $b_k, b_{k+1}$ 的<strong>旋转参数</strong> $\mathbf{q}_{b_k}^{c_0}, \mathbf{q}_{b_k+1}^{c_0}$ ，以及从IMU 预积分中获取两者之间的<strong>相对约束</strong> $\hat{\gamma}^{b_k}_{b_{k+1}}$ ，作者将IMU 预积分项做关于陀螺仪偏差的<strong>线性化</strong>，并最小化下面的<strong>损失方程</strong>：</p>
<p><img src="/2024/01/31/vins/image-20240131151351640.png" alt="image-20240131151351640" title="formula 7"></p>
<p>其中，$\mathcal{B}$ 表示窗口内的所有图像帧。这样可以得到陀螺仪偏差 $\mathbf{b}_w$ 的初始标定值，然后使用新的陀螺仪偏差来传递所有的IMU 预积分项 $\hat{\alpha}^{b_k}_{b_{k+1}}, \hat{\beta}^{b_k}_{b_{k+1}}, \hat{\gamma}^{b_k}_{b_{k+1}}$ 。</p>
<h3 id="5-2-2-Velocity-Gravity-Vector-and-Metric-Scale-Initialization"><a href="#5-2-2-Velocity-Gravity-Vector-and-Metric-Scale-Initialization" class="headerlink" title="5.2.2 Velocity, Gravity Vector, and Metric Scale Initialization"></a>5.2.2 Velocity, Gravity Vector, and Metric Scale Initialization</h3><p>在对陀螺仪偏差进行初始化之后，继续对其他导航状态参数进行初始化，包括<strong>速度</strong>、<strong>重力向量</strong>以及<strong>尺度参数</strong>。</p>
<p><img src="/2024/01/31/vins/image-20240131152147563.png" alt="image-20240131152147563" title="formula 8"></p>
<p>其中，$\mathbf{v}_{b_k}^{b_k}$ 为第<em>k</em> 张图片时在物体坐标系中的速度；$\mathbf{g}^{c_0}$ 表示 $c_0$ 帧的重力向量；<em>s</em> 代表单目SfM 的尺度单位。</p>
<p>对于窗口内的连续两帧 $b_k, b_{k+1}$ ，有以下等式表示：</p>
<p><img src="/2024/01/31/vins/image-20240131152817355.png" alt="image-20240131152817355" title="formula 9"></p>
<p>结合式6和式9，得到以下的线性观测模型：</p>
<p><img src="/2024/01/31/vins/image-20240131152915603.png" alt="image-20240131152915603" title="formula 10-11"></p>
<p>其中，$\mathbf{R}_{b_k}^{c_0}, \mathbf{R}_{b_{k+1}}^{c_0}, \overline{\mathbf{p}}_{c_k}^{c_0}, \overline{\mathbf{p}}_{c_{k+1}}^{c_0}$ 可从单目视觉SfM 获取；$\Delta t_k$ 表示连续两帧的时间间隔。通过求解下面的最小二乘问题可得到每一帧图片代表的物体坐标系下的<strong>速度参数</strong>、视觉参考坐标系下的<strong>重力向量</strong>，以及<strong>尺度参数</strong>：</p>
<p><img src="/2024/01/31/vins/image-20240131153711975.png" alt="image-20240131153711975" title="formula 12"></p>
<h3 id="5-2-3-Gravity-Refinement"><a href="#5-2-3-Gravity-Refinement" class="headerlink" title="5.2.3 Gravity Refinement"></a>5.2.3 Gravity Refinement</h3><p>从上述线性初始化中获取的重力向量可通过<strong>约束数值大小</strong>进行细调，大部分情况下重力向量的大小是已知的，这就导致该重力向量是2自由度的，因此，作者使用正切空间中的两个向量对重力进行扰动：$g(\hat{\overline{\mathbf{g}}} + \delta \mathbf{g}, \delta \mathbf{g} = w_1\mathbf{b}_1 + w_2\mathbf{b}_2)$，其中，<em>g</em> 为重力的已知大小，$\hat{\overline{\mathbf{g}}}$ 表示重力方向的单位向量，$\mathbf{b}_1, \mathbf{b}_2$ 为正切平面上的两个正交偏差，如Fig. 4所示，$w_1, w_2$ 为两个方向上的扰动值。</p>
<p><img src="/2024/01/31/vins/image-20240131154525405.png" alt="image-20240131154525405" title="figure 4"></p>
<p>使用 $g(\hat{\overline{\mathbf{g}}} + \delta \mathbf{g})$ 替代式9中的 $\mathbf{g}$ ，与其他参数共同求解2自由度的 $\delta \mathbf{g}$ 。</p>
<h3 id="5-2-4-Completing-Initialization"><a href="#5-2-4-Completing-Initialization" class="headerlink" title="5.2.4 Completing Initialization"></a>5.2.4 Completing Initialization</h3><p>在细调重力向量之后，通过将重力向量对齐世界坐标系的<em>z</em> 轴，来获取世界坐标系和相机坐标系 $c_0$ 之间的旋转参数 $\mathbf{q}_{c_0}^w$ ，然后即可将获取到的所有参数从相机坐标系转换至世界坐标系。至此，系统初始化完成。</p>
<h1 id="6-Tightly-Coupled-Monocular-VIO"><a href="#6-Tightly-Coupled-Monocular-VIO" class="headerlink" title="6 Tightly Coupled Monocular VIO"></a>6 Tightly Coupled Monocular VIO</h1><p>基于滑动窗口的紧耦合单目VIO 进行高精度与鲁棒状态估计，过程如Fig. 5所示：</p>
<p><img src="/2024/01/31/vins/image-20240131160055792.png" alt="image-20240131160055792" title="figure 5"></p>
<h2 id="6-1-Formulation"><a href="#6-1-Formulation" class="headerlink" title="6.1 Formulation"></a>6.1 Formulation</h2><p>滑动窗口内的状态向量定义为：</p>
<p><img src="/2024/01/31/vins/image-20240131160353726.png" alt="image-20240131160353726" title="formula 13"></p>
<p>其中，$\mathbf{x}_k$ 表示第<em>k</em> 张图片代表的IMU 状态，包含世界坐标系下的位置、速度及朝向，以及IMU 物体坐标系下的加速度偏差和陀螺仪偏差；<em>n</em> 表示关键帧的总数；<em>m</em> 表示窗口内的特征总数；$\lambda_l$ 表示第 <em>l</em> 个特征在首次观测的逆距离。</p>
<p>作者使用视觉-惯性BA 方程，最小化先验和所有观测残差M 范数的总和来获取最大后验估计：</p>
<p><img src="/2024/01/31/vins/image-20240131161246881.png" alt="image-20240131161246881" title="formula 14"></p>
<p>其中，$\mathbf{r}_{\mathcal{B}}, \mathbf{r}_{\mathcal{C}}$ 分别表示IMU 和视觉观测的残差；$\mathcal{B}$ 表示所有的IMU 观测；$\mathcal{C}$ 表示当前滑动窗口内至少被观测2次的特征集合；$\{\mathbf{r}_p, \mathbf{H}_p\}$ 表示边缘化先验信息。本系统使用Ceres 求解非线性问题。</p>
<h2 id="6-2-IMU-Measurement-Residual"><a href="#6-2-IMU-Measurement-Residual" class="headerlink" title="6.2 IMU Measurement Residual"></a>6.2 IMU Measurement Residual</h2><p>考虑在窗口内连续两帧间 $b_k, b_{k+1}$ 的IMU 观测，其预积分残差可被定义为：</p>
<p><img src="/2024/01/31/vins/image-20240131162230978.png" alt="image-20240131162230978" title="formula 16"></p>
<p>其中，$[.]_{xyz}$ 表示提取误差状态表示四元数的向量部分；$\delta \theta ^{b_k}_{b_{k+1}}$ 表示四元数的3D 误差状态表示； $[\hat{\alpha}^{b_k}_{b_{k+1}}, \hat{\beta}^{b_k}_{b_{k+1}}, \hat{\gamma}^{b_k}_{b_{k+1}}]$ 表示IMU 预积分观测项。</p>
<h2 id="6-3-Visual-Measurement-Residual"><a href="#6-3-Visual-Measurement-Residual" class="headerlink" title="6.3 Visual Measurement Residual"></a>6.3 Visual Measurement Residual</h2><p>相较于传统针孔相机模型将重投影误差定义在一个图像平面上的做法，本文作者将<strong>相机观测残差定义在一个单位球上</strong>。几乎所有类型相机的光学特性（包括广角相机、鱼眼相机、全向相机等）都可以建模为连接单位球表面的单位射线。假设第 $l$ 个特征是在第 $i$ 帧图片被首次观测到，则该特征在第 $j$ 帧图片上的观测残差定义为：</p>
<p><img src="/2024/01/31/vins/image-20240131163915327.png" alt="image-20240131163915327" title="formula 17"></p>
<p>其中，$[\hat{u}^{c_i}_l, \hat{v}^{c_i}_l]$ 表示第 $l$ 个特征在第 $i$ 帧图片被首次观测到的像素坐标；$[\hat{u}^{c_j}_l, \hat{v}^{c_j}_l]$ 表示该特征在图片 $j$ 中的像素坐标；由于视觉残差是2自由度的，所以作者将残差向量投影至正切平面，$\mathbf{b}_1, \mathbf{b}_2$ 表示 $\hat{\overline{\mathcal{P}}}^{c_j}_l$ 正切平面上的两个正交向量，如Fig. 6所示；式14中的协方差 $\mathbf{P}_l^{c_j}$ 也从像素坐标系传递至单位球上。</p>
<p><img src="/2024/01/31/vins/image-20240131170737502.png" alt="image-20240131170737502" title="figure 6"></p>
<h2 id="6-4-Marginalization"><a href="#6-4-Marginalization" class="headerlink" title="6.4 Marginalization"></a>6.4 Marginalization</h2><p>为了限制计算复杂度，作者使用边缘化策略，有选择性地从滑动窗口中边缘化掉IMU 状态 $\mathbf{x}_k$ 和特征 $\lambda_l$ ，同时将对应观测的边缘状态量转换为先验信息。边缘化过程如Fig. 7所示，作者选择不边缘化掉非关键帧的所有观测是为了保持系统的稀疏性；本系统的边缘化策略是为了保持窗口内关键帧的分隔状态，以保证特征三角化所需的充分视差，以及大激励加速度计观测的概率。边缘化是通过Schur complement 实现的。</p>
<p><img src="/2024/01/31/vins/image-20240131172037799.png" alt="image-20240131172037799" title="figure 7"></p>
<h2 id="6-5-Motion-Only-Visual-Inertial-Optimization-for-Camera-Rate-State-Estimation"><a href="#6-5-Motion-Only-Visual-Inertial-Optimization-for-Camera-Rate-State-Estimation" class="headerlink" title="6.5 Motion-Only Visual-Inertial Optimization for Camera-Rate State Estimation"></a>6.5 Motion-Only Visual-Inertial Optimization for Camera-Rate State Estimation</h2><p>由于非线性优化对算力的要求，低算力设备（如手机等）上的紧耦合单目VIO 无法实现相机采集率级别的输出，因此，除了full optimization 外，作者还实现了一个轻量级motion-only visual-inertial 优化来提高状态估计的速率至30Hz。</p>
<p>该轻量级优化方法的损失函数与单目VIO 相同（式14），但不会优化窗口内的所有参数，而是只优化几个的最新IMU 状态的位姿与速度；在优化中会使用所有的视觉和惯性观测信息，这会实现较单帧 PnP 方法更为顺滑的状态估计解，该方法的介绍如Fig. 8所示。</p>
<p><img src="/2024/01/31/vins/image-20240131174250572.png" alt="image-20240131174250572" title="figure 8"></p>
<p>两种优化方法的速度对比：在最新的嵌入式电脑中，full optimization 的处理时间约为50ms，而该轻量级优化的处理时间约为5ms。</p>
<h2 id="6-6-IMU-Forward-Propagation-for-IMU-Rate-State-Estimation"><a href="#6-6-IMU-Forward-Propagation-for-IMU-Rate-State-Estimation" class="headerlink" title="6.6 IMU Forward Propagation for IMU-Rate State Estimation"></a>6.6 IMU Forward Propagation for IMU-Rate State Estimation</h2><p>本文提出的VIO 输出频率被限制到相机采集速率上，但是仍然可以使用附近的IMU 观测信息来直接传递最新的VIO 估计，以实现IMU 采集速率级别的输出表现。高速率状态估计可被用于回环的状态反馈，作者使用该方法测试了无人机飞行试验。</p>
<h1 id="7-Relocalization"><a href="#7-Relocalization" class="headerlink" title="7 Relocalization"></a>7 Relocalization</h1><p>本系统采用了滑动窗口和边缘化策略来限制计算复杂度，但同时也引入了<strong>累积漂移</strong>；为了消除累积漂移，作者提出了一种可与单目VIO 无缝集成的<strong>紧耦合重定位模块</strong>。重定位过程首先使用<strong>回环检测模块</strong>来判断某场景是否出现过，然后建立回环候选帧与当前帧之间的<strong>特征级关联</strong>，这些特征关联<strong>紧耦合至单目VIO 模块</strong>，在使用最小计算的情况下实现<strong>无漂移状态估计</strong>。对多个特征的多次观测可被直接用于重定位，以实现更高的定位精度与顺滑度。重定位过程如Fig. 9 (a)所示。</p>
<p><img src="/2024/01/31/vins/image-20240131193322044.png" alt="image-20240131193322044" title="figure 9"></p>
<h2 id="7-1-Loop-Detection"><a href="#7-1-Loop-Detection" class="headerlink" title="7.1 Loop Detection"></a>7.1 Loop Detection</h2><p>作者使用<strong>DBoW2</strong> 进行回环检测，除了用于单目VIO 的角点特征外，还使用额外500个BRIEF 描述子代表的角点特征，以实现更高的回环检测召回率。经过时间与几何维度的一致性检验后，DBoW2 输出若干个回环检测候选帧。本系统保留所有的BRIEF 描述子以进行特征恢复，但为了节约内存消耗会舍弃所有的原始图像数据。</p>
<h2 id="7-2-Feature-Retrieval"><a href="#7-2-Feature-Retrieval" class="headerlink" title="7.2 Feature Retrieval"></a>7.2 Feature Retrieval</h2><p>当检测到回环后，局部窗口与回环候选帧之间会通过恢复特征关联（BRIEF 描述子匹配）建立联系，作者使用两步的几何外点方法来剔除错误匹配，如Fig. 10所示：</p>
<p><img src="/2024/01/31/vins/image-20240131194802806.png" alt="image-20240131194802806" title="figure 10"></p>
<ul>
<li>2D-2D：利用RANSAC 进行基础矩阵测试；</li>
<li>3D-2D：利用RANSAC 进行PnP 测试，基于窗口内特征点的已知3D 位置及回环候选帧中的2D 观测进行PnP 测试。</li>
</ul>
<h2 id="7-3-Tightly-Coupled-Relocalization"><a href="#7-3-Tightly-Coupled-Relocalization" class="headerlink" title="7.3 Tightly Coupled Relocalization"></a>7.3 Tightly Coupled Relocalization</h2><p>在重定位中，本系统将所有回环帧的位姿设为常量（不进行优化），使用所有的IMU 观测、局部视觉观测，以及恢复的特征关联对窗口进行联合优化。其中，视觉观测模型与式17相同，除了回环帧的位姿 $(\hat{\mathbf{q}}_v^w, \hat{\mathbf{p}}_v^w)$ 被设为常量，该常量来自位姿图，或直接取自里程计输出（若是第一次重定位）。基于此，将式14更改为如下所示：</p>
<p><img src="/2024/01/31/vins/image-20240131200032028.png" alt="image-20240131200032028" title="formula 18"></p>
<p>其中，$\mathcal{L}$ 表示从回环帧中恢复的特征观测，$(l, v)$ 表示在回环帧 $v$ 中观测到的第 $l$ 个特征。</p>
<h1 id="8-Global-Pose-Graph-Optimization-and-Map-Reuse"><a href="#8-Global-Pose-Graph-Optimization-and-Map-Reuse" class="headerlink" title="8 Global Pose Graph Optimization and Map Reuse"></a>8 Global Pose Graph Optimization and Map Reuse</h1><p>在重定位之后，使用额外的位姿图优化来确保历史位姿保持全局一致。</p>
<h2 id="8-1-Four-Accumulated-Drift-Direction"><a href="#8-1-Four-Accumulated-Drift-Direction" class="headerlink" title="8.1 Four Accumulated Drift Direction"></a>8.1 Four Accumulated Drift Direction</h2><p>受益于对重力的惯性测量，roll 与 pitch 角度可在VINS 中得到完整观测，如Fig. 11所示，随着物体运动，物体的 3D 位置和旋转会在参考坐标系下发生相对变化，可通过重力向量判断水平面，从而实现对roll 和 pitch 角的确定。因此，累积漂移仅出现在 $x, y, z, yaw$ 上，为了充分利用已知信息来校正漂移，作者固定roll 和 pitch 角度，利用位姿图实现<strong>对4自由度的优化</strong>。</p>
<p><img src="/2024/01/31/vins/image-20240131200758379.png" alt="image-20240131200758379" title="figure 11"></p>
<h2 id="8-2-Adding-Keyframes-Into-the-Pose-Graph"><a href="#8-2-Adding-Keyframes-Into-the-Pose-Graph" class="headerlink" title="8.2 Adding Keyframes Into the Pose Graph"></a>8.2 Adding Keyframes Into the Pose Graph</h2><p>关键帧在VIO 处理之后被添加进位姿图中，每个关键帧在位姿图中都作为一个顶点，与其他顶点之间有两种类型的边，如Fig. 12所示。</p>
<ol>
<li>Sequential Edge：一个关键帧会与之前的关键帧之间建立序列边，序列边表示两帧之间的相对位姿转换，该转换直接取自于VIO，且该转换关系只包含平移 $\hat{\mathbf{p}}^i_{ij}$ 和yaw 角 $\hat{\psi}_{ij}$；</li>
</ol>
<p><img src="/2024/01/31/vins/image-20240131204337669.png" alt="image-20240131204337669" title="formula 19"></p>
<ol>
<li>Loop-Closure Edge：回环边的值来自重定位，同样也只包含平移和yaw 角的4自由度参数。</li>
</ol>
<p><img src="/2024/01/31/vins/image-20240131203724102.png" alt="image-20240131203724102" title="figure 12"></p>
<h2 id="8-3-4-DOF-Pose-Graph-Optimization"><a href="#8-3-4-DOF-Pose-Graph-Optimization" class="headerlink" title="8.3 4-DOF Pose Graph Optimization"></a>8.3 4-DOF Pose Graph Optimization</h2><p>定义两帧间边的残差为：</p>
<p><img src="/2024/01/31/vins/image-20240131204626147.png" alt="image-20240131204626147" title="formula 20"></p>
<p>其中，$\hat{\phi}_i, \hat{\theta}_i$ 分别是固定的roll pitch 角。</p>
<p>序列边和回环边的联合损失函数如下式所示：</p>
<p><img src="/2024/01/31/vins/image-20240131204832609.png" alt="image-20240131204832609" title="formula 21"></p>
<p>尽管紧耦合的重定位模块已经对回环检测进行了筛选，作者还是增加了Huber 核函数 $\rho(.)$ 来进一步减少错误回环带来的影响。相对应的，序列边部分不使用额外处理，这是因为VIO 已经包含了充足的外点剔除机制。</p>
<p>位姿图优化和重定位在两个独立线程进行异步运行，这使得重定位可以使用最新处理过的位姿图优化结果。</p>
<h2 id="8-4-Pose-Graph-Merging"><a href="#8-4-Pose-Graph-Merging" class="headerlink" title="8.4 Pose Graph Merging"></a>8.4 Pose Graph Merging</h2><p>位姿图不仅可以优化当前地图，而且还可以利用<strong>回环检测</strong>将当前地图与历史构建地图进行融合，如Fig. 13所示。</p>
<p><img src="/2024/01/31/vins/image-20240131205629702.png" alt="image-20240131205629702" title="figure 13"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Multi-source Navigation</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>C++</tag>
        <tag>VINS</tag>
        <tag>IMU</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 X-View_Graph-Based Semantic Multi-View Localization</title>
    <url>/2024/01/30/x-view/</url>
    <content><![CDATA[<p>Gawel, Abel, Carlo Del Don, Roland Siegwart, Juan Nieto, and Cesar Cadena. “X-View: Graph-Based Semantic Multi-View Localization.” <em>IEEE Robotics and Automation Letters (RA-L)</em> 3, no. 3 (2018): 1687–94.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文做出的贡献：</p>
<ol>
<li>提出一个新颖的<strong>语义拓扑图表示</strong>方法；</li>
<li>引进了一个<strong>基于随机游走的图描述子</strong>，可以有效地使用既定的匹配方法进行高效匹配；</li>
<li>用于全局定位的语义分割完整pipeline；</li>
<li>开源<em>X-View</em> 算法；</li>
<li>公开数据集的测试结果。</li>
</ol>
<span id="more"></span>
<p><img src="/2024/01/30/x-view/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-X-View"><a href="#3-X-View" class="headerlink" title="3 X-View"></a>3 X-View</h1><p>X-View 利用从输入语义数据提取得到的图，并使用图描述子进行图匹配，本文提出的全局定位算法架构如Fig. 2所示，本系统被设计为可以用于任何给定的语义信息作为输入的里程估计系统，为了简化表示，Fig. 2中只列举了语义分割图片作为输入。</p>
<p><img src="/2024/01/30/x-view/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-System-input"><a href="#3-1-System-input" class="headerlink" title="3.1 System input"></a>3.1 System input</h2><p>将语义分割或实例分割图片作为系统输入，且假定外部里程系统的估计，以及一个参考语义图 $G_{db}$ 。</p>
<h2 id="3-2-Graph-extraction-and-assembly"><a href="#3-2-Graph-extraction-and-assembly" class="headerlink" title="3.2 Graph extraction and assembly"></a>3.2 Graph extraction and assembly</h2><p>将一系列语义分割图片 $I_q$ 转化为一个质询图 $G_q$ ，从连接区域中提取斑点 blobs，例如每幅图中具有相同语义标签 $l_j$ 的区域。为了应对语义分割的噪声（如标签中的洞、断连的边、边缘错误的标签等），作者使用 dilating 和 eroding 对每个班点的边界进行柔化，且设定最小像素数阈值为4以排除过小及次要物体的影响。斑点的中心位置 $p_j$ 被提取出来，与标签一起存储为顶点 $v_j = \{l_j, p_j\}$ 。</p>
<p>顶点之间的无向边可以是图像空间或3D 空间的，当考虑图像空间中的边时，认为图片在时间序列中根据几幅连续的输入图片流来生成图（利用连续图片获取3D距离信息），在3D 空间中不需要考虑这方面。</p>
<p>利用深度通道数据或者深度估计来组成3D空间结构，使用图片斑点的3D位置来计算欧氏距离，图提取与组合的过程如Fig. 3所示，当对多幅连续图片的图进行组合时，将距离较近且拥有相同语义标签的重复顶点融合为一个顶点，该顶点位置选择为初次观测到的位置。</p>
<p><img src="/2024/01/30/x-view/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="3-3-Descriptors"><a href="#3-3-Descriptors" class="headerlink" title="3.3 Descriptors"></a>3.3 Descriptors</h2><p>子图匹配是一个 NP-complete 难题，且为了实现机器人的实时定位问题，作者提出了为图中每个节点建立随机游走描述子并进行匹配，这样做的优势是根据给定的静态或者增长的参考图，匹配时间分别是常数或者线性变化的。每个顶点的描述子是一个 $n \times m$ 的矩阵，包含 <em>n</em> 个深度为 <em>m</em> 的随机游走，每个随机游走开始于源顶点 $v_j$ ，并存储访问过顶点的分类标签，随机游走描述子提取过程如Fig. 4所示。</p>
<p><img src="/2024/01/30/x-view/fig4.png" alt="fig4" title="figure 4"></p>
<h2 id="3-4-Descriptor-Matching"><a href="#3-4-Descriptor-Matching" class="headerlink" title="3.4 Descriptor Matching"></a>3.4 Descriptor Matching</h2><p>在质询图 $G_q$ 与 参考图 $G_{db}$ 完成之后，通过计算相应的图描述子之间的similarity score来建立两图顶点之间的联系：</p>
<ul>
<li>相似性是通过对质询图与参考图中顶点的语义描述子的每一行进行匹配计算获取的；</li>
<li>两个描述子拥有的相同随机游走数量决定了similarity score（分布在0与1之间）；</li>
<li>然后选取得分最高的前 k 个匹配顶点对来计算质询图在参考图中的定位信息。</li>
</ul>
<h2 id="3-5-Localization-Back-End"><a href="#3-5-Localization-Back-End" class="headerlink" title="3.5 Localization Back-End"></a>3.5 Localization Back-End</h2><p>一共有三种类型的约束：</p>
<ol>
<li>来自语义描述子匹配的约束；</li>
<li>联系定位图中连续位姿的机器人估计位姿约束；</li>
<li>每一个robot-vertex观测的转换信息编码得到的robot-vertex约束。</li>
</ol>
<p>作者根据以上三个约束参数计算最大后验估计 (MAP)来获取定位信息。</p>
<h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h1><p>在两个不同的合成户外数据集进行评估，包括<strong>前视——后视</strong>视角变化、<strong>前视——空视</strong>视角变化，以及一个真实世界户外数据集，包含<strong>前视——后视</strong>视角变化。</p>
<h2 id="4-1-Datasets"><a href="#4-1-Datasets" class="headerlink" title="4.1 Datasets"></a>4.1 Datasets</h2><p>SYNTHIA 数据集是在城市环境模拟生成的，采集汽车有8个摄像头，四个方向各有两个，相邻图片间的距离在0~1m。作者使用photo-realistic Arisim 生成了空对地观测的数据集，包含空中向地面观测与地面前向观测两种数据，两种数据之间只有在 z 轴存在偏差，其余均相同，相邻图片间的距离总是1m。真实户外场景的数据集是通过Google StreetView 获取的，类似于SYNTHIA 数据集，只使用前视与后视摄像头，相邻图片间的距离接近10m。</p>
<p><img src="/2024/01/30/x-view/fig5.png" alt="fig5" title="figure 5"></p>
<h2 id="4-3-Localization-performance"><a href="#4-3-Localization-performance" class="headerlink" title="4.3 Localization performance"></a>4.3 Localization performance</h2><p>设置两组实验：</p>
<ol>
<li>在SYNTHIA 数据集上测试不同的参数设置，如随机游走参数、质询图片数量、动态分类物体、图边缘构建技术等；</li>
<li>在SYNTHIA、Airsim 以及 StreetView 上进行对比分析实验。</li>
</ol>
<p>与基于外观匹配的算法进行对比，使用了BoW、NetVLAD 算法。</p>
<h2 id="4-4-Results"><a href="#4-4-Results" class="headerlink" title="4.4 Results"></a>4.4 Results</h2><p>Fig. 6是对SYNTHIA 数据集进行的参数对比试验结果。6(a) 中 n 代表随机游走的个数， m 代表随机游走的深度。</p>
<p><img src="/2024/01/30/x-view/fig6.png" alt="fig6" title="figure 6"></p>
<p>PR评估曲线以及不同定位误差下的成功率结果如Fig. 7所示。</p>
<p><img src="/2024/01/30/x-view/fig7.png" alt="fig7" title="figure 7"></p>
<p>系统组件的时间消耗如Table 1所示。</p>
<p><img src="/2024/01/30/x-view/t1.png" alt="t1" title="table 1"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Topology</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Topology</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Semantic SLAM Based on Object Detection and Improved Octomap</title>
    <url>/2024/01/30/zhang2018/</url>
    <content><![CDATA[<p>Zhang, Liang, Leqi Wei, Peiyi Shen, Wei Wei, Guangming Zhu, and Juan Song. “Semantic SLAM Based on Object Detection and Improved Octomap.” <em>IEEE Access</em> 6 (2018): 75545–59. <a href="https://doi.org/10.1109/ACCESS.2018.2873617">https://doi.org/10.1109/ACCESS.2018.2873617</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>基于ORB-SLAM2，利用YOLO 实现<strong>物体检测</strong>，剔除掉<strong>先验动态物体</strong>上的特征点，提高精度；并建立<strong>物体级语义八叉树地图</strong>，且优化了制图的速度。</p>
<p>本文的贡献：</p>
<ul>
<li>本系统可以检测80-200 种物体种类，而现有的语义制图系统只能检测不超过20个种类；</li>
<li>本系统不需要先验3D 模型就可以实现对环境中物体的3D 建模；</li>
<li>本系统利用的是物体级模型信息，而不是像素级的。</li>
</ul>
<span id="more"></span>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3 System Overview"></a>3 System Overview</h1><h2 id="3-1-SLAM-Analysis"><a href="#3-1-SLAM-Analysis" class="headerlink" title="3.1 SLAM Analysis"></a>3.1 SLAM Analysis</h2><p>大多数基于特征的SLAM 系统基于一个<strong>强假设</strong>：移动物体上的特征点数量要远小于静态物体特征点数量。</p>
<p><strong>八叉树地图Octomap</strong> 不仅可以存储RGB 和位置信息，还可以保存<strong>语义信息</strong>，且利用<strong>概率模型</strong>来构建更为精准的地图。</p>
<h2 id="3-2-ORB-SLAM-Analysis"><a href="#3-2-ORB-SLAM-Analysis" class="headerlink" title="3.2 ORB-SLAM Analysis"></a>3.2 ORB-SLAM Analysis</h2><p>ORB-SLAM2 的架构如Fig. 1所示，包含三个并行处理的线程：</p>
<ul>
<li>Tracking：负责实时定位每一帧图片对应的<strong>位姿</strong>，并决定哪些图片作为<strong>关键帧</strong>。与前一帧进行特征匹配并利用BA 进行位姿优化，如果跟踪失败，利用Bag of Word 进行全局<strong>重定位</strong>，将地图中的特征点进行<strong>重投影</strong>并进行位姿优化。</li>
<li>Local Mapping：在获取一个新的关键帧之后，该线程对新地图点进行<strong>三角化</strong>，利用BA 对关键帧和地图点进行<strong>位姿优化</strong>，并对冗余的关键帧和低质量的地图点进行剔除。</li>
<li>Loop Closing：对关键帧进行<strong>回环检测</strong>，若检测成功，计算相似度转换作为环路<strong>累积漂移</strong>，然后进行对齐、融合与位姿优化。</li>
</ul>
<p><img src="/2024/01/30/zhang2018/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-3-Overview-of-Semantic-SLAM-system"><a href="#3-3-Overview-of-Semantic-SLAM-system" class="headerlink" title="3.3 Overview of Semantic SLAM system"></a>3.3 Overview of Semantic SLAM system</h2><p>本文提出的语义SLAM 系统架构如Fig. 2所示，本系统是基于ORB-SLAM2 构建的，ORB-SLAM2 负责相机定位，并利用每一个RGB-D 图片帧进行制图。</p>
<p>tracking 线程使用<strong>关键帧进行跟踪</strong>，来减小移动物体的影响；local mapping 线程添加<strong>少量</strong>的关键帧来创造语义信息，因为语义信息的提取满足实时的要求。</p>
<p>本系统使用YOLO 网络对关键帧进行物体检测，并使用<strong>CRF</strong> (Conditional Random Field) 进行<strong>物体正则化</strong>来对YOLO 检测出的物体置信度进行<strong>矫正优化</strong>，该过程使用MS-COCO 数据集的统计数据计算得到的物体间<strong>约束</strong>。</p>
<p><img src="/2024/01/30/zhang2018/fig2.png" alt="fig2" title="figure 2"></p>
<p>当获取了每个物体的<strong>准确标签</strong>后，利用滤波器对特征进行筛选，并利用投影的点云来创建<strong>临时的物体模型</strong>；在此基础上，利用<strong>数据关联</strong>决定创建新的模型或是与地图中现有的模型进行融合。</p>
<p>最后，Map Generation 利用存储在物体中的点云生成八叉树地图，并使用多线程和Fast Line Rasterization 算法进行加速。</p>
<h2 id="3-4-Relationship-between-Keyframes-and-Objects"><a href="#3-4-Relationship-between-Keyframes-and-Objects" class="headerlink" title="3.4 Relationship between Keyframes and Objects"></a>3.4 Relationship between Keyframes and Objects</h2><p>借鉴ORB-SLAM2 系统中<strong>关键帧和地图点</strong>之间的关系来创建本系统<strong>关键帧和物体</strong>之间的联系。</p>
<p>本系统中，每个<strong>物体 $O_i$</strong> 需包含：</p>
<ul>
<li>物体中点云的坐标；</li>
<li>固定数量的类别标签，以及通过递归贝叶斯更新recursive Bayesian update 计算的置信度；</li>
<li>观测到该物体的关键帧；</li>
<li>物体中点云的Kd-tree 结构（用于快速查找）；</li>
<li>物体所属的类别标签；</li>
<li>物体被观测的数量。</li>
</ul>
<p>每个<strong>关键帧 $K_i$</strong> 需包含：</p>
<ul>
<li>用于物体检测的相应RGB 图片；</li>
<li>用于产生点云的相应深度图片；</li>
<li>本关键帧中观测到的物体。</li>
</ul>
<h1 id="4-Semantic-Mapping"><a href="#4-Semantic-Mapping" class="headerlink" title="4 Semantic Mapping"></a>4 Semantic Mapping</h1><h2 id="4-1-Improved-SLAM"><a href="#4-1-Improved-SLAM" class="headerlink" title="4.1 Improved SLAM"></a>4.1 Improved SLAM</h2><p>ORB-SLAM2 中，tracking 线程的步骤：</p>
<ol>
<li>提取<strong>ORB 特征</strong>；</li>
<li>ORB 特征与<strong>reference 帧</strong>进行<strong>特征匹配</strong>，初步计算相机位姿并返回匹配地图点（通过搜索<strong>相关的关键帧</strong>来获取）的数量；</li>
<li>利用匹配地图点进行<strong>位姿优化</strong>；</li>
<li>决定哪些帧作为<strong>关键帧</strong>。</li>
</ol>
<p>为了减小动态物体的影响，对跟踪线程的第二步进行更改：由之前的reference 帧改为与<strong>关键帧进行特征匹配</strong>，这是因为<strong>旧的关键帧不包含动态物体的特征点</strong>。</p>
<p>跟踪线程的第三步改为：与第二步的结果比较<strong>匹配内点数量</strong>，来判断当前帧是否跟踪失败。</p>
<h2 id="4-2-Object-Detection"><a href="#4-2-Object-Detection" class="headerlink" title="4.2 Object Detection"></a>4.2 Object Detection</h2><p>利用在COCO 数据集上训练的Tiny YOLO 网络进行物体检测。</p>
<h2 id="4-3-Object-Regularization"><a href="#4-3-Object-Regularization" class="headerlink" title="4.3 Object Regularization"></a>4.3 Object Regularization</h2><p>常规的物体检测等网络没有考虑<strong>上下文信息（场景信息）</strong>，使用CRF 可为语义提取过程添加上下文信息约束，CRF 擅长对分类器的<strong>类别得分</strong>与图片的<strong>局部信息</strong>进行建模，可以视为一个<strong>最大后验概率问题</strong>。定义<strong>unary potentials</strong> 来对像素或图像块的所属类别进行概率建模，定义<strong>pairwise potentials</strong> 来对像素间或图像块间的关系进行建模。</p>
<p>作者构建了一个<strong>基于物体的概率稠密CRF 算法</strong>，较基于像素的方法大大减少了计算复杂度，相应的Gibbs 能量方程如下所示：</p>
<p><img src="/2024/01/30/zhang2018/f1-2.png" alt="f1" title="f1-2"></p>
<p>与文章(Runz 和 Agapito, 2017)相似，$x$ 表示类别标签；$i, j$ 取值1 到 $k$ ，其中 $k$ 表示地图中物体的数量；$Z$ 是归一化参数。本物体级 CRF 的目标是最小化 $E(x)$ ，相应的unary potentials 和 pairwise potentials 表示为：</p>
<p><img src="/2024/01/30/zhang2018/f3-4.png" alt="f3" title="f3-4"></p>
<p>其中，$\mu$ 函数被称为<strong>标签兼容性函数</strong>，用来描述相邻位置两个不同标签同时出现的可能性；$f_{i, j}$ 是第 i 个物体和第 j 个物体的约束。</p>
<p><img src="/2024/01/30/zhang2018/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$p_{i, j}$ 表示两个物体出现在同一视野中的概率，通过对COCO 数据集进行统计分析获取，如Fig. 4所示，对角线数字表示该物体在数据集中出现的次数。</p>
<p><img src="/2024/01/30/zhang2018/fig4.png" alt="fig4" title="figure 4"></p>
<p>由此，本系统实现了同时利用YOLO 和 CRF 实现对物体分类的<strong>置信度</strong>确定。</p>
<h2 id="4-4-Temporary-Objects-Generation"><a href="#4-4-Temporary-Objects-Generation" class="headerlink" title="4.4 Temporary Objects Generation"></a>4.4 Temporary Objects Generation</h2><p>在确定物体的类别标签后，使用feature filter 根据物体类别实现对特征点和地图点的筛选。作者将属于<strong>先验动态物体</strong>的ORB 特征、地图点、DBoW 特征进行剔除，只保留先验静态物体的特征。</p>
<p><img src="/2024/01/30/zhang2018/fig5.png" alt="fig5" title="figure 5"></p>
<p>在动态物体特征剔除之后，生成包含物体尺寸、类别、置信度的得分以及点云的<strong>临时物体模型</strong>，并对噪声点进行剔除。</p>
<p><img src="/2024/01/30/zhang2018/fig6.png" alt="fig6" title="figure 6"></p>
<h2 id="4-5-Data-Association"><a href="#4-5-Data-Association" class="headerlink" title="4.5 Data Association"></a>4.5 Data Association</h2><p>本系统中的DA 是用来判定检测物体是新观测到的还是地图中已存在的。</p>
<p>首先，为每个临时物体模型寻找<strong>候选匹配模型</strong>。可轻易找出与当前关键帧相关联的历史关键帧，以此确定候选匹配模型，该过程如Fig. 7所示。</p>
<p><img src="/2024/01/30/zhang2018/fig7.png" alt="fig7" title="figure 7"></p>
<p>然后，在候选匹配模型中选取<strong>最相似的模型</strong>。本过程在临时模型和候选模型的点云间进行nearest neighbor search ，计算匹配点对之间的<strong>欧氏距离</strong>；该过程利用<strong>k-d 树</strong>来加速匹配过程；选取匹配数量最多且超过一定阈值的候选匹配模型作为成功匹配的模型，若没有成功匹配，则视为<strong>新模型</strong>添加进地图中。</p>
<h2 id="4-6-Object-Model-Update"><a href="#4-6-Object-Model-Update" class="headerlink" title="4.6 Object Model Update"></a>4.6 Object Model Update</h2><p>若模型成功匹配，则<strong>点云</strong>与<strong>类别置信度</strong>需要进行融合。利用recursive Bayesian update 来更新相应的概率分布：</p>
<p><img src="/2024/01/30/zhang2018/f9.png" alt="f9" title="formula 9"></p>
<h2 id="4-7-Map-Generation"><a href="#4-7-Map-Generation" class="headerlink" title="4.7 Map Generation"></a>4.7 Map Generation</h2><p><img src="/2024/01/30/zhang2018/fig9.png" alt="fig9" title="figure 9"></p>
<p>在制图中，移动物体以及测距误差会造成很多误差，八叉树地图使用概率模型来解决该问题。八叉树地图的每个叶子节点存储被占用或空闲的概率，当新的3D 点被插入时会更新概率：</p>
<p><img src="/2024/01/30/zhang2018/f10-11.png" alt="f10-11" title="f10-11"></p>
<p>其中，$n$ 表示叶子节点；$z_t$ 表示观测；$P(n|z_t)$ 表示在给定观测 $z_t$ 时体素 $n$ 被占用的概率。</p>
<p>此外，除了占用概率，体素中还会存储<strong>固定数量的类别标签</strong>以及<strong>置信度得分</strong>。</p>
<p>八叉树地图绘制过程中会消耗大量的时间来计算空的体素，作者利用优化来加速该过程，如Fig. 10所示。</p>
<p><img src="/2024/01/30/zhang2018/fig10.png" alt="fig10" title="figure 10"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>zsh、Oh-My-Zsh及相关设置</title>
    <url>/2024/01/26/zsh-setup/</url>
    <content><![CDATA[<p>注：<strong>本文主要参考</strong><a href="https://www.kwchang0831.dev/dev-env/ubuntu/oh-my-zsh">该文章</a>。</p>
<h1 id="1-安装并设置zsh"><a href="#1-安装并设置zsh" class="headerlink" title="1 安装并设置zsh"></a>1 安装并设置zsh</h1><p>安装命令：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install zsh</span><br></pre></td></tr></table></figure>
<p>设置zsh为默认shell：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">chsh -s $(<span class="built_in">which</span> zsh)</span><br></pre></td></tr></table></figure>
<h1 id="2-安装Oh-My-Zsh"><a href="#2-安装Oh-My-Zsh" class="headerlink" title="2 安装Oh My Zsh"></a>2 安装Oh My Zsh</h1><p>安装命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh -c <span class="string">&quot;<span class="subst">$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)</span>&quot;</span></span><br></pre></td></tr></table></figure>
<h1 id="3-安装主题PowerLevel10k"><a href="#3-安装主题PowerLevel10k" class="headerlink" title="3 安装主题PowerLevel10k"></a>3 安装主题PowerLevel10k</h1><p>安装命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/romkatv/powerlevel10k.git <span class="variable">$ZSH_CUSTOM</span>/themes/powerlevel10k</span><br></pre></td></tr></table></figure>
<p>进入.zshrc设置主题：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ZSH_THEME=<span class="string">&quot;powerlevel10k/powerlevel10k&quot;</span></span><br></pre></td></tr></table></figure>
<h1 id="4-安装插件"><a href="#4-安装插件" class="headerlink" title="4 安装插件"></a>4 安装插件</h1><h2 id="4-1-日常插件"><a href="#4-1-日常插件" class="headerlink" title="4.1 日常插件"></a>4.1 日常插件</h2><p>zsh-autosuggestions:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/zsh-users/zsh-autosuggestions <span class="variable">$&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;</span>/plugins/zsh-autosuggestions</span><br></pre></td></tr></table></figure>
<p>zsh-syntax-highlighting:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/zsh-users/zsh-syntax-highlighting.git <span class="variable">$&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;</span>/plugins/zsh-syntax-highlighting</span><br></pre></td></tr></table></figure>
<p>设置要启动的插件（Plugins）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">plugins=(</span><br><span class="line">         git</span><br><span class="line">         zsh-autosuggestions </span><br><span class="line">         zsh-syntax-highlighting</span><br><span class="line">         extract</span><br><span class="line">         sudo</span><br><span class="line">         tmux</span><br><span class="line"></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>值得注意的是，除了zsh-autosuggestions 与zsh-syntax-highlighting 需要单独安装外，上述的其他插件均是内嵌的，可直接添加至配置文件中直接使用。</p>
<h2 id="4-2-Tmux设置"><a href="#4-2-Tmux设置" class="headerlink" title="4.2 Tmux设置"></a>4.2 Tmux设置</h2><p><strong>该部分主要参考</strong><a href="https://louiszhai.github.io/2017/09/30/tmux/#导读">文章</a>。</p>
<p>安装命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install tmux</span><br></pre></td></tr></table></figure>
<p>tmux 的个性化设置需在主目录编辑.tmux.conf 文件，本人的文件设置如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> -g prefix C-z</span><br><span class="line">unbind C-b</span><br><span class="line"><span class="built_in">bind</span> C-a send-prefix</span><br><span class="line"></span><br><span class="line"><span class="built_in">bind</span> r source-file ~/.tmux.conf \; display-message <span class="string">&quot;Config reloaded..&quot;</span></span><br><span class="line"></span><br><span class="line">unbind <span class="string">&#x27;&quot;&#x27;</span></span><br><span class="line"><span class="built_in">bind</span> - splitw -v -c <span class="string">&#x27;#&#123;pane_current_path&#125;&#x27;</span></span><br><span class="line">unbind %</span><br><span class="line"><span class="built_in">bind</span> | splitw -h -c <span class="string">&#x27;#&#123;pane_current_path&#125;&#x27;</span></span><br><span class="line"></span><br><span class="line">set-option -g mouse on</span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -g base-index 0</span><br><span class="line"><span class="built_in">set</span> -g pane-base-index 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># setw -g utf8 on</span></span><br><span class="line"><span class="built_in">set</span> -g status-interval 1</span><br><span class="line"><span class="built_in">set</span> -g status-justify left</span><br><span class="line">setw -g monitor-activity on</span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -g status-bg black</span><br><span class="line"><span class="built_in">set</span> -g status-fg yellow</span><br><span class="line"><span class="built_in">set</span> -g status-left <span class="string">&quot;#[bg=#FF661D] ❐ #S &quot;</span></span><br><span class="line"><span class="built_in">set</span> -g status-right <span class="string">&quot;%H:%M:%S %d-%b&quot;</span></span><br><span class="line"><span class="built_in">set</span> -g status-left-length 300</span><br><span class="line"><span class="built_in">set</span> -g status-right-length 500</span><br><span class="line"><span class="built_in">set</span> -wg window-status-current-format <span class="string">&quot; #I:#W#F &quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -g message-style <span class="string">&quot;bg=#202529, fg=#91A8BA&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -g default-terminal <span class="string">&quot;screen-256color&quot;</span></span><br></pre></td></tr></table></figure>
<p>其他使用说明可参考上述链接。在oh my zsh 插件中添加了 tmux 后，可以使用如下快捷键：</p>
<p><img src="/2024/01/26/zsh-setup/hotkey.png" alt="hotkey" title="快捷键设置"></p>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>zsh</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 SO-SLAM_Semantic Object SLAM With Scale Proportional and Symmetrical Texture Constraints</title>
    <url>/2024/02/19/so-slam/</url>
    <content><![CDATA[<p>Liao, Ziwei, Yutong Hu, Jiadong Zhang, Xianyu Qi, Xiaoyu Zhang, and Wei Wang. “SO-SLAM: Semantic Object SLAM With Scale Proportional and Symmetrical Texture Constraints.” <em>IEEE Robotics and Automation Letters</em> 7, no. 2 (April 2022): 4008–15. <a href="https://doi.org/10.1109/LRA.2022.3148465">https://doi.org/10.1109/LRA.2022.3148465</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文主要解决单目 Object SLAM 的<strong>两个挑战</strong>：</p>
<ol>
<li>单目相机包含<strong>较少的物体约束信息</strong>，特别是在部分观测、遮挡等情况下更为严重，使得单目 Object SLAM 较为<strong>脆弱</strong>；</li>
<li>当前的 Object SLAM 主要用于约束<strong>物体的占用空间</strong>，没有充分利用物体的<strong>朝向信息</strong>。</li>
</ol>
<p>针对以上两个挑战，作者提出了<strong>单目 Semantic Object SLAM (SO-SLAM) 系统</strong>，如Fig. 1 所示，除了物体的语义信息，作者还引入了三种代表性的<strong>物体空间约束</strong>：<strong>尺寸比例约束、对称纹理约束</strong>以及<strong>平面支撑约束</strong>，作者推导约束模型并同时应用于前端初始化与后端优化中。</p>
<p>本文的贡献如下：</p>
<ol>
<li>提出一个面向室内环境的完全耦合三种空间约束的<strong>单目 Object SLAM</strong>；</li>
<li>基于空间约束提出两个新方法：<strong>单帧物体初始化</strong>方法和<strong>物体朝向优化</strong>方法；</li>
<li>在两个公开数据集与自采数据集上验证了本算法的有效性。</li>
</ol>
<span id="more"></span>
<p><img src="/2024/02/19/so-slam/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-Monocular-Object-SLAM-Framework"><a href="#3-Monocular-Object-SLAM-Framework" class="headerlink" title="3 Monocular Object SLAM Framework"></a>3 Monocular Object SLAM Framework</h1><p>表示物体的椭球体包含<strong>9自由度</strong>，可利用SVD 方法来获取，该方法至少3帧具有足够视差的观测才可以获取(Nicholson 等, 2019)。作者利用位姿图对相机和物体位姿进行优化：</p>
<p><img src="/2024/02/19/so-slam/f1.png" alt="f1" title="formula 1"></p>
<p>其中，X 表示相机位姿，Q 表示物体位姿；$F_Z$ 表示相机-物体观测约束，$F_O$ 表示里程计约束，这两个约束项在文章(Nicholson 等, 2019)中有详细介绍；本文着重介绍新增加的空间约束 $F_S$ ，包含平面支撑约束 $f_{sup}$ ，尺度比例约束 $f_{ssc}$ ，以及对称纹理约束 $f_{sym}$ ；H() 是鲁棒核用以提高系统对外点的鲁棒性，本文使用 Huber Kernel。</p>
<h1 id="4-Single-Frame-Initialization-with-Semantic-Priors"><a href="#4-Single-Frame-Initialization-with-Semantic-Priors" class="headerlink" title="4 Single-Frame Initialization with Semantic Priors"></a>4 Single-Frame Initialization with Semantic Priors</h1><p>作者根据人类认知习惯来设定物体坐标系：人造物体的上表面一般与物体的支撑面相反，前向一般是对称的方向，由此构建物体的 Z 轴和 X 轴，由此构建了物体坐标系。然后，就可以应用更多的约束，如Fig. 2所示，实现单帧图片对9自由度物体进行初始化的方法，克服了传统SVD 方法难以满足的要求。</p>
<p><img src="/2024/02/19/so-slam/fig2.png" alt="figure2" title="figure 2"></p>
<h2 id="4-1-Object-Detection-Constraints"><a href="#4-1-Object-Detection-Constraints" class="headerlink" title="4.1 Object Detection Constraints"></a>4.1 Object Detection Constraints</h2><p>物体检测框的四条边线经相机投影矩阵进行逆投影，可得到<strong>四个切面约束</strong>，如Fig. 2中的对 $l_i$ 逆投影得到切平面 $\pi_i$ ，由此形成对椭球体 $Q^*$ 的四自由度约束，如下所示：</p>
<p><img src="/2024/02/19/so-slam/f4.png" alt="f4" title="formula 4"></p>
<p>其中，$\sum_{det}$ 表示物体检测协方差矩阵，本文中令其为10。</p>
<h2 id="4-2-Plane-Supporting-Constraints"><a href="#4-2-Plane-Supporting-Constraints" class="headerlink" title="4.2 Plane Supporting Constraints"></a>4.2 Plane Supporting Constraints</h2><p>根据物体与其支撑平面的关系，可以构建<strong>三个约束</strong>，如Fig. 2所示，物体的X、Y轴与平面 $\pi_s$ 的法向量正交，椭球体与平面 $\pi_s$ 相切。由此构建如下函数模型：</p>
<p><img src="/2024/02/19/so-slam/f8.png" alt="f8" title="formula 8"></p>
<p>其中，$Rot_x(Q^\ast), Rot_y(Q^\ast)$ 分别为椭球体的 X、Y 轴；支撑平面为 $\pi_s=(n_s, d)$，  $n_s$ 为支撑平面的法向量。</p>
<h2 id="4-3-Semantic-Scale-Proportional-Constraint"><a href="#4-3-Semantic-Scale-Proportional-Constraint" class="headerlink" title="4.3 Semantic Scale Proportional Constraint"></a>4.3 Semantic Scale Proportional Constraint</h2><p>作者提出了一种灵活的物体尺度先验约束——Scale Proportional Constraint (SPC)，用来约束物体的<strong>尺寸比例</strong>而不是其精确的尺寸。假设物体的尺寸为 $s = [a, b, c]^T$ ，分别为X、Y、Z轴的尺寸，作者由此定义尺寸比例 $r = [\sigma, \beta]^T$ ：</p>
<p><img src="/2024/02/19/so-slam/f9.png" alt="f9" title="formula 9"></p>
<p>根据常见的物体尺寸可制作一个<strong>尺寸比例表</strong>。</p>
<p>给定一个物体 $Q_O^\ast$ ，可根据定义计算其尺寸比例 $r_O = r(Q^\ast_O)$ ，根据其语义类别标签 $l_O$ 经查表可得其对应的尺寸比例 $r_s = SemTable(l_O)$ 。由此构建物体的尺寸比例约束：</p>
<p><img src="/2024/02/19/so-slam/f11.png" alt="f11" title="formula 11"></p>
<h2 id="4-4-Solving-the-Single-Frame-Initialization"><a href="#4-4-Solving-the-Single-Frame-Initialization" class="headerlink" title="4.4 Solving the Single Frame Initialization"></a>4.4 Solving the Single Frame Initialization</h2><p>利用Levenberg-Marquardt 算法对目标方程进行迭代求解：</p>
<p><img src="/2024/02/19/so-slam/f12.png" alt="f12" title="formula 12"></p>
<h1 id="5-Orientation-Optimization-with-Texture-Symmetry"><a href="#5-Orientation-Optimization-with-Texture-Symmetry" class="headerlink" title="5 Orientation Optimization with Texture Symmetry"></a>5 Orientation Optimization with Texture Symmetry</h1><h2 id="5-1-Mathematical-Description-of-Object-Symmetry"><a href="#5-1-Mathematical-Description-of-Object-Symmetry" class="headerlink" title="5.1 Mathematical Description of Object Symmetry"></a>5.1 Mathematical Description of Object Symmetry</h2><p>人造物体一般是对称的，作者将其<strong>对称平面的方向</strong>定义为前向，也就是物体坐标系的 X 轴，如Fig. 3所示。物体对称性可<strong>数学表示</strong>为：物体上的任意一点 $v_0 \in V$ 都可经由其<strong>对称面</strong> $\pi_{xz}$ 找到物体上与其对称的点 $v_0^S \in V$ 。</p>
<p><img src="/2024/02/19/so-slam/fig3.png" alt="fig3" title="figure 3"></p>
<p>物体在图像中的一点 u 经逆投影得到其在3D 空间中的点 v，满足以下方程：</p>
<p><img src="/2024/02/19/so-slam/f13.png" alt="f13" title="formula 13"></p>
<p>将式（a）代入式（b），在（c）的约束下最多只有一个解，由此可以得到 u 的对称像素点：</p>
<p><img src="/2024/02/19/so-slam/f14.png" alt="f14" title="formula 14"></p>
<p>其中，$\mathcal{P}$ 表示相机的投影矩阵；$v_0^S = \mathcal{S}(v_0, Q)$ 表示3D 点 $v_0$  的对称点。该过程如Fig. 3（b）所示，根据找到的<strong>对称点对</strong>，作者构建一个<strong>描述子映射关系</strong> $\beta(<em>)$ ，作者称其为<em>*对称投影不变</em></em> symmetric projection invariant，其满足以下性质：</p>
<p><img src="/2024/02/19/so-slam/f15.png" alt="f15" title="formula 15"></p>
<p>由此，当观测噪声较严重时，可以构建如下的代价方程来优化椭球体 Q：</p>
<p><img src="/2024/02/19/so-slam/f16.png" alt="f16" title="formula 16"></p>
<h2 id="5-2-The-Construction-of-Symmetry-Descriptor"><a href="#5-2-The-Construction-of-Symmetry-Descriptor" class="headerlink" title="5.2 The Construction of Symmetry Descriptor"></a>5.2 The Construction of Symmetry Descriptor</h2><p>对于 $\beta(*)$ 的确定，作者先后考虑了灰度值（广泛用于直接法）、BRIEF 描述子等，之后，作者考虑像素的 Distance Transform：</p>
<p><img src="/2024/02/19/so-slam/f17.png" alt="f17" title="formula 17"></p>
<p>上式的含义为：<strong>从一个像素点到图像中任意边上像素的最近距离</strong>，部分反应了物体的<strong>纹理</strong>。但是这个描述子不一定可以满足对称投影不变的性质，如Fig. 4（a）所示。进而，作者发现了the nearest edge distance of point $v_0$ ，记为 $B_{3DT}(v_0)$ 可满足要求（但<strong>计算量太大</strong>）：</p>
<p><img src="/2024/02/19/so-slam/f19.png" alt="f19" title="formula 19"></p>
<p><img src="/2024/02/19/so-slam/fig4.png" alt="fig4" title="figure 4"></p>
<p>作者结合以上两种方法的优势，提出了 Improved-DT descriptor：</p>
<p><img src="/2024/02/19/so-slam/f20.png" alt="f20" title="formula 20"></p>
<p><img src="/2024/02/19/so-slam/f21.png" alt="f21" title="formula 21"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Pose Optimization</tag>
        <tag>Object SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Global Localization with Object-Level Semantics and Topology</title>
    <url>/2024/02/20/liu2019/</url>
    <content><![CDATA[<p>Liu, Yu, Yvan Petillot, David Lane, and Sen Wang. “Global Localization with Object-Level Semantics and Topology.” In <em>2019 International Conference on Robotics and Automation (ICRA)</em>, 4909–15. Montreal, QC, Canada: IEEE, 2019. <a href="https://doi.org/10.1109/ICRA.2019.8794475">https://doi.org/10.1109/ICRA.2019.8794475</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文作者研究了使用3D 物体级语义信息来实现基于视觉的全局定位技术，主要贡献如下：</p>
<ol>
<li>综合利用现有的<strong>稠密语义</strong>、<strong>3D 拓扑</strong>、<strong>图匹配</strong>以及<strong>3D 对齐技术</strong>，实现一个新颖的物体级全局定位算法；</li>
<li>展现了物体级语义信息对于鲁棒地点识别与全局定位的作用，对于光强变化、场景改变等具有较强的鲁棒性；</li>
<li>证明了物体级对齐技术可以处理具有挑战性的3D 点对齐，并在没有完整观察数据的情况下实现精确定位。</li>
</ol>
<span id="more"></span>
<h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h1><h2 id="2-3-Graph-Matching"><a href="#2-3-Graph-Matching" class="headerlink" title="2.3 Graph Matching"></a>2.3 Graph Matching</h2><p>图匹配可以被视为一种<strong>匹配问题</strong>，解决两个图之间节点与边的确切对应关系。不幸的是，用这种方式解决该问题是典型的NP-hard 问题。选择另一种非确切图匹配方式来解决对应问题，如:</p>
<p>一些作者将一个图利用其邻接矩阵来表示重要的拓扑属性，在此基础上使用矩阵相似度来简化图匹配问题。</p>
<p>一些作者尝试使用 graph kernels based on walks 来解决图匹配：</p>
<blockquote>
<p>the authors compute <strong>pair-wise similarity between walks’ composing nodes and edges</strong>, and calculate a final matching score for the scene modeling problem.</p>
</blockquote>
<p>一些作者比较每个节点的random walk descriptors，其中每个描述子对相应节点的局部连接进行编码each descriptor encodes the local connectivity of the corresponding node.</p>
<h1 id="3-Global-Localization-With-Object-Level-Semantic-and-Topology"><a href="#3-Global-Localization-With-Object-Level-Semantic-and-Topology" class="headerlink" title="3 Global Localization With Object-Level Semantic and Topology"></a>3 Global Localization With Object-Level Semantic and Topology</h1><p><img src="/2024/02/20/liu2019/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-Semantic-Segmentation-and-Fusion"><a href="#3-1-Semantic-Segmentation-and-Fusion" class="headerlink" title="3.1 Semantic Segmentation and Fusion"></a>3.1 Semantic Segmentation and Fusion</h2><p>本文中，全局地图是在制图阶段利用稠密SLAM 算法构建的，后续增加语义特征，使用简单的投票方案voting scheme进行融合操作，地图中的点若从多个角度观测均被划分为某类则表明有较高的置信度。在最终的语义地图中，只保留那些一直具有高度一致语义标签的点云。query的局部语义地图以同样的方式来获取。</p>
<h2 id="3-2-Graph-Extraction"><a href="#3-2-Graph-Extraction" class="headerlink" title="3.2 Graph Extraction"></a>3.2 Graph Extraction</h2><p>从语义地图到语义图的转换过程：</p>
<ol>
<li>首先，使用Euclidean clustering来提取具有相同语义标签的近点，忽略墙壁、地板、天花板等不能提供有用拓扑关系的语义点云集合；</li>
<li>在图中，使用bounding sphere来表示每一个物体，因为球体具有旋转不变性，球形的尺寸表示物体的尺寸，由最远点到中心点的距离决定；</li>
<li>节点与边表示地图中的3D语义拓扑，当两个物体节点处于邻接距离内时使用无向边进行连接，此外，若两个物体的球体有相交，那么也用无向边进行连接，表明模型包含了<strong>空间与尺寸</strong>关系。</li>
</ol>
<p><img src="/2024/02/20/liu2019/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="3-3-Random-Walk-Descriptor"><a href="#3-3-Random-Walk-Descriptor" class="headerlink" title="3.3 Random Walk Descriptor"></a>3.3 Random Walk Descriptor</h2><p>本文使用随机游走描述子来对图中的节点进行描述：从根节点开始，探索邻接节点并记录访问过的标签序列，每次探索的深度被定义为该游走的长度，探索的数量被定义为该节点的特定描述子尺寸，过程如Fig. 4所示。在描述子中，除了访问过的标签，还需要跟踪每个标签在图中所对应的节点，来确保在association步骤中的空间一致性。</p>
<p><img src="/2024/02/20/liu2019/fig4.png" alt="fig4" title="figure 4"></p>
<h2 id="3-4-Object-Association"><a href="#3-4-Object-Association" class="headerlink" title="3.4 Object Association"></a>3.4 Object Association</h2><p>一旦全局与query图的随机游走描述子被建立之后，就开始基于它们<strong>共有的相同描述子数量</strong>来建立节点间的联系。由于环境中可能存在同一类的多个物体，两个匹配的描述子只表明是<strong>一个潜在的候选对</strong>，对该候选对节点的游走路径进行追溯，对空间一致性进行确认，如果两个路径拥有相同的标签，但是任意两个节点之间的距离超出一个阈值就判定为匹配失败。对于每个质询节点都与全局图中的所有节点进行描述子匹配，最终选取k个具有最多匹配描述子的节点作为匹配节点，由此可以得到质询图与全局图中物体的对应关系。</p>
<p>注意，本文允许非精确匹配inexact association，因为某些场景中的物体可能会出现过度分割的现象，导致一个物体被分割为多块，需要解决多对一的对应问题。</p>
<h2 id="3-5-Localization-based-on-Object-Level-Alignment"><a href="#3-5-Localization-based-on-Object-Level-Alignment" class="headerlink" title="3.5 Localization based on Object-Level Alignment"></a>3.5 Localization based on Object-Level Alignment</h2><p>实现质询图与全局图中物体的关联之后，使用物体的几何信息进行位姿估计：对两图中关联物体的点云进行稠密对齐。</p>
<p>使用Fast Point Feature Histograms (FPFH), Sample Consensus initial alignment method (SAC-IA) 算法来对关联点云进行对齐，由此得到的位姿转换关系提供了质询物体在地图中的初始定位信息，然后使用 Iterative Closest Point (ICP) 算法进行优化，最终得到6自由度的相机位姿。</p>
<h1 id="4-Experimental-Results"><a href="#4-Experimental-Results" class="headerlink" title="4 Experimental Results"></a>4 Experimental Results</h1><h2 id="4-1-Dataset"><a href="#4-1-Dataset" class="headerlink" title="4.1 Dataset"></a>4.1 Dataset</h2><p>作者选取了两个数据库：公开数据集SceneNN ，以及作者在living room，kitchen以及dining room自采的数据集——LKD，在LKD 数据集中作者采集了不同光线变化下的场景数据。</p>
<p><img src="/2024/02/20/liu2019/fig5.png" alt="fig5" title="figure 5"></p>
<h2 id="4-4-Localization-Performance"><a href="#4-4-Localization-Performance" class="headerlink" title="4.4 Localization Performance"></a>4.4 Localization Performance</h2><p>Fig. 7与Table 1是在两个数据集上的实验结果，可以发现在数据集SceneNN 上效果更好，作者给出的原因是：</p>
<blockquote>
<p>most errors are skewed toward the lower range.</p>
</blockquote>
<p><img src="/2024/02/20/liu2019/fig7.png" alt="fig7" title="figure 7"></p>
<p><img src="/2024/02/20/liu2019/t1.png" alt="t1" title="table 1"></p>
<p>Fig. 6展示了在SceneNN 数据集中质询范例成功在全局地图中进行匹配的例子。</p>
<p><img src="/2024/02/20/liu2019/fig6.png" alt="fig6" title="figure 6"></p>
<p>进一步地，作者使用FAB-MAP，NetVLAD 算法进行对比验证，其中FAB-MAP 是基于BoW词袋算法的，NetVLAD 利用CNN 网络在不同视角与光线条件下的同一场景数据进行训练来获取深度特征。实验结果如Fig. 8所示。</p>
<p><img src="/2024/02/20/liu2019/fig8.png" alt="fig8" title="figure 8"></p>
<h2 id="4-5-Discussion-on-Challenging-Scenarios"><a href="#4-5-Discussion-on-Challenging-Scenarios" class="headerlink" title="4.5 Discussion on Challenging Scenarios"></a>4.5 Discussion on Challenging Scenarios</h2><p>作者讨论了当观测数据不完整（可能原因包括遮挡、过度分割导致物体碎片等）、存在动态物体的场景（移除物体或新增物体）情况下本文算法的鲁棒性。</p>
<p><img src="/2024/02/20/liu2019/fig9.png" alt="fig9" title="figure 9"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Topology</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Topology</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 VDO-SLAM_A Visual Dynamic Object-aware SLAM System</title>
    <url>/2024/02/21/vdo-slam/</url>
    <content><![CDATA[<p>Zhang, Jun, Mina Henein, Robert Mahony, and Viorela Ila. “VDO-SLAM: A Visual Dynamic Object-Aware SLAM System,” 2020.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者提出了VDO-SLAM (Visual Dynamic Object-aware SLAM)，一个基于Stereo/RGB-D 相机的<strong>动态SLAM 系统</strong>，利用图像语义信息同时实现<strong>机器人定位</strong>、<strong>静动态结构制图</strong>，并在场景中<strong>跟踪物体的运动</strong>。本文的贡献如下：</p>
<ul>
<li>将动态场景建模为一个<strong>统一的估计框架</strong>，包括机器人位姿、静动态3D 点以及物体运动；</li>
<li>对动态物体 SE(3) <strong>位姿变换</strong>的精确估计，并提取<strong>物体速度</strong>；</li>
<li>一个利用语义信息来<strong>跟踪移动物体</strong>的鲁棒方法，且能够处理由语义分割失败导致的<strong>间接遮挡</strong>。</li>
</ul>
<span id="more"></span>
<h1 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3 Methodology"></a>3 Methodology</h1><h2 id="3-1-Background-and-Notation"><a href="#3-1-Background-and-Notation" class="headerlink" title="3.1 Background and Notation"></a>3.1 Background and Notation</h2><p>本文的符号表示如Fig. 2所示，本文利用<strong>光流</strong>来发掘连续帧之间的联系。</p>
<p><img src="/2024/02/21/vdo-slam/fig2.png" alt="fig2" title="figure 2"></p>
<p>作者根据<strong>物体特征点在物体上的位置不变</strong>的性质，得到<strong>特征点</strong>在全局参考坐标系下的运动模型：</p>
<p><img src="/2024/02/21/vdo-slam/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$^0_{k-1}H_k\in SE(3)$ 表示物体特征点在全局参考坐标系下的<strong>位姿转换（相当于物体的位姿变换）</strong>。上式是本文运动估计的<strong>核心</strong>所在，因为它从物体特征点的角度描述了物体的位姿变换，而不需要将物体的3D 位姿设定为一个随机变量。</p>
<h2 id="3-2-Camera-Pose-and-Object-Motion-Estimation"><a href="#3-2-Camera-Pose-and-Object-Motion-Estimation" class="headerlink" title="3.2 Camera Pose and Object Motion Estimation"></a>3.2 Camera Pose and Object Motion Estimation</h2><h3 id="3-2-1-Camera-Pose-Estimation"><a href="#3-2-1-Camera-Pose-Estimation" class="headerlink" title="3.2.1 Camera Pose Estimation"></a>3.2.1 Camera Pose Estimation</h3><p>通过<strong>最小重投影误差</strong>来估计相机位姿：</p>
<p><img src="/2024/02/21/vdo-slam/f7.png" alt="f7" title="formula 7"></p>
<p>其中，$^0\mathbf{m}^i_{k-1}$ 表示在时间k-1 观测到的静态3D 点；$^{I_k}\tilde{\mathbf{p}}^i_k$ 表示在图片 $I_k$ 中相应的2D 点。</p>
<h3 id="3-2-2-Object-Motion-Estimation"><a href="#3-2-2-Object-Motion-Estimation" class="headerlink" title="3.2.2 Object Motion Estimation"></a>3.2.2 Object Motion Estimation</h3><p>相似地，利用重投影误差来求解物体的运动估计 $^0_{k-1}H_k$ ：</p>
<p><img src="/2024/02/21/vdo-slam/f10.png" alt="f10" title="formula 10"></p>
<h3 id="3-2-3-Joint-Estimation-with-Optical-Flow"><a href="#3-2-3-Joint-Estimation-with-Optical-Flow" class="headerlink" title="3.2.3 Joint Estimation with Optical Flow"></a>3.2.3 Joint Estimation with Optical Flow</h3><p>跟踪移动物体上的特征点难度很大，比如当物体运动较大或相机距离物体较远的情况。因此，本文提出的技术目标在于同时对<strong>光流估计</strong>和<strong>运动估计</strong>进行优化。</p>
<h2 id="3-3-Graph-Optimization"><a href="#3-3-Graph-Optimization" class="headerlink" title="3.3 Graph Optimization"></a>3.3 Graph Optimization</h2><p>作者将动态SLAM 问题建模为一个<strong>因子图优化</strong>，如Fig. 3所示，该因子图包含<strong>四种类型的观测信息</strong>：</p>
<ol>
<li>3D 点测量（白色圆圈）</li>
<li>视觉里程计观测（黄色圆圈）</li>
<li>动态物体特征点运动（品红色圆圈）：同一个动态物体上的特征点的位姿转换相同</li>
<li>物体平滑运动观测（靛蓝色圆圈）：考虑相机帧率、物理规则会阻止相对大型物体（汽车）快速剧烈的运动，因此引入该平滑运动因子来最小化物体连续运动的改变</li>
</ol>
<p><img src="/2024/02/21/vdo-slam/fig3.png" alt="fig3" title="figure 3"></p>
<h1 id="4-System"><a href="#4-System" class="headerlink" title="4 System"></a>4 System</h1><p>系统整体框架如Fig. 4所示，系统主要包含三个部分：Pre-processing， Tracking 以及 Mapping。</p>
<p><img src="/2024/02/21/vdo-slam/fig4.png" alt="fig4" title="figure 4"></p>
<h2 id="4-1-Pre-processing"><a href="#4-1-Pre-processing" class="headerlink" title="4.1 Pre-processing"></a>4.1 Pre-processing</h2><p>该模块需要满足两个挑战：</p>
<ol>
<li>区分静态背景与物体；</li>
<li>确保对动态物体的长期跟踪。</li>
</ol>
<p>为了实现目标，作者使用<strong>实例分割</strong>与<strong>稠密光流估计</strong>。</p>
<p>其中，作者利用稠密光流估计来最大化<strong>动态物体的跟踪点数量</strong>，本方法利用稠密光流在语义掩码中的所有点进行采样来大幅增加物体特征点的数量；此外，稠密光流法通过对同一物体掩码内的所有点赋予一个独特的<strong>物体识别码</strong>，实现同时跟踪多个物体，且在实例分割失败的情况下可以<strong>恢复物体掩码</strong>。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 A Unified Framework for Mutual Improvement of SLAM and Semantic Segmentation</title>
    <url>/2024/02/21/wang2019/</url>
    <content><![CDATA[<p>Wang, Kai, Yimin Lin, Luowei Wang, Liming Han, Minjie Hua, Xiang Wang, Shiguo Lian, and Bill Huang. “A Unified Framework for Mutual Improvement of SLAM and Semantic Segmentation.” In <em>2019 International Conference on Robotics and Automation (ICRA)</em>, 5224–30. Montreal, QC, Canada: IEEE, 2019. <a href="https://doi.org/10.1109/ICRA.2019.8793499">https://doi.org/10.1109/ICRA.2019.8793499</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献 总结如下：</p>
<ul>
<li>提出了一个<strong>同时增强</strong>vSLAM 和语义分割的统一框架；</li>
<li>通过识别并处理<strong>移动物体</strong>和<strong>潜在动态物体</strong>来增强制图与定位的<strong>精度</strong>；</li>
<li>提出一个利用3D 位姿来有效<strong>优化语义分割</strong>的策略。</li>
</ul>
<span id="more"></span>
<h1 id="3-Framework"><a href="#3-Framework" class="headerlink" title="3 Framework"></a>3 Framework</h1><h2 id="3-1-Overall-Workflow"><a href="#3-1-Overall-Workflow" class="headerlink" title="3.1 Overall Workflow"></a>3.1 Overall Workflow</h2><p>系统框架如图一所示，系统以RGBD 图像作为输入，主要包含两个模块：<strong>vSLAM</strong> 以及<strong>语义分割模块</strong>。</p>
<p><img src="/2024/02/21/wang2019/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-2-Initial-Segmentation"><a href="#3-2-Initial-Segmentation" class="headerlink" title="3.2 Initial Segmentation"></a>3.2 Initial Segmentation</h2><p>本系统使用FCIS 进行<strong>初始化（粗糙coarse）分割</strong>，FCIS 可以计算每个物体的bbox，如果bbox 内的像素值the pixel value大于一定阈值，则被视为物体的一部分，否则被标记为背景。分割之后可以得到所有的<strong>先验动态物体</strong>。</p>
<h2 id="3-3-vSLAM-based-on-Segmentation-Result"><a href="#3-3-vSLAM-based-on-Segmentation-Result" class="headerlink" title="3.3 vSLAM based on Segmentation Result"></a>3.3 vSLAM based on Segmentation Result</h2><p>作者使用ORB-SLAM2来处理RGBD 图片。对于每一帧新的图片，提取ORB 特征并将其<strong>与深度图对齐</strong>以得到其3D 坐标，然后根据<strong>最小重投影误差</strong>计算<strong>初始（粗糙coarse）位姿估计</strong>。</p>
<p>在初始位姿估计基础上来判断物体的<strong>运动状态</strong>：作者根据分割结果将图片像素分为A组：代表背景区域，$\{B_i|i=1…n\}$ 表示不同的物体区域；根据初始位姿估计，计算相应匹配点在当前帧中的投影位置，根据两者之间的欧式距离判断该点是否是动态特征点；若组 $B_i$ 内判定为动态特征点的数量超过一个阈值，则判定该物体为<strong>动态物体</strong>。</p>
<p><img src="/2024/02/21/wang2019/fig2.png" alt="fig2" title="figure 2"></p>
<p>在识别出动态物体后，根据背景A及静态物体 $\{B_s\}$ 上的特征点重新计算重投影误差，得到<strong>精确的位姿估计</strong>；然后，将该精确位姿传递至分割模块，利用精确的位姿估计实现对<strong>语义分割的优化</strong>。</p>
<p>在vSLAM 模块中创建并维护两个地图：<strong>跟踪地图</strong>和<strong>长期地图</strong>。跟踪地图用于跟踪过程中计算相机的轨迹，利用精确的位姿估计将<strong>背景特征点</strong>和<strong>静态物体特征点</strong>添加进跟踪地图中。而长期地图是用于长期使用的，如回环检测等，因此需要将更加稳定的特征点添加进长期地图中，在本系统中<strong>只考虑背景中</strong>的特征点。</p>
<h2 id="3-4-Refinement-of-Segmentation-Result"><a href="#3-4-Refinement-of-Segmentation-Result" class="headerlink" title="3.4 Refinement of Segmentation Result"></a>3.4 Refinement of Segmentation Result</h2><p>利用前一帧图片的精确位姿估计 $(R_f, T_f)$ 和分割结果，结合当前帧的初始位姿估计 $(R_c, T_c)$ 可以实现对<strong>当前帧分割结果的优化</strong>。</p>
<p>将前一帧分割区域的每个点 $(p_u, p_v)$ 投影至当前帧 $(p_u’, p_v’)$ 处，转换过程如下所示：</p>
<p><img src="/2024/02/21/wang2019/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$R = R_c^{-1}R_f, T = T_f - T_x$ 代表两帧之间的相对位姿变换。然后采用如下策略实现对当前帧初始位姿估计的优化，值得注意的是，本策略的前提是基于连续两帧图片<strong>不会发生剧烈改变的假设</strong>。</p>
<p><img src="/2024/02/21/wang2019/a1.png" alt="a1" title="algorithm 1"></p>
<p><img src="/2024/02/21/wang2019/f5.png" alt="f5" title="formula 5"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 QuadricSLAM_Dual Quadrics From Object Detections as Landmarks in Object-Oriented SLAM</title>
    <url>/2024/02/21/quadricslam/</url>
    <content><![CDATA[<p>Nicholson, Lachlan, Michael Milford, and Niko Sunderhauf. “QuadricSLAM: Dual Quadrics From Object Detections as Landmarks in Object-Oriented SLAM.” <em>IEEE Robotics and Automation Letters</em> 4, no. 1 (January 2019): 1–8. <a href="https://doi.org/10.1109/LRA.2018.2866205">https://doi.org/10.1109/LRA.2018.2866205</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者认为语义地图应该是<strong>面向对象的 object-oriented</strong>，即将对象视为地图的中心实体，而<strong>二次曲面</strong>（如椭球体 ellipsoids）有着很多具有吸引力的特性来作为面向对象语义地图的地标表示方法，如Fig. 1 所示，二次曲面可以紧凑表示，且可在投影集合框架中实现高效操纵；二次曲面可以表示物体的<strong>尺寸、位置及朝向信息</strong>，而且如果必要的话，可以作为更详细3D 重建的anchors。此外，二次曲面表示形式在整合角度来看也具有吸引力，作者会在后文中证明，二次曲面可以<strong>直接利用物体检测bbox</strong> 进行构建，而且很方便整合进<strong>基于因子图的SLAM 框架</strong>中。</p>
<span id="more"></span>
<p><img src="/2024/02/21/quadricslam/image-20240221095315076.png" alt="image-20240221095315076" title="figure 1"></p>
<p>本文做出的贡献：</p>
<ol>
<li>将SLAM 中的物体地标使用<strong>受限二次曲面</strong> constrained dual quadrics 进行<strong>参数化表示</strong>；</li>
<li>作者将物体检测网络（如YOLOv3）视为SLAM系统中的传感器，将其观测（物体 bbox）通过<strong>几何误差方程</strong>来直接约束二次曲面参数；</li>
<li>为了将二次曲面参数表示整合进SLAM 系统中，作者构建了一个基于因子图的SLAM 方程，可联合估计二次曲面和载体位姿参数（在假设解决了物体关联前提下 assuming solved data association）。</li>
</ol>
<h1 id="3-Dual-Quadrics——Fundamental-Concepts"><a href="#3-Dual-Quadrics——Fundamental-Concepts" class="headerlink" title="3 Dual Quadrics——Fundamental Concepts"></a>3 Dual Quadrics——Fundamental Concepts</h1><h2 id="3-1-Dual-Quadrics"><a href="#3-1-Dual-Quadrics" class="headerlink" title="3.1 Dual Quadrics"></a>3.1 Dual Quadrics</h2><p>二次曲面是3D 空间中的曲面，可使用<strong>4x4 对称矩阵</strong> $Q$ 来表示；in dual form，一个二次曲面可以使用一组正切平面来定义，这些平面组成一个二次曲面的包络envelope，因此，对偶二次曲面dual quadric $Q^\ast$ 定义为满足 $\pi^TQ^\ast\pi = 0$ 的所有平面 $\pi$ 。二次曲面包含的立方体包括球体、椭球体、双曲面、圆锥体和圆柱体。</p>
<p>二次曲面包含<strong>9自由度</strong>，对应于4x4 对称矩阵 $Q$ 中10个独立的元素，多余的一个元素表示尺寸信息。因此，可以使用10维向量来表示一个<strong>对偶二次曲面</strong>：$\hat{\mathbf{q}} = (\hat{q}_1, …, \hat{q}_{10})$ ，其中的每个元素对应于 $Q^\ast$ 的10个独立元素。</p>
<p>对偶二次曲面投影至图像平面上为一个对偶二次曲线：$\mathbf{C}^\ast = \mathbf{P}\mathbf{Q}^\ast\mathbf{P}^T$ ，其中 $\mathbf{P} = \mathbf{K}[\mathbf{R} | \mathbf{t}]$ 为相机投影矩阵，包含内参和外参。二次曲面包含的曲线包括圆、椭圆、抛物线和双曲线。</p>
<h2 id="3-2-Constrained-Dual-Quadric-Parameterization"><a href="#3-2-Constrained-Dual-Quadric-Parameterization" class="headerlink" title="3.2 Constrained Dual Quadric Parameterization"></a>3.2 Constrained Dual Quadric Parameterization</h2><p>由于二次曲面可以是闭合曲面也可以是非闭合曲面（如抛物面和双曲面），只有<strong>闭合曲面</strong>才可以表示有意义的物体地标，所以作者使用<strong>受限对偶二次曲面</strong>表示形式来确保曲面是闭合的椭球体或球体。作者将对偶二次曲面表示为：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221104708225.png" alt="image-20240221104708225" title="formula 1"></p>
<p>其中，$\overset{\frown}{\mathbf{Q}}^\ast$ 为一个中心位于原点的椭球体，$\mathbf{Z}$ 表示齐次变换矩阵，包含旋转和平移：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221105215815.png" alt="image-20240221105215815" title="formula 2"></p>
<p>其中，$\mathbf{t} = (t_1, t_2, t_3)$ 为平移向量；$\mathbf{R}(\mathbf{\theta})$ 为由角度 $\mathbf{\theta} = (\theta_1, \theta_2, \theta_3)$ 定义的旋转矩阵；$\mathbf{s} = (s_1, s_2, s_3)$ 为椭球体三个轴的尺度参数。在后续中，作者使用9维向量表示一个<strong>受限对偶二次曲面</strong>：$\mathbf{q} = (\theta_1, \theta_2, \theta_3, t_1, t_2, t_3, s_1, s_2, s_3)$ ，并根据式1来重建完整的对偶二次曲面 $\mathbf{Q}^\ast$ 。</p>
<h1 id="4-A-Sensor-Model-for-Image-Based-Object-Detections"><a href="#4-A-Sensor-Model-for-Image-Based-Object-Detections" class="headerlink" title="4 A Sensor Model for Image-Based Object Detections"></a>4 A Sensor Model for Image-Based Object Detections</h1><h2 id="4-1-Motivation"><a href="#4-1-Motivation" class="headerlink" title="4.1 Motivation"></a>4.1 Motivation</h2><p>作者的目标是将SOTA 物体检测器<strong>作为一个传感器</strong>整合进SLAM中，因此，需要构建一个<strong>传感器模型</strong>，在给定预估相机位姿 $\mathbf{x}_i$ 和预估地图结构（如二次曲面参数 $\mathbf{q}_j$ ）时可以预测物体检测器的观测参数。物体检测器的观测包含物体检测bbox 和对应的物体标签，本文主要关注bbox 参数，作者使用 $\mathbf{b} = (x_{min}, y_{min}, x_{max}, y_{max})$ 来表示bbox ，因此，该传感器模型可表示为：$\beta(\mathbf{x}_i, \mathbf{q}_j) = \hat{\mathbf{b}}_{ij}$ ，即根据相机位姿 $\mathbf{x}_i$ 和二次曲面参数 $\mathbf{q}_j$ 来预测bbox 观测 $\hat{\mathbf{b}}_{ij}$ 。</p>
<p>在构建了该传感器模型后，作者可以<strong>在预测和观测之间形成几何误差项</strong>，这对于本文提出的SLAM 系统至关重要。</p>
<h2 id="4-2-Deriving-the-Object-Detection-Sensor-Model-beta"><a href="#4-2-Deriving-the-Object-Detection-Sensor-Model-beta" class="headerlink" title="4.2 Deriving the Object Detection Sensor Model $\beta$"></a>4.2 Deriving the Object Detection Sensor Model $\beta$</h2><p>将二次曲面 $\mathbf{q}_j$ 根据相机位姿 $\mathbf{x}_i$ 投影至图像中：$\mathbf{C}^\ast_{ij} = \mathbf{P}_i\mathbf{Q}^\ast_{(q_j)}\mathbf{P}^T_i$ ，其中 $\mathbf{P} = \mathbf{K}[\mathbf{R} | \mathbf{t}]$ 为相机投影矩阵，包含相机内参 $\mathbf{K}$ 和相机位姿；由此获得对偶双曲线 $\mathbf{C}^\ast$ ，并进一步获取其原始形式 $\mathbf{C}$ 。简单的模型会直接计算二次曲线 $\mathbf{C}$ 的闭合bbox，并将该bbox 截断来适应图片边界，如Fig. 2（a）所示，当该二次曲线大部分位于图像边界外时，二次曲线 $\mathbf{C}$ 在图片内对应的bbox（蓝色虚线）与真值bbox （绿实线）之间存在较大的差异，即该方法会引入极大的误差。</p>
<p><img src="/2024/02/21/quadricslam/image-20240221112358306.png" alt="image-20240221112358306" title="figure 2"></p>
<p>一个准确的传感器模型需要获取二次曲线和图片边界的交点，对于物体检测bbox 的正确预测应该是最小轴对齐的矩形，该矩形在将图片尺寸内的二次曲线完全包括进去，该矩形bbox 表示为：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221113652000.png" alt="image-20240221113652000" title="formula 3"></p>
<h2 id="4-3-Calculating-the-On-Image-Conic-Bounding-Box"><a href="#4-3-Calculating-the-On-Image-Conic-Bounding-Box" class="headerlink" title="4.3 Calculating the On-Image Conic Bounding Box"></a>4.3 Calculating the On-Image Conic Bounding Box</h2><p>通过以下算法来获取图片中双曲线的正确bbox：</p>
<ol>
<li>寻找该二次曲线上包含极值坐标的四个点 $\{\mathbf{p}_1, …, \mathbf{p}_4\}$ ；</li>
<li>寻找该二次曲线与图片边界的交点（最多8个） $\{\mathbf{p}_5, …, \mathbf{p}_{12}\}$ ；</li>
<li>移除 $\mathcal{P} = \{\mathbf{p}_1, …, \mathbf{p}_4\}$  中所有的non-real 点和位于图片边界外的点；</li>
<li>从 $\mathcal{P}$ 中的剩余点获取极值坐标点。</li>
</ol>
<p>经过以上步骤，可以得到向量：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221114446827.png" alt="image-20240221114446827" title="formula 4"></p>
<p>上式表示的bbox 即可完整包含二次曲线 $\mathbf{C}$ 在图片可见区域的部分。</p>
<h1 id="5-SLAM-With-Dual-Quadric-Landmark-Representations"><a href="#5-SLAM-With-Dual-Quadric-Landmark-Representations" class="headerlink" title="5 SLAM With Dual Quadric Landmark Representations"></a>5 SLAM With Dual Quadric Landmark Representations</h1><h2 id="5-1-General-Problem-Setup"><a href="#5-1-General-Problem-Setup" class="headerlink" title="5.1 General Problem Setup"></a>5.1 General Problem Setup</h2><p>作者构建如下的SLAM 问题方程：$\mathbf{x}_{i+1} = f(\mathbf{x}_i, \mathbf{u}_i) + \mathbf{w}_i$ ，其中，$\mathbf{u}_i$ 表示连续两个位姿 $\mathbf{x}_i, \mathbf{x}_{i+1}$ 之间的里程计观测；$f$ 通常为非线性方程；$\mathbf{w}_i$ 为零均值高斯分布，协方差为 $\sum_i$ 。此处不具体讨论里程计观测 $\mathbf{u}_i$ 的具体获取形式。</p>
<p>此外，还有物体路标观测 $B = \{\mathbf{b}_{ij}\}$ ，表示在位姿 $\mathbf{x}_i$ 处对物体 $j$ 观测到的bbox 。</p>
<blockquote>
<p> 值得注意的是，作者在此假设数据关联问题已经得到解决。</p>
</blockquote>
<h2 id="5-2-Building-and-Solving-a-Factor-Graph-Representation"><a href="#5-2-Building-and-Solving-a-Factor-Graph-Representation" class="headerlink" title="5.2 Building and Solving a Factor Graph Representation"></a>5.2 Building and Solving a Factor Graph Representation</h2><p>条件概率分布可表示为因子式：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221150906286.png" alt="image-20240221150906286" title="formula 5"></p>
<p>包含：载体位姿 $X = \{\mathbf{x}_i\}$ ，地标  $Q= \{\mathbf{q}_i\}$ ，里程计观测 $U= \{\mathbf{u}_i\}$，物体观测 $B= \{\mathbf{b}_{ij}\}$。</p>
<p>里程计因子 $P(\mathbf{x}_{i+1} | \mathbf{x}_i, \mathbf{u}_i)$ 假设服从高斯分布，即 $\mathbf{x}_{i+1} \sim \mathcal{N}(f(\mathbf{x}_i, \mathbf{u}_i), \sum_i)$ 。为了将地标因子集成至高斯因子图中，应用贝叶斯法则：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221152014709.png" alt="image-20240221152014709" title="formula 6"></p>
<p>因为目标是求取MAP，所以忽略分母项（只发挥归一化作用）。进一步地，作者假设 $P(\mathbf{q}_{j} | \mathbf{x}_i)$ 服从均匀分布，则式6可转化为求取最大似然 $P(\mathbf{b}_{ij} | \mathbf{q}_j, \mathbf{x}_i)$ 。该似然项可建模为高斯分布 $\mathcal{N}(\beta_{(\mathbf{x}_i, \mathbf{q}_j)}, \Lambda_{ij})$ ，其中，$\beta$ 为上节构建的传感器观测模型，$\Lambda$ 为协方差矩阵，表示观测到的检测物体在图片空间中的不确定性。</p>
<p>式5的解可视为求解<strong>非线性最小二乘问题</strong>：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221153522001.png" alt="image-20240221153522001" title="formula 7"></p>
<p>其中，第一项的差值是SE(3) 空间的差值。</p>
<h2 id="5-3-The-Geometric-Error-Term"><a href="#5-3-The-Geometric-Error-Term" class="headerlink" title="5.3 The Geometric Error Term"></a>5.3 The Geometric Error Term</h2><p>式7的第二项 $||\mathbf{b}_{ij} - \beta_{(\mathbf{x}_{i}, \mathbf{q}_{j})}||^2_{\Lambda_{ij}}$ 是一个几何误差项，因为 $\mathbf{b}, \beta$ 都是包含像素坐标的向量，作者发现该几何误差项要比之前工作中的代数误差项更具优势。且，可以通过协方差矩阵项 $\Lambda_{ij}$ 更方便地将物体检测器的<strong>空间不确定性</strong>传递至SLAM 系统中。</p>
<h2 id="5-4-Variable-Initialization"><a href="#5-4-Variable-Initialization" class="headerlink" title="5.4 Variable Initialization"></a>5.4 Variable Initialization</h2><p>载体位姿 $\mathbf{x}_i$ 可通过里程计观测值 $\mathbf{u}_i$ 进行初始化，接下来讨论对偶二次曲面地标 $\mathbf{q}_j$ 的初始化。可通过最小二乘满足以下定义式来进行初始化：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221155058842.png" alt="image-20240221155058842" title="formula 8"></p>
<p>其中，$\hat{\mathbf{q}_j}$ 为3.1节的通用对偶二次曲面：$\hat{\mathbf{q}} = (\hat{q}_1, …, \hat{q}_{10})$ 。</p>
<p>可使用地标bbox 观测 $\mathbf{b}_{ij}$ 和对应的线 $\mathbf{l}_{ijk}$ （bbox的4条线）来定义平面 $\mathbf{\pi}_{ijk}$ : $\mathbf{\pi}_{ijk} = \mathbf{P}^T_i \mathbf{l}_{ijk}$，其中，相机位姿 $\mathbf{P}_i$ 可通过初始位姿估计获取，最终，式8可转换为：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221160103483.png" alt="image-20240221160103483" title="formula 9"></p>
<p>使用不同视角 $i$ 和平面 $k$ ，基于式9构建线性方程组，通过SVD 分解获取最小二乘解 $\hat{\mathbf{q}}_j$ 。该解是一个通用的二次曲面，并没有被约束为一个椭球体，这里会进一步使用3.2节的方式将其进一步参数化表示，参数求取过程如下所述：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221162842265.png" alt="image-20240221162842265" title="formula 10"></p>
<p>其中，$\mathbf{Q}_{33}$ 为二次曲面矩阵 $\mathbf{Q}$ 的左上侧3x3 子矩阵；$\lambda_i$ 为 $\mathbf{Q}$ 的特征值；旋转矩阵 $\mathbf{R}(\theta)$ 为 $\mathbf{Q}_{33}$ 的特征矩阵；平移向量表示为 $\mathbf{t} = (\hat{q}_4, \hat{q}_7, \hat{q}_9)/\hat{q}_{10}$ 。</p>
<h1 id="6-Experiments-and-Evaluation"><a href="#6-Experiments-and-Evaluation" class="headerlink" title="6 Experiments and Evaluation"></a>6 Experiments and Evaluation</h1><p>作者在TUM 数据集上进行测试，与两种里程计算法进行比较，分别是Fovis 和 ORB-SLAM2 算法，物体检测器使用YOLOv3网络；里程计噪声模型中的平移和旋转标准差均设置为0.001，物体bbox 的标准差是计算该物体所有观测bbox 长与宽的标准差之和。定位误差ATE 如Table 1所示，</p>
<p><img src="/2024/02/21/quadricslam/image-20240221171102454.png" alt="image-20240221171102454" title="table 1"></p>
<p>可以发现，相较于fovis 视觉里程计，加入二次曲面物体地标后定位精度提升明显，但较ORB-SLAM2 有所下降，作者认为原因可能有两个：</p>
<ol>
<li>bbox 观测噪声不太符合高斯分布；</li>
<li>物体遮挡导致过小的bbox观测，从而影响定位精度。</li>
</ol>
<p>作者通过进一步实验：对于fr1_desk2, fr3_office 数据序列，作者舍弃宽高标准差大于一定值的物体bbox ，从而实现两个数据序列的定位精度分别达到0.0239和0.0087。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Pose Optimization</tag>
        <tag>Object SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 SaD-SLAM_A Visual SLAM Based on Semantic and Depth Information</title>
    <url>/2024/02/22/sad-slam/</url>
    <content><![CDATA[<p>Yuan, Xun, and Song Chen. “SaD-SLAM: A Visual SLAM Based on Semantic and Depth Information.” In <em>2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 4930–35. Las Vegas, NV, USA: IEEE, 2020. <a href="https://doi.org/10.1109/IROS45743.2020.9341180">https://doi.org/10.1109/IROS45743.2020.9341180</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献如下：</p>
<ol>
<li>在 ORB-SLAM2 的基础上提出了一种基于特征的RGB-D SLAM 算法——SaD-SLAM，该算法结合<strong>语义信息</strong>、<strong>几何信息</strong>和<strong>深度信息</strong>，可在<strong>动态环境</strong>中运行良好；</li>
<li>在当前帧和历史帧的特征点之间进行<strong>对极几何约束</strong>，从动态物体及静态但可移动的物体（如椅子等）上提取<strong>静态特征点</strong>，来提高相机位姿估计的准确度与鲁棒性。</li>
</ol>
<span id="more"></span>
<h1 id="3-System-Description"><a href="#3-System-Description" class="headerlink" title="3 System Description"></a>3 System Description</h1><p>本系统利用语义掩码和深度信息实行三步走算法：<strong>位姿初始化</strong>、<strong>移动一致性测试</strong>，以及<strong>位姿细调</strong>。</p>
<p>如Fig. 1所示，作者将室内场景中的特征点分为三类：<strong>动态点</strong>（人）、<strong>静态但可移动点</strong>（椅子），以及<strong>静态点</strong>。</p>
<p><img src="/2024/02/22/sad-slam/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-1-Framework-of-SaD-SLAM"><a href="#3-1-Framework-of-SaD-SLAM" class="headerlink" title="3.1 Framework of SaD-SLAM"></a>3.1 Framework of SaD-SLAM</h2><p>SaD-SLAM 是在ORB-SLAM2 基础上进一步开发的，其系统架构如Fig. 2所示：</p>
<p><img src="/2024/02/22/sad-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-2-Semantic-Segmentation"><a href="#3-2-Semantic-Segmentation" class="headerlink" title="3.2 Semantic Segmentation"></a>3.2 Semantic Segmentation</h2><p>本文使用Mask R-CNN 网络获取输入图片的像素级语义信息，并挑选出<strong>潜在动态物体</strong>及<strong>相对较大的但是可移动的物体</strong>的掩码，对于TUM 数据集而言，这里指的是人与椅子，位于人身上的特征点被标记为<strong>动态特征点</strong>，位于椅子上的特征点被标记为<strong>静态但可移动特征点</strong>，其他物体上的特征点被标记为<strong>静态特征点</strong>。</p>
<h2 id="3-3-Pose-Initialization"><a href="#3-3-Pose-Initialization" class="headerlink" title="3.3 Pose Initialization"></a>3.3 Pose Initialization</h2><p>一些<strong>小物件</strong>（如笔、书等）对于CNN 而言较难识别出来，或者精度较差。作者采取以下策略进行<strong>辅助检测</strong>：对于不属于人或椅子的特征点，检测以自身为中心、半径分别为5像素和10像素的区域内是否有像素属于人和椅子，若存在，且该像素的<strong>深度信息</strong>与本特征点相似，则将本特征点分别标记为动态点和静态但可移动点，其中，动态点（人）的<strong>优先级更高</strong>；以此可在不需要CNN 的情况下方便识别出人类手上的物体等。此外，作者还提到CNN 在<strong>物体边缘</strong>附近的识别精度较差，该方法也可以<strong>弥补</strong>这些错误识别。最终，可让CNN 识别更少的物体种类，实现CNN 的<strong>快速与小型化</strong>。</p>
<p>在实现对物体类别信息获取的基础上，系统利用<strong>静态点</strong>和<strong>静态但可移动点</strong>构建<strong>重投影误差</strong>，实现对位姿的<strong>初始估计</strong>。值得注意的是，这里也使用了静态但可移动特征点进行位姿求解，这是因为这些点更有可能是保持静止的，或者移动速度较慢。</p>
<h2 id="3-4-Moving-Consistency-Testing"><a href="#3-4-Moving-Consistency-Testing" class="headerlink" title="3.4 Moving Consistency Testing"></a>3.4 Moving Consistency Testing</h2><p>作者在<strong>连续5帧</strong>中跟踪<strong>可能移动的特征点</strong>及<strong>静态特征点</strong>，如果当前帧与之前第4帧（即连续5帧中的最前面一帧）中的对应特征点满足<strong>对极几何约束</strong>，则判定连续5帧中的这些点<strong>均为静态点</strong>。这里的对极几何约束与其他文章中的方法类似，判定<strong>匹配点</strong>与<strong>极线</strong>之间的距离是否超过阈值。</p>
<h2 id="3-5-Pose-Fine-tuning"><a href="#3-5-Pose-Fine-tuning" class="headerlink" title="3.5 Pose Fine-tuning"></a>3.5 Pose Fine-tuning</h2><p>作者将特征点的<strong>动态属性</strong>分为以下四种：1. 静态点，2. 动态点， 3. 静态但可移动点， 4. 从动态点或静态但可移动点转换过来的静态点。</p>
<p>当前阶段，已有的输入数据包括：相机初始位姿估计，静态特征点，动态特征点，静态但可移动物体上的特征点，以及利用移动一致性测试检测到的从移动特征点转换的静态特征点。位姿细调过程包含两步：</p>
<ol>
<li>第一步：利用静态特征点、静态但可移动物体上的特征点、利用移动一致性测试检测到的从移动特征点转换的静态特征点进行<strong>最小化重投影误差</strong>来优化相机位姿；</li>
<li>第二步：利用上步得到的相机位姿，将<strong>局部地图</strong>上的更多特征点重投影至当前帧，并保留地图点的动态属性，只选取属于<strong>静态的地图点</strong>与上步中使用的特征点，结合起来进行再一次的相机位姿优化。</li>
</ol>
<h1 id="4-Experimental-Results"><a href="#4-Experimental-Results" class="headerlink" title="4 Experimental Results"></a>4 Experimental Results</h1><p>为了证明移动一致性检验的作用，作者将该模块去掉，只使用先验静态物体上的特征点进行位姿求解，将该方法记为SaD-SLAM—，结果对比如表2所示：</p>
<p><img src="/2024/02/22/sad-slam/t2.png" alt="t2" title="table 2"></p>
<p>同时，作者与DynaSLAM、DS-SLAM 进行对比，结果如下所示：</p>
<p><img src="/2024/02/22/sad-slam/t1.png" alt="t1" title="table 1"></p>
<p><img src="/2024/02/22/sad-slam/t3.png" alt="t3" title="table 3"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 DM-SLAM_A Feature-Based SLAM System for Rigid Dynamic Scenes</title>
    <url>/2024/02/22/dm-slam/</url>
    <content><![CDATA[<p>Cheng, Junhao, Zhi Wang, Hongyan Zhou, Li Li, and Jian Yao. “DM-SLAM: A Feature-Based SLAM System for Rigid Dynamic Scenes.” <em>ISPRS International Journal of Geo-Information</em> 9, no. 4 (April 2020): 202. <a href="https://doi.org/10.3390/ijgi9040202">https://doi.org/10.3390/ijgi9040202</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文做出的贡献：</p>
<ul>
<li>提出一个完整的视觉SLAM 系统——DM-SLAM，该系统结合<strong>实例分割网络</strong>和<strong>光流信息</strong>，可在高动态环境中消除动态物体对位姿估计的影响，且本系统适用于<strong>单目、双目和RGB-D 相机</strong>；</li>
<li>针对RGB-D/双目和单目相机，分别提出<strong>两种高效提取动态点</strong>的策略；</li>
<li>在公开数据集上进行测试，证明了本算法的有效性。</li>
</ul>
<span id="more"></span>
<h1 id="3-System-Introduction"><a href="#3-System-Introduction" class="headerlink" title="3 System Introduction"></a>3 System Introduction</h1><h2 id="3-2-Proposed-Method"><a href="#3-2-Proposed-Method" class="headerlink" title="3.2 Proposed Method"></a>3.2 Proposed Method</h2><p>DM-SLAM 包含四个模块：语义分割、自运动估计、动态点检测，以及基于特征的SLAM 架构。</p>
<h3 id="3-2-1-Overview-of-the-Proposed-Approach"><a href="#3-2-1-Overview-of-the-Proposed-Approach" class="headerlink" title="3.2.1 Overview of the Proposed Approach"></a>3.2.1 Overview of the Proposed Approach</h3><p>本方法的整体架构如Fig. 2所示：</p>
<p><img src="/2024/02/22/dm-slam/fig2.png" alt="fig2" title="figure 2"></p>
<p>Fig. 3展示了本方法的细节，Mask R-CNN 处理过的图片包含像素级的语义信息，本系统<strong>暂时舍弃</strong>属于<strong>先验动态物体</strong>的特征点，利用剩余的特征点进行<strong>粗略的自运动估计</strong>；因为更少的特征点参与位姿解算，而且没有使用局部BA ，所以当前模块获取的初始位姿是<strong>不准确</strong>的。</p>
<p>然后，针对双目/RGB-D 和单目相机采用<strong>不同的</strong>动态点检测策略：</p>
<ul>
<li>针对单目相机，使用<strong>对极几何约束</strong>实现对动态点的检测：计算匹配点距离极线的距离，然后进行判断；</li>
<li>对于双目/RGB-D 相机，可利用三角化或深度图获取特征点的深度信息，将前一帧上的特征点重投影到当前帧，计算<strong>重投影偏差</strong>。</li>
</ul>
<p>之后，利用检测到的动态特征点判断所属物体是否属于动态物体。</p>
<p><img src="/2024/02/22/dm-slam/fig3.png" alt="fig3" title="figure 3"></p>
<h3 id="3-2-3-Ego-Motion-Estimation"><a href="#3-2-3-Ego-Motion-Estimation" class="headerlink" title="3.2.3 Ego-Motion Estimation"></a>3.2.3 Ego-Motion Estimation</h3><p>在暂时剔除掉先验动态特征点之后，本系统采用一个<strong>轻量级的跟踪算法</strong>进行位姿估计，该算法与ORB-SLAM2 不同，不进行局部BA 和新关键帧筛选，只与历史帧的特征点进行匹配，并将关联的地图点投影到当前帧，利用<strong>最小化重投影误差</strong>来获取粗略的初始位姿。</p>
<h3 id="3-2-4-Dynamic-Feature-Points-Extraction"><a href="#3-2-4-Dynamic-Feature-Points-Extraction" class="headerlink" title="3.2.4 Dynamic Feature Points Extraction"></a>3.2.4 Dynamic Feature Points Extraction</h3><p>针对RGB-D/双目相机，作者首先利用<strong>LK 光流</strong>算法获取两帧之间的匹配特征点，然后利用初始位姿估计将前一帧的特征点投影至当前帧，由于初始位姿估计的精度问题，必然会存在一定的<strong>重投影偏差向量</strong>，如Fig. 5、Fig. 6所示。</p>
<p><img src="/2024/02/22/dm-slam/fig5.png" alt="fig5" title="figure 5"></p>
<p><img src="/2024/02/22/dm-slam/fig6.png" alt="fig6" title="figure 6"></p>
<p>可以看到，即便初始位姿估计精度较差，但仍然可以从重投影偏差向量识别出静态点与动态点。基于此，作者使用静态区域（去除掉先验动态物体区域）的特征点，经加权平均求得<strong>阈值</strong>：</p>
<p><img src="/2024/02/22/dm-slam/f6.png" alt="f6" title="formula 6"></p>
<p><img src="/2024/02/22/dm-slam/f7.png" alt="f7" title="formula 7"></p>
<p>其中，$\epsilon$ 表示偏差向量的模值；$\theta$ 表示角度。</p>
<p>根据计算出的阈值，判断先验动态物体区域内的特征点是否属于动态点：</p>
<p><img src="/2024/02/22/dm-slam/f8.png" alt="f8" title="formula 8"></p>
<p>对于单目相机，简单地使用<strong>对极约束</strong>来提取动态点：</p>
<p><img src="/2024/02/22/dm-slam/f9.png" alt="f9" title="formula 9"></p>
<p>其中，$q_i, p_i$ 分别是当前帧和历史帧的匹配特征点；F 是基础矩阵。对当前帧的每一点计算其<strong>与极线之间的距离</strong>：</p>
<p><img src="/2024/02/22/dm-slam/f10.png" alt="f10" title="formula 10"></p>
<p>利用该距离判断是否属于动态点。</p>
<p>在得到动态点之后，根据物体掩码内的动态点数量判断该物体是否是动态的。</p>
<p>作者比较了本方法与DS-SLAM、DynaSLAM 的优势：</p>
<ul>
<li>DS-SLAM 只将人类作为典型的动态物体代表，而本方法将20种物体作为潜在动态物体；且DS-SLAM 不能处理立体相机数据；</li>
<li>DynaSLAM 直接将潜在动态物体判定为动态的，与现实不符；且该方法只适用于RGB-D 数据采用多视角几何进行动态点检测。</li>
</ul>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 RDS-SLAM_Real-Time Dynamic SLAM Using Semantic Segmentation Methods</title>
    <url>/2024/02/23/rds-slam/</url>
    <content><![CDATA[<p>Liu, Yubao, and Jun Miura. “RDS-SLAM: Real-Time Dynamic SLAM Using Semantic Segmentation Methods.” <em>IEEE Access</em> 9 (2021): 23772–85. <a href="https://doi.org/10.1109/ACCESS.2021.3050617">https://doi.org/10.1109/ACCESS.2021.3050617</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献如下：</p>
<ol>
<li>提出一个基于语义的<strong>实时动态vSLAM 算法</strong>——RDS-SLAM，该算法的跟踪线程不需要等待语义结果，可在保持实时的情况下高效利用语义分割结果进行<strong>动态物体检测和外点剔除</strong>；</li>
<li>作者提出了一种<strong>关键帧选取策略</strong>，使得在利用任意语义分割方法的情况下，尽可能使用<strong>最新的语义信息</strong>进行外点剔除；</li>
<li>在TUM 数据集上证明了本方法的实时性能。</li>
</ol>
<span id="more"></span>
<p>Fig .2 是利用语义信息进行动态特征点检测方法常用的模式，该模式会为了获取语义信息而阻挡跟踪、制图等线程，作者称这种模式为<strong>blocked model</strong>。</p>
<p><img src="/2024/02/23/rds-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3 System Overview"></a>3 System Overview</h1><p>本系统的架构如Fig. 3所示，语义线程与其他线程<strong>并行运行</strong>，因此不会阻挡跟踪线程的运行，并将语义信息保存至atlas 中。语义标签用于产生先验动态物体的掩码，并利用语义信息更新与关键帧特征点相匹配的<strong>地图点的移动概率</strong>，最终，使用atlas 中的语义信息优化相机位姿。</p>
<p>atlas 管理两种地图：<strong>动态地图</strong> active map 与<strong>非动态地图</strong> non-active map，当相机跟踪失败且数帧重定位失败时，动态地图转换为非动态地图，并初始化一个新的地图。在atlas 中，利用共视图和spanning tree 管理关键帧和地图点。</p>
<p><img src="/2024/02/23/rds-slam/fig3.png" alt="fig3" title="figure 3"></p>
<h1 id="4-Semantic-Thread"><a href="#4-Semantic-Thread" class="headerlink" title="4 Semantic Thread"></a>4 Semantic Thread</h1><p>语义线程的过程如Fig. 4所示，作者将选取关键帧序列KF 中的关键帧去请求语义标签的过程称为<strong>semantic keyframe selection 语义关键帧选取过程</strong>。</p>
<p><img src="/2024/02/23/rds-slam/fig4.png" alt="fig4" title="figure 4"></p>
<p>语义线程的实施细节如算法1所示：</p>
<p><img src="/2024/02/23/rds-slam/a1.png" alt="a1" title="algorithm 1"></p>
<h2 id="4-1-Semantic-Keyframe-Selection-Algorithm"><a href="#4-1-Semantic-Keyframe-Selection-Algorithm" class="headerlink" title="4.1 Semantic Keyframe Selection Algorithm"></a>4.1 Semantic Keyframe Selection Algorithm</h2><p>作者定义<strong>semantic delay 语义延迟</strong>来表示具有最新语义信息的帧id 与当前帧id 的距离：</p>
<p><img src="/2024/02/23/rds-slam/f1.png" alt="f1" title="formula 1"></p>
<p>Fig. 7展示了本系统的关键帧选取策略，作者选取关键帧序列KF 中的<strong>头尾关键帧</strong>进行语义信息获取：选取KF 尾部的关键帧进行语义获取可以使得系统可以获取<strong>相对新的语义信息</strong>；选取KF 头部关键帧的原因在于，与blocked model 不同的是，本系统会缺失前面几帧图片的语义信息，由于跟踪线程处理速度远快于语义分割线程速度，因此前期会因为动态物体的存在而积累大量的误差，因此需要利用前期关键帧中的语义信息来<strong>矫正相机位姿</strong>。</p>
<p><img src="/2024/02/23/rds-slam/fig7.png" alt="fig7" title="figure 7"></p>
<h2 id="4-3-Semantic-Mask-Generation"><a href="#4-3-Semantic-Mask-Generation" class="headerlink" title="4.3 Semantic Mask Generation"></a>4.3 Semantic Mask Generation</h2><p>本系统将实例分割的二维掩码图片融合进一张掩码图片中，如Fig. 8所示，然后利用该掩码图片计算地图点的<strong>先验移动概率</strong>；为了减小因分割精度不足导致部分边缘动态点检测失败的情况，作者使用<strong>掩码膨胀策略</strong>将属于物体边缘的特征点也囊括进去，如Fig. 9所示。</p>
<p><img src="/2024/02/23/rds-slam/fig8.png" alt="fig8" title="figure 8"></p>
<p><img src="/2024/02/23/rds-slam/fig9.png" alt="fig9" title="figure 9"></p>
<h2 id="4-4-Moving-Probability-Update"><a href="#4-4-Moving-Probability-Update" class="headerlink" title="4.4 Moving Probability Update"></a>4.4 Moving Probability Update</h2><p>本系统利用<strong>移动概率</strong>来将语义信息从语义线程传递至跟踪线程，该移动概率被用于跟踪线程的<strong>外点检测和剔除</strong>。</p>
<h3 id="4-4-1-Definition-of-Moving-Probability"><a href="#4-4-1-Definition-of-Moving-Probability" class="headerlink" title="4.4.1 Definition of Moving Probability"></a>4.4.1 Definition of Moving Probability</h3><p>由于CNN 存在准确性、鲁棒性的问题，所以不能简单使用单帧图片的结果进行判定；针对该问题，作者考虑使用多帧图片的<strong>空间-时间一致性</strong>来实现对动态特征点的精确检测，因此，使用移动概率来利用<strong>连续关键帧</strong>的语义信息。</p>
<p><img src="/2024/02/23/rds-slam/fig11.png" alt="fig11" title="figure 11"></p>
<h3 id="4-4-2-Definition-of-Observed-Moving-Probability"><a href="#4-4-2-Definition-of-Observed-Moving-Probability" class="headerlink" title="4.4.2 Definition of Observed Moving Probability"></a>4.4.2 Definition of Observed Moving Probability</h3><p>由于分割结果不是100%准确的，因此，作者定义<strong>观测移动概率</strong>：</p>
<p><img src="/2024/02/23/rds-slam/f1-1.png" alt="f1-1" title="formula 1-1"></p>
<p>其中，$\alpha, \beta$ 的值是与语义分割网络的精度相关的。本系统中，将两者的值设定为0.9，即认为语义分割的结果是<strong>相对可靠</strong>的。</p>
<h3 id="4-4-3-Moving-Probability-Update"><a href="#4-4-3-Moving-Probability-Update" class="headerlink" title="4.4.3 Moving Probability Update"></a>4.4.3 Moving Probability Update</h3><p>当前时间的移动概率 $bel(m_t)$ 是基于语义观测 $z_{1:t}$ 和初始状态 $m_0$ 进行预测的，利用<strong>贝叶斯滤波器</strong>进行更新：</p>
<p><img src="/2024/02/23/rds-slam/f2.png" alt="f2" title="formula 2"></p>
<h1 id="5-Tracking-Thread"><a href="#5-Tracking-Thread" class="headerlink" title="5 Tracking Thread"></a>5 Tracking Thread</h1><p>算法2展示了在跟踪上帧模型中的数据关联方法，系统会设定一个<strong>阈值</strong> $\tau$ ，<strong>优先使用</strong>静态特征点进行位姿解算；如果静态特征点的数量小于该阈值，会进一步使用<strong>未知状态的特征点</strong>进行位姿解算。</p>
<p><img src="/2024/02/23/rds-slam/a2.png" alt="a2" title="algorithm 2"></p>
<h1 id="6-Optimization"><a href="#6-Optimization" class="headerlink" title="6 Optimization"></a>6 Optimization</h1><p>本系统使用语义关键帧选取策略给定的关键帧来优化相机位姿：根据地图点的运动概率该修改ORB-SLAM3 的误差项，优化过程中<strong>只使用静态特征点</strong>。</p>
<p><img src="/2024/02/23/rds-slam/f6.png" alt="f6" title="formula 6"></p>
<p>其中，X 表示地图点的3D 位姿；T 表示待优化关键帧位姿；x 表示关键帧中的匹配特征点。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 RDMO-SLAM_Real-Time Visual SLAM for Dynamic Environments Using Semantic Label Prediction With Optical Flow</title>
    <url>/2024/02/23/rdmo-slam/</url>
    <content><![CDATA[<p>Liu, Yubao, and Jun Miura. “RDMO-SLAM: Real-Time Visual SLAM for Dynamic Environments Using Semantic Label Prediction With Optical Flow.” <em>IEEE Access</em> 9 (2021): 106981–97. <a href="https://doi.org/10.1109/ACCESS.2021.3100426">https://doi.org/10.1109/ACCESS.2021.3100426</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献如下：</p>
<ol>
<li>提出一个面向动态环境的<strong>实时语义vSLAM 算法</strong>——RDMO-SLAM，该算法使用Mask R-CNN（语义分割）和 PWC-Net（光流估计），可同时实现良好的<strong>跟踪表现以及实时特性</strong>；</li>
<li>利用<strong>光流法</strong>来预测Mask R-CNN 的语义结果，使得跟踪线程可利用<strong>尽可能多的语义信息</strong>；</li>
<li>实验验证了本算法的实时性能（<strong>30 Hz</strong>）。</li>
</ol>
<span id="more"></span>
<h1 id="3-Rigid-Scene-Assumption-Problem"><a href="#3-Rigid-Scene-Assumption-Problem" class="headerlink" title="3 Rigid Scene Assumption Problem"></a>3 Rigid Scene Assumption Problem</h1><p>如Fig. 1所示，展示了动态物体对SLAM 算法的影响。</p>
<p><img src="/2024/02/23/rdmo-slam/fig1.png" alt="fig1" title="figure 1"></p>
<p>对于场景中的动态物体，正确的误差项应为：</p>
<p><img src="/2024/02/23/rdmo-slam/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$H_{t-1}^t$ 表示地标的<strong>运动参数</strong>。据作者所言，有两种方法来解算该问题：方法一是使用<strong>多物体跟踪</strong>对物体的运动参数 $H$ 和相机位姿 $T(\epsilon)$ 进行<strong>联合优化</strong>（如VDO-SLAM、DynaSLAMⅡ），但该方法是基于<strong>刚性物体的假设</strong>，即假设物体及其表面的特征点拥有相同的运动模式，该假设不适用于人类等非刚性物体；而且，由于这些方法使用<strong>blocked model</strong>（如Fig. 3所示）导致无法做到实时。</p>
<p><img src="/2024/02/23/rdmo-slam/fig3.png" alt="fig3" title="figure 3"></p>
<p>第二种方法是<strong>将动态特征点进行剔除</strong>，构建的损失函数如下所示：</p>
<p><img src="/2024/02/23/rdmo-slam/f5.png" alt="f5" title="formula 5"></p>
<p>大部分算法都是根据特征点的动态属性将<strong>权重参数</strong> $W_j$ 设置为0和1，而作者在其前作RDS-SALM 中将 $W_j$ 设置为<strong>静态概率</strong>（1-移动概率），作者不选择直接将动态地图点进行剔除，因为可以避免静态匹配点过少的场景；作者会随着时间<strong>更新每个地图点的移动概率</strong>来减少动态物体带来的影响。</p>
<h1 id="4-System-Overview"><a href="#4-System-Overview" class="headerlink" title="4 System Overview"></a>4 System Overview</h1><p>RDMO-SLAM 算法框架如Fig. 4所示，本系统是基于ORB-SLAM3 和 RDS-SLAM 开发的，本算法继续使用RDS-SLAM 中的语义线程，根据<strong>移动概率</strong>将地标分为三组：未知、静态以及动态，然后在跟踪线程中使用尽可能多的静态特征点。此外，作者新增了一个Label Prediction 模块，在等待语义分割结果的时候<strong>使用光流来预测语义标签</strong>；速度估计线程是利用光流来计<strong>算并更新地图点的速度</strong>，利用地图点的速度信息可作为<strong>额外的约束</strong>来滤除跟踪线程中的错误关联。最终，包含<strong>运动概率和速度的语义信息</strong>被用于滤除外点。</p>
<p><img src="/2024/02/23/rdmo-slam/fig4.png" alt="fig4" title="figure 4"></p>
<h1 id="5-Optical-Flow-Thread"><a href="#5-Optical-Flow-Thread" class="headerlink" title="5 Optical Flow Thread"></a>5 Optical Flow Thread</h1><p>作者使用PWC-NET 进行<strong>光流估计</strong>，对1024*436分辨率的图片实现35FPS 的处理速度。每个像素的光流结果存储两个浮点数，分别表示像素在相邻两帧之间的位移，结果如Fig. 5所示，光流只检测身体的运动部位，如手臂与腿部。</p>
<p><img src="/2024/02/23/rdmo-slam/fig5.png" alt="fig5" title="figure 5"></p>
<p>光流的结果有两个用途：</p>
<ol>
<li>在<strong>语义线程</strong>中协助预测关键帧的语义标签，来提高语义信息的生成速度；</li>
<li>在<strong>速度估计线程</strong>中计算地图点的速度。</li>
</ol>
<h1 id="6-Semantic-Thread"><a href="#6-Semantic-Thread" class="headerlink" title="6 Semantic Thread"></a>6 Semantic Thread</h1><p>Mask R-CNN 运行速度太慢，接近200ms，为了获取更多的语义信息，作者提出一个算法：在等待 Mask R-CNN 结果的同时，利用<strong>之前获取的语义标签</strong>和参考关键帧的<strong>光流模式</strong>来预测当前关键帧的语义标签。</p>
<h2 id="6-1-Semantic-Keyframe-Selection"><a href="#6-1-Semantic-Keyframe-Selection" class="headerlink" title="6.1 Semantic Keyframe Selection"></a>6.1 Semantic Keyframe Selection</h2><p>由于Mask R-CNN 的处理速度，如果所有的关键帧都经过语义分割处理，那么语义线程与跟踪线程之间的<strong>语义延迟</strong>会逐渐增大，导致跟踪线程无法实时获取丰富的最新语义信息。为了减少语义延迟，RDS-SLAM 的关键帧选取策略会导致许多关键帧<strong>无法获取语义结果</strong>，不充分的语义信息导致RDS-SLAM 处理<strong>速度较慢</strong>（15Hz）。</p>
<p>为解决该问题，作者提出了新的关键帧选取策略，如Fig. 6所示，每次使用Mask R-CNN 对<strong>最新的关键帧</strong>（如 $KF_0, KF_3, KF_7$ ）进行处理，而之间未处理的关键帧（如 $KF_1, KF_2$ ）使用<strong>预测方法</strong>处理。</p>
<p><img src="/2024/02/23/rdmo-slam/fig6.png" alt="fig6" title="figure 6"></p>
<h2 id="6-3-Semantic-Label-Prediction"><a href="#6-3-Semantic-Label-Prediction" class="headerlink" title="6.3 Semantic Label Prediction"></a>6.3 Semantic Label Prediction</h2><p>使用已分割关键帧（如 $KF_0$ ）作为参考关键帧，利用参考关键帧的<strong>语义标签</strong>和目标关键帧的<strong>光流结果</strong>来对目标帧（如 $KF_1, KF_2$ ）的语义标签进行预测，如下式所示：</p>
<p><img src="/2024/02/23/rdmo-slam/f7.png" alt="f7" title="formula 7"></p>
<p>其中，$I_r(x_r, y_r)$ 表示参考关键帧的语义分割标签；$(f_x, f_y)$ 表示光流向量；$I_p(x_p, y_p)$ 表示待预测的标签。</p>
<h2 id="6-5-Moving-Probability-Update"><a href="#6-5-Moving-Probability-Update" class="headerlink" title="6.5 Moving Probability Update"></a>6.5 Moving Probability Update</h2><p>作者定义地图点的<strong>移动概率</strong> $p(m_t^j), m_t^j \in M=\{static(s), dynamic(d)\}$ 。本系统在语义线程中利用<strong>贝叶斯滤波器</strong>来更新移动概率：</p>
<p><img src="/2024/02/23/rdmo-slam/f8.png" alt="f8" title="formula 8"></p>
<p>其中，$p(m_0) = 0.5$ 是初始概率；$p(z_t | m_t)$ 是观测似然，根据<strong>语义标签</strong>进行设定。</p>
<h2 id="6-6-Algorithm-Implementation"><a href="#6-6-Algorithm-Implementation" class="headerlink" title="6.6 Algorithm Implementation"></a>6.6 Algorithm Implementation</h2><p>语义线程的具体算法如下所示：</p>
<p><img src="/2024/02/23/rdmo-slam/a1.png" alt="a1" title="algorithm 1"></p>
<p>其中，各个指针代表的含义如Fig. 12所示，workId 指的是当前工作的指针，会遍历每一个关键帧；refId 指的是参考关键帧；reqId 是最新语义请求的关键帧；latestId 是当前最新的关键帧。</p>
<p><img src="/2024/02/23/rdmo-slam/fig12.png" alt="fig12" title="figure 12"></p>
<p>利用语义分割或者预测获取到语义标签后，更新语义信息、获取<strong>动态物体的掩码</strong>并更新地图点的<strong>移动概率</strong>。</p>
<p><img src="/2024/02/23/rdmo-slam/a2.png" alt="a2" title="algorithm 2"></p>
<h1 id="7-Velocity-Estimation-Thread"><a href="#7-Velocity-Estimation-Thread" class="headerlink" title="7 Velocity Estimation Thread"></a>7 Velocity Estimation Thread</h1><p>语义分割只能识别出预定义的动态物体，且分割结果不一定都是准确的，因此，作者引入<strong>速度约束</strong>来进一步减少外点的影响。为了获取更为准确的速度估计，作者使用<strong>卡尔曼滤波器</strong>来更新速度：</p>
<p><img src="/2024/02/23/rdmo-slam/f17.png" alt="f17" title="formula 17"></p>
<p><img src="/2024/02/23/rdmo-slam/fig13.png" alt="fig13" title="figure 13"></p>
<h1 id="8-Tracking"><a href="#8-Tracking" class="headerlink" title="8 Tracking"></a>8 Tracking</h1><p>为了让vSLAM 实时运行，作者将语义线程、速度估计线程与跟踪线程分离开，利用地图中存储的地标<strong>移动概率</strong>和<strong>速度</strong>来滤除外点。实验中，作者设定 $\theta_d = 0.6, \theta_s = 0.4$ 。跟踪线程是用来进行初始位姿估计的，作者优先使用静态特征点；若不足，则进一步使用未知特征点；若还不足，进一步使用动态特征点。</p>
<p><img src="/2024/02/23/rdmo-slam/fig10.png" alt="fig10" title="figure 10"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 DP-SLAM_A visual SLAM with moving probability towards dynamic environments</title>
    <url>/2024/02/24/dp-slam/</url>
    <content><![CDATA[<p>Li, Ao, Jikai Wang, Meng Xu, and Zonghai Chen. “DP-SLAM: A Visual SLAM with Moving Probability towards Dynamic Environments.” <em>Information Sciences</em> 556 (May 1, 2021): 128–42. <a href="https://doi.org/10.1016/j.ins.2020.12.019">https://doi.org/10.1016/j.ins.2020.12.019</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ol>
<li>作者提出了一种通过实时传递每个关键点<strong>移动概率</strong>的动态物体检测方法，该移动概率传递方法克服了<strong>几何约束和语义信息的偏差</strong>，可提高vSLAM 的准确性与鲁棒性；</li>
<li>利用静态信息<strong>补全</strong>遮挡的背景区域，获取没有动态物体的合成RGB 图片以及相应的深度图片，有益于虚拟现实等应用；</li>
<li>作者将该移动物体检测方法集成至ORB-SLAM2 系统中，利用公开数据集进行测试，证明了本方法对SLAM <strong>精度与鲁棒性</strong>的提高。</li>
</ol>
<span id="more"></span>
<h1 id="3-DP-SLAM"><a href="#3-DP-SLAM" class="headerlink" title="3 DP-SLAM"></a>3 DP-SLAM</h1><h2 id="3-1-The-approach-overview"><a href="#3-1-The-approach-overview" class="headerlink" title="3.1 The approach overview"></a>3.1 The approach overview</h2><p>本文提出的移动物体检测算法流程图如Fig. 1所示：</p>
<p><img src="/2024/02/24/dp-slam/fig1.png" alt="fig1" title="figure 1"></p>
<p>本方法结合几何模型和语义分割进行动态物体检测，几何模型和语义分割的结果转化为<strong>观测概率</strong>，前一帧中关键点的移动概率被视为<strong>先验概率</strong>，基于<strong>贝叶斯理论</strong>，每个关键点的移动概率可通过观测概率和先验概率进行更新。</p>
<p><img src="/2024/02/24/dp-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-2-Semantic-segmentation"><a href="#3-2-Semantic-segmentation" class="headerlink" title="3.2 Semantic segmentation"></a>3.2 Semantic segmentation</h2><p>本方法使用Mask R-CNN 网络获取<strong>像素级语义信息</strong>，将场景中的<strong>先验动态物体</strong>（人、自行车、汽车等）掩码生成到一张图片中；由于CNN 网络的精度限制，导致部分关键点分类错误的情况，特别是在物体<strong>轮廓边界</strong>附近，为了实现更精确的分类结果，作者使用<strong>二项式逻辑回归binomial logistic regression 模型</strong>来计算每个关键点的<strong>语义分割动态概率</strong>，如Fig. 3所示，掩码内具有低得不正常的移动概率（0.75）的关键点更有可能是识别错误的，应该是位于静态背景中的关键点。</p>
<p><img src="/2024/02/24/dp-slam/fig3.png" alt="fig3" title="figure 3"></p>
<p>用来估计关键点 $p_i$ <strong>语义分割动态概率</strong>的二项式逻辑回归模型如下所示：</p>
<p><img src="/2024/02/24/dp-slam/f0.png" alt="f0" title="formula 0"></p>
<p>其中，$\alpha$ 为影响因子来平滑即时检测结果，本文中被设定为0.1；$dist(p_i, z_t)$ 为关键点 $p_i$ 与语义分割掩码边界之间的距离。上式的含义在于，位于先验动态物体掩码内的关键点<strong>距离边界越近</strong>，那么该关键点的<strong>语义分割动态概率越小</strong>，即该点被错误分类的可能性越高。</p>
<h2 id="3-3-Epipolar-geometry-constraint"><a href="#3-3-Epipolar-geometry-constraint" class="headerlink" title="3.3 Epipolar geometry constraint"></a>3.3 Epipolar geometry constraint</h2><p>Mask R-CNN 网络是由COCO 数据集训练的，只能对<strong>预定义的种类</strong>进行识别，不能对其他物体进行检测，因此，作者额外使用<strong>对极几何约束</strong>来检测关键点的移动概率。如Fig. 4所示，计算当前帧<strong>匹配点与极线</strong>之间的距离判断该点的移动概率。</p>
<p><img src="/2024/02/24/dp-slam/fig4.png" alt="fig4" title="figure 4"></p>
<p>利用对极几何约束计算特征点的移动概率过程如算法1所示，具体步骤如下所示：</p>
<ol>
<li>首先，通过计算当前帧的<strong>光流金字塔</strong>来获取匹配的关键点，剔除掉那些距离图片边缘过近或匹配点对之间像素差异过大的点；</li>
<li>然后，利用RANSAC 算法获取<strong>基础矩阵</strong>，在此基础上计算关键点距离极线的距离，若大于一定阈值（0.75）则判定为外点。</li>
</ol>
<p><img src="/2024/02/24/dp-slam/a1.png" alt="a1" title="algorithm 1"></p>
<p>由于动态物体的存在，图片中每个关键点不会严格位于对应的极线上，距离越大，移动概率就越大；因此，作者假设关键点与其极线的距离满足<strong>高斯分布</strong>：</p>
<p><img src="/2024/02/24/dp-slam/f1.png" alt="f1" title="formula 1"></p>
<h2 id="3-4-Iteratively-moving-probability-update"><a href="#3-4-Iteratively-moving-probability-update" class="headerlink" title="3.4 Iteratively moving probability update"></a>3.4 Iteratively moving probability update</h2><p>持续跟踪动态物体会<strong>极大提高</strong>定位表现，因此，作者提出一种<strong>移动概率传递算法</strong>，从多帧图片中结合几何模型和语义分割信息进行动态物体检测。</p>
<p>定义关键点 $p_i$ 在时间 t 的<strong>运动状态</strong>为 $D_t(p_i)$ ，若该关键点被判定为动态点，则 $D_t(p_i)=1$ ，否则为0。结合来自<strong>语义模型和语义分割的移动概率</strong> $P(D_t(p_i) | c_{p_i}^t), P(D_t(p_i) | s_{p_i}^t)$ ：</p>
<p><img src="/2024/02/24/dp-slam/f2.png" alt="f2" title="formula 2"></p>
<p>其中，</p>
<p><img src="/2024/02/24/dp-slam/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$N_c, N_s$ 分别表示几何模型、语义分割识别出当前帧中<strong>动态点的数量</strong>。</p>
<p>在这个过程中，作者假设动态概率传递模型拥有<strong>马尔可夫特性</strong>，则基于贝叶斯理论的关键点<strong>移动概率传递模型</strong>表示为：</p>
<p><img src="/2024/02/24/dp-slam/f4.png" alt="f4" title="formula 4"></p>
<p>观测概率表示为：</p>
<p><img src="/2024/02/24/dp-slam/f5.png" alt="f5" title="formula 5"></p>
<p>其中，</p>
<p><img src="/2024/02/24/dp-slam/f6.png" alt="f6" title="formula 6"></p>
<p>最后，将关键点移动概率 $P(D_t(p_i)|c_{p_i}^t, s_{p_i}^t) &gt; 0.5$ 的关键点视为<strong>外点</strong>，在后续的跟踪制图进程中不再使用。</p>
<p><img src="/2024/02/24/dp-slam/a2.png" alt="a2" title="algorithm 2"></p>
<h2 id="3-5-Background-inpainting"><a href="#3-5-Background-inpainting" class="headerlink" title="3.5 Background inpainting"></a>3.5 Background inpainting</h2><p>在移除掉动态物体后，根据之前的<strong>静态观测</strong>来补全遮挡的背景区域，生成一个不包含动态物体的静态图片，该静态图片包含环境中的静态结构，有益于后续的<strong>回环检测和制图</strong>。</p>
<p>作者将本算法与其他类似的方法进行比较，结果如Fig. 10所示，证明了本算法的优势。</p>
<p><img src="/2024/02/24/dp-slam/fig10.png" alt="fig10" title="figure 10"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Long-Term Visual Localization Using Semantically Segmented Images</title>
    <url>/2024/02/24/stenborg2018/</url>
    <content><![CDATA[<p>Stenborg, Erik, Carl Toft, and Lars Hammarstrand. “Long-Term Visual Localization Using Semantically Segmented Images.” In <em>2018 IEEE International Conference on Robotics and Automation (ICRA)</em>, 6484–90. Brisbane, QLD: IEEE, 2018. <a href="https://doi.org/10.1109/ICRA.2018.8463150">https://doi.org/10.1109/ICRA.2018.8463150</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文利用近期在语义分割中的发展，设计了一个基于<strong>语义分割图片</strong>和<strong>语义点特征地图</strong>的定位算法，不再使用传统的描述子来描述特征，而是使用<strong>3D 位置坐标</strong>和<strong>语义标签</strong>来描述。</p>
<span id="more"></span>
<h1 id="2-Problem-Statement"><a href="#2-Problem-Statement" class="headerlink" title="2 Problem Statement"></a>2 Problem Statement</h1><h2 id="2-1-Observations"><a href="#2-1-Observations" class="headerlink" title="2.1 Observations"></a>2.1 Observations</h2><p>坐标系与传感器安装位置如Fig 2所示：</p>
<p><img src="/2024/02/24/stenborg2018/fig2.png" alt="fig2" title="figure 2"></p>
<p>相机采集到的图片经过预处理，得到一组包含 $n_t$ 个特征点（坐标）与描述子对（即<strong>观测量</strong>），$\mathbf{f}_t=\{(u_t^i,d_t^i)\}_{i=1}^{n_t}$ ，在本文中，依据我们使用的方法不同，$\mathbf{f}_t$ 会有不同的形式：如果使用<strong>语义点云地图方法</strong>，$\mathbf{f}_t$ 是稠密的，包含图片中每一个像素，且描述子为像素对应的语义标签；若使用<strong>SIFT 算法</strong>，则特征点是稀疏的，且描述子为128x1 的向量。</p>
<h2 id="2-2-Maps"><a href="#2-2-Maps" class="headerlink" title="2.2 Maps"></a>2.2 Maps</h2><p>假设预构建的特征地图包含M 个特征点，则地图可表示为 $\mathcal{M} = \{&lt;\mathbf{U}_i, \mathbf{D}_i, \mathcal{V}_i&gt;\}_{i=1}^M$ ，每个特征点包含<strong>位置信息</strong> $\mathbf{U}_i$ ，<strong>特征描述子</strong> $\mathbf{D}_i$ ，以及<strong>可视参数</strong> $\mathcal{V}_i = [\rho_i, \gamma_i^a,\gamma_i^b,r_i]^T$  （参考Fig. 2），其中 $\rho_i$ 表示特征被探测到的概率。</p>
<h2 id="2-3-Problem-Definition"><a href="#2-3-Problem-Definition" class="headerlink" title="2.3 Problem Definition"></a>2.3 Problem Definition</h2><p>问题被描述为：给定观测和地图 $\mathcal{M}$ ，通过迭代计算出汽车相对于地图的<strong>位姿后验概率估计</strong>。</p>
<p>假设 $t$ 时刻的汽车位姿为 $\mathbf{x}_t$ ，则我们的目标为计算 $p(\mathcal{x}_t|\mathcal{f}_{1:t}, \mathcal{M})$ 。</p>
<h1 id="3-Models"><a href="#3-Models" class="headerlink" title="3 Models"></a>3 Models</h1><h2 id="3-1-Process-model"><a href="#3-1-Process-model" class="headerlink" title="3.1 Process model"></a>3.1 Process model</h2><p><strong>状态方程</strong></p>
<p>作者使用一个简单的<strong>点质量模型 point mass model</strong> 来对汽车进行建模。本状态方程由两部分组成，<strong>部分一</strong>使用速度测量作为输入来<strong>对运动进行建模</strong>：</p>
<p><img src="/2024/02/24/stenborg2018/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$M(.) \in SE(3)$ ，是表示位姿的4x4 矩阵。</p>
<p><strong>部分二</strong>是一项来确保总有一小部分density 被留在路上，这保证了即便一个lost filter 丢失了，也可以重新获取其横向位置。</p>
<p>综合以上两部分，状态模型表示为：</p>
<p><img src="/2024/02/24/stenborg2018/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$p_m(.)$ 是由部分一给定的，$p_r(.)$ 是 $p_m(.)$ 到路上的投影。</p>
<h2 id="3-2-Measurement-model"><a href="#3-2-Measurement-model" class="headerlink" title="3.2 Measurement model"></a>3.2 Measurement model</h2><p>假定数据关联步骤已经解决，得到一个DA 向量 $\mathbf{\lambda}_t = [\lambda_t^1, …, \lambda_t^{n_t}]^T$ ，其中，$\lambda_t^i = j$ 表示，如果 $j&gt;0$ ，则图片特征 i 与地图特征 j 相关联；否则该特征在地图中不存在。</p>
<p>利用已有的DA，并假定特征位置坐标和描述子 $u_t^i,d_t^i$ 满足<strong>条件独立</strong>，则有：</p>
<p><img src="/2024/02/24/stenborg2018/f4.png" alt="f4" title="formula 4"></p>
<p>其中，$\mathcal{M}_{\lambda_t^i}$ 表示地图中带有描述子和可见参数的<strong>3D 点与图片中的特征点 i</strong> 根据DA $\mathbf{\lambda}_t$ 关联起来。</p>
<h3 id="3-2-1-SIFT-map"><a href="#3-2-1-SIFT-map" class="headerlink" title="3.2.1 SIFT map"></a>3.2.1 SIFT map</h3><p>当给定数据关联时，特征中的<strong>描述子对似然函数没有帮助</strong>，可以忽略掉。同时，假定图片中SIFT 特征点的探测误差服从零均值高斯分布，则有：</p>
<p><img src="/2024/02/24/stenborg2018/f5.png" alt="f5" title="formula 5"></p>
<p>其中，$\pi(.)$ 表示带有镜头畸变的标准针孔相机投影模型；$\sigma_{\pi}^2$ 表示探测器误差的方差。</p>
<h3 id="3-2-2-Semantic-map"><a href="#3-2-2-Semantic-map" class="headerlink" title="3.2.2 Semantic map"></a>3.2.2 Semantic map</h3><p>在该方法中，地图中的特征点描述子 $D_j$ ，以及图片特征的描述子 $d_t^i$ 都是来自CityScapes 数据集中的<strong>语义标签</strong>。</p>
<p>即便图片中相邻像素的语义分割结果是相关的，但是作者让做出了<strong>简化假设</strong>：假设像素坐标与语义类别是独立的，则有：</p>
<p><img src="/2024/02/24/stenborg2018/f6.png" alt="f6" title="formula 6"></p>
<p>这个因式分解会在观测中导致<strong>overconfidence</strong>，而且观测越多影响越严重，这使得作者在第四章中加入了尺度参数 s，以及测量截断值 $N_c$ 。</p>
<p>上式的第一个因子 $p(u_t^i|x_t, U_{\lambda_t^i})$ 表示在像素 i 处检测到特征的概率，然而本方法是对整幅图片进行语义分割，即每一个像素都被视为一个特征，则该因子对于任意的 i 是一个<strong>常数值</strong>。因此，上式可进一步化简为：</p>
<p><img src="/2024/02/24/stenborg2018/f7.png" alt="f7" title="formula 7"></p>
<p>对于上式的右半部分，分两种情况讨论：</p>
<p><strong>情况一：</strong>没有地图点投影到该像素，即 $\lambda_t^i = 0$ ：此时，我们无法从地图中获取该像素类别的信息，作者为所有这种像素（没有地图特征点投影的像素）假设一种分布，即在所有种类上的<strong>边缘分布</strong>：</p>
<p><img src="/2024/02/24/stenborg2018/f8.png" alt="f8" title="formula 8"></p>
<p>根据后文描述，该边缘分布是通过汇总制图所用到的所有图片的语义分割结果，统计各个种类所占的比例即为该边缘分布。</p>
<p><strong>情况二：</strong>有地图点投影到该像素，即 $\lambda_t^i &gt; 0$ ：虽然地图上的一个特征点投影到了该像素上，但是<strong>仍然不能确定</strong>是探测到了这个地图特征点，还是该处被别的物体遮挡住了，如汽车或行人。为解决该<strong>不确定度</strong>，作者引入了一个<strong>探测变量</strong> $\delta$ ：当该值为1时表示该地图特征点确实被探测到了，否则为0。利用该探测变量，将像素与对应的地图特征点似然函数表示为：</p>
<p><img src="/2024/02/24/stenborg2018/f9.png" alt="f9" title="formula 9"></p>
<p>其中，$Pr\{d_t^i|\delta=0, D_{\lambda^i_k}\}, Pr\{d_t^i|\delta=1, D_{\lambda^i_k}\}$ 分别表示给定特定地图点特征<strong>被遮挡</strong>或者<strong>可视状态</strong>下像素类别的概率；根据后文描述， $Pr\{d_t^i|\delta=0, D_{\lambda^i_k}\}$ 是通过手动调整设定的，动态物体的概率设定高一点，如汽车、行人等，静态物体设置低一点； $Pr\{d_t^i|\delta=1, D_{\lambda^i_k}\}$ 的确定是通过汇总确定该地图特征点的所有图片中，该点周围7x7 邻域像素的语义分类，并归一化为一个直方图作为各个种类的PMF；剩下的部分表示给定的地图点是可视的概率：</p>
<p><img src="/2024/02/24/stenborg2018/f10.png" alt="f10" title="formula 10"></p>
<p>其中，$v(.)$ 是一个函数，当位姿 $\mathbf{x}_t$ 在地图特征点的可视区域内时该函数为1；$P_0$ 表示一个可视地图特征点被遮挡的概率。</p>
<p>综上所述，语义地图法的似然计算需要考虑地图特征点是否映射到像素上的两种情况：</p>
<p><img src="/2024/02/24/stenborg2018/f11.png" alt="f11" title="formula 11"></p>
<h1 id="4-Algorithmic-Details"><a href="#4-Algorithmic-Details" class="headerlink" title="4 Algorithmic Details"></a>4 Algorithmic Details</h1><h2 id="4-1-SIFT-filter"><a href="#4-1-SIFT-filter" class="headerlink" title="4.1 SIFT filter"></a>4.1 SIFT filter</h2><p>针对该方法，作者使用了无迹卡尔曼滤波器UKF 进行优化，伪代码如下所示：</p>
<p><img src="/2024/02/24/stenborg2018/a1.png" alt="a1" title="algorithm 1"></p>
<p>其中，数据关联DA 步骤如下所示：作者基于当前的位姿估计以及地图特征点的可视参数 $\mathcal{V}$ 来选取一个<strong>子地图</strong> $\mathcal{M}_t$ ，将图片中提取到的SIFT 特征点与子地图 $\mathcal{M}_t$ 中的特征点进行<strong>匹配</strong>，并使用<strong>RANSAC 算法</strong>进行外点剔除，最终得到DA 。</p>
<h2 id="4-2-Semantic-filter"><a href="#4-2-Semantic-filter" class="headerlink" title="4.2 Semantic filter"></a>4.2 Semantic filter</h2><p>针对该方法，作者使用了bootstrap particle filter 进行递归优化，该算法的伪代码如下所示：</p>
<p><img src="/2024/02/24/stenborg2018/a2.png" alt="a2" title="algorithm 2"></p>
<p>为了可以评估一个粒子的似然，需要确认地图中的哪些特征点是<strong>潜在可视</strong>的，这与SIFT 方法类似，但是只需要做到近似即可，因此可以<strong>同时计算</strong>附近的几个粒子，例如使用这些粒子的<strong>平均位置</strong>和地图特征点的<strong>可视参数</strong>进行计算。然后，将这些潜在可视的地图特征点映射到图片中，从而<strong>为每个粒子</strong>构建了<strong>独有的数据关联</strong> $\lambda_t$ 。</p>
<p><img src="/2024/02/24/stenborg2018/fig3.png" alt="fig3" title="figure 3"></p>
<p>将式（11）除以常数 $\prod_iPr\{d_t^i\}$ 可以简化权重更新过程，因为只需要考虑那些有地图特征点投影到的像素，</p>
<p><img src="/2024/02/24/stenborg2018/f12.png" alt="f12" title="formula 12"></p>
<p>在3.2.2节我们做了像素位置与语义分类<strong>条件独立的假设</strong>，但实际情况是该假设不满足，所以会导致对观测的<strong>overly confident</strong>，为减小该假设的影响，作者增加测量似然到一个小于1的正数，因此，对于粒子 j 的<strong>权重更新</strong>变为了：</p>
<p><img src="/2024/02/24/stenborg2018/f13.png" alt="f13" title="formula 13"></p>
<p>其中，$w_t^{(j)}$ 是在状态 $\mathbf{x}_t^{(j)}$ 下粒子 j 的<strong>权重</strong>；s 是<strong>尺度参数</strong>，本文设定为3；$n_{\lambda_t}$ 是映射到图像中的地图特征点个数；$N_c = 400$ 是<strong>截断值</strong>，作者认为更多的映射特征点不会提供更多的信息，因为更多的特征点意味着它们之间的间距越小，那么它们之间的相关性越强。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Semantic Visual Localization</title>
    <url>/2024/02/24/schonberger2018/</url>
    <content><![CDATA[<p>Schonberger, Johannes L., Marc Pollefeys, Andreas Geiger, and Torsten Sattler. “Semantic Visual Localization.” In <em>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 6896–6906. Salt Lake City, UT: IEEE, 2018. <a href="https://doi.org/10.1109/CVPR.2018.00721">https://doi.org/10.1109/CVPR.2018.00721</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文遵循基于结构的视觉定位方法，并使用一个三维语义图作为数据库场景database scene 的表示；给定一个带有语义分割和深度图的查询图片，作者<strong>建立一个三维的语义查询图，并从中提取到的局部描述子</strong>，然后在查询描述子和数据库描述子之间使用3D-3D 匹配，来对齐图并获取所要查询的位姿估计。</p>
<p>所有的视觉定位方法（包括本文提出的方法）都直接或间接地测量一个查询图片与数据库场景表示之间的相似度（视觉或结构），因此，这些方法都默认一个<strong>先验假设：查询图与数据库图是在充分相似的条件（视角、光照以及场景外观等）下描述一个场景的</strong>。如Fig. 1所示，这种先验假设在实际应用场景中会被轻易地打破。</p>
<span id="more"></span>
<p><img src="/2024/02/24/schonberger2018/fig1.png" alt="fig1" title="figure 1"></p>
<p>主要挑战在于如何建立查询图与数据库图之间的data association。现有的基于图片或基于结构的方法都使用具有区别性的局部特征：同样物理点的描述子是相似的，不相关点的描述子差距很大。然而，视觉观测条件的巨变（外观或几何）却对同一物理点在不同外观下要求具有不变的embedding 表示，这与上述方法的区别性描述子相矛盾。</p>
<p>为解决该问题，作者提出了一种<strong>基于生成式的描述子学习方法</strong>。该方法的核心在于学习一个欧氏空间中的embedding，该embedding 包含有恢复不同视角条件下场景外观所需的所有信息。作者提出的该embedding 包含高等级的3D 几何信息和语义信息，使得我们可以处理强烈的视角改变以及轻微的场景几何改变（如由季节变化引起的）。此外，作者还提出一个基于3D 语义场景补全辅助任务的声称是描述子模型，该描述子可通过自监督学习进行获取，且泛化能力较好。</p>
<p>本文贡献如下：</p>
<ol>
<li>提出一个基于3D 几何信息和语义信息的视觉定位方法；</li>
<li>利用面向3D 语义场景补全的生成式模型进行描述子学习；</li>
<li>证明了本方法在两个挑战性问题中的有效性：在强烈视角变化以及光照/季节改变情况下相机位姿的精确估计。</li>
</ol>
<p>即使不用语义信息，本方法也远远超过现有的SOTA 方法，这证明了生成式描述子在定位应用中的强大，而包含语义信息后会取得更大的性能提升。</p>
<h1 id="3-Semantic-Visual-Localization"><a href="#3-Semantic-Visual-Localization" class="headerlink" title="3 Semantic Visual Localization"></a>3 Semantic Visual Localization</h1><p>本文提出的语义视觉定位方法包含以下三个步骤：</p>
<ol>
<li>离线操作：利用语义场景补全辅助任务来学习鲁棒的局部描述子；</li>
<li>在线操作：使用该局部描述子建立查询图与数据库图之间的3D-3D匹配；</li>
<li>在线操作：利用匹配来估计两个图之间的对齐关系，进而获取查询图对应的位姿。</li>
</ol>
<h2 id="3-1-Semantic-Segmentation-and-Fusion"><a href="#3-1-Semantic-Segmentation-and-Fusion" class="headerlink" title="3.1 Semantic Segmentation and Fusion"></a>3.1 Semantic Segmentation and Fusion</h2><p>将语义分割后的图片融合为语义3D体素地图，该地图在体素分辨率上具有强烈照明改变以及几何改变的不变性。尽管语义信息包含高层次的场景信息，但是为实现对场景的明确、实例级的理解就需要较大的空间上下文信息，这就会导致由于遮挡造成的观测缺失，比如不同视角（甚至是相反视角）下查询图与数据库图之间的结构重叠会非常小。因此，本文方法的一个主要挑战就在于<strong>在共同观测缺失的情况下如何实现查询图与数据库图之间的鲁棒匹配。</strong></p>
<h2 id="3-2-Generative-Descriptor-Learning"><a href="#3-2-Generative-Descriptor-Learning" class="headerlink" title="3.2 Generative Descriptor Learning"></a>3.2 Generative Descriptor Learning</h2><p>因为查询图和数据库图在尺寸和覆盖范围上的不同，作者选择建立两图中具有相同尺寸的子块 $ v_D \in M_D, v_Q \in M_Q$ 之间的联系，利用一个function来识别相似的子块，其中，语义场景理解是该function的关键。</p>
<p>针对该function的两种传统解决方法是：匹配function $f(v_D, v_Q)$ 和embedding $f(v)$ 。embedding $f(v)$ 是将相同的子块映射到欧氏空间内的相似点处；而匹配function $f(v_D, v_Q)$ 理论上具有更强大区分能力，但是其使用穷举配对比较方法具有太大的计算复杂度，在大规模数据上难以高效运行。因此，本文选用embedding $f(v)$ 方法，每个子块只需要评估一次即可。 $f(v)$ 将一个子块映射为一个低维空间描述子，同时编码场景语义和几何信息。</p>
<p>为了从不同视角甚至是相反视角下识别出同一物体，需要<strong>对子块的未观测部分进行补全</strong>，作者引入了语义场景补全辅助任务：使用function $h(v)$ 来补全输入未观测部分的语义和几何信息。作者使用一个3D 编码器-解码器结构 $h(v) = g(f(v))$ , 其中 $f$ 是对不完整子块进行编码的神经网络，$g$ 是对子块进行补全的神经网络。$h$ 的架构和示分别如Fig.2 和 Fig. 3所示。值得注意的是，网络训练完成后，本文的<strong>语义定位pipeline只使用 $h$ 的编码器部分 $f$ 来产生描述子</strong> $\mu$ 。</p>
<p><img src="/2024/02/24/schonberger2018/fig2.png" alt="fig2" title="figure 2"></p>
<p><img src="/2024/02/24/schonberger2018/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="3-3-Bag-of-Semantic-Words"><a href="#3-3-Bag-of-Semantic-Words" class="headerlink" title="3.3 Bag of Semantic Words"></a>3.3 Bag of Semantic Words</h2><p>利用编码器部分 $f$ 产生描述子 $\mu$ 来创建一个 Bag of Semantic Words。</p>
<h2 id="3-4-Semantic-Vocabulary-for-Indexing-and-Search"><a href="#3-4-Semantic-Vocabulary-for-Indexing-and-Search" class="headerlink" title="3.4 Semantic Vocabulary for Indexing and Search"></a>3.4 Semantic Vocabulary for Indexing and Search</h2><p>作者在离线步骤中使用训练数据集的bag of semantic words 建立一个语义词库 semantic vocabulary，使用层级k-means 和 一个 Hamming embedding 来量化描述子空间，将数据库图的所有语义词 semantic words 建立词库树索引，作为实现高效匹配的数据结构。在匹配过程中遍历词库树索引并找到汉明距离最近的匹配结果。</p>
<h2 id="3-5-Semantic-Alignment-and-Verification"><a href="#3-5-Semantic-Alignment-and-Verification" class="headerlink" title="3.5 Semantic Alignment and Verification"></a>3.5 Semantic Alignment and Verification</h2><p>根据3.4节得到的匹配结果，寻找将query与数据库地图实现最佳对齐的转换关系 $P \in SE(3)$ ，一个好的对齐定义为同时满足几何及语义对齐。本描述子<strong>不具有</strong>旋转不变性，因此，转换关系P 包含了旋转与平移参数。</p>
<p>作者穷尽匹配对所有的转换假设，对于每一个转换假设P，将query经P转换后与数据库地图进行对齐，计算正确对齐的体素个数，体素的正确对齐包含几何（占用）与语义的一致性。作者使用ICP (Iterative Closest Point) 算法进一步细调对齐关系。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Object-aware data association for the semantically constrained visual SLAM</title>
    <url>/2024/02/26/liu2023a/</url>
    <content><![CDATA[<p>Liu, Yang, Chi Guo, and Yingli Wang. “Object-Aware Data Association for the Semantically Constrained Visual SLAM.” <em>Intelligent Service Robotics</em> 16, no. 2 (April 1, 2023): 155–76. <a href="https://doi.org/10.1007/s11370-023-00455-9">https://doi.org/10.1007/s11370-023-00455-9</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者认为，将物体级语义信息和传统vSLAM 融合需要考虑两个关键问题：</p>
<ol>
<li>第一个问题是构建连续帧之间的<strong>数据关联</strong>问题。作者认为物体级vSLAM 需要充分利用相机运动以及3D 几何信息，最终产生一个带有全局一致性物体信息的3D 地图。现有的物体级vSLAM 大多依赖于点云的低级几何特征匹配(Yang 和 Scherer, 2019)、聚类或者统计分析进行数据关联，而忽略了物体整个的<strong>外观信息</strong>；</li>
<li>第二个问题是怎么利用物体级信息进行<strong>位姿估计</strong>。CubeSLAM(Yang 和 Scherer, 2019)、QuadricSLAM (Nicholson 等, 2019) 都是将物体建模为可观测的几何参数在位姿估计中建立约束；但是作者认为，根据物体级约束来提高传统数据关联的<strong>准确性和鲁棒性</strong>也可以提高位姿估计精度。</li>
</ol>
<p>总结来看，本文的贡献如下：</p>
<ol>
<li>提出一个基于<strong>物体外观和特征点路标</strong>的物体级数据关联框架，产生一个带有物体信息的语义地图；</li>
<li>提出一个<strong>语义重投影误差项</strong>并将其整合至位姿优化中，该重投影误差项<strong>结合了语义与特征点观测</strong>作为相机位姿的联合约束；</li>
<li>在公开数据集上进行了测试，证明了本方法在物体级数据关联中实现了较高的准确性，并超越了位姿估计基线方法。</li>
</ol>
<span id="more"></span>
<h1 id="3-Methods"><a href="#3-Methods" class="headerlink" title="3 Methods"></a>3 Methods</h1><h2 id="3-1-System-overview"><a href="#3-1-System-overview" class="headerlink" title="3.1 System overview"></a>3.1 System overview</h2><p>系统框架如Fig. 1所示，本系统基于ORB-SLAM2 并使用<strong>双目/RGB-D 图片</strong>作为输入。</p>
<p><img src="/2024/02/26/liu2023a/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-2-Build-object-level-features-and-landmarks"><a href="#3-2-Build-object-level-features-and-landmarks" class="headerlink" title="3.2 Build object-level features and landmarks"></a>3.2 Build object-level features and landmarks</h2><h3 id="3-2-1-Build-2D-objects"><a href="#3-2-1-Build-2D-objects" class="headerlink" title="3.2.1 Build 2D-objects"></a>3.2.1 Build 2D-objects</h3><p>2D 物体语义信息用来构建新的3D 物体、更新已有的3D 物体并为位姿优化提供约束。作者使用YOLACT 实例分割网络来获取2D 物体语义信息，实例级语义信息包含物体掩码、类别标签以及置信度，2D 物体集合表示为 $S = \{s_j\}$ ，其中，每个<strong>2D 物体</strong> $s_j = \{l_j, q_j, m_j, b_j, f_j, Z_j\}$ ：</p>
<ul>
<li>$l_j$  表示物体类别标签</li>
<li>$q_j$  表示分类置信度</li>
<li>$m_j$  表示物体二值掩码图像</li>
<li>$b_j$  表示物体的2D bbox</li>
<li>$f_j$  表示物体掩码区域的HSV (Hue-Saturation-Value) 直方图向量，从RGB 转换为HSV，向量维度为94，来描述物体外观</li>
<li>$Z_j$  <strong>物体掩码区域内的ORB 特征点集合</strong></li>
</ul>
<h2 id="3-2-2-Build-and-update-3D-objects"><a href="#3-2-2-Build-and-update-3D-objects" class="headerlink" title="3.2.2 Build and update 3D-objects"></a>3.2.2 Build and update 3D-objects</h2><p>3D 物体作为<strong>物体级地标</strong>存储了物体的外观和几何信息，$o = \{l, P, B, F, C\}$ :</p>
<ul>
<li>$l$  表示类别标签</li>
<li>$P$  表示物体的稀疏路标点云</li>
<li>$B$  表示3D bbox</li>
<li>$F$  表示之前所有帧中该2D 物体的HSV 特征向量集合</li>
<li>$C$  表示存储当前与之前物体点云中心点的集合</li>
</ul>
<h3 id="3-2-3-Outliers-elimination"><a href="#3-2-3-Outliers-elimination" class="headerlink" title="3.2.3 Outliers elimination"></a>3.2.3 Outliers elimination</h3><p>外点指的是那些位于本物体的特征点云中，但是不属于本物体的特征点。造成外点的原因包括：</p>
<ol>
<li>双目相机估计的<strong>深度误差</strong>或者RGB-D 相机的<strong>深度测量误差</strong>；</li>
<li><strong>错误的实例分割结果</strong>导致包含背景区域或其他物体；</li>
<li>在<strong>物体边界处的特征点</strong>难以区分类别。</li>
</ol>
<p><img src="/2024/02/26/liu2023a/fig2.png" alt="fig2" title="figure 2"></p>
<p>作者综合利用三种手段进行外点剔除：<strong>统计检验</strong>、<strong>聚类</strong>以及<strong>概率分析</strong>。</p>
<ul>
<li><strong>统计检验</strong>：作者假设一个物体的点云分布较为紧凑且近似符合高斯分布，点云中心作为其均值，统计所有点云计算标准差；</li>
<li><strong>聚类</strong>：作者假设物体点云应该是相连的，一小部分外点会被主体点云所孤立。作者使用DBSCAN 对物体点云进行聚类，将点云分为几个独立的聚类点云，计算各个聚类中点云所占的比例。如果某个点所属的聚类占总体点云比例较小且该点距离点云中心的距离超过3倍标准差，则将该点视为外点。值得注意的是，DBSCAN 算法消耗很大的计算资源，若某个物体的点云数量超过一定阈值则不使用聚类检测手段，只使用剩余两个方法；</li>
<li><strong>概率分析</strong>：利用<strong>语义一致性</strong>进行检测，考虑到由于模糊分割造成的外点其分类一般具有随机性，很难长期保持一个稳定的类别标签，因此，作者统计每个特征点的分类概率，若该点属于对应物体的概率值较低，则可判定该点为外点。</li>
</ul>
<h2 id="3-3-Object-level-data-association"><a href="#3-3-Object-level-data-association" class="headerlink" title="3.3 Object-level data association"></a>3.3 Object-level data association</h2><p>作者考虑多种度量方式来检查物体的<strong>外观和几何一致性</strong>，以此进行<strong>短期与长期的</strong>物体级数据关联。</p>
<h3 id="3-3-1-Data-association-metrics"><a href="#3-3-1-Data-association-metrics" class="headerlink" title="3.3.1 Data association metrics"></a>3.3.1 Data association metrics</h3><h4 id="Appearance-metric"><a href="#Appearance-metric" class="headerlink" title="Appearance metric"></a>Appearance metric</h4><p>本文的物体级数据关联核心度量手段是物体的<strong>外观相似度检查</strong>。基于从物体掩码区域提取的<strong>HSV 特征向量</strong>进行外观检测，考虑了物体的整体外观特征，且不需要物体具有突出的几何特征。外观相似度的计算过程如下所示，表征两个归一化HSV 向量 $f_1, f_2$ 的cosine 相似度：</p>
<p><img src="/2024/02/26/liu2023a/f2.png" alt="f2" title="formula 2"></p>
<h4 id="2D-IoU-metric"><a href="#2D-IoU-metric" class="headerlink" title="2D IoU metric"></a>2D IoU metric</h4><p>计算两个2D 物体的bbox IoU，被广泛应用于<strong>短期物体跟踪</strong>。</p>
<p><img src="/2024/02/26/liu2023a/f3.png" alt="f3" title="formula 3"></p>
<p><img src="/2024/02/26/liu2023a/fig3.png" alt="fig3" title="figure 3"></p>
<h4 id="3D-overlap-ratio-metric"><a href="#3D-overlap-ratio-metric" class="headerlink" title="3D overlap ratio metric"></a>3D overlap ratio metric</h4><p>由于相机在单一视角下只能观测到物体的一部分，因此，作者不直接计算3D IoU，而是使用重叠部分占两个视角3D 区域的最大比例：</p>
<p><img src="/2024/02/26/liu2023a/f4.png" alt="f4" title="formula 4"></p>
<h4 id="Object-center-metric"><a href="#Object-center-metric" class="headerlink" title="Object center metric"></a>Object center metric</h4><p>3D 物体的点云是随着观测数据的加入而<strong>增量式更新</strong>的，相应的点云中心坐标也会随着新的观测数据加入而改变；针对新观测的物体点云，作者计算该点云中心坐标与物体之前所有点云中心坐标的最近距离作为一个度量参数：</p>
<p><img src="/2024/02/26/liu2023a/f5.png" alt="f5" title="formula 5"></p>
<h3 id="3-3-2-Data-association-scheme"><a href="#3-3-2-Data-association-scheme" class="headerlink" title="3.3.2 Data association scheme"></a>3.3.2 Data association scheme</h3><p>本数据关联框架包含两种物体数据关联以及冗余的物体检查。</p>
<h4 id="Short-term-data-association"><a href="#Short-term-data-association" class="headerlink" title="Short-term data association"></a>Short-term data association</h4><p>作者在连续两帧中利用<strong>2D IoU</strong> 和<strong>外观检测</strong>来进行物体级的短期数据关联。</p>
<p>查看作者源代码发现，作者利用两帧的2D物体匹配更新当前帧的3D物体，判断2D 物体匹配成功的准则为：两者外观相似度大于0.8，且2D IoU不小于0.5。</p>
<h4 id="Long-term-data-association"><a href="#Long-term-data-association" class="headerlink" title="Long-term data association"></a>Long-term data association</h4><p>当物体无法利用该短期数据关联完成时，作者利用前端的<strong>特征点匹配</strong>得到相机的<strong>初始位姿估计</strong>，基于此将物体的特征点映射至空间中以产生临时的3D 点云，结合<strong>外观检测、3D 重叠</strong>以及<strong>物体中心度量</strong>来实现图片中2D 物体与地图中3D 物体的关联。</p>
<p>完整的数据关联如下所示：</p>
<p><img src="/2024/02/26/liu2023a/a1.png" alt="a1" title="algorithm 1"></p>
<h2 id="3-4-Pose-optimization-constrained-by-semantic-information"><a href="#3-4-Pose-optimization-constrained-by-semantic-information" class="headerlink" title="3.4 Pose optimization constrained by semantic information"></a>3.4 Pose optimization constrained by semantic information</h2><p>本文的语义重投影误差采用与文章(Lianos 等, 2018) VSO 相似的方法，主要不同之处在于本文建立<strong>物体对应关系的语义约束</strong>，此外还建立了一个新的损失函数以及优化策略。</p>
<p>理论上来讲，只要一个物体出现在视角中，那么该物体点云的投影像素应该位于相应的掩码区域内，如Fig. 4所示，物体级语义信息对于<strong>视角变换和尺度变换</strong>具有鲁棒性。</p>
<p><img src="/2024/02/26/liu2023a/fig4.png" alt="fig4" title="figure 4"></p>
<p>对于一个新观测帧，获取到一组<strong>匹配点对</strong> $\mathcal{M}_p = \{(z_i^g,p_i)\}$ ，其中，$z_i^g$  为图片中的ORB 点，$p_i$ 为地图中的特征点地标；进行物体级数据关联之后，得到一组<strong>物体匹配对</strong> $\mathcal{M}_o = \{(s_j, o_j)\}$ ，其中，$s_j$ 表示图片中的2D 物体，$o_j$ 表示地图中的3D 物体。利用物体匹配 $\mathcal{M}_o$ 来检查匹配点对 $\mathcal{M}_p$ 的语义一致性：</p>
<p><img src="/2024/02/26/liu2023a/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$\emptyset$ 表示背景类，示例如Fig. 5所示。其中，$\mathcal{M}_p^{base}$ 中的点对表示低等级几何一致性和物体级语义一致性；特别地，式（6）中 $\mathcal{M}_p^{base}$ 中属于不同语义类别的两种情况：</p>
<ul>
<li>一个平面语义特征点与一个背景点地标匹配时（上式第二行），该背景点地标可视为该物体的备选元素；</li>
<li>一个背景特征点与一个语义点地标匹配时（上式第三行），该3D 物体没能与2D 物体匹配上，可能是由于数据关联失败或者实例分割误差的存在。</li>
</ul>
<p>$\mathcal{M}_p^{joint}$ 中的匹配点对不满足语义一致性。</p>
<p><img src="/2024/02/26/liu2023a/fig5.png" alt="fig5" title="figure 5"></p>
<p>整个优化过程分为两步：<strong>初始位姿优化</strong>和<strong>语义约束优化</strong>。</p>
<p>在<strong>初始位姿优化</strong>中，利用 $\mathcal{M}_p^{base}$ 中匹配点对计算传统的重投影误差：</p>
<p><img src="/2024/02/26/liu2023a/f7.png" alt="f7" title="formula 7"></p>
<p>而对于 $\mathcal{M}_p^{joint}$ 中的匹配点对不具有语义一致性，因此<strong>需要更多的约束</strong>来进行位姿优化，此处，作者添加一个<strong>语义重投影误差项</strong>来作为额外约束，定义重投影像素与所属2D 物体最近的距离作为语义残差项，如Fig. 5（a）所示。</p>
<p><img src="/2024/02/26/liu2023a/f8.png" alt="f8" title="formula 8"></p>
<p>式（9）中的 $h(z, m)$ 表示在 m 中搜寻距离 z 最近的255数值点（即2D 物体的掩码区域）；$\hat{T}$ 表示相机的初始位姿。</p>
<p><strong>疑问：</strong>式8中是否还需要第一项，是否只有第二项时会有更优的结果？？？</p>
<p>作者将上述两个误差项在初始优化中进行结合：</p>
<p><img src="/2024/02/26/liu2023a/f10.png" alt="f10" title="formula 10"></p>
<p>在初始位姿优化后，作者为优化<strong>增加额外的语义约束</strong>。基于初始位姿优化得到的位姿，作者将3D 物体中<strong>没有被匹配的地图点</strong>投影至图像中，并搜寻2D 图像中对应最近的像素作为语义观测项，剔除掉那些距离过远的点，如Fig. 5（b）所示，由此构建新的误差项：</p>
<p><img src="/2024/02/26/liu2023a/f11.png" alt="f11" title="formula 11"></p>
<p>从而构建整个位姿优化的误差函数：</p>
<p><img src="/2024/02/26/liu2023a/f14.png" alt="f14" title="formula 14"></p>
<p>上述优化策略可应用于对相机位姿和特征点地标的同时优化中，但是会大幅增加计算复杂度。在作者的实验中，仅利用其对相机位姿进行优化（match local map）来限制约束数量以及计算负载。作者还利用Kd-tree 来加速最邻近点的搜寻。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Pose Optimization</tag>
        <tag>Object SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 View-Invariant Loop Closure with Oriented Semantic Landmarks</title>
    <url>/2024/02/26/li2020/</url>
    <content><![CDATA[<p>Li, Jimmy, Karim Koreitem, David Meger, and Gregory Dudek. “View-Invariant Loop Closure with Oriented Semantic Landmarks.” In <em>2020 IEEE International Conference on Robotics and Automation (ICRA)</em>, 7943–49. Paris, France: IEEE, 2020. <a href="https://doi.org/10.1109/ICRA40945.2020.9196886">https://doi.org/10.1109/ICRA40945.2020.9196886</a>.</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>作者使用物体标识与物体间几何关系来实现具有视角不变性的回环检测与偏移矫正。</p>
<blockquote>
<p>use <strong>object identity and inter-object geometry</strong> for view-invariant loop detection and drift correction</p>
</blockquote>
<p>此外，作者还提出了一个对物体方向进行估计的方法，来克服由于物体对称性造成的模糊度；最终，作者构建了可绘制带有几何细节语义地图（包含物体方向、距离与尺寸信息）的SLAM系统。</p>
<span id="more"></span>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者在前作(Li 等, 2019)（使用物体地标进行视角不变性的重定位）的基础上，使用物体作为高等级的语义地标进行回环检测。</p>
<p><img src="/2024/02/26/li2020/fig1.png" alt="fig1" title="figure 1"></p>
<p>本文利用移动相机捕捉的多视角图像来估计物体的方向，进而解决物体对称性模糊度，这使得本系统可以匹配一个更大的环境来确定位姿。</p>
<h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h1><p>作者使用bounding cuboids这种可以很好容纳目标物体的表示方式来代表物体；且没有选择RGB-D 相机，而是使用更为常见的RGB 相机，有利于本系统的广泛应用。</p>
<p>作者利用SLAM 进行多视角物体方向推断；相较于已有的实例级别方法，作者是在种类级别category level进行方向估计的，允许系统在陌生环境中正常运行。</p>
<h1 id="3-SLAM-System"><a href="#3-SLAM-System" class="headerlink" title="3 SLAM System"></a>3 SLAM System</h1><h2 id="3-1-Overview"><a href="#3-1-Overview" class="headerlink" title="3.1 Overview"></a>3.1 Overview</h2><p>作者认为，基于基于外观的局部特征对于精确跟踪相机的局部位姿是有意义的，但对于大视角变化等明显改变场景外观的情况下不具有鲁棒性；而环境中的物体信息（如常见的家居用品）对于视角变化等情况具有较强的鲁棒性，但在缺少其他信息（如基于外观的局部特征）时难以得到利用，因为在图片平面中计算物体的精确位置是很困难的。因此，作者采取如下的混合策略：</p>
<ol>
<li>使用基于外观的局部特征来跟踪相机局部位姿；</li>
<li>使用已知的相机位姿来简化物体在3D 空间中位姿的推断；</li>
<li>当相机在跨越长基线情况下需要进行回环检测或者重定位时，使用物体地标进行视角不变性的匹配。</li>
</ol>
<p>作者在前作(Li 等, 2019)进行了重定位，本文针对回环检测进行研究。回环识别是通过对由于相机位姿漂移造成的重复物体进行匹配（包括物体标识与几何布局）而实现的。</p>
<p><img src="/2024/02/26/li2020/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-2-Semantic-Mapping"><a href="#3-2-Semantic-Mapping" class="headerlink" title="3.2 Semantic Mapping"></a>3.2 Semantic Mapping</h2><p>制图系统输入为RGB 图像流，输出为估计的相机轨迹，以及以9自由度bounding cuboids表示的物体地标，文章(Lin 等, 2021)借鉴了本文的9自由度bounding cuboids表示方法，包含位置、旋转以及尺寸信息。</p>
<p>本系统是以ORB-SLAM 为基础进行构建的，bounding box探测是由Faster-RCNN 完成的，且作者引入了一种<strong>实例级物体方向回归算法</strong>来提升制图系统的性能。</p>
<p>如Fig. 2所示，作者使用 expectation maximization (EM) 算法来更新物体地标：</p>
<ol>
<li>物体地标投影到每个关键帧中，并与object detections 进行匹配；</li>
<li>将匹配到的detections 与关键帧相机位姿进行结合，进行三角测量并更新物体地标位姿；</li>
<li>没有与已知物体地标匹配成功的detections 初始化为新的地标。</li>
</ol>
<h2 id="3-3-Loop-Detection"><a href="#3-3-Loop-Detection" class="headerlink" title="3.3 Loop Detection"></a>3.3 Loop Detection</h2><p>在长轨迹上进行语义地图绘制时，由于相机位姿漂移会造成重复物体地标被添加进语义地图中。本文系统是建立在ORB-SLAM基础上的，因此保留了其基于外观特征的回环检测，为了应对大视角变化场景下的回环检测，增加了额外的物体级回环检测机制：<strong>在制图环节的每一次EM迭代后，将最近添加的物体地标视为一组近期地标，并与早期的地标进行匹配。</strong>假设地标 <em>l</em> 与 <em>m</em> 分别属于关键帧组合 $K_l, K_m$ （因为同一地标可能会出现在多个关键帧中），定义<em>keyframe separation</em> ：</p>
<p><img src="/2024/02/26/li2020/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$u - w$ 表示两个关键帧索引值（表示两个关键帧被加入地图中的次序）相减，定义一个阈值，若式 (1) 小于某个阈值则判定地标 <em>l</em> 与 <em>m</em> 相近。定义一个集合 <em>L</em>，包含最新的地标 <em>l</em> 以及根据上式得到的与其距离相近的地标集合，目标是找到轨迹中之前遇到过的一组地标与 <em>L</em> 的子集拥有相似的空间分布，这样就可能找到了回环。由于物体地标的稀疏性，可以穷尽迭代所有可能的匹配。</p>
<h2 id="3-4-Geometric-Loop-Verification"><a href="#3-4-Geometric-Loop-Verification" class="headerlink" title="3.4 Geometric Loop Verification"></a>3.4 Geometric Loop Verification</h2><p>假设两组物体地标 $(l_1, l_2, l_3), (m_1, m_2, m_3)$ 实现了上节的初步匹配，进行如下的匹配测试：</p>
<h3 id="3）-Object-layout"><a href="#3）-Object-layout" class="headerlink" title="3） Object layout"></a>3） Object layout</h3><p>若要比较匹配物体的几何布局，需要使用一个共同的参考框架来描述物体，本文使用点来表示各个物体的坐标，基于两组物体地标 $(l_1, l_2, l_3), (m_1, m_2, m_3)$ 分别构建局部坐标系统 A 与 B ，然后计算包括旋转 R、平移 t 以及尺寸 s 的相似度转换，将A中的任意点 $p_l$  映射到 B 中的相应位置 $p_m$ ：</p>
<p><img src="/2024/02/26/li2020/f2.png" alt="f2" title="formula 2"></p>
<p>将 $(l_1, l_2, l_3)$ 从 A 投影到 B 中，然后进行如下测试：</p>
<ul>
<li>Scale consistency：尺度变化不能过大</li>
<li>Translational consistency：位置尺寸不能过大，由于单目RGB相机无法提供绝对距离，作者选取一个尺度归一化的距离进行判断；</li>
<li>Rotational consistency：物体方向不能差异过大。</li>
</ul>
<h2 id="3-5-Loop-Correction"><a href="#3-5-Loop-Correction" class="headerlink" title="3.5 Loop Correction"></a>3.5 Loop Correction</h2><p>对于给定的回环检测，利用相似转换来将物体匹配与观测到这些物体地标的关键帧相机位姿进行对齐，并使用ORB-SLAM 的 essential graph optimizer 将矫正参数通过非线性最小二乘优化散发到所有关键帧，在此基础上更新语义地图。</p>
<p>对于每一个物体地标，选取一个观测到该地标的关键帧，计算该关键帧更新前后的相对位姿变换，并将该变换参数更新到地标位姿上去，使用EM算法细调所有的地标位姿。</p>
<h1 id="4-Orientation-Regression"><a href="#4-Orientation-Regression" class="headerlink" title="4 Orientation Regression"></a>4 Orientation Regression</h1><h2 id="4-1-Overview"><a href="#4-1-Overview" class="headerlink" title="4.1 Overview"></a>4.1 Overview</h2><p>Fig. 3展现了物体的对称性造成的物体方向回归算法不收敛的难题，作者在本文主要针对的是镜面对称问题，并简单讨论了圆柱对称问题。</p>
<p><img src="/2024/02/26/li2020/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="4-2-Multi-view-Orientation-Inference"><a href="#4-2-Multi-view-Orientation-Inference" class="headerlink" title="4.2 Multi-view Orientation Inference"></a>4.2 Multi-view Orientation Inference</h2><p>作者将视角球形分为几部分，其中任意部分的不同视角展现不同的外观，如Fig. 4所示，作者将视角分为4个象限，并对每一部分训练一个方向回归函数。</p>
<p><img src="/2024/02/26/li2020/fig4.png" alt="fig4" title="figure 4"></p>
<h1 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5 Experiments"></a>5 Experiments</h1><h2 id="5-1-Loop-Closure"><a href="#5-1-Loop-Closure" class="headerlink" title="5.1 Loop Closure"></a>5.1 Loop Closure</h2><p>现有的SLAM数据集在回环检测方面基本上使用视角变化不大的同一场景照片，本方法的优势无法展现，故作者采用自采的数据集进行实验，回环检测图片使用的是具有较大视角变化的图片，结果如Fig. 6所示，实验使用5种立方物体（屏幕、键盘、手机、遥控器以及微波炉）、4种圆柱物体（水杯、瓶子、碗以及花盆绿植）作为地标物体并进行回环检测。</p>
<p><img src="/2024/02/26/li2020/fig6.png" alt="fig6" title="figure 6"></p>
<p>实验中漂移减少百分比定义如下：</p>
<p><img src="/2024/02/26/li2020/f4.png" alt="f4" title="formula 4"></p>
<p>S是共同的起始位置坐标，E是本文方法估计的结束位置坐标，e是ORB-SLAM 估计的位置坐标。Table 1 展示了所有序列的偏移减少量。</p>
<p><img src="/2024/02/26/li2020/t1.png" alt="t1" title="table 1"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Topology</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Topology</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Towards Real-time Semantic RGB-D SLAM in Dynamic Environments</title>
    <url>/2024/02/27/ji2021/</url>
    <content><![CDATA[<p>Ji, Tete, Chen Wang, and Lihua Xie. “Towards Real-Time Semantic RGB-D SLAM in Dynamic Environments.” In <em>2021 IEEE International Conference on Robotics and Automation (ICRA)</em>, 11175–81. Xi’an, China: IEEE, 2021. <a href="https://doi.org/10.1109/ICRA48506.2021.9561743">https://doi.org/10.1109/ICRA48506.2021.9561743</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文做出的贡献：</p>
<ul>
<li>提出一个<strong>基于关键帧</strong>的语义RGB-D SLAM 系统，可以减少动态环境中移动物体的影响；</li>
<li>提出一个高效的<strong>几何模块</strong>，与语义SLAM 框架相结合来处理<strong>未知的移动物体</strong>；</li>
<li>通过实验证明本算法可在<strong>嵌入式系统中实时运行</strong>，同时可实现与SOTA 方法相当的精度。</li>
</ul>
<span id="more"></span>
<h1 id="3-Proposed-Method"><a href="#3-Proposed-Method" class="headerlink" title="3 Proposed Method"></a>3 Proposed Method</h1><p>本系统是基于ORB-SLAM2 算法的，系统框架如下图所示：</p>
<p><img src="/2024/02/27/ji2021/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-Semantic-Module"><a href="#3-1-Semantic-Module" class="headerlink" title="3.1 Semantic Module"></a>3.1 Semantic Module</h2><p>考虑到精度与速度的平衡，作者选用<strong>SegNet</strong> 作为语义分割的网络；为了进一步实现实时处理性能，作者<strong>只对关键帧</strong>进行语义分割，将潜在动态物体（如人、汽车、自行车等）上的特征点进行<strong>剔除</strong>；值得注意的是，作者<strong>并不进一步使用几何模块</strong>对潜在动态物体的<strong>真实运动情况</strong>进行确认，而是认为，对于长期一致的制图目的而言，即便这些潜在动态物体在某些时刻是静态的，但是长远考虑的话它们<strong>并不可靠</strong>，所以作者直接将这些潜在动态物体进行剔除。</p>
<h2 id="3-2-Geometry-Module"><a href="#3-2-Geometry-Module" class="headerlink" title="3.2 Geometry Module"></a>3.2 Geometry Module</h2><p>语义分割只能对训练时的标记种类进行识别，<strong>无法对未知类别</strong>的物体进行识别，因此，作者引入<strong>几何模块</strong>来对<strong>未知动态物体</strong>进行检测。</p>
<p>作者首先利用<strong>K-Means 算法</strong>将深度图中的点分割为N 个集群，3D 空间中<strong>距离较近的点</strong>被分为一组集群；作者假设每个集群属于一个物体表面，且同一集群内的点的<strong>运动状态一致</strong>。因为同一个物体可能被分为几个不同的集群，所以本算法中的物体<strong>可以不满足刚体假设</strong>。</p>
<p>对于每个集群 $c_j$ ，作者计算该集群内所有特征点 $\mathbf{u}_i’$ 与其在3D 空间中的关联点 $\mathbf{P}_i$ 之间的重投影误差，求得集群内的重投影误差均值 $r_j$ ：</p>
<p><img src="/2024/02/27/ji2021/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$\rho$ 是正则化方程（惩罚方程）。如果某个集群的平均重投影误差相较于其他集群更大，则<strong>标记该集群是动态的</strong>，并移除掉属于该集群的所有特征点。部分实验结果如Fig. 3所示，观察可得，在部分情况下语义分割会出现识别失败或错误的现象，而此时几何模块可以正常识别出动态物体。</p>
<p><img src="/2024/02/27/ji2021/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="3-3-Keyframe-and-Local-Map-Update"><a href="#3-3-Keyframe-and-Local-Map-Update" class="headerlink" title="3.3 Keyframe and Local Map Update"></a>3.3 Keyframe and Local Map Update</h2><p>由于一帧图片是使用<strong>关键帧</strong>和<strong>局部地图</strong>进行<strong>跟踪</strong>的，所以只需要确保关键帧和局部地图中只有静态特征点即可。当一个新的关键帧被选取后，利用语义分割识别出动态特征点，局部地图中也会<strong>同步移除</strong>相应的动态地图点，由此，可以保持一个只包含静态特征与地图点的数据库。</p>
<h2 id="3-4-Tracking"><a href="#3-4-Tracking" class="headerlink" title="3.4 Tracking"></a>3.4 Tracking</h2><p>本系统对每一帧新图片的跟踪采用类似于ORB-SLAM2 的<strong>两步法</strong>：</p>
<ol>
<li>首先，利用当前帧和与当前帧有最大重合区域的关键帧进行初始位姿估计，因为关键帧<strong>已经移除了潜在动态物体</strong>，所以初始位姿估计<strong>更可靠</strong>；</li>
<li>然后，几何模块利用初始位姿估计进行<strong>动态物体检测</strong>，并从当前帧中移除所有的动态特征点，在此基础上跟踪当前帧中所有观测到的地图点进行<strong>局部BA 优化</strong>，获取最终的位姿估计。</li>
</ol>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Fusing Semantic Segmentation and Object Detection for Visual SLAM in Dynamic Scenes</title>
    <url>/2024/02/27/yu2021/</url>
    <content><![CDATA[<p>Yu, Peilin, Chi Guo, yang Liu, and Huyin Zhang. “Fusing Semantic Segmentation and Object Detection for Visual SLAM in Dynamic Scenes.” In <em>Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology</em>, 1–7. VRST ’21. New York, NY, USA: Association for Computing Machinery, 2021. <a href="https://doi.org/10.1145/3489849.3489882">https://doi.org/10.1145/3489849.3489882</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文针对SLAM 算法环境中存在的动态物体，结合<strong>物体检测</strong>和<strong>语义分割</strong>来获取<strong>潜在动态物体</strong>的先验轮廓，在此基础上应用<strong>几何约束</strong>实现对动态特征点的剔除。</p>
<p>本文做出的贡献：</p>
<ol>
<li>提出一种<strong>自适应机制</strong>，使得系统可根据不同的环境选择使用语义分割还是物体检测；</li>
<li>提出一种<strong>静态点恢复技术</strong>，以减少可用静态点的损失，并使用光流和对极约束来检查物体的状态；</li>
<li>在公开数据集TUM 上进行评估，在<strong>高动态环境</strong>下实现了良好的表现。</li>
</ol>
<span id="more"></span>
<h1 id="3-Our-Method"><a href="#3-Our-Method" class="headerlink" title="3 Our Method"></a>3 Our Method</h1><p>本文方法的架构如Fig. 1所示，利用<strong>自适应机制</strong>来切换物体检测分支和语义分割分支，然后结合<strong>几何约束</strong>技术实现对动态特征点的剔除。</p>
<p><img src="/2024/02/27/yu2021/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-1-Extract-Semantic-Information"><a href="#3-1-Extract-Semantic-Information" class="headerlink" title="3.1 Extract Semantic Information"></a>3.1 Extract Semantic Information</h2><p>在<strong>自适应机制模块</strong>中，作者通过多次实验得到一个<strong>阈值</strong>，来决定是否有必要使用语义分割进行处理：</p>
<ul>
<li>如果得分超过这个阈值，说明动态物体占据了当前视野的较大部分。此时，需要进一步使用语义分割来<strong>尽可能地保留静态特征点</strong>（Fig. 2）；</li>
<li>如果得分低于这个阈值，说明动态物体占据了当前视野地较小部分。此时，仅利用物体检测技术即可（Fig. 3）。</li>
</ul>
<p><img src="/2024/02/27/yu2021/fig2.png" alt="fig2" title="figure 2"></p>
<p><img src="/2024/02/27/yu2021/fig3.png" alt="fig3" title="figure 3"></p>
<p>得分的计算方程：</p>
<p><img src="/2024/02/27/yu2021/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$f(u,v)$ 表示若像素被判定属于潜在动态物体则为1，否则为0；$P_I$ 表示所有像素数量。即，上式表示判定为动态物体像素占总体像素的比例。</p>
<h2 id="3-2-Remove-Dynamic-Features"><a href="#3-2-Remove-Dynamic-Features" class="headerlink" title="3.2 Remove Dynamic Features"></a>3.2 Remove Dynamic Features</h2><p>使用以下策略对动态特征点进行准确剔除：</p>
<h3 id="3-2-1-Dynamic-check"><a href="#3-2-1-Dynamic-check" class="headerlink" title="3.2.1 Dynamic check"></a><strong>3.2.1 Dynamic check</strong></h3><p>使用DS-SLAM 中的<strong>移动一致性检测</strong>来判断是否属于动态特征点：潜在动态物体内部的动态特征点超过一定阈值，则判定该物体是动态物体，并移除属于该物体的所有特征点。该策略同时应用于两个分支中。</p>
<h3 id="3-2-2-Static-point-recovery"><a href="#3-2-2-Static-point-recovery" class="headerlink" title="3.2.2 Static point recovery"></a><strong>3.2.2 Static point recovery</strong></h3><p>作者提出一个静态点恢复技术 static point recovery (SPR) 恢复bbox 内的部分静态点来增强系统的鲁棒性。对于物体检测网络处理过的图片，作者将潜在动态物体bbox 外部的点视为静态点，记为第一部分；bbox 内部的点视为潜在动态点，记为第二部分。SPR 技术是针对第二部分的特征点进行恢复操作的，具体操作步骤如下所示：</p>
<p>首先，利用第一部分（即静态特征点）中的所有特征点计算两两之间线段的距离，在相邻帧之间构建 line segment constraint，如Fig. 4所示，</p>
<p><img src="/2024/02/27/yu2021/fig4.png" alt="fig4" title="figure 4"></p>
<p><img src="/2024/02/27/yu2021/f2.png" alt="f2" title="formula 2"></p>
<p>所谓的 line segment constraint，作者认为<strong>静态特征点对</strong>之间的距离在相邻帧之间变化不大，则统计第一部分所有点对在相邻帧之间的距离变化值，用其平均值作为阈值 $\phi_t$ 。</p>
<p><img src="/2024/02/27/yu2021/f5.png" alt="f5" title="formula 5"></p>
<p>然后，利用所有静态点和bbox 内的潜在动态点 $k$ 进行组合，计算潜在动态点 $k$ 的得分：</p>
<p><img src="/2024/02/27/yu2021/f6.png" alt="f6" title="formula 6"></p>
<p>作者判定，若得分 $s_{k, t}$ 超过组合数量的一半，则判定潜在动态点 $k$ 为真实动态点并剔除，否则保留为静态特征点。SPR 效果如Fig. 5所示。</p>
<p><img src="/2024/02/27/yu2021/fig5.png" alt="fig5" title="figure 5"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 The STDyn-SLAM_A Stereo Vision and Semantic Segmentation Approach for VSLAM in Dynamic Outdoor Environments</title>
    <url>/2024/02/28/stdyn-slam/</url>
    <content><![CDATA[<p>Esparza, Daniela, and Gerardo Flores. “The STDyn-SLAM: A Stereo Vision and Semantic Segmentation Approach for VSLAM in Dynamic Outdoor Environments.” <em>IEEE Access</em> 10 (2022): 18201–9. <a href="https://doi.org/10.1109/ACCESS.2022.3149885">https://doi.org/10.1109/ACCESS.2022.3149885</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献如下：</p>
<ul>
<li>提出一个针对动态环境的立体SLAM 算法——STDyn-SLAM，结合<strong>语义分割</strong>神经网络和<strong>几何约束</strong>对动态物体进行剔除；</li>
<li>立体相机的深度图用于构建3D 八叉树地图重建，深度图对于本SLAM 不是必要的；</li>
<li>利用公开数据集进行测试，并于SOTA 方法进行对比；</li>
<li>开源代码：<a href="https://github.com/DanielaEsparza/STDyn-SLAM">https://github.com/DanielaEsparza/STDyn-SLAM</a></li>
</ul>
<span id="more"></span>
<h1 id="3-Method"><a href="#3-Method" class="headerlink" title="3 Method"></a>3 Method</h1><p>本文提出的STDyn-SLAM 算法框架如Fig. 2所示，本系统基于ORB-SLAM2 算法：</p>
<p><img src="/2024/02/28/stdyn-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-Stereo-Process"><a href="#3-1-Stereo-Process" class="headerlink" title="3.1 Stereo Process"></a>3.1 Stereo Process</h2><p>对立体图片的处理主要包含三个部分：</p>
<ul>
<li>ORB 特征提取</li>
<li>光流跟踪</li>
<li>对极几何</li>
</ul>
<p>首先对当前左右图像和前一帧左边图像进行ORB 特征提取；为了避免神经网络检测动态物体失败的情况，本系统计算当前与前一帧左图之间的<strong>光流</strong>，值得注意的是，这些光流点（Harris 点）需要与ORB 特征点<strong>保持不同</strong>，舍弃那些在/靠近边缘角点上的光流点；根据<strong>基础矩阵</strong>、<strong>ORB 特征点</strong>以及<strong>光流点</strong>来计算<strong>极线</strong>，然后将当前帧匹配的特征点映射到前一帧，根据极线与投影特征点之间的距离来判断是否属于外点。</p>
<h2 id="3-2-Artificial-Neural-Network‘s-Architecture"><a href="#3-2-Artificial-Neural-Network‘s-Architecture" class="headerlink" title="3.2 Artificial Neural Network‘s Architecture"></a>3.2 Artificial Neural Network‘s Architecture</h2><p>本算法使用SegNet 作为语义分割网络。</p>
<h3 id="3-2-1-Outliers-Removal"><a href="#3-2-1-Outliers-Removal" class="headerlink" title="3.2.1 Outliers Removal"></a>3.2.1 Outliers Removal</h3><p>Fig. 3展示了判断投影点的三种情况，设定一个阈值进行外点的筛选。</p>
<p><img src="/2024/02/28/stdyn-slam/fig3.png" alt="fig3" title="figure 3"></p>
<p>利用语义分割结果来剔除属于<strong>潜在动态物体</strong>上的ORB 特征点，如汽车。</p>
<h3 id="3-2-3-3D-Reconstruction"><a href="#3-2-3-3D-Reconstruction" class="headerlink" title="3.2.3 3D Reconstruction"></a>3.2.3 3D Reconstruction</h3><p>利用左图、语义分割图以及利用视觉里程计得到的深度图进行3D 重建，值得注意的是，本算法<strong>只对左图</strong>进行语义分割、光流跟踪以及几何约束，以减少时间消耗。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Multimodal Semantic SLAM with Probabilistic Data Association</title>
    <url>/2024/02/28/doherty2019/</url>
    <content><![CDATA[<p>Doherty, Kevin, Dehann Fourie, and John Leonard. “Multimodal Semantic SLAM with Probabilistic Data Association.” In <em>2019 International Conference on Robotics and Automation (ICRA)</em>, 2419–25. Montreal, QC, Canada: IEEE, 2019. <a href="https://doi.org/10.1109/ICRA.2019.8794244">https://doi.org/10.1109/ICRA.2019.8794244</a>.</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p><strong>语义SLAM</strong> 可被分解为：</p>
<ul>
<li>一个<strong>离散</strong>的推理问题：决定物体种类与观测地标之间的联系（data association, DA)</li>
<li>一个<strong>连续</strong>的推理问题：获取机器人的位姿和地标的位置。</li>
</ul>
<p>在模糊的DA 情况下，语义SLAM 通常不是一个高斯推理过程，现有的工作多是基于潜在的假设或者使用多重可能假设进行求解的。而作者提出了一种将DA 假设表示为一种<strong>等效非高斯模型的多模态形式</strong>的解算方法。</p>
<span id="more"></span>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>如图1所示，即便那些可以用高斯分布表示的观测模型在<strong>DA 和地标种类模糊</strong>的情况下也可以使用一个<strong>非高斯观测模型</strong>来表示。</p>
<p><img src="/2024/02/28/doherty2019/fig1.png" alt="fig1" title="figure 1"></p>
<p>本文做出的贡献：</p>
<ol>
<li>提出一个<strong>nonparametric belief propagation</strong> 方法，为带有模糊DA 的语义SLAM 进行<strong>后验推理</strong>；</li>
<li>描述了<strong>multimodal semantic factors</strong> ，使得我们可以将<strong>不确定性</strong>包含到DA 和语义中，作为因子图中的<strong>非高斯因子</strong>，从而利用mm-iSAM (multimodal incremental smoothing and mapping) 进行位姿与地标的<strong>连续优化</strong>；</li>
<li>在仿真与真实数据集中进行实验，证明了该方法在DA 和地标种类模糊方面的鲁棒性。</li>
</ol>
<h1 id="3-Semantic-SLAM-with-Ambiguous-Data-Association"><a href="#3-Semantic-SLAM-with-Ambiguous-Data-Association" class="headerlink" title="3 Semantic SLAM with Ambiguous Data Association"></a>3 Semantic SLAM with Ambiguous Data Association</h1><h2 id="3-1-Semantic-SLAM-with-Known-Data-Association"><a href="#3-1-Semantic-SLAM-with-Known-Data-Association" class="headerlink" title="3.1 Semantic SLAM with Known Data Association"></a>3.1 Semantic SLAM with Known Data Association</h2><p>本文使用最大后验估计理论进行求解：</p>
<p><img src="/2024/02/28/doherty2019/f2.png" alt="f2" title="formula 2"></p>
<h2 id="3-2-Probabilistic-Data-Association"><a href="#3-2-Probabilistic-Data-Association" class="headerlink" title="3.2 Probabilistic Data Association"></a>3.2 Probabilistic Data Association</h2><p>为解决带有模糊DA 的语义SLAM 问题，作者分两步<strong>交替计算</strong>DA 概率与位姿、地标位置：</p>
<p>第一步，边缘化位姿与地标来计算DA 概率：</p>
<p><img src="/2024/02/28/doherty2019/f7.png" alt="f7" title="formula 7"></p>
<p>其中，$\eta_{\mathcal{D}}$ 为归一化常数；$\Theta = \{\mathcal{X}, \mathcal{L}\}$ 分别表示<strong>相机位姿与地标</strong>。</p>
<p>第二步，边缘化DA 来获取位姿与地标的分布：</p>
<p><img src="/2024/02/28/doherty2019/f8.png" alt="f8" title="formula 8"></p>
<p>作者只在接收到观测之后进行一次迭代的DA 概率计算，从而减少了计算负担。</p>
<h1 id="4-Multimodal-Semantic-SLAM"><a href="#4-Multimodal-Semantic-SLAM" class="headerlink" title="4 Multimodal Semantic SLAM"></a>4 Multimodal Semantic SLAM</h1><h2 id="4-1-Multimodal-iSAM"><a href="#4-1-Multimodal-iSAM" class="headerlink" title="4.1 Multimodal iSAM"></a>4.1 Multimodal iSAM</h2><p>SLAM 中的因子图表示为：</p>
<p><img src="/2024/02/28/doherty2019/f10.png" alt="f10" title="formula 10"></p>
<p>其中，$\varphi$ 表示<strong>观测因子</strong>，$\psi$ 表示<strong>先验因子</strong>。这里，因子图是一个间接的图模型，位姿与地标是被观测因子与先验因子连接起来的<strong>隐变量</strong>。每个变量的边缘分布可以使用<strong>belief propagation</strong> 进行解算，该方法在变量服从高斯分布时可以产生方便的分析形式。</p>
<p>multimodal iSAM 利用<strong>nonparametric belief propagation</strong> 来容纳非高斯变量，该方法在没有高斯假设的情况下，使用Gibbs 采样和kernel density estimation 组合方法来近似所有连续状态变量的<strong>置信度</strong>。对于随机变量X：</p>
<p><img src="/2024/02/28/doherty2019/f11.png" alt="f11" title="formula 11"></p>
<p>其中，$\mathcal{N}$ 是多元高斯核 multivariate Gaussian kernel，每个核的中心位于一个采样 $x^{[n]}$ ， $w^{[n]}$ 是相应核的权重（采用均匀分布），而 $\sum^{[n]}$ 是相应高斯核的带宽，通过leave-one-out 交叉验证来获取。</p>
<p>该方法的一个好处在于，在后验估计中不再需要直接表示多种潜在可能的modes，这种<strong>隐含表示</strong>将推理的复杂度从假设数量中<strong>解耦</strong>了出来，因为在近似边缘化中的计算只依赖于采样的<strong>固定数量</strong>。结果就是，具有非常低概率的modes 在近似边缘分布中不太可能会出现，但是这些modes 也不会直接被舍弃，会继续存在于因子图中，这样那些后面变得更有可能的modes 可以得到<strong>恢复</strong>。</p>
<h2 id="3-2-Multimodal-Semantic-Factors"><a href="#3-2-Multimodal-Semantic-Factors" class="headerlink" title="3.2 Multimodal Semantic Factors"></a>3.2 Multimodal Semantic Factors</h2><p>设定一个语义观测模型因子为</p>
<p>$p(y_t^k | x_t, l_j) = p(y_t^{k,c} | l_j^c)p(y_t^{k,r} | x_t, l_j)p(y_t^{k,b} | x_t, l_j)$</p>
<p>其中，$y_t^{k,c}$ 表示路标<strong>类别</strong>，通过物体检测网络获取；$y_t^{k,r}$  表示到路标的<strong>距离</strong>；$y_t^{k,b}$ 表示物体的<strong>方位</strong>。</p>
<p>假设 $p(y_t^{k,r} | x_t, l_j), p(y_t^{k,b} | x_t, l_j)$ 服从高斯分布，均值与方差分别为 $y_t^{k,r}$  ，$y_t^{k,b}$  和 $\sigma_t^{2,k,r}, \sigma_t^{2,k,b}$ 。</p>
<p>假设一个统一的先验DA：</p>
<p><img src="/2024/02/28/doherty2019/f12.png" alt="f12" title="formula 12"></p>
<p>其中，$\mathbb{D}\{d_t^k = j\} = \{\mathcal{D}_t \in \mathbb{D}_t | d_t^k = j\}$ 表示在时间t 所有可能的数据关联集合，其中观测 k 与地标 j 相关联。</p>
<p>在给定数据关联 $d_t^k$  情况下边缘化位姿估计、地标位置和种类，来计算每个观测 $y_t^k$  的似然：</p>
<p><img src="/2024/02/28/doherty2019/f13.png" alt="f13" title="formula 13"></p>
<p>上式中，使用<strong>采样</strong>来逼近位姿分布上的积分计算，对于DA 计算，作者采纳了一个最大似然模型来简化地标位置的积分计算，作者发现该方法在高斯分布模型上效果很好，对于非高斯模型可以使用基于采样的逼近策略来近似。</p>
<p>对于属于集合 $\mathcal{J} \subseteq\mathcal{L}$ 中的所有地标 $l_j$ ，给定 $\hat{p}(d_t^k = j)$ ，一个multimodal 语义因子将位姿 $x_t$ 与 $\mathcal{J}$  中的每一个候选联系起来：</p>
<p><img src="/2024/02/28/doherty2019/f14.png" alt="f14" title="formula 14"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Visual-Based Semantic SLAM with Landmarks for Large-Scale Outdoor Environment</title>
    <url>/2024/02/28/zhao2019/</url>
    <content><![CDATA[<p>Zhao, Zirui, Yijun Mao, Yan Ding, Pengju Ren, and Nanning Zheng. “Visual-Based Semantic SLAM with Landmarks for Large-Scale Outdoor Environment.” In <em>2019 2nd China Symposium on Cognitive Computing and Hybrid Intelligence (CCHI)</em>, 149–54, 2019. <a href="https://doi.org/10.1109/CCHI.2019.8901910">https://doi.org/10.1109/CCHI.2019.8901910</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ul>
<li>通过将视觉SLAM 地图和语义分割信息进行融合，来构建大型户外环境的<strong>语义3D 地图</strong>；</li>
<li>扩充KITTI 数据集以包含<strong>GPS 信息</strong>，以及从<strong>Google Map 上获取的相关地标标签</strong>；</li>
<li>提出一个基于语义地图将真实世界的地标和点云地图联系起来，以构建一个<strong>拓扑地图</strong>的方法。</li>
</ul>
<span id="more"></span>
<h1 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3 Approach"></a>3 Approach</h1><h2 id="3-1-System-overview"><a href="#3-1-System-overview" class="headerlink" title="3.1 System overview"></a>3.1 System overview</h2><p>系统框架如Fig. 1所示：</p>
<p><img src="/2024/02/28/zhao2019/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-2-Semantic-mapping"><a href="#3-2-Semantic-mapping" class="headerlink" title="3.2 Semantic mapping"></a>3.2 Semantic mapping</h2><p>作者使用PSPNet-101 模型对图片进行语义分割，利用TensorRT 进行实施<strong>推理加速</strong>。</p>
<p>利用ORB-SLAM2 进行<strong>3D 重建</strong>和<strong>轨迹估计</strong>。</p>
<p>利用<strong>贝叶斯更新准则</strong>来为每个地图点的<strong>语义标签概率分布</strong>进行更新。</p>
<p>3D 点云坐标与像素坐标的转换关系：</p>
<p><img src="/2024/02/28/zhao2019/f1.png" alt="f1" title="formula 1"></p>
<p>在特征点被投影至相机坐标系后，可得到每个特征点在19个类别标签上的<strong>概率分布</strong>：</p>
<p><img src="/2024/02/28/zhao2019/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$L_m(x_m, y_m, z_m)$ 表示地图点 $(x_m, y_m, z_m)$ 的类别标签；$F_s$ 表示语义分割后当前帧图片像素在每个类别标签上的概率分布。此外，由于同一特征点可在多帧图像中被观测到，因此需要进行<strong>多观测数据融合</strong>操作，此处利用<strong>贝叶斯更新</strong>：</p>
<p><img src="/2024/02/28/zhao2019/f4.png" alt="f4" title="formula 4"></p>
<p>其中，Z 是归一化常数；$l_k^m$ 表示地图点 m 在帧 k 上的标签；$p(l_l^m | F_{1:k}, P_{1:k})$ 表示从第一帧到 k 帧的<strong>累积概率分布</strong>。</p>
<p>最终，每个地图点的标签通过选取最大概率值来确定：</p>
<p><img src="/2024/02/28/zhao2019/f6.png" alt="f6" title="formula 6"></p>
<p>在实时融合过程中，每个地图点会包含一个语义标签以及一个语义概率分布。</p>
<h2 id="3-3-GPS-fusion"><a href="#3-3-GPS-fusion" class="headerlink" title="3.3 GPS fusion"></a>3.3 GPS fusion</h2><p>为了将<strong>建筑地标</strong>和<strong>点云</strong>在像素级别上联系起来以产生<strong>语义点云</strong>，作者将建筑地标的WGS84 坐标转换至点云坐标系下。作者发现从Goole Map API 获取的WGS84 坐标系经纬度不适合直接转换，因此，首先将其转换至<strong>Cartesian 坐标系</strong>下，单位是米；然后利用现有的方法将其与<strong>点云坐标系</strong>进行对齐。设$P_A$ 为Cartesian 坐标下的点云，其中心为 $centroid_A$ ；$P_B$ 为位姿坐标系下的点云，其中心为 $centroid_B$ ，由于两者的尺度不一致，还需进行尺度转换。旋转矩阵与平移矩阵的计算如下所示：</p>
<p><img src="/2024/02/28/zhao2019/f7.png" alt="f7" title="formula 7"></p>
<p><img src="/2024/02/28/zhao2019/f9.png" alt="f9" title="formula 9"></p>
<p>其中，$\lambda$ 为<strong>尺度因子</strong>，计算方式如下：</p>
<p><img src="/2024/02/28/zhao2019/f11.png" alt="f11" title="formula 11"></p>
<h2 id="3-4-Post-process"><a href="#3-4-Post-process" class="headerlink" title="3.4 Post process"></a>3.4 Post process</h2><p>利用地标的GPS 信息和语义标签，可以将地标级数据和3D 重建结果融合起来，可方便用于任务导向的导航问题。作者使用<strong>基于模糊数学的方法</strong> fuzzy-mathematics-based method 进行地标数据融合：不关注地标位置的准确性，而是关注<strong>地标位置的归属分布</strong>，这点是模仿人类感知习惯的。作者尝试基于<strong>高斯概率分布</strong>来评估位置归属性，如果一个位置接近某个地标，那么根据高斯分布该位置的归属性会更高；<strong>归属性</strong>定义如下所示：</p>
<p><img src="/2024/02/28/zhao2019/f13.png" alt="f13" title="formula 13"></p>
<p>其中，$(x_l, y_l)$ 表示地标位置；$\sigma$ 表示高斯分布的标准差。</p>
<p>拓扑地图如Fig. 3所示，其只包含地标之间的<strong>可抵达关系</strong>以及它们的<strong>几何关系</strong>，拓扑地图中只有节点和边，适合全局路径规划。</p>
<p><img src="/2024/02/28/zhao2019/fig3.png" alt="fig3" title="figure 3"></p>
<h1 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h1><p>现有的工作主要聚焦于准确性或者实时性能，但这对于<strong>机器人整体感知层面</strong>的提升较小；作者利用带有GPS 信息的KITTI 数据集进行<strong>地标语义融合</strong>以及<strong>拓扑语义制图</strong>，结合Google Map API，可构建包含<strong>真实名字与位置信息的地标</strong>，使得离线人机语言交互、任务导向的导航或者地标级的定位成为可能。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Semantic Segmentation-Based Lane-Level Localization Using Around View Monitoring System</title>
    <url>/2024/02/29/deng2019/</url>
    <content><![CDATA[<p>Deng, Liuyuan, Ming Yang, Bing Hu, Tianyi Li, Hao Li, and Chunxiang Wang. “Semantic Segmentation-Based Lane-Level Localization Using Around View Monitoring System.” <em>IEEE Sensors Journal</em> 19, no. 21 (November 1, 2019): 10077–86. <a href="https://doi.org/10.1109/JSEN.2019.2929135">https://doi.org/10.1109/JSEN.2019.2929135</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>一般来讲，对于使用视觉相机进行定位的方法是：通过从图片中提取特征，并与一个先验地图进行匹配从而得到汽车位置；许多方法是使用前置单目相机或立体相机来采集数据，但是在交通繁忙的场景中，其field of view (FOV) 容易被其他车辆干扰导致效果不佳。本文提出的方法采用<strong>Around View Monitoring (AVM)</strong> 系统来减小环境干扰的影响。</p>
<p>AVM 系统中鱼眼相机采集到的图片通常会进行去畸变、合成一张顶视图片（鸟瞰视角图片，如下图所示），若要通过检测车道线等标志进行车辆定位，经过处理后的图片覆盖范围有限，仅能获取当前车道线内的路标，难以区分道路上不同车道线之间的区别；故本文使用鱼眼相机采集到的原始图片进行车道线级别的定位。</p>
<span id="more"></span>
<p><img src="/2024/02/29/deng2019/fig1.png" alt="fig1" title="figure 1"></p>
<p>对于鱼眼相机图片的使用存在两个问题：</p>
<ol>
<li><strong>车辆周围存在动态物体的干扰</strong>：动态物体的干扰会导致定位的不稳定，为解决此问题，本文采用基于CNN 的语义分割来提取道路特征（如道路边界、标志等），同时利用语义场景理解，将动态物体进行剔除；</li>
<li><strong>道路边界可以提供横向位置信息，但是当距离车辆比较远时会有比较大的不确定度</strong>：本文提出了<strong>Coarse-Scale Localization (CSL)</strong> 方法与<strong>Fine-Scale Localization (FSL)</strong> 方法相结合来实现精准定位，CSL 利用道路边界来估计一个粗略的位置，将该粗略位置作为FSL的初始值进行高精度定位。由于边界点具有区别度很大的不确定性，CSL 采用weight Iterative Closest Point （ICP) 来提高匹配准确度，并给出置信度。FSL 通过将车辆附近的路标与先验地图进行匹配，将该匹配结果息与运动信息相结合得到最终的车辆位置信息。</li>
</ol>
<p>本文做出的主要贡献：</p>
<ol>
<li>对AVM 系统的鱼眼相机采集到的图片进行语义分割实现对道路特征的提取：<ul>
<li>利用语义环境感知来识别出正确的道路边界点，剔除动态物体的干扰；</li>
<li>检测线形标志（车道线等）与非线形标志（箭头等）来提高横向与纵向的定位精度。</li>
</ul>
</li>
<li>提出了 CSL 与 FSL 方法，充分利用具有不同精确度的道路特征来实现高精度定位；</li>
<li>仅使用AVM系统、GPS、proprioceptive sensors (IMU 、里程计) 以及先验地图来实现城市环境车辆的分米级定位。</li>
</ol>
<h1 id="3-System-Framework"><a href="#3-System-Framework" class="headerlink" title="3 System Framework"></a>3 System Framework</h1><p>系统包括四大模块：detection, map manager, CSL and FSL。</p>
<p><img src="/2024/02/29/deng2019/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-detection-module"><a href="#3-1-detection-module" class="headerlink" title="3-1 detection module"></a>3-1 detection module</h2><p>该模块检测道路边界与路标，并将检测到的图像像素转换为2D点：使用路标的轮廓点进行表示，使得不同的路标可以用统一的方式进行表示，有着更小的契合误差。</p>
<h2 id="3-2-map-manager"><a href="#3-2-map-manager" class="headerlink" title="3-2 map manager"></a>3-2 map manager</h2><p>该模块包括先验<strong>道路边界地图与路标地图</strong>，GPS提供一个粗略的位置信息，根据地图中的道路结构进行约束，沿着车辆行驶方向对齐到距离最近的车道线中间线上的点。</p>
<h2 id="3-3-CSL"><a href="#3-3-CSL" class="headerlink" title="3-3 CSL"></a>3-3 CSL</h2><p>该模块通过匹配道路边界线来提供一个粗略的位置信息，目标是输出一个横向定位精度小于半个车道宽度的位置信息作为FSL 的初始值。<strong>置信度计算与weighted ICP 是同时、独立进行的</strong>，如果置信度值低于设定的阈值，则判定匹配结果无效。</p>
<h2 id="3-4-FSL"><a href="#3-4-FSL" class="headerlink" title="3-4 FSL"></a>3-4 FSL</h2><p>将检测到的2D道路标志点进行累积形成一个本地道路标志地图，然后将该地图与先验路标地图使用 ICP 进行匹配，得到车辆的位置信息。由于本地地图匹配结果通常包含<strong>未知的时间相关性</strong>，本文使用<strong>Split Covariance Intersection Filter (Split CIF)</strong> ——可被视为Kalman滤波器的泛化，擅长处理具有未知相关性的数据——来融合匹配结果与动作数据。</p>
<h2 id="4-Semantic-Segmentation-Based-Road-Boundary-and-Road-Marking-Detection"><a href="#4-Semantic-Segmentation-Based-Road-Boundary-and-Road-Marking-Detection" class="headerlink" title="4 Semantic Segmentation-Based Road Boundary and Road Marking Detection"></a>4 Semantic Segmentation-Based Road Boundary and Road Marking Detection</h2><h2 id="4-1-Semantic-Segmentation-on-Raw-Fisheye-Images-From-AVM"><a href="#4-1-Semantic-Segmentation-on-Raw-Fisheye-Images-From-AVM" class="headerlink" title="4-1 Semantic Segmentation on Raw Fisheye Images From AVM"></a>4-1 Semantic Segmentation on Raw Fisheye Images From AVM</h2><p><img src="/2024/02/29/deng2019/f1.png" alt="f1" title="formula 1"></p>
<p>I是输入图片，$\theta$ 是CNN 模型参数，L是语义分割种类数，本文设置为18，包括free space，静态与动态物体。</p>
<h2 id="4-2-Road-Boundary-and-Road-Marking-Detection"><a href="#4-2-Road-Boundary-and-Road-Marking-Detection" class="headerlink" title="4-2 Road Boundary and Road Marking Detection"></a>4-2 Road Boundary and Road Marking Detection</h2><p>本文利用像素级语义信息的优势来区分真实与虚假的道路边界。</p>
<p>将18个类别分为3个大类：free space (F), static objects (S), dynamic objects (D)。<strong>定义F与S的边界作为真正的道路边界，将F与D的边界作为虚假的边界</strong>。</p>
<p><img src="/2024/02/29/deng2019/fig4.png" alt="fig4" title="figure 4"></p>
<p>本文采用自下而上、列独立的搜索策略，如上图所示，搜索过程可并行处理，而且可以在GPU上高速运行。针对每一张图片，<strong>边界像素的数量等于图片的宽度（包括真边界与假边界）</strong>。真实边界点通过逆透视变换法 <strong>(Inverse Perspective Mapping, IPM)</strong> 转换至2D VCS (Vehicle Coordinate System) 边界点。</p>
<p><img src="/2024/02/29/deng2019/fig5.png" alt="fig5" title="figure 5"></p>
<p>路标的提取过程如上图所示，首先将鱼眼相机的语义分割图片经过IPM变换为顶视图片，然后根据像素级语义分割顶视图片计算路标的边缘：</p>
<ul>
<li>首先将语义分割图片转化为灰度图；</li>
<li>然后利用一系列形态操作子morphological operators (Opening, Closing, Gradient) 对灰度图片去除噪点、获取路标边缘；</li>
<li>将图像坐标系中的像素转换为2D VCS。</li>
</ul>
<h1 id="5-Coarse-Scale-Localization-Method"><a href="#5-Coarse-Scale-Localization-Method" class="headerlink" title="5 Coarse-Scale Localization Method"></a>5 Coarse-Scale Localization Method</h1><h2 id="5-1-Map-Matching-Based-on-Weighted-ICP"><a href="#5-1-Map-Matching-Based-on-Weighted-ICP" class="headerlink" title="5-1 Map Matching Based on Weighted ICP"></a>5-1 Map Matching Based on Weighted ICP</h2><p>通过IPM从鱼眼相机图片中获取的道路边界点有着不同的测量不确定度，该不确定度与相机和道路边界点之间的距离有关。因此，ICP 中的点对需要根据距离来赋予不同的权重。本文采用<strong>weighted ICP 进行地图匹配</strong>。</p>
<p><img src="/2024/02/29/deng2019/f2.png" alt="f2" title="formula 2"></p>
<p><img src="/2024/02/29/deng2019/f3.png" alt="f3" title="formula 3"></p>
<p>权重$w_i$ 只与相机和边界点之间的距离$d_i$ 相关，当$d_i$足够小时权重设为0，即忽略该边界点。</p>
<h2 id="5-2-Confidence-Computation"><a href="#5-2-Confidence-Computation" class="headerlink" title="5-2 Confidence Computation"></a>5-2 Confidence Computation</h2><p>当出现极端情况时CSL 不会输出位置信息，如当所有边界点都被车辆遮挡，或者边界点距离采样相机太远时。<strong>CSL 计算置信度来预估当前场景是否可靠</strong>。置信度取决于真实边界点的数量以及这些边界点与相机之间的距离。</p>
<p><img src="/2024/02/29/deng2019/f4.png" alt="f4" title="formula 4"></p>
<h1 id="6-Fine-Scale-Localization-Method"><a href="#6-Fine-Scale-Localization-Method" class="headerlink" title="6 Fine-Scale Localization Method"></a>6 Fine-Scale Localization Method</h1><h2 id="6-2-Fine-Position-Estimation"><a href="#6-2-Fine-Position-Estimation" class="headerlink" title="6-2 Fine Position Estimation"></a>6-2 Fine Position Estimation</h2><p>利用低成本IMU与里程计，积累短距离内采集到的路标边缘点来生成本地路标地图，比起只使用一帧图片中采集到的路标边缘点会提供更高的稳定性；与此同时，过多的采样点也会导致计算量激增，所以本文采取了一系列的措施来限制本地路标地图中的采样点数量。</p>
<p>本地地图与先验地图通过ICP 进行匹配，计算ICP 的协方差，然后使用Split CIF 将匹配结果与运动数据相结合得到最终的定位结果。</p>
<h1 id="7-Experimental-Results"><a href="#7-Experimental-Results" class="headerlink" title="7 Experimental Results"></a>7 Experimental Results</h1><h2 id="7-1-Road-Boundary-and-Road-Marking-Detection-Results"><a href="#7-1-Road-Boundary-and-Road-Marking-Detection-Results" class="headerlink" title="7-1 Road Boundary and Road Marking Detection Results"></a>7-1 Road Boundary and Road Marking Detection Results</h2><p>基于CNN 的鱼眼相机语义分割网络采用作者前作(Deng 等, 2020)中的架构，将真实采集到的鱼眼相机图片与转化得到的鱼眼相机图片共同训练多任务学习架构网络。</p>
<p><img src="/2024/02/29/deng2019/fig7.png" alt="fig7" title="figure 7"></p>
<p>根据上图道路边界检测结果，本文提出的方法可以很好地检测到边界点，而且可以区分正确与错误的边界点。</p>
<p><img src="/2024/02/29/deng2019/fig8.png" alt="fig8" title="figure 8"></p>
<p>上图是利用语义分割对路标点进行提取，路标点很少受到遮挡，这是因为我们只关注邻近区域内的路标点。</p>
<h2 id="7-2-The-Results-of-Coarse-Scale-Localization"><a href="#7-2-The-Results-of-Coarse-Scale-Localization" class="headerlink" title="7-2 The Results of Coarse-Scale Localization"></a>7-2 The Results of Coarse-Scale Localization</h2><p><img src="/2024/02/29/deng2019/fig11.png" alt="fig11" title="figure 11"></p>
<p>上图是标准ICP 与weighted ICP 解算结果对比，表明weighted ICP 的优势。</p>
<p><img src="/2024/02/29/deng2019/fig12.png" alt="fig12" title="figure 12"></p>
<p>上图是不同车道线时CSL 的输出结果，可以发现随着车道线数量的增加，<strong>横向误差逐渐增大、置信度逐渐下降，原因是道路边界与相机之间的距离变大，导致检测误差、系统误差增大，以及边界点的权重降低</strong>。</p>
<p><img src="/2024/02/29/deng2019/fig13.png" alt="fig13" title="figure 13"></p>
<p>上图是当道路边界被遮挡时的横向误差与置信度，图（a）右侧道路边界被大量遮挡，导致横向误差增大、置信度降低（因为真实边界点数量的减少）；图（b）左右两侧的道路边界几乎被完全遮挡，虽然匹配的横向误差不大，但是置信度过低表明结果不可靠。</p>
<p><img src="/2024/02/29/deng2019/fig14.png" alt="fig14" title="figure 14"></p>
<p>上图是在2.5km的测试路段CSL 的实验结果，算法使用单帧图片的数据进行解算，不使用跟踪：</p>
<ul>
<li>Root Mean Square Error (RMSE) 是0.26m，Max Absolute Error (MAE) 是1.55m；</li>
<li>当置信度较低时说明边界线被大量遮挡（不考虑处于交叉路口的情况）；</li>
<li>第2222帧到2774帧图片采集区域为三车道线区域（其余区域为4车道线）内置信度较高、横向误差较小，与理论分析相契合；</li>
<li>当置信度低于阈值时，关闭CSL 模块，阈值是通过在大量不同道路场景下的测试来确定的。</li>
</ul>
<h2 id="7-3-The-Results-of-Fine-Scale-Localization"><a href="#7-3-The-Results-of-Fine-Scale-Localization" class="headerlink" title="7-3 The Results of Fine-Scale Localization"></a>7-3 The Results of Fine-Scale Localization</h2><p><img src="/2024/02/29/deng2019/fig15.png" alt="fig15" title="figure 15"></p>
<p>对采集到的原始数据进行下采样，最终选择最小距离分辨率为0.08m，在保留地图细节的同时尺寸更小。</p>
<p><img src="/2024/02/29/deng2019/fig16.png" alt="fig16" title="figure 16"></p>
<p>注意上图中绿色点为本地生成的路标点，距离车辆越近点越稠密，这是因为采用了6-2节提到的dropping策略：<strong>越靠近车辆当前位置附近的点会有更小的轨迹推断误差dead reckoning error</strong>。</p>
<ul>
<li>图（a）是FSL 的初始化过程，使用CSL 的结果作为初始值，然后使用dead reckoning。<strong>一旦本地路标地图距离达到30m就开始进行地图匹配以矫正位置信息</strong>；</li>
<li>图（b）、（c）分别是道路段、人行横道段的结果，箭头与斑马线会提供良好的横向与径向距离信息；</li>
<li>图（d）显示了路口处的结果，在<strong>此处仅使用dead reckoning进行位置推断</strong>。</li>
</ul>
<p><img src="/2024/02/29/deng2019/t1.png" alt="t1" title="table 1"></p>
<p><img src="/2024/02/29/deng2019/fig17.png" alt="fig17" title="figure 17"></p>
<p>上图为FSL 的实验结果，观察可以发现：</p>
<ul>
<li>横向误差大部分情况下都小于0.1m，除了在路口阶段缺少路标的情况；</li>
<li>径向误差比横向误差大得多，原因之一是径向距离信息（停止线、斑马线等）远少于横向距离信息（车道线等），另一个原因是径向距离更容易受到颠簸或俯仰角改变的影响bumps and pitch changes。</li>
</ul>
<p>本文方法是基于平面道路或者有着较小俯仰角变化的假设，利用IPM 来获取路标位置的；所以当出现颠簸，或者汽车刹车时，IPM 的假设基础不再满足，定位误差（特别是径向误差）会增大。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Robust Loop Closure Detection Integrating Visual–Spatial–Semantic Information via Topological Graphs and CNN Features</title>
    <url>/2024/02/29/wang2020/</url>
    <content><![CDATA[<p>Wang, Yuwei, Yuanying Qiu, Peitao Cheng, and Xuechao Duan. “Robust Loop Closure Detection Integrating Visual–Spatial–Semantic Information via Topological Graphs and CNN Features.” <em>Remote Sensing</em> 12, no. 23 (November 27, 2020): 3890. <a href="https://doi.org/10.3390/rs12233890">https://doi.org/10.3390/rs12233890</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的主要贡献：</p>
<ul>
<li>提出一个包含<strong>视觉、空间以及语义信息</strong>的鲁棒<strong>回环检测</strong>方法，提高大视角变化及动态场景下的鲁棒性；</li>
<li>使用预训练的语义分割网络和AlexNet 特征提取网络，可不经再训练直接应用于其他场景。</li>
</ul>
<span id="more"></span>
<h1 id="2-Materials-and-Methods"><a href="#2-Materials-and-Methods" class="headerlink" title="2 Materials and Methods"></a>2 Materials and Methods</h1><p>算法架构如Fig. 1所示，包含以下关键模块：</p>
<ol>
<li>语义地标提取；</li>
<li>消除动态地标并选择区分度高的地标；</li>
<li>地标区域的CNN 特征计算，及特征维度压缩处理；</li>
<li>语义拓扑图及随机游走描述子的构建；</li>
<li>使用随机游走描述子计算几何相似度；</li>
<li>整体相似度计算以进行回环检测。</li>
</ol>
<p><img src="/2024/02/29/wang2020/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="2-1-Semantic-Topology-Graph"><a href="#2-1-Semantic-Topology-Graph" class="headerlink" title="2.1 Semantic Topology Graph"></a>2.1 Semantic Topology Graph</h2><p><strong>语义拓扑图</strong>的构建过程如下所示：</p>
<p><img src="/2024/02/29/wang2020/fig2.png" alt="fig2" title="figure 2"></p>
<h3 id="2-1-1-Landmark-Extraction"><a href="#2-1-1-Landmark-Extraction" class="headerlink" title="2.1.1 Landmark Extraction"></a>2.1.1 Landmark Extraction</h3><p>作者使用ADE20K 训练的DeepLabV3+ 网络进行<strong>语义分割</strong>来提取地标。</p>
<h3 id="2-1-2-Landmark-Selection"><a href="#2-1-2-Landmark-Selection" class="headerlink" title="2.1.2 Landmark Selection"></a>2.1.2 Landmark Selection</h3><p>对语义分割图片进行处理，来去除面积小于一定阈值（本文中设定为100）的区域，并滤掉动态物体，最终获取具有清晰边界的地标区域，过程如Fig. 3所示。</p>
<p><img src="/2024/02/29/wang2020/fig3.png" alt="fig3" title="figure 3"></p>
<p>为了克服<strong>动态场景</strong>的影响，作者利用语义信息消除<strong>行人动态地标</strong>，将行人与长期停放的车辆区域进行<strong>融合</strong>，以作为后续工作的汽车地标。</p>
<p>作者结合地标包含的<strong>像素个数</strong>以及<strong>语义信息</strong>来选择区分度高的地标；而动态物体地标是由场景内容和语义信息来决定的，也就是说，根据地标在每个实验场景中的移动状态，来移除数据集中的移动地标，从而阻止其参与后续的算法步骤。</p>
<h3 id="2-1-3-CNN-Features"><a href="#2-1-3-CNN-Features" class="headerlink" title="2.1.3 CNN Features"></a>2.1.3 CNN Features</h3><p>CNN 特征具有<strong>外观不变性 appearance invariance</strong>。</p>
<p>根据前人的研究，AlexNet 的<strong>第三卷积层</strong>输出的特征图在<strong>外观变化</strong>下具有优异的表现，作者发现全连接层输出的特征图有丰富的语义信息，对于视角变化具有很强的鲁棒性，但是在外观变化下较弱。因此，作者选取AlexNet Conv3 的输出作为地标区域的<strong>全局特征</strong>。</p>
<p>为了保持地标的<strong>原始尺寸信息</strong>，作者将地标轮廓的Hu moment 添加进CNN 特征中来描述地标。</p>
<h3 id="2-1-4-Graph-Representation"><a href="#2-1-4-Graph-Representation" class="headerlink" title="2.1.4 Graph Representation"></a>2.1.4 Graph Representation</h3><p>本文中，每个地标被描述为包含有<strong>类别与像素数量信息</strong>的节点，节点的位置位于地标区域的中心。描述子的构建过程如Fig. 5所示：</p>
<p><img src="/2024/02/29/wang2020/fig5.png" alt="fig5" title="figure 5"></p>
<h2 id="2-2-Loop-Closure-Detection"><a href="#2-2-Loop-Closure-Detection" class="headerlink" title="2.2 Loop Closure Detection"></a>2.2 Loop Closure Detection</h2><p>回环检测的流程如Fig. 6所示，同时检测外观相似度（CNN 与轮廓特征）与几何相似度（随机游走描述子），从而得到总体相似度。</p>
<p><img src="/2024/02/29/wang2020/fig6.png" alt="fig6" title="figure 6"></p>
<h3 id="2-2-1-Obtain-Candidate-Images"><a href="#2-2-1-Obtain-Candidate-Images" class="headerlink" title="2.2.1 Obtain Candidate Images"></a>2.2.1 Obtain Candidate Images</h3><p>通过控制query image 与历史图片的<strong>相同标签地标数量</strong>，来获取候选匹配图片；作者设定为1，即当query image 与历史图片有一个相同标签的地标时，就将该历史图片作为候选图片。</p>
<h3 id="2-2-2-Appearance-Similarity"><a href="#2-2-2-Appearance-Similarity" class="headerlink" title="2.2.2 Appearance Similarity"></a>2.2.2 Appearance Similarity</h3><p>作者使用基于cos 距离（式 1）的最近邻域搜索方法来对具有相同标签的地标进行匹配，</p>
<p><img src="/2024/02/29/wang2020/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$v_i^q$ 表示query 图片的第 i 个地标的特征向量，$v_j^c$ 表示候选图片的第 j 个地标的特征向量。</p>
<p>此外，利用地标的<strong>几何形状</strong>作为<strong>惩罚因子</strong>来消除误匹配，使用Hu moments 来描述地标不规则的轮廓特征，具有旋转、平移及尺寸的不变性。</p>
<h3 id="2-2-3-Geometric-Similarity"><a href="#2-2-3-Geometric-Similarity" class="headerlink" title="2.2.3 Geometric Similarity"></a>2.2.3 Geometric Similarity</h3><p>将随机游走描述子矩阵M 进行级联转化为向量 G，然后计算两个图描述子向量之间的相似度：</p>
<p><img src="/2024/02/29/wang2020/f6.png" alt="f6" title="formula 6"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Topology</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Topology</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Semantic SLAM Based on Improved DeepLabv3⁺ in Dynamic Scenarios</title>
    <url>/2024/02/29/hu2022/</url>
    <content><![CDATA[<p>Hu, Zhangfang, Jiang Zhao, Yuan Luo, and Junxiong Ou. “Semantic SLAM Based on Improved DeepLabv3+ in Dynamic Scenarios.” <em>IEEE Access</em> 10 (2022): 21160–68. <a href="https://doi.org/10.1109/ACCESS.2022.3154086">https://doi.org/10.1109/ACCESS.2022.3154086</a>.</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文提出动态场景中的语义SLAM 系统DeepLabv3+_SLAM 包含三个线程：<strong>ORB-SLAM3</strong>，<strong>语义分割</strong>线程以及<strong>几何</strong>线程。</p>
<span id="more"></span>
<h1 id="3-System-Description"><a href="#3-System-Description" class="headerlink" title="3 System Description"></a>3 System Description</h1><p>系统架构如Fig. 1所示，本系统是基于ORB-SLAM3 进一步开发的。DeepLabv3+ 模型会获取像素级的<strong>先验动态物体</strong>，同时，利用<strong>多视角几何线程</strong>区分图片中的动态与静态特征点；然后，将语义分割结果和多视角几何方法得到的信息相结合得到<strong>动态物体的边界线</strong>，从而剔除掉所有的动态物体。</p>
<p>注：个人认为，多视角几何线程的图示不是很准确，应该将多视角几何与新的蚁群策略合为一个框，或将新蚁群策略提到多视角结合方法之前。因为本算法是利用新蚁群算法来减少多视角几何方法中的候选点数量的，两者应该是处于共同的层级，甚至新蚁群策略应该在多视角几何方法之前。</p>
<p><img src="/2024/02/29/hu2022/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-1-Semantic-Segmentation-DeepLabv3"><a href="#3-1-Semantic-Segmentation-DeepLabv3" class="headerlink" title="3.1 Semantic Segmentation DeepLabv3+"></a>3.1 Semantic Segmentation DeepLabv3+</h2><p>DeepLabv3+ 的架构如Fig. 2所示，本文使用ResNest 网络作为backbone，可以取得更优秀的分割精度。</p>
<p><img src="/2024/02/29/hu2022/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-2-ASPP-Module"><a href="#3-2-ASPP-Module" class="headerlink" title="3.2 ASPP Module"></a>3.2 ASPP Module</h2><p>作者对ASPP 模块进行了更改：</p>
<p><img src="/2024/02/29/hu2022/fig4.png" alt="fig4" title="figure 4"></p>
<h2 id="3-3-Dynamic-Object-Detection-Based-on-Multi-view-Geometry"><a href="#3-3-Dynamic-Object-Detection-Based-on-Multi-view-Geometry" class="headerlink" title="3.3 Dynamic Object Detection Based on Multi-view Geometry"></a>3.3 Dynamic Object Detection Based on Multi-view Geometry</h2><p>作者使用基于<strong>多视角几何</strong>处理的动态物体分割算法，如Fig. 5所示，该算法考虑历史帧 (hf) 和当前帧 (cf) 对同一特征点的<strong>视角变化</strong>，若视角变化值大于一个给定阈值，则判定该特征点属于动态特征点；此外，还计算关键点在当前帧的<strong>深度值</strong> $d_{cf}$ ，及历史关键帧在当前帧的投影深度值 $d_{proj}$ ，若两者之差大于给定阈值，则判定该特征点为动态特征点。</p>
<p><img src="/2024/02/29/hu2022/fig5.png" alt="fig5" title="figure 5"></p>
<h2 id="3-4-Ant-Colony-Strategy"><a href="#3-4-Ant-Colony-Strategy" class="headerlink" title="3.4 Ant Colony Strategy"></a>3.4 Ant Colony Strategy</h2><p>蚁群算法是一种模拟蚂蚁的觅食行为的优化算法，算法包含两个主要步骤：state transfer，pheromone update。</p>
<p>状态转换方程：</p>
<p><img src="/2024/02/29/hu2022/f4.png" alt="f4" title="formula 4"></p>
<p>信息素更新方程：</p>
<p><img src="/2024/02/29/hu2022/f5.png" alt="f5" title="formula 5"></p>
<h2 id="3-5-New-Ant-Colony-Strategy"><a href="#3-5-New-Ant-Colony-Strategy" class="headerlink" title="3.5 New Ant Colony Strategy"></a>3.5 New Ant Colony Strategy</h2><p>利用<strong>多视角几何方法</strong>将历史帧投影至当前帧过程，会得到大量的投影特征点，为了判断是否属于静态特征点需要<strong>遍历</strong>所有的点，运算量过大。作者提出一种<strong>新的蚁群算法</strong>，通过<strong>最优路径</strong>来找到所有的动态特征点群。</p>
<p>蚁群算法的策略为，从起点到终点过程中，蚁群会绕过它们遇到的障碍物来寻找到达终点的一条<strong>最优路径</strong>。图片中的动态点或静态点都是<strong>成块分布</strong>的，而不是随机散布在整幅图片上；因此，当遇到一个动态特征点时，会在该动态特征点<strong>所在的块内</strong>进行搜索，直到整块特征点都被探索过或到达块的范围，然后继续搜索下一块动态特征点集合。</p>
<p>根据特征点在图片中的分布，作者设计了一条从 S 到 T 的轨迹 l，如Fig. 6所示，搜索策略是蚁群从特征点 $m_i$ 持续移动到下一个特侦点，直到抵达终点 T。针对每一个特征点 $m_i$ ，以<strong>自身为原点、半径为 R 进行动态点搜索</strong>，每找到一个新的特征点就<strong>拓宽带宽</strong> $\Delta h$ ，直至在该区域内没有新的动态特征点，便移至下一个位置。</p>
<p><img src="/2024/02/29/hu2022/fig6.png" alt="fig6" title="figure 6"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Monocular Localization in HD Maps by Combining Semantic Segmentation and Distance Transform</title>
    <url>/2024/03/01/pauls2020/</url>
    <content><![CDATA[<p>Pauls, Jan-Hendrik, Kursat Petek, Fabian Poggenhans, and Christoph Stiller. “Monocular Localization in HD Maps by Combining Semantic Segmentation and Distance Transform.” In <em>2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 4595–4601. Las Vegas, NV, USA: IEEE, 2020. <a href="https://doi.org/10.1109/IROS45743.2020.9341003">https://doi.org/10.1109/IROS45743.2020.9341003</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ol>
<li>提出使用语义分割网络来克服之前常用的物体探测网络生成<strong>bboxing 的空间限制</strong>，这使得我们可以检测到稀疏、可共享、独立于传感器的HD 地图中的所有地图元素；</li>
<li>使用distance transform 来解决稠密语义信息的<strong>数据关联</strong>问题，且该关联过程本质上是动态的；</li>
<li>将<strong>语义信息和标准的汽车里程计</strong>组成为一个鲁棒的位姿图优化，克服了语义定位只使用图片的缺点。</li>
</ol>
<span id="more"></span>
<p><img src="/2024/03/01/pauls2020/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-Semantic-Segmentation-and-Post-processing"><a href="#3-Semantic-Segmentation-and-Post-processing" class="headerlink" title="3 Semantic Segmentation and Post-processing"></a>3 Semantic Segmentation and Post-processing</h1><p>作者使用包含多个检测头的ResNet-38 网络进行语义分割：</p>
<ul>
<li>其中一个检测头按照<strong>增强型Cityscapes 数据集种类</strong>进行分割，种类的增强在于增加了所有的道路标志lane marking 类别，该结果被命名为Cityscapes+<em>LM</em>。</li>
<li>第二个检测头预测<strong>车道</strong>，特别是本车所在的车道区域。</li>
</ul>
<p>只关注与地图元素对应的语义类别，curbs (C), lane borders (LB), lane markings (LM), traffic lights (TL), traffic signs (TS)。由于本网络无法检测车道边界LB，作者采用<strong>形态学方法</strong>根据<strong>车道区域</strong>提取出相应的LB。</p>
<h1 id="4-Distance-Transform"><a href="#4-Distance-Transform" class="headerlink" title="4 Distance Transform"></a>4 Distance Transform</h1><p>将某个类别的<strong>二值图片</strong>记为 $B_c$ ，distance transform 用于将 $B_c$ 转化为一个<strong>距离图片</strong> $D_c$ ，具有相同的维度，但是是连续的像素数值。</p>
<p><img src="/2024/03/01/pauls2020/f1.png" alt="f1" title="formula 1"></p>
<h1 id="5-Semantic-Localization"><a href="#5-Semantic-Localization" class="headerlink" title="5 Semantic Localization"></a>5 Semantic Localization</h1><p>作者将不同地标采样成<strong>3D 点</strong>来表示，这样可以适用于任何形状的地标。利用<strong>初始位姿估计</strong>将附近的地表点映射至距离图片 $D_c$ ，构建如下所示的<strong>损失函数</strong>：</p>
<p><img src="/2024/03/01/pauls2020/f2.png" alt="f2" title="formula 2"></p>
<p>原理在于最优位姿p 会使得所有投影后的地标位于相应的<strong>语义标签图片块</strong>中，对于非最优的情况，每个地标需要移动至下一个合适的图片块，该信息可通过对距离图片进行<strong>插值</strong>以生成一个<strong>顺滑的梯度</strong>。因此，插值后的距离图片可以视为一个<strong>快速查找表</strong>，只需计算一次然后就可用于每一次优化步骤。另一个优势在于这个查找可<strong>动态</strong>构建地标与图片块之间的联系，也就是说，不需要额外的处理就可以在每一次优化中改变。</p>
<p>对于外点剔除，作者使用带有<strong>变量宽度</strong>的Tukey’s biweight 损失作为<strong>鲁棒损失函数</strong> $\rho$ ，距离较远的地图元素，如交通牌、交通灯等可以相应调整变量宽度。</p>
<h1 id="6-Pose-Graph-Optimization"><a href="#6-Pose-Graph-Optimization" class="headerlink" title="6 Pose Graph Optimization"></a>6 Pose Graph Optimization</h1><p>作者构建了一个包含图片定位和汽车VO 的<strong>滑动窗口位姿图优化</strong>，VO 主要用来获取径向longitudinal 速度与 yaw 转向速率，作者假定在连续帧间VO 满足常数速度和转向速率 $v,w$ ，因此，部分2D 位姿在汽车坐标系中的非线性更新 $\widetilde{p} = (x, y, \theta)$ ：</p>
<p><img src="/2024/03/01/pauls2020/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$\Delta T$  表示连续两帧之间的时间间隔。然后该部分2D 位姿被转换至相机坐标系，并使用在高度、pitch、roll 方向的弱正则化进行补充，作为一个简化的6自由度运动模型。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Monocular Localization with Vector HD Map (MLVHM)_A Low-Cost Method for Commercial IVs</title>
    <url>/2024/03/01/xiao2020/</url>
    <content><![CDATA[<p>Xiao, Zhongyang, Diange Yang, Tuopu Wen, Kun Jiang, and Ruidong Yan. “Monocular Localization with Vector HD Map (MLVHM): A Low-Cost Method for Commercial IVs.” <em>Sensors</em> 20, no. 7 (March 27, 2020): 1870. <a href="https://doi.org/10.3390/s20071870">https://doi.org/10.3390/s20071870</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ol>
<li>提出一个基于<strong>语义向量提取</strong>和<strong>鲁棒地图匹配算法</strong>的低成本高精度定位算法；</li>
<li>提出一个<strong>基于滑动窗口的帧间运动融合（单目相机视觉里程计作为帧间约束）</strong>来有效提高定位的稳定性，特别是在<strong>稀疏定位特征的场景</strong>中也可以实现实时的稳定定位；</li>
<li>在真实世界中进行实验证明了精度与可靠性。</li>
</ol>
<span id="more"></span>
<p>不同种类的高精度地图：</p>
<p><img src="/2024/03/01/xiao2020/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3 System Overview"></a>3 System Overview</h1><p>MLVHM 系统概述如Fig. 2所示：</p>
<p><img src="/2024/03/01/xiao2020/fig2.png" alt="fig2" title="figure 2"></p>
<p>值得注意的是，ORB 特征只用来进行里程计计算作为<strong>帧间动作约束</strong>，并不参与地图匹配。</p>
<h1 id="4-Map-Based-Localization"><a href="#4-Map-Based-Localization" class="headerlink" title="4 Map-Based Localization"></a>4 Map-Based Localization</h1><h2 id="4-1-Line-and-Point-Based-Camera-Localization"><a href="#4-1-Line-and-Point-Based-Camera-Localization" class="headerlink" title="4.1 Line-and Point-Based Camera Localization"></a>4.1 Line-and Point-Based Camera Localization</h2><p>MLVHM 使用的是<strong>带有语义信息的几何特征</strong>，如Fig. 3所示，在Image processing 阶段，这些点与线特征被识别为带有语义信息，相应地，地图中的地标也是环境中关键元素的几何描述。</p>
<p><img src="/2024/03/01/xiao2020/fig3.png" alt="fig3" title="figure 3"></p>
<p>建立图片与地图间点、线特征的Mahalanobis 范数非线性优化：</p>
<p><img src="/2024/03/01/xiao2020/f13.png" alt="f13" title="formula 13"></p>
<p>其中，$r^{\mathcal{P}}(z_{i,t}^{(P)},x_t),r^{\mathcal{L}}(z_{m,t}^{(L)},x_t)$  分别是点与线的<strong>观测残差</strong>。如Fig. 4所示：</p>
<p><img src="/2024/03/01/xiao2020/fig4.png" alt="fig4" title="figure 4"></p>
<p><strong>点特征</strong>的观测残差表示为 $r^{\mathcal{P}}(z_{i,t}^{(P)},x_t) = h_j^{(P)} - \hat{p}_i^{(P)}$ 。<strong>线特征两个端点</strong>的观测残差分别为 $r^{\mathcal{L}}(z_{m,t}^{(L)},x_t) = [d_1, d_2]^T$ 。</p>
<h2 id="4-2-Data-Association-Method"><a href="#4-2-Data-Association-Method" class="headerlink" title="4.2 Data Association Method"></a>4.2 Data Association Method</h2><p>作者采用一个RANSAC 升级版进行数据关联。</p>
<h3 id="Basic-RANSAC-method"><a href="#Basic-RANSAC-method" class="headerlink" title="Basic RANSAC method"></a>Basic RANSAC method</h3><p>随机选取语义标签正确匹配的一组<strong>可能匹配点子集</strong>来评估该子集的质量，传统RANSAC 是通过测量内点的数量来评价子集的质量，即将地图投影到像素坐标系中，根据符合一定阈值内的<strong>匹配点数量</strong>来评判该子集的质量。</p>
<h3 id="Improved-RANSAC-method"><a href="#Improved-RANSAC-method" class="headerlink" title="Improved RANSAC method"></a>Improved RANSAC method</h3><p>伪代码如Algorithm 1所示，$\mathbf{c}_{1-3}^{(L)}$  表示选取三个（计算位姿的最小匹配数量）<strong>线匹配</strong>计算相机位姿 $\hat{\mathbf{x}}^{\ast}$ ，根据该相机位姿 $\hat{\mathbf{x}}^{\ast}$ 将地图投影到像素坐标系，计算内点（投影地标与图片特征小于阈值视为内点）集合 $\mathbf{c}^\ast$ 。此外，作者计算相机位姿 $\hat{\mathbf{x}}^{\ast}$ 与初始位姿估计 $\bar{\mathbf{p}}$ 之间的<strong>偏移量</strong>，若该偏移量小于 D，则将内点集合 $\mathbf{c}^\ast$ 并入关联集合 C 中（Algorithm 1：step 6）。</p>
<p>阈值 D 的设置是根据初始位姿估计 $\bar{\mathbf{p}}$ 的<strong>置信度</strong>来决定的，第一帧的初始位姿估计 $\bar{\mathbf{p}}$ 是通过低成本GNSS 接收机获取的，后续的初始位姿估计是结合上一帧的位姿与VO 进行估计的。</p>
<p>最终选取内点数最多的一组C 作为最终的数据关联结果。</p>
<p><img src="/2024/03/01/xiao2020/a1.png" alt="a1" title="algorithm 1"></p>
<h2 id="4-3-Integrating-Frame-to-Frame-Motion"><a href="#4-3-Integrating-Frame-to-Frame-Motion" class="headerlink" title="4.3 Integrating Frame-to-Frame Motion"></a>4.3 Integrating Frame-to-Frame Motion</h2><p>利用ORB-SLAM 算法计算帧间VO ，作为帧间运动约束实现对位姿估计的优化。</p>
<p><img src="/2024/03/01/xiao2020/fig5.png" alt="fig5" title="figure 5"></p>
<p>设定：帧 $c_i$ 较第一帧图像 $C_0$ 的位姿变换是 $\mathbf{R}_{c_i}^{C_0},\mathbf{t}_{c_i}^{C_0}$ ；由于单目相机的<strong>尺度不确定性</strong>，$\mathbf{t}_{c_i}^{C_0}$ 较真正的平移向量存在一个尺度因子 $s$ 的差距；相机坐标系与世界坐标系之间的转换关系为  $\mathbf{R}^{w}_{C_0},\mathbf{t}^{w}_{C_0}$ ；则相应的帧 $c_i$ 在世界坐标系下的位姿为：</p>
<p><img src="/2024/03/01/xiao2020/f18.png" alt="f18" title="formula 18"></p>
<p><img src="/2024/03/01/xiao2020/f19.png" alt="f19" title="formula 19"></p>
<p>由于语义分割消耗时间较久，故地图匹配的位姿结果通常与当前帧的时间不同，假定中间的<strong>延迟为M 帧</strong>。即假设当前时间为t，则距离最近的地图匹配位姿为 t-M 帧的结果，设定帧 $c_i$ 根据<strong>地图匹配</strong>求得的位姿为 $\mathbf{\hat{R}}_{c_i}^{w},\mathbf{\hat{t}}_{c_i}^{w}$ 。设定一个宽度为N 的滑动窗口对位姿进行优化，最终，使用<strong>帧间VO 约束</strong>的基于地图匹配的定位算法优化目标定义为：</p>
<p><img src="/2024/03/01/xiao2020/f20.png" alt="f20" title="formula 20"></p>
<p>由此，可以得到<strong>帧t 对应的位姿</strong>：</p>
<p><img src="/2024/03/01/xiao2020/f21.png" alt="f21" title="formula 21"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
</search>
