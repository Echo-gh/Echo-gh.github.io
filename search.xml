<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Ubuntu系统安装显卡驱动、CUDA、cuDNN</title>
    <url>/2024/01/26/Ubuntu-dl-setup/</url>
    <content><![CDATA[<p><strong>本文主要参考</strong><a href="https://zhuanlan.zhihu.com/p/643954422">该文章</a>。</p>
<h2 id="1-安装显卡驱动"><a href="#1-安装显卡驱动" class="headerlink" title="1 安装显卡驱动"></a>1 安装显卡驱动</h2><h3 id="1-1-前期准备"><a href="#1-1-前期准备" class="headerlink" title="1.1 前期准备"></a>1.1 前期准备</h3><p>根据显卡型号在<a href="https://www.nvidia.com/Download/Find.aspx?lang=en-us#">Nvidia 官网</a>下载相应的驱动程序，然后安装必备软件：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 更新源</span></span><br><span class="line">sudo apt-get update</span><br><span class="line"><span class="comment">## 安装必备软件</span></span><br><span class="line">sudo apt-get install g++</span><br><span class="line">sudo apt-get install gcc</span><br><span class="line">sudo apt-get install make</span><br></pre></td></tr></table></figure>
<h3 id="1-2-禁用默认驱动"><a href="#1-2-禁用默认驱动" class="headerlink" title="1.2 禁用默认驱动"></a>1.2 禁用默认驱动</h3><p>在安装NVIDIA驱动前需要禁止系统自带的显卡驱动 nouveau：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 编辑blacklist.conf 文件</span></span><br><span class="line">sudo vi /etc/modprobe.d/blacklist.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在文件末尾添加以下内容并保存</span></span><br><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新initramfs，然后重启电脑</span></span><br><span class="line">sudo update-initramfs -u</span><br><span class="line">sudo reboot now</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查是否关闭nouveau，若没有输出，则说明已成功关闭</span></span><br><span class="line">lsmod | grep nouveau</span><br></pre></td></tr></table></figure>
<h3 id="1-3-安装驱动"><a href="#1-3-安装驱动" class="headerlink" title="1.3 安装驱动"></a>1.3 安装驱动</h3><p>进入tty 模式，并关闭图形进程：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo service gdm3 stop</span><br></pre></td></tr></table></figure>
<p>开始安装驱动：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 赋予执行权限</span></span><br><span class="line">sudo <span class="built_in">chmod</span> 777 NVIDIA-Linux-x86_64-535.54.03.run</span><br><span class="line"><span class="comment">## 安装：不安装OpenGL,安装时关闭X服务  </span></span><br><span class="line">sudo ./NVIDIA-Linux-x86_64-535.54.03.run –no-opengl-files -no-x-check</span><br><span class="line"><span class="comment">## Install Nvidia&#x27;s 32-bit compatibility libraries?</span></span><br><span class="line"><span class="comment">## 选择 &quot;No&quot;</span></span><br><span class="line"><span class="comment">## Would you like to run the nvidia-xconfig utility to automatically update your X configuration file so that the NVIDIA X driver dill be used dhen you restart X? Any pre-existing X configuration file will be backed up.</span></span><br><span class="line"><span class="comment">## 选择 &quot;Yes&quot;</span></span><br></pre></td></tr></table></figure>
<p>成功安装之后，会进入图形界面，此时使用命令nvidia-smi 检查驱动是否安装成功，若出现下图界面，证明驱动安装成功。</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/nvidia-smi.png" alt="nvidia-smi" title="nvidia-smi显示界面"></p>
<h3 id="1-4-显卡驱动失效问题记录"><a href="#1-4-显卡驱动失效问题记录" class="headerlink" title="1.4 显卡驱动失效问题记录"></a>1.4 显卡驱动失效问题记录</h3><p>一次重启电脑后，发现Ubuntu提示某个文件发生错误，是否需要反馈给Ubuntu，本人当时没在意点了“否”，在后续操作过程中发现nvidia-smi命令报错，显示找不到显卡驱动：</p>
<blockquote>
<p>NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</p>
</blockquote>
<p>但使用nvcc -V 命令可以正常显示CUDA 版本。由于之前也遇到过类似驱动失效的问题，所以选择重装显卡驱动，按照之前的步骤检查nouveau、关闭图形界面、安装驱动……然后就一路报错了……</p>
<blockquote>
<p>NVIDIA-SMI has failed because it couldn‘t communicate with the NVIDIA driver.</p>
</blockquote>
<p>网上找解决方案，这篇<a href="https://blog.csdn.net/wjinjie/article/details/108997692">文章</a>提到一个解决方案，使用dkms：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 首先，查看显卡驱动版本</span></span><br><span class="line"><span class="built_in">ls</span> /usr/src | grep nvidia</span><br><span class="line">nvidia-535.146.02</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 然后，安装dkms，并修复显卡驱动</span></span><br><span class="line">sudo apt-get install dkms</span><br><span class="line">sudo dkms install -m nvidia -v 535.146.02</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 输入nvidia-smi命令，一切恢复正常</span></span><br></pre></td></tr></table></figure>
<p>然后搜了一下dkms的作用，wikipedia 中介绍如下：</p>
<blockquote>
<p>动态内核模块支持 （Dynamic Kernel Module Support，DKMS）是用来生成Linux的内核模块的一个框架，其源代码一般不在Linux内核源代码树。 当新的内核安装时，DKMS 支持的内核设备驱动程序 到时会自动重建。 DKMS 可以用在两个方向：如果一个新的内核版本安装，自动编译所有的模块，或安装新的模块（驱动程序）在现有的系统版本上，而不需要任何的手动编译或预编译软件包需要。例如，这使得新的显卡可以使用在旧的Linux系统上。</p>
</blockquote>
<p>唔……很奇怪，我应该没有更新Ubuntu的内核，不晓得为什么会出现这个情况，先记录一下吧，当前本人的Ubuntu内核版本为：</p>
<blockquote>
<p>Linux echo-dell 6.5.0-14-generic #14~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Nov 20 18:15:30 UTC 2 x86_64 x86_64 x86_64 GNU/Linux</p>
</blockquote>
<h2 id="2-安装CUDA"><a href="#2-安装CUDA" class="headerlink" title="2 安装CUDA"></a>2 安装CUDA</h2><h3 id="2-1-下载与安装"><a href="#2-1-下载与安装" class="headerlink" title="2.1 下载与安装"></a>2.1 下载与安装</h3><p>可同时安装不同的CUDA 版本，根据不同环境需求选择使用不同版本，本处以CUDA-11.8为例进行安装说明。</p>
<p>在<a href="https://developer.nvidia.com/cuda-toolkit-archive">NVIDIA官网</a>下载对应版本，推荐使用runfile (local) 进行安装：</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/CUDA.png" alt="cuda" title="CUDA安装命令"></p>
<p>根据官网给出的下载和安装命令执行即可：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run</span><br><span class="line">sudo sh cuda_11.8.0_520.61.05_linux.run</span><br></pre></td></tr></table></figure>
<h3 id="2-2-环境变量配置"><a href="#2-2-环境变量配置" class="headerlink" title="2.2 环境变量配置"></a>2.2 环境变量配置</h3><p>安装完成后，打开账户的配置文件，进行以下修改：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## CUDA ENV</span></span><br><span class="line"><span class="built_in">export</span> CUDA_HOME=/usr/local/cuda</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>:<span class="variable">$&#123;CUDA_HOME&#125;</span>/lib64</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;CUDA_HOME&#125;</span>/bin:<span class="variable">$&#123;PATH&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新配置文件设置</span></span><br><span class="line"><span class="built_in">source</span> ~/.zshrc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证安装是否成功</span></span><br><span class="line">nvcc -V</span><br></pre></td></tr></table></figure>
<p>若出现下图界面，则证明CUDA 安装成功:</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/nvcc.png" alt="cuda" title="检验CUDA安装成功"></p>
<h3 id="2-3-多版本切换"><a href="#2-3-多版本切换" class="headerlink" title="2.3 多版本切换"></a>2.3 多版本切换</h3><p>CUDA 安装位置的文件如下图所示：</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/CUDA-version.png" alt="cuda-version" title="CUDA版本切换"></p>
<p>根据所需版本，切换cuda 的软链接即可：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo <span class="built_in">ln</span> -snf /usr/local/cuda-11.8 /usr/local/cuda</span><br></pre></td></tr></table></figure>
<h2 id="3-安装cuDNN"><a href="#3-安装cuDNN" class="headerlink" title="3 安装cuDNN"></a>3 安装cuDNN</h2><p>注：此处可参考<a href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#verify">NVIDIA官网教程</a>进行安装。</p>
<h3 id="3-1-前期准备与下载"><a href="#3-1-前期准备与下载" class="headerlink" title="3.1 前期准备与下载"></a>3.1 前期准备与下载</h3><p>首先安装依赖包：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install zlib1g</span><br></pre></td></tr></table></figure>
<p>然后在<a href="https://developer.nvidia.com/rdp/cudnn-download">NVIDIA官网</a>下载相应版本的安装包，注意，下载cuDNN 需要注册NVIDIA账号。选取合适的版本进行下载：</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/cudnn.png" alt="cudnn" title="cuDNN安装包"></p>
<h3 id="3-2-安装"><a href="#3-2-安装" class="headerlink" title="3.2 安装"></a>3.2 安装</h3><p>解压并安装：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i cudnn-local-repo-ubuntu2204-8.9.3.28_1.0-1_amd64.deb</span><br></pre></td></tr></table></figure>
<p>按照提示导入CUDA GPG key：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo <span class="built_in">cp</span> /var/cudnn-local-repo-ubuntu2204-8.9.3.28/cudnn-local-*-keyring.gpg /usr/share/keyrings/</span><br></pre></td></tr></table></figure>
<p>更新源并安装：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line"><span class="comment">## 1. Install the runtime library.</span></span><br><span class="line"><span class="comment">## sudo apt-get install libcudnn8=8.x.x.x-1+cudaX.Y</span></span><br><span class="line">sudo apt-get install libcudnn8=8.9.3.28-1+cuda11.8</span><br><span class="line"><span class="comment">## 2. Install the developer library.</span></span><br><span class="line"><span class="comment">## sudo apt-get install libcudnn8-dev=8.x.x.x-1+cudaX.Y</span></span><br><span class="line">sudo apt-get install libcudnn8-dev=8.9.3.28-1+cuda11.8</span><br><span class="line"><span class="comment">## 3. Install the code samples.</span></span><br><span class="line"><span class="comment">## sudo apt-get install libcudnn8-samples=8.x.x.x-1+cudaX.Y</span></span><br><span class="line">sudo apt-get install libcudnn8-samples=8.9.3.28-1+cuda11.8</span><br></pre></td></tr></table></figure>
<h3 id="3-3-测试"><a href="#3-3-测试" class="headerlink" title="3.3 测试"></a>3.3 测试</h3><p>输入检查命令，出现下图证明安装初步成功。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo dpkg -l | grep cudnn</span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/Ubuntu-dl-setup/cudnn-dpkg.png" alt="cudnn-version" title="cuDNN安装成功"></p>
<p>按照官网教程，进行代码测试：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将cuDNN samples 拷贝至主目录</span></span><br><span class="line"><span class="built_in">cp</span> -r /usr/src/cudnn_samples_v8/ <span class="variable">$HOME</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入samples 文件夹，并编译</span></span><br><span class="line"><span class="built_in">cd</span>  <span class="variable">$HOME</span>/cudnn_samples_v8/mnistCUDNN</span><br><span class="line">make clean &amp;&amp; make</span><br></pre></td></tr></table></figure>
<p>此时可能出现如下报错：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">fatal error: FreeImage.h: No such file <span class="keyword">or</span> directory</span><br><span class="line">    <span class="number">1</span> | <span class="meta">#<span class="keyword">include</span> <span class="string">&quot;FreeImage.h&quot;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/Ubuntu-dl-setup/cudnn-error.png" alt="cudnn-error" title="cuDNN报错"></p>
<p>可安装相应包进行解决：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libfreeimage3 libfreeimage-dev</span><br></pre></td></tr></table></figure>
<p>安装之后重新编译，并运行生成文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 重新编译</span></span><br><span class="line">make clean &amp;&amp; make</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行生成文件</span></span><br><span class="line">./mnistCUDNN</span><br></pre></td></tr></table></figure>
<p>若出现如下结果，则证明cuDNN 安装成功：</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/cudnn-success.png" alt="cudnn-success" title="cuDNN安装成功"></p>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Deep Learning</tag>
        <tag>Nvidia</tag>
        <tag>CUDA</tag>
        <tag>cuDNN</tag>
      </tags>
  </entry>
  <entry>
    <title>寒武纪MLU220 开发环境Docker搭建</title>
    <url>/2024/01/26/mlu220/</url>
    <content><![CDATA[<p>首先进入<a href="https://cair.cambricon.com/#/home/catalog">寒武纪开发主页</a>并登录寒武纪账号，进入cambricon_pytorch docker 页面。由于本人只使用MLU220 进行边缘端推理，所以不需要在主机上安装MLU 驱动，因此可跳过第一步直接按照提示安装docker。本人在尝试按照页面说明时遇到了一些问题，在此记录一下。</p>
<span id="more"></span>
<p>若是初次使用docker，则建议将Ubuntu个人账户添加进docker组里，这样就避免每次使用时都要输入sudo了，该部分参考<a href="https://www.cnblogs.com/jzcn/p/16591083.html">文章</a>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">## 将当前用户添加进docker组，并更新</span><br><span class="line">sudo gpasswd -a user docker</span><br><span class="line">newgrp docker</span><br><span class="line"></span><br><span class="line">## 然后重启电脑才可永久生效</span><br></pre></td></tr></table></figure>
<p>然后按照寒武纪提示，依次进行如下操作：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">## 登录harbor</span><br><span class="line">docker login cair.cambricon.com</span><br><span class="line"></span><br><span class="line">## 输入用户名、API密钥（网页用户名下拉框中有API密钥选项）</span><br><span class="line">Username: (username)</span><br><span class="line">Password: (API密钥)</span><br></pre></td></tr></table></figure>
<p>注意，这里网页提示使用docker pull命令下载相应的镜像文件，但经过本人尝试之后发现，在后续的docker run命令中会重复下载，因此<strong>跳过使用docker pull，直接使用docker run命令创建容器</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -it --name mlu220 -v /home/***/work:/work cair.cambricon.com/cambricon/cambricon_pytorch:ubuntu18.04_sdk_v1.7.0_pytorch_v0.15.0-2 /bin/bash</span><br></pre></td></tr></table></figure>
<p>上述命令会自动下载镜像文件，并改名为“mlu220”，且将主机的“/home/<em>*</em>/work”映射至docker端的“/work”。</p>
<p>此时，可以查看新建的容器：</p>
<p><img src="/2024/01/26/mlu220/docker1.png" alt="docker" title="容器查看"></p>
<p>启动mlu220容器并查看：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">❯ docker start mlu220</span><br><span class="line">mlu220</span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/mlu220/docker2.png" alt="docker" title="启动容器"></p>
<p>最后进入docker并激活开发环境：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it mlu220 /bin/bash</span><br><span class="line"><span class="built_in">source</span> torch/venv3/pytorch/bin/activate</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Cambricon</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Co-fusion_Real-time segmentation, tracking and fusion of multiple objects</title>
    <url>/2024/01/27/Co-fusion/</url>
    <content><![CDATA[<p>Runz, Martin, and Lourdes Agapito. “Co-Fusion: Real-Time Segmentation, Tracking and Fusion of Multiple Objects.” In <em>2017 IEEE International Conference on Robotics and Automation (ICRA)</em>, 4471–78. Singapore, Singapore: IEEE, 2017. <a href="https://doi.org/10.1109/ICRA.2017.7989518">https://doi.org/10.1109/ICRA.2017.7989518</a>.</p>
<h1 id="3-Overview-of-our-Method"><a href="#3-Overview-of-our-Method" class="headerlink" title="3 Overview of our Method"></a>3 Overview of our Method</h1><p>Co-Fusion 是一个可以<strong>实时处理</strong>每一帧输入图片的RGB-D SLAM 系统，本系统为场景中每一个分割的物体<strong>存储模型</strong>，而且可以<strong>独立跟踪</strong>它们的运动，每个模型是由<strong>一组3D 点</strong>构成的。本系统维护两组物体模型：当前在视野中可见的<strong>active 模型</strong>，以及曾经观测到的模型，但是目前不在视野中，记为<strong>inactive 模型</strong>。本系统的框架如Fig. 2所示，在初始化阶段，场景只包含一个active 模型——<strong>背景</strong>，初始化完成后，按照Fig .2的流程处理每一帧图片。</p>
<span id="more"></span>
<p><img src="/2024/01/27/Co-fusion/overview.png" alt="overview" title="Overview"></p>
<p><strong>tracking</strong> 与 <strong>fusion</strong> 步骤是在GPU 上完成的，而<strong>segmentation</strong> 步骤是在CPU 上完成的。</p>
<h2 id="3-1-Tracking"><a href="#3-1-Tracking" class="headerlink" title="3.1 Tracking"></a><strong>3.1 Tracking</strong></h2><p>在当前帧中跟踪每一个active 模型的6DOF 位姿，通过最小化每个模型独立的目标方程来实现，该目标方程包含：</p>
<ul>
<li><strong>几何误差</strong>：基于稠密的iterative closest point (ICP) 对齐；</li>
<li><strong>光度误差</strong>：基于当前帧和存储的3D 模型之间的颜色差异。</li>
</ul>
<h2 id="3-2-Segmentation"><a href="#3-2-Segmentation" class="headerlink" title="3.2 Segmentation"></a><strong>3.2 Segmentation</strong></h2><p>该阶段将当前帧中的每一个像素与某个active 模型/物体联系起来，有两种手段来实现该过程：motion 以及 semantic labels。</p>
<h3 id="3-2-1-Motion-segmentation"><a href="#3-2-1-Motion-segmentation" class="headerlink" title="3.2.1 Motion segmentation"></a>3.2.1 Motion segmentation</h3><p>将运动分割构建为一个使用全连接的Conditional Random Field (CRF) 解决的<strong>分类问题</strong>，可在CPU 上实时处理。当将一个像素与一个运动模型联系起来时，unary potentials 编码一个几何 <strong>ICP 损失函数</strong>。</p>
<h3 id="3-2-2-Multi-class-image-segmentation"><a href="#3-2-2-Multi-class-image-segmentation" class="headerlink" title="3.2.2 Multi-class image segmentation"></a>3.2.2 Multi-class image segmentation</h3><p>利用基于<strong>深度学习</strong>的方法实现像素级的语义分割，作为动作分割的备选方案。</p>
<h2 id="3-3-Fusion"><a href="#3-3-Fusion" class="headerlink" title="3.3 Fusion"></a>3.3 Fusion</h2><p>本系统使用surfel-based 融合方法，利用新估计的6DOF 位姿将属于某个模型的点更新至其active 模型。其中，每个模型是由一个sufel 列表构成的，$\mathcal{M}_m^s \in (\mathbf{p}\in \mathbb{R}^3, \mathbf{n}\in \mathbb{R}^3, \mathbf{c}\in \mathbb{N}^3, w \in \mathbb{R}, r \in \mathbb{R}, \mathbf{t}\in \mathbb{R}^2)$ ，分别表示位置、法向量、颜色、权重、半径以及两个时间戳。</p>
<p>为了解决<strong>动态物体</strong>的跟踪问题，本系统使用 $\mathcal{T}_t = \{\mathbf{T}_{tm}()\}$ 来表示每个active 模型 $\mathcal{M}_m$ 在时间 t 相对于全局参考坐标系的位姿转换几何，即 $\mathbf{T}_{tm}$ 表示时间 t 时模型 $\mathcal{M}_m$ 的全局位姿。特别地，作者使用 $\mathbf{T}_{tb}$ 来表示<strong>背景模型</strong>的位姿转换。</p>
<h1 id="5-Tracking-Active-Models"><a href="#5-Tracking-Active-Models" class="headerlink" title="5 Tracking Active Models"></a>5 Tracking Active Models</h1><p>对于时间t 的图像帧中的每一个active 模型  $\mathcal{M}_m$ ，系统通过配准<strong>当前的深度图</strong>和<strong>前一帧的深度图</strong>（通过将存储的3D 模型利用t-1 的估计位姿进行投影而获取）来跟踪其全局位姿 $\mathbf{T}_{tm}$ ，对每一个active 模型进行<strong>独立优化和跟踪</strong>。</p>
<h2 id="5-1-Energy"><a href="#5-1-Energy" class="headerlink" title="5.1 Energy"></a>5.1 Energy</h2><p>误差项包含<strong>ICP 几何误差</strong>和<strong>光度误差</strong>，其中光度误差是由预测的图片（将之前帧中存储的3D 模型投影而获取）与当前图片的<strong>颜色差异</strong>构成，</p>
<p><img src="/2024/01/27/Co-fusion/formula1.png" alt="formula1"></p>
<h2 id="5-2-Geometry-Term"><a href="#5-2-Geometry-Term" class="headerlink" title="5.2 Geometry Term"></a>5.2 Geometry Term</h2><p>ICP 几何误差定义为以下两者之间的误差：</p>
<ul>
<li>当前帧深度图的<strong>逆向投影</strong>3D 点；</li>
<li>前一帧t-1 <strong>预测</strong>的深度图。</li>
</ul>
<p><img src="/2024/01/27/Co-fusion/formula2.png" alt="formula2"></p>
<p>其中，$\mathbf{v}_t^i$ 是当前帧深度图 $\mathcal{D}_t$ 中第i 个点的反向映射3D 点；$\mathbf{v}^i, \mathbf{n}^i$ 分别是t-1 时刻<strong>预测的深度图</strong>中模型m 第i 个点的反向映射点以及其法向量；$\mathbf{T}_m$ 是将前一帧与当前帧的模型m 对齐的<strong>位姿转换</strong>。</p>
<h2 id="5-3-Photometric-Color-Term"><a href="#5-3-Photometric-Color-Term" class="headerlink" title="5.3 Photometric Color Term"></a>5.3 Photometric Color Term</h2><p>在给定(1)当前深度图、(2)每个active 模型的3D 几何估计，以及(3)将每个模型与前一帧对齐的位姿转换关系，即可将当前场景合成为一个与前一帧对齐的虚拟投影，进而，跟踪问题就变为了当前帧与合成的虚拟投影之间的<strong>光度配准</strong>问题：</p>
<p><img src="/2024/01/27/Co-fusion/formula3.png" alt="formula3"></p>
<p>其中，$\mathbf{T}_m$ 是将前一帧与当前帧的模型m 对齐的<strong>位姿转换</strong>；$\mathbf{I}_{t-1}()$ 表示提供模型在前一帧上顶点的颜色参数。</p>
<p>为了鲁棒性和效率，本优化使用一个<strong>4层的空间金字塔</strong>来集成到一个<strong>由粗到细</strong>的方法中，并在GPU 中完成运算。</p>
<h1 id="6-Motion-Segmentation"><a href="#6-Motion-Segmentation" class="headerlink" title="6 Motion Segmentation"></a>6 Motion Segmentation</h1><p>在跟踪步骤之后，在t 时刻有 $M_t$ 个新的位姿转换 $\{\mathbf{T}_{tm}\}$ ，来描述每个active 模型相对于全局坐标系的绝对位姿；接下来作者将帧t 的运动分割问题构建为一个<strong>分类标记问题</strong>，而标签为 $M_t$ 个位姿转换 $\{\mathbf{T}_{tm}\}$ ，作者将 $M_t+1$ 种可能分配到<strong>每一个像素</strong>中，即 $\mathcal{l} \in \mathcal{L}_t = \{1, …, |M_t|+1\}$ ，除了 $M_t$ 个位姿转换 $\{\mathbf{T}_{tm}\}$外还包含一个<strong>外点标签</strong> $\mathcal{l}_{|M_t|+1}$ 。</p>
<p>为了可以在CPU上实施完成分割步骤，系统首先将当前帧分割为SLIC 超像素，并在超像素级别上进行分类标记，超像素的位置、颜色与深度由从属的所有像素均值得到。代价函数如下所示：</p>
<p><img src="/2024/01/27/Co-fusion/formula4.png" alt="formula4"></p>
<p>其中，i 和 j 图片中超像素的索引（从1到S）。</p>
<h2 id="6-1-The-Unary-Potentials"><a href="#6-1-The-Unary-Potentials" class="headerlink" title="6.1 The Unary Potentials"></a>6.1 The Unary Potentials</h2><p>对于 $\psi_u(x_i)$ 表示为超像素 $s_i$ 分配标签为 $x_i$ 的代价，对于运动分割模式，该代价为<strong>ICP 几何对齐损失函数</strong>（式 2）。</p>
<h2 id="6-2-The-Pairwise-Potentials"><a href="#6-2-The-Pairwise-Potentials" class="headerlink" title="6.2 The Pairwise Potentials"></a>6.2 The Pairwise Potentials</h2><p>对于 $\psi_p(x_i, x_j)$ 可表示为：</p>
<p><img src="/2024/01/27/Co-fusion/formula5.png" alt="formula5"></p>
<p>其中，$\mu(x_i, x_j)$ 惩罚<strong>临近像素标签不同</strong>的情况；$k_m(f_i, f_j)$ 测量像素外观之间的<strong>相似度</strong>，代表的含义是：两个超像素的<strong>特征向量</strong>之间的距离较小时应具有相同的标签，所谓的特征向量 $f_i$  包含2D 位置、RGB 颜色以及深度值。</p>
<h2 id="6-3-Post-processing"><a href="#6-3-Post-processing" class="headerlink" title="6.3 Post-processing"></a>6.3 Post-processing</h2><p>在分割之后，采用一系列<strong>后处理步骤</strong>来获取更为鲁棒的结果：</p>
<ol>
<li>对具有相似位姿转换关系的模型进行<strong>融合操作</strong>；</li>
<li>抑制相同标签中除最大之外的所有区域来保证不连接区域的<strong>独立建模</strong>；</li>
<li>小于一定阈值的区域被<strong>舍弃</strong>。</li>
</ol>
<h2 id="6-4-Addition-of-New-Models"><a href="#6-4-Addition-of-New-Models" class="headerlink" title="6.4 Addition of New Models"></a>6.4 Addition of New Models</h2><p>一个区域内的外点数量若大于总像素数的3%，则判定该物体为一个新物体；若该新物体的部分几何结构已存在于地图中，会对重复的构建进行剔除。</p>
<p>如果一个物体消失在视野中，并在一定帧内不再出现，则将该模型添加进inactive 列表中。</p>
<h1 id="7-Object-Instance-Segmentation"><a href="#7-Object-Instance-Segmentation" class="headerlink" title="7 Object Instance Segmentation"></a>7 Object Instance Segmentation</h1><p>使用实例分割网络SharpMask 进行物体语义信息的获取。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 ORB-SLAM2_An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras</title>
    <url>/2024/01/29/orb-slam2/</url>
    <content><![CDATA[<p>Mur-Artal, Raul, and Juan D. Tardos. “ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras.” <em>IEEE Transactions on Robotics</em> 33, no. 5 (October 2017): 1255–62. <a href="https://doi.org/10.1109/TRO.2017.2705103">https://doi.org/10.1109/TRO.2017.2705103</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ol>
<li>第一个适用于<strong>单目</strong>、<strong>双目</strong>以及<strong>RGB-D 相机</strong>的开源SLAM 系统，该系统包含回环检测、重定位以及地图重用；</li>
<li>本系统运行RGB-D 的结果证明：使用<strong>BA</strong> 可以实现比基于ICP 或者光度深度误差最小化的SOTA 方法更高的精度；</li>
<li>通过使用<strong>近远立体点和单目观测</strong>，本系统运行双目的结果要比直接双目SLAM 的 SOTA 算法精度更高；</li>
<li>提出了一种关闭制图功能情况下，有效<strong>重使用地图</strong>的轻量级定位模式。</li>
</ol>
<span id="more"></span>
<p><img src="/2024/01/29/orb-slam2/fig1.png" alt="fig1" title="fig 1"></p>
<h1 id="3-ORB-SLAM2"><a href="#3-ORB-SLAM2" class="headerlink" title="3 ORB-SLAM2"></a>3 ORB-SLAM2</h1><p>ORB-SLAM2 的整体架构如下所示：</p>
<p><img src="/2024/01/29/orb-slam2/fig2.png" alt="fig2" title="fig 2"></p>
<p>系统包含三个主要的并行线程：</p>
<ol>
<li><strong>跟踪线程：</strong>寻找与<strong>局部地图</strong>相匹配的特征点，利用<strong>motion-only BA</strong> <strong>最小化重投影误差</strong>，来解算<strong>每帧图片</strong>对应的相机位姿；</li>
<li><strong>局部制图：</strong>管理<strong>局部地图</strong>，并使用<strong>局部BA</strong> 对其进行优化；</li>
<li><strong>回环检测：</strong>检测大回环，并使用<strong>位姿图优化</strong>来消除累积漂移；然后开启第四个线程，进行<strong>全局BA</strong> 优化解算地图与位姿的最优解。</li>
</ol>
<p>本系统还嵌入了一个基于<strong>DBoW2</strong> 的地点重识别模块进行<strong>重定位</strong>，在跟踪失败或利用现有地图进行重初始化时使用；本系统维护一个<strong>共视图</strong>，来关联任意两个具有共同观测特征点的关键帧，并使用一个<strong>最小化spanning tree</strong> 来连接所有的关键帧；这些图结构方便恢复关键帧的<strong>局部窗口</strong>以进行局部的跟踪与制图，并为回环检测中的位姿图优化提供结构。</p>
<h2 id="3-1-Monocular-Close-Stereo-and-Far-Stereo-Keypoints"><a href="#3-1-Monocular-Close-Stereo-and-Far-Stereo-Keypoints" class="headerlink" title="3.1 Monocular, Close Stereo, and Far Stereo Keypoints"></a>3.1 Monocular, Close Stereo, and Far Stereo Keypoints</h2><p>本系统经过如图Fig. 2（b）的<strong>图片预处理操作</strong>，提取出关键点的特征，系统后续的操作均是基于这些<strong>特征点（立体关键点和单目关键点）</strong>的，实现独立于所使用的传感器类型。后续的操作均基于立体关键点和单目关键点。</p>
<p><strong>立体关键点</strong>使用三个坐标进行定义：$\mathbf{x}_s = (u_L, v_L, u_R)$ ，其中 $(u_L, v_L)$ 是特征点在左边图片中的坐标，$u_R$ 是特征点在右边图片的水平座标。对于RGB-D 相机，作者将深度值 d 转化为一个<strong>虚拟的右图坐标</strong>：</p>
<p><img src="/2024/01/29/orb-slam2/f1.png" alt="f1" title="formula 1"></p>
<p>作者定义，如果一个关键点的<strong>深度小于基线长度的40倍</strong>，则被视为近点，否则视为远点。对于近点，可以使用一帧图片进行安全的三角化，因为其深度信息得到了准确估计，可以提供相应的<strong>尺度、平移和旋转信息</strong>；而对于远点，可以提供<strong>准确的旋转信息</strong>，但是尺度与平移信息较不可靠，只对多视角观测的远点进行三角化。</p>
<p><strong>单目关键点</strong>使用左图的两个坐标进行定义 $\mathbf{x}_m = (u_L, v_L)$ ，是针对那些立体匹配失败或RGB-D 深度参数不可靠的点；这些点只通过多视角观测进行三角化，且不提供尺度信息，但会参与旋转与平移估计的解算。</p>
<h2 id="3-2-System-Bootstrapping"><a href="#3-2-System-Bootstrapping" class="headerlink" title="3.2 System Bootstrapping"></a>3.2 System Bootstrapping</h2><p>使用立体相机或RGB-D 相机的一个主要优势在于：可仅使用一帧图片获取<strong>深度信息</strong>，而不需要单目相机的动作初始化操作。系统启动后，使用第一帧作为关键帧，将其位姿定为<strong>原点</strong>，并利用所有的<strong>立体关键点</strong>创建一个初始地图。</p>
<h2 id="3-3-Bundle-Adjustment-with-Monocular-and-Stereo-Constraints"><a href="#3-3-Bundle-Adjustment-with-Monocular-and-Stereo-Constraints" class="headerlink" title="3.3 Bundle Adjustment with Monocular and Stereo Constraints"></a>3.3 Bundle Adjustment with Monocular and Stereo Constraints</h2><p>BA 在本系统中的应用：</p>
<ul>
<li>在跟踪线程中优化相机位姿（motion-only BA）</li>
<li>在局部制图线程中优化局部窗口内的关键帧和特征点（local BA）</li>
<li>回环检测之后优化所有的关键帧与特征点（full BA）</li>
</ul>
<h3 id="Motion-only-BA"><a href="#Motion-only-BA" class="headerlink" title="Motion-only BA"></a>Motion-only BA</h3><p>对关键点 $\mathbf{x}_{(.)}^i$ （包括单目点 $\mathbf{x}_m^i \in \mathbb{R}^2$ 和立体点 $\mathbf{x}_s^i \in \mathbb{R}^3$ ）进行最小化重投影误差，其中 $i\in \mathcal{X}$ 为所有匹配点集合：</p>
<p><img src="/2024/01/29/orb-slam2/f2.png" alt="f2" title="formula 2"></p>
<p>其中，$\rho$ 是<strong>鲁棒Huber 损失函数</strong>，$\sum$ 是关键点尺度参数对应的协方差矩阵。单目点和立体点的投影矩阵如下所示：</p>
<p><img src="/2024/01/29/orb-slam2/f3.png" alt="f3" title="formula 3"></p>
<h3 id="Local-BA"><a href="#Local-BA" class="headerlink" title="Local BA"></a>Local BA</h3><p><img src="/2024/01/29/orb-slam2/f4.png" alt="f4" title="formula 4"></p>
<p>其中，$\mathcal{K}_L$ 为一组共视关键帧；$\mathcal{P}_L$ 为这些共视关键帧中的所有点；至于其他观测到 $\mathcal{P}_L$ 中的点且不属于 $\mathcal{K}_L$ 的关键帧 $\mathcal{K}_F$ ，会参与损失函数的构建，但是在优化中<strong>保持固定</strong>；$\mathcal{X}_k$ 表示 $\mathcal{P}_L$ 中与关键帧 k 匹配的点列表。</p>
<h3 id="Full-BA"><a href="#Full-BA" class="headerlink" title="Full BA"></a>Full BA</h3><p>是一种局部BA 的特殊情况，除了<strong>初始关键帧</strong>是固定的，地图中其余的所有关键帧和点都参与优化过程。</p>
<h2 id="3-4-Loop-Closing-and-Full-BA"><a href="#3-4-Loop-Closing-and-Full-BA" class="headerlink" title="3.4 Loop Closing and Full BA"></a>3.4 Loop Closing and Full BA</h2><p>回环检测包含两步：</p>
<ol>
<li>回环的检测与验证；</li>
<li>通过位姿图优化来矫正回环。</li>
</ol>
<h2 id="3-5-Keyframe-Insertion"><a href="#3-5-Keyframe-Insertion" class="headerlink" title="3.5 Keyframe Insertion"></a>3.5 Keyframe Insertion</h2><p>本系统遵循ORB-SLAM 的关键帧插入策略，此外，基于立体远近点创建了一个新的关键帧挑选策略：如果跟踪的近点数量低于 $\tau_t = 100$，且可新增近点数量大于 $\tau_c = 70$ 时，将其作为新的关键帧进行插入。</p>
<p><img src="/2024/01/29/orb-slam2/fig3.png" alt="fig3" title="fig 3"></p>
<h2 id="3-6-Localization-Mode"><a href="#3-6-Localization-Mode" class="headerlink" title="3.6 Localization Mode"></a>3.6 Localization Mode</h2><p>本系统引入了一个<strong>定位模式</strong>，可在已经制图的区域进行<strong>轻量级长期定位</strong>。该过程利用<strong>视觉里程计匹配点</strong>和<strong>地图匹配点</strong>进行定位，其中视觉里程计匹配是基于当前帧的ORB 和历史帧创建的3D 点之间进行的，这些匹配可在无地图区域进行定位，但会存在累积漂移；而地图点匹配会得到与地图无偏的定位结果。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>ORB-SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>C++</tag>
        <tag>ORB-SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu系统安装</title>
    <url>/2024/01/26/Ubuntu-setup/</url>
    <content><![CDATA[<h1 id="1-系统盘制作"><a href="#1-系统盘制作" class="headerlink" title="1 系统盘制作"></a>1 系统盘制作</h1><p>推荐使用 Rufus 这个软件进行制作：</p>
<span id="more"></span>
<p><img src="/2024/01/26/Ubuntu-setup/rufus.png" alt="rufus" title="rufus设置界面"></p>
<h1 id="2-进入BIOS更改启动顺序"><a href="#2-进入BIOS更改启动顺序" class="headerlink" title="2 进入BIOS更改启动顺序"></a>2 进入BIOS更改启动顺序</h1><p>将制作好的U盘插入电脑，启动电脑进入BIOS，设置第一启动项为U盘启动，保存BIOS设置并重新启动电脑，启动后选择”Try or install Ubuntu”选项，注意，此时可能会出现电脑黑屏的情况，但是屏幕是点亮的、鼠标有灯效，证明确实进入了安装系统，但是由于Ubuntu显卡驱动的问题，此时需要进行额外设置：</p>
<ol>
<li><p>重新启动电脑，在光标选择”Try or install Ubuntu”选项后，不要点击Enter键，而是点击”e”键进入命令行编辑模型；</p>
</li>
<li><p>删除”quite splash”后的”—-“，并添加”nomodeset”（依照不同显卡进行不同显卡驱动选项的添加，对于Nvidia显卡，添加nomodeset）；</p>
</li>
<li><p>然后，点击”F10”开始安装，此时电脑屏幕会正常。</p>
</li>
</ol>
<p>值得注意的是，装机成功重启后可能也会出现黑屏的情况（本人没有遇到），此时应在开机后点击”e”键，同样找到”quite splash” 并在后面添加”nomodeset”，按”F10”启动系统 ，进去系统之后编辑”/etc/default/grub”这个文件，具体操作在此不做赘述，可参考<a href="https://blog.csdn.net/qq_32285693/article/details/118900765">该文章</a>。</p>
<h1 id="3-设置硬盘分区"><a href="#3-设置硬盘分区" class="headerlink" title="3 设置硬盘分区"></a>3 设置硬盘分区</h1><p>本人想将之前的双系统进行清空，但又不想使用默认设置，故选择”something else”：</p>
<ol>
<li>清空之前双系统的所有磁盘空间，将硬盘全置为free；</li>
<li>根据个人需求设置硬盘分区，以下是重点部分，本人设置情况如下所示（<strong>注意分区类型</strong>）：</li>
</ol>
<p><img src="/2024/01/26/Ubuntu-setup/disk_set.png" alt="磁盘分区设置" title="磁盘分区设置"></p>
<p>具体空间分配可根据个人使用习惯和硬盘大小进行设置，值得注意的是，<strong>EFI 分区一定是第一个设置的！！！</strong></p>
<h1 id="4-Ubuntu设置"><a href="#4-Ubuntu设置" class="headerlink" title="4 Ubuntu设置"></a>4 Ubuntu设置</h1><p>进入Ubuntu系统后的设置不再赘述。</p>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>gcc、g++版本管理</title>
    <url>/2024/01/26/gcc-g/</url>
    <content><![CDATA[<p>本文主要参考<a href="https://zhuanlan.zhihu.com/p/261001751">文章</a>。</p>
<h2 id="1-版本查看"><a href="#1-版本查看" class="headerlink" title="1 版本查看"></a>1 版本查看</h2><p>对系统中的现有gcc、g++版本进行查看，安装所需版本：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 查看当前版本</span></span><br><span class="line">gcc -v</span><br><span class="line">g++ -v</span><br><span class="line"></span><br><span class="line"><span class="comment">## 查看系统已安装版本</span></span><br><span class="line"><span class="built_in">ls</span> /usr/bin/gcc*</span><br><span class="line"><span class="built_in">ls</span> /usr/bin/g++*</span><br><span class="line"></span><br><span class="line"><span class="comment">## 安装新版本</span></span><br><span class="line">sudo apt install gcc-11</span><br><span class="line">sudo apt install g++-11</span><br></pre></td></tr></table></figure>
<h2 id="2-版本切换"><a href="#2-版本切换" class="headerlink" title="2 版本切换"></a>2 版本切换</h2><p>首先，将已有版本添加到update-alternatives中：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 最后的数值代表该版本的权重参数，越大优先级越高</span></span><br><span class="line"><span class="comment"># gcc</span></span><br><span class="line">sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 100</span><br><span class="line">sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 70</span><br><span class="line"></span><br><span class="line"><span class="comment"># g++</span></span><br><span class="line">sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 100</span><br><span class="line">sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-11 70</span><br></pre></td></tr></table></figure>
<p>若想删除某个版本的管理：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo update-alternatives --remove gcc /usr/bin/gcc-11</span><br></pre></td></tr></table></figure>
<p>手动切换版本：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo update-alternatives --config gcc</span><br><span class="line">sudo update-alternatives --config g++</span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/gcc-g/g++.png" alt="g++" title="g++ version"></p>
<p>如上图所示，输入相应的id 即可实现不同版本之间的切换。</p>
<p>切换完之后，查看版本是否切换成功：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 查看当前版本</span></span><br><span class="line">gcc -v</span><br><span class="line">g++ -v</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>g++</tag>
        <tag>gcc</tag>
      </tags>
  </entry>
  <entry>
    <title>Object SLAM部署过程</title>
    <url>/2024/01/26/object-slam/</url>
    <content><![CDATA[<p>系统代码包下载地址为<a href="https://github.com/yangliu9527/Object_SLAM.git">github地址</a>，论文为(Liu 等, 2023).</p>
<h2 id="1-ORB-SLAM2基础问题"><a href="#1-ORB-SLAM2基础问题" class="headerlink" title="1 ORB-SLAM2基础问题"></a>1 ORB-SLAM2基础问题</h2><p>该算法是在ORB-SLAM2 的基础上进行改进的，编译过程可参考ORB-SLAM2的部署教程，此处不再赘述。除此之外，本人在部署过程中还遇到了其他问题，这里记录一下。</p>
<span id="more"></span>
<h2 id="2-其他问题"><a href="#2-其他问题" class="headerlink" title="2 其他问题"></a>2 其他问题</h2><ul>
<li>PCL 报错：</li>
</ul>
<blockquote>
<p>error: #error PCL requires C++14 or above</p>
</blockquote>
<p>参考这篇<a href="https://blog.csdn.net/handily_1/article/details/122421305">文章</a>解释，使用C++14编译器，更改主目录下的CMakeLists.txt：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">### 修改前</span><br><span class="line"># Check C++<span class="number">11</span> <span class="keyword">or</span> C++<span class="number">0</span><span class="function">x support</span></span><br><span class="line"><span class="function"><span class="title">include</span><span class="params">(CheckCXXCompilerFlag)</span></span></span><br><span class="line"><span class="function"><span class="title">CHECK_CXX_COMPILER_FLAG</span><span class="params">(<span class="string">&quot;-std=c++11&quot;</span> COMPILER_SUPPORTS_CXX11)</span></span></span><br><span class="line"><span class="function"><span class="title">CHECK_CXX_COMPILER_FLAG</span><span class="params">(<span class="string">&quot;-std=c++0x&quot;</span> COMPILER_SUPPORTS_CXX0X)</span></span></span><br><span class="line"><span class="function"><span class="title">if</span><span class="params">(COMPILER_SUPPORTS_CXX11)</span></span></span><br><span class="line"><span class="function">   <span class="title">set</span><span class="params">(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++11&quot;</span>)</span></span></span><br><span class="line"><span class="function">   <span class="title">add_definitions</span><span class="params">(-DCOMPILEDWITHC11)</span></span></span><br><span class="line"><span class="function">   <span class="title">message</span><span class="params">(STATUS <span class="string">&quot;Using flag -std=c++11.&quot;</span>)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">### 修改后</span></span><br><span class="line"><span class="function"># Check C++11 <span class="keyword">or</span> C++0x support</span></span><br><span class="line"><span class="function"><span class="title">include</span><span class="params">(CheckCXXCompilerFlag)</span></span></span><br><span class="line"><span class="function"><span class="title">CHECK_CXX_COMPILER_FLAG</span><span class="params">(<span class="string">&quot;-std=c++11&quot;</span> COMPILER_SUPPORTS_CXX11)</span></span></span><br><span class="line"><span class="function"><span class="title">CHECK_CXX_COMPILER_FLAG</span><span class="params">(<span class="string">&quot;-std=c++0x&quot;</span> COMPILER_SUPPORTS_CXX0X)</span></span></span><br><span class="line"><span class="function"><span class="title">if</span><span class="params">(COMPILER_SUPPORTS_CXX11)</span></span></span><br><span class="line"><span class="function">   <span class="title">set</span><span class="params">(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++14&quot;</span>)</span></span></span><br><span class="line"><span class="function">   <span class="title">add_definitions</span><span class="params">(-DCOMPILEDWITHC11)</span></span></span><br><span class="line"><span class="function">   <span class="title">message</span><span class="params">(STATUS <span class="string">&quot;Using flag -std=c++14.&quot;</span>)</span></span></span><br></pre></td></tr></table></figure>
<p>修改完之后不再报错，可正常编译。</p>
<ul>
<li>针对不同的输入图片，需要设置不同的通道变换方式，否则会报与数据通道相关的错误，如下图所示；修改方法为修改frame.cc 代码 392行附近的内容：</li>
</ul>
<p><img src="/2024/01/26/object-slam/err1.png" alt="err1" title="err1"></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//* for gray pictures (e.g. KITTI-odometry dataset)</span></span><br><span class="line">cv::Mat imRGB_1;</span><br><span class="line">cv::<span class="built_in">cvtColor</span>(imRGB.<span class="built_in">clone</span>(), imRGB_1, CV_GRAY2BGR);</span><br><span class="line">cv::<span class="built_in">cvtColor</span>(imRGB_1.<span class="built_in">clone</span>(), Img_HSV, CV_BGR2HSV);</span><br><span class="line"></span><br><span class="line"><span class="comment">//* for RGB pictures (e.g. TUM dataset)</span></span><br><span class="line"><span class="comment">// cv::cvtColor(imRGB.clone(), Img_HSV, CV_BGR2HSV);</span></span><br></pre></td></tr></table></figure>
<ul>
<li>在VS Code 里对程序进行调试运行时，可能会报错：</li>
</ul>
<p><img src="/2024/01/26/object-slam/err2.png" alt="err2"></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 只有在VS Code 中调试才会出现该错误，解决办法是在VS Code 中unset GTK_PATH即可：</span></span><br><span class="line"><span class="built_in">unset</span> GTK_PATH</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Object SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>C++</tag>
        <tag>Object SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 ORB-SLAM_A Versatile and Accurate Monocular SLAM System</title>
    <url>/2024/01/29/orb-slam/</url>
    <content><![CDATA[<p>Mur-Artal, Raul, J. M. M. Montiel, and Juan D. Tardos. “ORB-SLAM: A Versatile and Accurate Monocular SLAM System.” <em>IEEE Transactions on Robotics</em> 31, no. 5 (October 2015): 1147–63. <a href="https://doi.org/10.1109/TRO.2015.2463671">https://doi.org/10.1109/TRO.2015.2463671</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献如下：</p>
<ol>
<li>对所有任务使用相同的特性：tracking, mapping, relocalization and loop closing, 这使得我们的系统更加<strong>高效</strong>、<strong>简单</strong>且<strong>可靠</strong>；使用ORB 特征，可在CPU 上实现实时运行，且具有较好的<strong>视角、光照不变性</strong>；</li>
<li>利用<strong>共视图</strong>使得跟踪与制图都聚焦于一个局部共视区域，从而实现在大规模环境中地<strong>实时</strong>操作，可不受全局地图尺寸地影响；</li>
<li>基于<strong>位姿图优化</strong>的实时回环检测（作者称其为 Essential Graph），其构建于系统维护的spanning tree、回环连接以及共视图中的线；</li>
<li>基于良好的视角和光照不变性实现的<strong>实时相机重定位</strong>，可在跟踪失败时进行<strong>重初始化</strong>，并增强了<strong>地图的重用性</strong>；</li>
<li>提出一个基于模型选择的自动鲁棒的<strong>初始化程序</strong>，可为平面和非平面场景创建一个<strong>初始化地图</strong>；</li>
<li>对于地图点和关键帧采取“适者生存” <strong>survival of the fittest</strong> 策略，在生成时非常宽松，而在剔除时非常严格，该策略提高了跟踪的鲁棒性，且由于冗余的关键帧被舍弃了，从而增强了 lifelong operation。</li>
</ol>
<span id="more"></span>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3 System Overview"></a>3 System Overview</h1><h2 id="3-1-Feature-Choice"><a href="#3-1-Feature-Choice" class="headerlink" title="3.1 Feature Choice"></a>3.1 Feature Choice</h2><p>本系统的一个重要设计是：制图与跟踪所使用的<strong>同样特性</strong>会用于<strong>地点重识别</strong>，来进行帧级的重定位和回环检测。</p>
<h2 id="3-2-Three-Threads-Tracking-Local-Mapping-and-Loop-Closing"><a href="#3-2-Three-Threads-Tracking-Local-Mapping-and-Loop-Closing" class="headerlink" title="3.2 Three Threads: Tracking, Local Mapping, and Loop Closing"></a>3.2 Three Threads: Tracking, Local Mapping, and Loop Closing</h2><p>系统整体架构如Fig. 1所示：</p>
<p><img src="/2024/01/29/orb-slam/overview.png" alt="overview" title="overview"></p>
<h2 id="3-3-Map-Points-Keyframes-and-Their-Selection"><a href="#3-3-Map-Points-Keyframes-and-Their-Selection" class="headerlink" title="3.3 Map Points, Keyframes, and Their Selection"></a>3.3 Map Points, Keyframes, and Their Selection</h2><p>每个地图点 $p_i$ 存储以下信息：</p>
<ol>
<li>世界坐标系中的3D 位置信息 $\mathbf{X}_{w, i}$ ；</li>
<li>视角朝向 $\mathbf{n}_i$，是所有视角方向（观测到该点的关键帧的相机光心与该点的连线）的平均单位向量；</li>
<li>一个代表性的 ORB 描述子 $\mathbf{D}_i$ ，利用所有观测到该点的关键帧中的描述子计算一个最小汉明距离的ORB 描述子；</li>
<li>根据ORB 特征的尺度不变约束，计算该点可被观测到的最大距离和最小距离 $d_{max}, d_{min}$ 。</li>
</ol>
<p>每个关键帧 $K_i$ 存储以下信息：</p>
<ol>
<li>相机<strong>位姿</strong> $\mathbf{T}_{iw}$ ，是从世界坐标系到相机坐标系的刚体转换关系；</li>
<li>相机<strong>内参</strong>，包含焦距和光心；</li>
<li>该帧图片中提取的<strong>所有ORB 特征</strong>，是否与地图点关联。</li>
</ol>
<h2 id="3-4-Covisibility-Graph-and-Essential-Graph"><a href="#3-4-Covisibility-Graph-and-Essential-Graph" class="headerlink" title="3.4 Covisibility Graph and Essential Graph"></a>3.4 Covisibility Graph and Essential Graph</h2><p>关键帧之间的<strong>共视信息</strong>对于本系统的许多任务而言至关重要，该共视信息使用<strong>无向加权图</strong>来表示，图中每个节点代表一个关键帧，如果两个关键帧之间的共视地图点超过15个，则进行节点间的连线，并使用共视地图点的数量来表示权重参数 $\theta$ 。</p>
<p>作者使用<strong>位姿图优化</strong>对回环检测到的位姿进行整体优化，为了避免包含共视图中所有的边（过于稠密），作者提出 <strong>Essential Graph</strong>只保留<strong>所有的节点与部分边</strong>，仍然可以保留强壮的网络结构来产生精确的结果。</p>
<p>系统会从初始帧开始构建一个<strong>增量式spanning tree</strong>，提供一个具有最小边数量的共视图的<strong>连接子图</strong>，当一个新的关键帧被插入时，被包含在该树中，并和与其<strong>有最多共视点</strong>的关键帧连接；当某个关键帧被剔除后，会更新相应的受影响的连线。</p>
<p>而Essential Graph 包含<strong>spanning tree</strong>、<strong>共视图</strong>中共视点数超过100个的边，以及<strong>回环检测边</strong>，从而形成一个强壮的相机轨迹网络。</p>
<p><img src="/2024/01/29/orb-slam/reconstruction.png" alt="reconstruction" title="reconstruciton and graphs"></p>
<h2 id="3-5-Bags-of-Words-Place-Recognition"><a href="#3-5-Bags-of-Words-Place-Recognition" class="headerlink" title="3.5 Bags of Words Place Recognition"></a>3.5 Bags of Words Place Recognition</h2><p>系统集成了一个基于DBoW2 的词袋库地点重识别模块来进行回环检测和重定位，本系统创建一个增量式的数据集以进行查询和更新。</p>
<h1 id="4-Automatic-Map-Initialization"><a href="#4-Automatic-Map-Initialization" class="headerlink" title="4 Automatic Map Initialization"></a>4 Automatic Map Initialization</h1><p>地图初始化的目标是计算两帧图片之间的<strong>相对位姿</strong>来<strong>三角化</strong>一组地图点，作者提出并行计算两种几何模型：平面场景下的<strong>单应矩阵（homography）</strong>，及非平面场景下的<strong>基础矩阵</strong>。本系统的地图初始化步骤如下所示：</p>
<ol>
<li><strong>寻找初始关系：</strong>提取当前帧中的ORB 特征，并寻找与参考帧之间的匹配，如果没有找到足够的匹配，重置参考帧；</li>
<li><strong>并行计算两个模型：</strong>分别计算单应矩阵和基础矩阵，并在迭代中计算相应的得分，最后保留得分最高的矩阵；</li>
<li><strong>模型选择：</strong>若场景属于平面、近似平面或者存在较小的视差，可选用单应矩阵；如果是有足够视差的非平面场景，应当选用基础矩阵；</li>
<li><strong>从移动恢复（motion recovery）中得到动作和结构：</strong>对于单应矩阵，作者直接对8组解进行三角化，并检查是否存在一个解使得大部分点位于相机前部且有着较低的重投影误差，如果不存在一个具有明显优势的解，则返回第一步重新开始初始化，该方法被认为是本系统鲁棒性的关键所在；对于基础矩阵，利用内参计算出本质矩阵，利用奇异值分解恢复出四个运动假设，然后采用与单应矩阵相同的方法进行求解；</li>
<li><strong>BA：</strong>最后，利用<strong>full BA</strong> 来优化初始重建。</li>
</ol>
<p>一个初始化的例子如下图所示，PTAM 和 LSD-SLAM 初始化一个平面上的所有点，而本系统会等到有足够的视差后利用基础矩阵才进行正确的初始化。</p>
<p><img src="/2024/01/29/orb-slam/initialization.png" alt="initiallization" title="initialization"></p>
<h1 id="5-Tracking"><a href="#5-Tracking" class="headerlink" title="5 Tracking"></a>5 Tracking</h1><h2 id="5-2-Initial-Pose-Estimation-From-Previous-Frame"><a href="#5-2-Initial-Pose-Estimation-From-Previous-Frame" class="headerlink" title="5.2 Initial Pose Estimation From Previous Frame"></a>5.2 Initial Pose Estimation From Previous Frame</h2><p>如果上一帧跟踪成功，本系统使用一个<strong>固定速度运动模型</strong>来预测相机位姿，并对上一帧观测到的地图点进行一个<strong>引导式搜索</strong>，然后利用寻找到的关联对位姿进行优化。</p>
<h2 id="5-3-Initial-Pose-Estimation-via-Global-Relocalization"><a href="#5-3-Initial-Pose-Estimation-via-Global-Relocalization" class="headerlink" title="5.3 Initial Pose Estimation via Global Relocalization"></a>5.3 Initial Pose Estimation via Global Relocalization</h2><p>如果跟踪失败，则将该帧图片转换为<strong>词袋</strong>并进行词袋库搜索以实现<strong>全局重定位</strong>，对每个候选关键帧进行RANSAC 迭代并使用PnP 计算相机的位姿；若找到具有足够内点的相机位姿，则根据匹配关键帧的地图点搜寻更多的匹配以进行位姿优化。</p>
<h2 id="5-4-Track-Local-Map"><a href="#5-4-Track-Local-Map" class="headerlink" title="5.4 Track Local Map"></a>5.4 Track Local Map</h2><p>获取相机位姿的估计和一组初始化特征匹配后，将<strong>局部地图</strong>投影至当前帧中来搜寻更多的匹配。该局部地图包含与当前帧有匹配特征点的关键帧 $\mathcal{K}_1$ ，以及在共视图中 $\mathcal{K}_1$ 的邻接关键帧 $\mathcal{K}_2$ ，关键帧 $\mathcal{K}_1$ 、$\mathcal{K}_2$ 中的所有点进行以下搜索策略：</p>
<ol>
<li>将点投影至当前帧中，舍弃掉超出图片界限的点；</li>
<li>比较当前帧中点和地图中点的视角朝向，舍弃小于60读夹角的点；</li>
<li>计算地图点到相机光心的距离，舍弃掉超范围的点 $d \notin [d_{min}, d_{max}]$ ；</li>
<li>计算尺度信息 $d/d_{min}$ ；</li>
<li>将地图点的代表性描述子与当前帧中未匹配的ORB 特征进行比较，寻找地图点的最佳匹配。</li>
</ol>
<p>最终利用所有匹配的点对相机位姿进行优化。</p>
<h2 id="5-5-New-Frame-Decision"><a href="#5-5-New-Frame-Decision" class="headerlink" title="5.5 New Frame Decision"></a>5.5 New Frame Decision</h2><p><strong>新的关键帧</strong>需满足以下所有的要求：</p>
<ol>
<li>距离上一次全局重定位不少于20帧（为了更好地重定位）；</li>
<li>局部制图线程空闲，或者距离上次插入关键帧已超过20帧；</li>
<li>当前帧至少跟踪了50个地图点（为了更好地跟踪）；</li>
<li>当前帧比参考帧少跟踪90%的点（限制最小的视角变化）。</li>
</ol>
<h1 id="6-Local-Mapping"><a href="#6-Local-Mapping" class="headerlink" title="6 Local Mapping"></a>6 Local Mapping</h1><h2 id="6-1-Keyframe-Insertion"><a href="#6-1-Keyframe-Insertion" class="headerlink" title="6.1 Keyframe Insertion"></a>6.1 Keyframe Insertion</h2><p>每当插入一个新的关键帧，进行一下操作：</p>
<ul>
<li>更新<strong>共视图</strong>，增加新的节点和边；</li>
<li>更新<strong>spanning tree</strong>；</li>
<li>计算该关键帧的<strong>词袋表示</strong>。</li>
</ul>
<h2 id="6-2-Recent-Map-Points-Culling"><a href="#6-2-Recent-Map-Points-Culling" class="headerlink" title="6.2 Recent Map Points Culling"></a>6.2 Recent Map Points Culling</h2><p>地图点要想保留在地图中，需要在创建后的前三个关键帧中经过严格的测试：</p>
<ol>
<li>跟踪线程需要在其被预测可见的关键帧中至少有25%的比例被观测到；</li>
<li>若该点创建后已经过了一个关键帧，那么至少需要被三个关键帧观测到。</li>
</ol>
<h2 id="6-3-New-Map-Point-Creation"><a href="#6-3-New-Map-Point-Creation" class="headerlink" title="6.3 New Map Point Creation"></a>6.3 New Map Point Creation</h2><p>新的地图点通过共视图中的相连关键帧对ORB 特征点进行三角化来创建。</p>
<h2 id="6-4-Local-Bundle-Adjustment"><a href="#6-4-Local-Bundle-Adjustment" class="headerlink" title="6.4 Local Bundle Adjustment"></a>6.4 Local Bundle Adjustment</h2><p>局部BA 的优化对象为：当前关键帧，共视图中与当前关键帧相连的所有关键帧，以及这些关键帧中的所有地图点。至于其余可观测到这些地图点但是未与当前关键帧相连的那些关键帧也会参与到优化过程中，但是其自身保持固定。</p>
<h2 id="6-5-Local-Keyframe-Culling"><a href="#6-5-Local-Keyframe-Culling" class="headerlink" title="6.5 Local Keyframe Culling"></a>6.5 Local Keyframe Culling</h2><p>之所以要控制关键帧的数量，是因为BA 的计算复杂度与该数量正相关，而且在lifelong operation 中，同一场景中的关键帧数量不能无限制增长。</p>
<p>本系统根据以下准则舍弃掉关键帧：其90%的地图点在其他至少三个关键帧中以相同或更好的尺度被观测到。这个尺度条件确保了地图点可以保留那些自身被最好精度观测到的关键帧。</p>
<h1 id="7-Loop-Closing"><a href="#7-Loop-Closing" class="headerlink" title="7 Loop Closing"></a>7 Loop Closing</h1><h2 id="7-1-Loop-Candidates-Detection"><a href="#7-1-Loop-Candidates-Detection" class="headerlink" title="7.1 Loop Candidates Detection"></a>7.1 Loop Candidates Detection</h2><p>首先，将当前帧和共视图中与其相连的所有帧（共视点数量不少于30个）计算BoW 向量的相似性，得到一个最低的阈值 $s_{min}$ ；然后再词袋库中进行匹配，舍弃掉低于该阈值的关键帧，此外，还需舍弃掉与当前帧直接相连的关键帧；只有当检测到连续三个一致的回环候选帧（在共视图中是相连的），才判定存在回环。</p>
<h2 id="7-2-Compute-the-Similarity-Transformation"><a href="#7-2-Compute-the-Similarity-Transformation" class="headerlink" title="7.2 Compute the Similarity Transformation"></a>7.2 Compute the Similarity Transformation</h2><p>在单目SLAM 中，地图有<strong>7自由度</strong>：旋转、平移，以及一个尺度因子。因此，为了闭合一个回环，需要计算一个从当前帧到回环帧之间的<strong>相似转换</strong>，以获取回环的累积误差；同时，该相似度计算也作为该回环的<strong>几何验证</strong>。</p>
<p>首先计算当前帧与回环候选帧之间的<strong>地图点关联</strong>，然后为每个候选回环帧进行RANSAC ，若获取足够多的内点，则进行优化并进一步获取更多的关联，并进一步进行优化，此时若有足够的内点数量支持该相似度，则确定回环。</p>
<h2 id="7-3-Loop-Fusion"><a href="#7-3-Loop-Fusion" class="headerlink" title="7.3 Loop Fusion"></a>7.3 Loop Fusion</h2><p>回环矫正的第一步是将<strong>重复的地图点</strong>进行融合，并在<strong>共视图</strong>中插入新的边以完成回环闭合。使用计算得到的<strong>相似转换</strong>来矫正当前关键帧的位姿，并将该矫正传递至与其邻接的关键帧，使得回环两侧进行<strong>对齐</strong>。</p>
<h2 id="7-4-Essential-Graph-Optimization"><a href="#7-4-Essential-Graph-Optimization" class="headerlink" title="7.4 Essential Graph Optimization"></a>7.4 Essential Graph Optimization</h2><p>利用essential graph 将回环闭合误差分布至整个图上以进行优化；利用<strong>相似转换</strong>进行优化来矫正尺度偏移。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>ORB-SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>C++</tag>
        <tag>ORB-SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>SSH免密登录设置</title>
    <url>/2024/01/26/ssh-setup/</url>
    <content><![CDATA[<h1 id="1-ssh服务"><a href="#1-ssh服务" class="headerlink" title="1 ssh服务"></a>1 ssh服务</h1><p>Ubuntu开启ssh服务需要下载openssh-server，命令为：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br></pre></td></tr></table></figure>
<h1 id="2-ssh-key生成"><a href="#2-ssh-key生成" class="headerlink" title="2 ssh key生成"></a>2 ssh key生成</h1><p>生成ssh key的命令是：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure>
<p>后续一路默认设置即可，该命令会在主目录默认生成.ssh文件，内包含以下文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">id_rsa: 私钥</span><br><span class="line">id_rsa.pub: 公钥</span><br></pre></td></tr></table></figure>
<h1 id="3-无密码远程登陆服务器"><a href="#3-无密码远程登陆服务器" class="headerlink" title="3 无密码远程登陆服务器"></a>3 无密码远程登陆服务器</h1><p>将本地产生的公钥上传至服务器：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-copy-id -p <span class="comment">#port master@ubuntu</span></span><br></pre></td></tr></table></figure>
<h1 id="4-Windows免密登录Ubuntu"><a href="#4-Windows免密登录Ubuntu" class="headerlink" title="4 Windows免密登录Ubuntu"></a>4 Windows免密登录Ubuntu</h1><p>Windows在终端中执行以下命令生成公私密钥：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>
<p>会在C:\Users\xxx.ssh 文件夹中生成以下三个文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">id_rsa 本地私钥</span><br><span class="line">id_rsa.pub 公钥</span><br><span class="line">known_hosts 已知的ip</span><br></pre></td></tr></table></figure>
<p>将公钥上传至Ubuntu，可使用scp命令，然后执行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将公钥拷贝至Ubuntu的authorized_keys文件中</span></span><br><span class="line"><span class="built_in">cat</span> ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改sshd_config文件</span></span><br><span class="line">sudo vim /etc/ssh/sshd_config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将一下三行命令添加至sshd_config文件</span></span><br><span class="line">RSAAuthentication <span class="built_in">yes</span></span><br><span class="line">PubkeyAuthentication <span class="built_in">yes</span></span><br><span class="line">PasswordAuthentication no</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启ssh</span></span><br><span class="line">sudo service sshd restart</span><br></pre></td></tr></table></figure>
<p>然后即可实现Windows免密登录Ubuntu。</p>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title>ORB-SLAM2环境搭建与运行</title>
    <url>/2024/01/26/orbslam2-env/</url>
    <content><![CDATA[<p>本文主要参考<a href="https://blog.csdn.net/meng_152634/article/details/127570220">该文章</a>。</p>
<h2 id="1-Eigen3安装与卸载"><a href="#1-Eigen3安装与卸载" class="headerlink" title="1 Eigen3安装与卸载"></a>1 Eigen3安装与卸载</h2><h3 id="1-1-安装"><a href="#1-1-安装" class="headerlink" title="1.1 安装"></a>1.1 安装</h3><p>可通过apt命令安装，由于使用源码安装的方式在后续编译ORB-SLAM2 过程中可能会遇到一些问题，因此，本人<strong>建议使用apt命令进行安装</strong>：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install libeigen3-dev</span><br></pre></td></tr></table></figure>
<p>也可在<a href="https://eigen.tuxfamily.org/index.php?title=Main_Page">官网</a>下载eigen3 源码，然后编译安装（本人不推荐该方法，后续会出现程序找不到eigen3的问题）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> eigen-xxx</span><br><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>版本查看命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">❯ pkg-config --modversion eigen3</span><br><span class="line">3.4.0</span><br></pre></td></tr></table></figure>
<h3 id="1-2-卸载"><a href="#1-2-卸载" class="headerlink" title="1.2 卸载"></a>1.2 卸载</h3><p>通过apt 方式安装的采用以下方式进行卸载：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 通过remove 卸载</span></span><br><span class="line">sudo apt remove libeigen3-dev</span><br><span class="line"></span><br><span class="line"><span class="comment">## 可通过locate 进一步定位残余文件进行手动删除，并手动删除/usr/local/和/usr/include/目录下的eigen目录</span></span><br><span class="line">sudo updatedb</span><br><span class="line">locate eigen</span><br></pre></td></tr></table></figure>
<p>通过源码安装的，需要手动删除/usr/local/include/等目录下的eigen3目录：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo updatedb</span><br><span class="line">locate eigen3  <span class="comment"># 查看eigen3的位置</span></span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/include/eigen3</span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/lib/cmake/eigen3</span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/local//include/eigen3</span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/share/doc/libeigen3-dev</span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/local/share/pkgconfig/eigen3.pc /usr/share/pkgconfig/eigen3.pc /var/lib/dpkg/info/libeigen3-dev.list /var/lib/dpkg/info/libeigen3-dev.md5sums</span><br></pre></td></tr></table></figure>
<h2 id="2-Pangolin-安装与卸载"><a href="#2-Pangolin-安装与卸载" class="headerlink" title="2 Pangolin 安装与卸载"></a>2 Pangolin 安装与卸载</h2><h3 id="2-1-安装"><a href="#2-1-安装" class="headerlink" title="2.1 安装"></a>2.1 安装</h3><p>首先安装依赖项：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install libglew-dev</span><br><span class="line">sudo apt install libboost-dev libboost-thread-dev libboost-filesystem-dev</span><br><span class="line">sudo apt install libboost-all-dev</span><br><span class="line">sudo apt install libwayland-dev wayland-protocols</span><br><span class="line">sudo apt install libxkbcommon-dev</span><br><span class="line">sudo apt install libegl1-mesa-dev</span><br><span class="line">sudo apt install ninja-build</span><br></pre></td></tr></table></figure>
<p>然后下载源码，编译安装</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/stevenlovegrove/Pangolin.git</span><br><span class="line"></span><br><span class="line"><span class="comment">## 编译安装</span></span><br><span class="line"><span class="built_in">cd</span> Pangolin</span><br><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>然后可在/usr/local/include/和/usr/local/lib/目录下找到pangolin相关的目录及库文件：</p>
<p><img src="/2024/01/26/orbslam2-env/pangolin.png" alt="pangolin" title="Pangolin目录及库文件"></p>
<p>安装完成后进行测试：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> Pangolin/build/examples/HelloPangolin/</span><br><span class="line">./HelloPangolin</span><br></pre></td></tr></table></figure>
<p>出现以下界面说明安装成功：</p>
<p><img src="/2024/01/26/orbslam2-env/pangolin-success.png" alt="pangolin success" title="Pangolin安装成功"></p>
<h3 id="2-2-卸载"><a href="#2-2-卸载" class="headerlink" title="2.2 卸载"></a>2.2 卸载</h3><p>Pangolin 的卸载需要手动删除相关文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo updatedb</span><br><span class="line">locate pangolin</span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/local/include/pangolin</span><br><span class="line">sudo <span class="built_in">rm</span> /usr/local/lib/libpango_*.so</span><br></pre></td></tr></table></figure>
<h2 id="3-opencv-安装与卸载"><a href="#3-opencv-安装与卸载" class="headerlink" title="3 opencv 安装与卸载"></a>3 opencv 安装与卸载</h2><p>分别到<a href="https://opencv.org/releases/">opencv</a>、<a href="https://github.com/opencv/opencv_contrib/tags">contrib</a> 开发库下载对应版本，注意，两个文件的版本需要保持一致。</p>
<h3 id="3-0-pkg-config"><a href="#3-0-pkg-config" class="headerlink" title="3.0 pkg-config"></a>3.0 pkg-config</h3><h4 id="3-0-1-介绍"><a href="#3-0-1-介绍" class="headerlink" title="3.0.1 介绍"></a>3.0.1 介绍</h4><p>opencv 多版本管理主要参考<a href="https://www.cntofu.com/book/46/opencv/ubuntuxia_duo_ban_ben_opencv_qie_huan.md">文章1</a>和<a href="https://ivanzz1001.github.io/records/post/linux/2017/09/08/linux-pkg-config#1-pkg-config简单介绍">文章2</a>，该文章主要利用pkg-config 包管理工具来管理多版本的opencv。</p>
<p>一般用第三方库的时候，就少不了要使用到<strong>第三方的头文件</strong>和<strong>库文件</strong>。我们在编译、链接的时候必须要指定这些头文件和库文件的位置。对于一个比较大的第三方库，其头文件和库文件的数量是比较多的，如果我们一个个手动地写，那将是相当的麻烦的。因此，pkg-config就应运而生了。pkg-config能够把这些头文件和库文件的位置指出来，给编译器使用。pkg-config主要提供了下面几个功能：</p>
<ul>
<li>检查库的版本号。 如果所需要的库的版本不满足要求，它会打印出错误信息，避免链接错误版本的库文件</li>
<li>获得编译预处理参数，如宏定义、头文件的位置</li>
<li>获得链接参数，如库及依赖的其他库的位置，文件名及其他一些链接参数</li>
<li>自动加入所依赖的其他库的设置</li>
</ul>
<p>如以下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">gcc -o <span class="built_in">test</span> test.c `pkg-config --libs --cflags glib-2.0`</span><br></pre></td></tr></table></figure>
<p>其中，—libs 用于指定<strong>库文件</strong>，—cflags 用于指定<strong>头文件</strong>。</p>
<p>pkg-config 命令基本用法如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pkg-config &lt;options&gt; &lt;library-name&gt;</span><br></pre></td></tr></table></figure>
<h4 id="3-0-2-配置环境变量"><a href="#3-0-2-配置环境变量" class="headerlink" title="3.0.2 配置环境变量"></a>3.0.2 配置环境变量</h4><p>事实上，pkg-config只是一个工具，所以不是你安装了一个第三方库，pkg-config就能知道第三方库的头文件和库文件的位置的。为了让pkg-config可以得到一个库的信息，就要求库的提供者提供一个<strong>.pc 文件</strong>。例如，本人安装的opencv-4.6.0 中包含了对应的opencv4.pc 文件：</p>
<p><img src="/2024/01/26/orbslam2-env/opencv4.png" alt="opencv4" title="opencv4.pc"></p>
<p>首先，将.pc 文件拷贝至pkgconfig 路径下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo <span class="built_in">cp</span> ~/Softwares/opencv/opencv-4.6.0/lib/pkgconfig/opencv4.pc /usr/lib/pkgconfig/opencv4.pc</span><br></pre></td></tr></table></figure>
<p>然后，添加链接库路径：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 新建文件并编辑</span></span><br><span class="line">sudo vi /etc/ld.so.conf.d/opencv4.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">## 添加链接库路径</span></span><br><span class="line">/home/echo/Softwares/opencv/opencv-4.6.0/lib/</span><br><span class="line"></span><br><span class="line"><span class="comment">## 刷新</span></span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure>
<p>刷新之后，即可检验相应版本是否添加成功：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; pkg-config --modversion opencv4</span><br><span class="line">4.6.0</span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/orbslam2-env/opencv4-1.png" alt="opencv4" title="opencv4库文件与头文件"></p>
<p>相应地，编译工程时即可使用如下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">g++ -o cv_test cv_test.cpp `pkg-config --libs --cflags opencv4`</span><br></pre></td></tr></table></figure>
<h3 id="3-1-opencv-4-6-0"><a href="#3-1-opencv-4-6-0" class="headerlink" title="3.1 opencv-4.6.0"></a>3.1 opencv-4.6.0</h3><h4 id="3-1-1-编译与安装"><a href="#3-1-1-编译与安装" class="headerlink" title="3.1.1 编译与安装"></a>3.1.1 编译与安装</h4><p>编译命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cmake -D CMAKE_BUILD_TYPE=RELEASE \</span><br><span class="line">	-D CMAKE_INSTALL_PREFIX=/home/***/Softwares/opencv/opencv-4.6.0 \ <span class="comment">## 更换为相应位置</span></span><br><span class="line">	-D INSTALL_C_EXAMPLES=OFF \</span><br><span class="line">	-D OPENCV_ENABLE_NONFREE=ON \</span><br><span class="line">	-D WITH_CUDA=ON \</span><br><span class="line">	-D WITH_CUDNN=ON \</span><br><span class="line">	-D OPENCV_DNN_CUDA=ON \</span><br><span class="line">	-D ENABLE_FAST_MATH=1 \</span><br><span class="line">	-D CUDA_FAST_MATH=1 \</span><br><span class="line">	-D CUDA_ARCH_BIN=8.6 \  <span class="comment">## 一定更改为显卡对应的算力版本</span></span><br><span class="line">	-D WITH_CUBLAS=1 \</span><br><span class="line">	-D OPENCV_EXTRA_MODULES_PATH=/home/***/Softwares/opencv/opencv-4.6.0/opencv_contrib/modules \</span><br><span class="line">	-D BUILD_EXAMPLES=ON ..</span><br></pre></td></tr></table></figure>
<p>因为要使用pkg-config 对不同版本的opencv 进行管理，但是<strong>opencv4默认将opencv.pc 的产生选项关闭了</strong>，查看CMakelist.txt 相关语句如下所示：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">OCV_OPTION(OPENCV_GENERATE_PKGCONFIG <span class="string">&quot;Generate .pc file for pkg-config build tool (deprecated)&quot;</span> OFF)</span><br></pre></td></tr></table></figure>
<p>所以需将CMakelist.txt 中对应语句的<strong>参数改为ON</strong>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 将对应命令更改如下：</span></span><br><span class="line">OCV_OPTION(OPENCV_GENERATE_PKGCONFIG <span class="string">&quot;Generate .pc file for pkg-config build tool (deprecated)&quot;</span> ON)</span><br></pre></td></tr></table></figure>
<p>编译成功之后，make、安装：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">make -j13 -w</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>值得注意的是，由于是安装到了自定义目录下，在利用CMakeLists.txt 进行编译前，需要进行以下操作：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 将build文件夹中的OpenCVConfig.cmake、OpenCVModules.cmake移至share/opencv4中</span></span><br><span class="line">sudo <span class="built_in">cp</span> OpenCVConfig.cmake OpenCVModules.cmake ../../share/opencv4/</span><br><span class="line"></span><br><span class="line"><span class="comment">## 修改CMakeLists.txt 文件，设置搜寻路径</span></span><br><span class="line"><span class="built_in">set</span>(OpenCV_DIR /home/echo/Softwares/opencv/opencv-4.6.0/share/opencv4)</span><br><span class="line">find_package(OpenCV)</span><br></pre></td></tr></table></figure>
<h4 id="3-1-2-多版本控制"><a href="#3-1-2-多版本控制" class="headerlink" title="3.1.2 多版本控制"></a>3.1.2 多版本控制</h4><p>按照pkgconfig  版本控制，使用opencv-4.6.0 对自带的示例进行测试，修改CMakeLists.txt 文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 设置搜寻路径</span></span><br><span class="line"><span class="built_in">set</span>(OpenCV_DIR /home/echo/Softwares/opencv/opencv-4.6.0/share/opencv4)</span><br><span class="line">find_package(OpenCV)</span><br></pre></td></tr></table></figure>
<p>进行cmake编译并执行：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cmake 过程中会出现相应的版本号、库文件等信息</span></span><br><span class="line">cmake .</span><br><span class="line">make</span><br><span class="line">./opencv_example</span><br></pre></td></tr></table></figure>
<p>得到如下输出结果，证明安装成功：</p>
<p><img src="/2024/01/26/orbslam2-env/opencv4-success.png" alt="opencv-success" title="opencv安装成功"></p>
<p>同样地，使用g++ 命令进行编译：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">g++ -o opencv_example example.cpp `pkg-config --libs --cflags opencv4`</span><br></pre></td></tr></table></figure>
<p>执行生成文件，得到与上图相同的结果证明安装成功。</p>
<h3 id="3-2-opencv-3-4-11"><a href="#3-2-opencv-3-4-11" class="headerlink" title="3.2 opencv-3.4.11"></a>3.2 opencv-3.4.11</h3><p>编译命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cmake -D CMAKE_BUILD_TYPE=RELEASE \</span><br><span class="line">	-D CMAKE_INSTALL_PREFIX=/home/***/Softwares/opencv/opencv-3.4.11 \</span><br><span class="line">	-D INSTALL_C_EXAMPLES=OFF \</span><br><span class="line">	-D OPENCV_ENABLE_NONFREE=ON \</span><br><span class="line">	-D WITH_CUDA=ON \</span><br><span class="line">	-D WITH_CUDNN=ON \</span><br><span class="line">	-D OPENCV_DNN_CUDA=ON \</span><br><span class="line">	-D ENABLE_FAST_MATH=1 \</span><br><span class="line">	-D CUDA_FAST_MATH=1 \</span><br><span class="line">	-D CUDA_ARCH_BIN=8.6 \</span><br><span class="line">	-D WITH_CUBLAS=1 \</span><br><span class="line">	-D OPENCV_EXTRA_MODULES_PATH=/home/***/Softwares/opencv/opencv-3.4.11/opencv_contrib/modules \</span><br><span class="line">	-D BUILD_EXAMPLES=ON ..</span><br></pre></td></tr></table></figure>
<p>该版本的opencv 会自动在share/OpenCV 文件夹下生成OpenCVConfig.cmake、OpenCVModules.cmake 等文件，所以无需从build 文件夹中进行复制。</p>
<p>其他使用pkg-config 进行多版本控制方法与opencv-4.6.0 一致，在此不做赘述。</p>
<h3 id="3-3-opencv-卸载"><a href="#3-3-opencv-卸载" class="headerlink" title="3.3 opencv 卸载"></a>3.3 opencv 卸载</h3><p>首先，到编译目录build 下执行卸载命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo make uninstall</span><br></pre></td></tr></table></figure>
<p>然后，到opencv 的安装目录下，将bin、lib、share、include 等文件删除即可。若不再安装本版本，则将pkg-config 的相关配置清除掉。</p>
<h2 id="4-ORB-SLAM2-编译与运行"><a href="#4-ORB-SLAM2-编译与运行" class="headerlink" title="4 ORB-SLAM2 编译与运行"></a>4 ORB-SLAM2 编译与运行</h2><h3 id="4-1-前期准备"><a href="#4-1-前期准备" class="headerlink" title="4.1 前期准备"></a>4.1 前期准备</h3><p>在此注明一下本人各个软件包的<strong>最终版本</strong>如下所示：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">opencv: 3.4.11</span><br><span class="line">eigen3: 3.4.0</span><br><span class="line">pabgolin: 0.6</span><br></pre></td></tr></table></figure>
<p>明确完软件包版本后，首先到<a href="https://github.com/raulmur/ORB_SLAM2">ORB-SLAM2仓库</a>下载，进入下载的文件夹后对以下文件进行更改：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 对主目录及DBoW2下的CMakeLists.txt 文件中的opencv版本及搜寻路径进行相应修改</span></span><br><span class="line"><span class="built_in">set</span>(OpenCV_DIR /home/echo/Softwares/opencv/opencv-3.4.11/share/OpenCV)</span><br><span class="line">find_package(OpenCV 3.4.11 QUIET)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 对主目录下的CMakeLists.txt 文件中的eigen3 版本进行修改</span></span><br><span class="line">find_package(Eigen3 3.4.0 REQUIRED)</span><br></pre></td></tr></table></figure>
<h3 id="4-2-编译"><a href="#4-2-编译" class="headerlink" title="4.2 编译"></a>4.2 编译</h3><p>按照官方介绍进行编译：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +x build.sh</span><br><span class="line">./build.sh</span><br></pre></td></tr></table></figure>
<p>正常编译成功的话会在<em>lib</em> 文件夹内生成<strong>libORB_SLAM2.so</strong> 函数库，在<em>Examples</em> 文件夹内生成对应的可执行文件：<strong>mono_tum, mono_kitti, rgbd_tum, stereo_kitti, mono_euroc</strong> 和 <strong>stereo_euroc</strong>。但一般都会有各种报错，这里记录一下本人的报错及相应处理。</p>
<ul>
<li>首先是可能因为软件包版本问题报错：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 报错内容：</span></span><br><span class="line">Pangolin could not be found because dependency Eigen3 could not be found</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 解决方案：</span></span><br><span class="line">根据网上找的资料，可能是Pangolin和Eigen3的版本问题；按照网上教程，卸载了Pangolin和Eigen3，由于要手动选取版本，所以这里Eigen3选择了源文件安装，但后续又出现了其他问题，故又卸载新安装的Eigen3，重新使用apt安装了Eigen3，所以最终情况是：Pangolin降级为v0.6，Eigen3仍是原来的3.4.0。至此，该问题得到解决</span><br></pre></td></tr></table></figure>
<ul>
<li>再次编译可能会出现<strong>无法找到Eigen3 函数库</strong>的报错，此时根据报错内容，将Eigen3 的安装位置：/usr/include/目录下的eigen目录软链接到相应位置即可。</li>
<li>过程中没有其他报错内容，DBoW2、g2o等均编译成功，但最终编译ORB-SLAM2 各个运行程序时会提示：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 输出提示</span></span><br><span class="line">make: *** No targets specified and no makefile found.  Stop.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 在ORB-SLAM2的issues中找到解决方案：</span></span><br><span class="line"><span class="comment">## 删掉build文件，直接执行build.sh中的最后一步</span></span><br><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br></pre></td></tr></table></figure>
<ul>
<li>出现usleep() 未声明的报错：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">## 报错内容：</span><br><span class="line">error: ‘usleep’ was <span class="keyword">not</span> declared in <span class="keyword">this</span> scope</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 解决方案：在system.h文件中加入</span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li>LoopClosing.h中的报错：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">## 报错内容：</span><br><span class="line">/usr/include/c++/<span class="number">9</span>/bits/stl_map.h: In instantiation of ‘<span class="keyword">class</span> <span class="title class_">std</span>::map&lt;ORB_SLAM2::KeyFrame*, g2o::Sim3, std::less&lt;ORB_SLAM2::KeyFrame*&gt;, Eigen::aligned_allocator&lt;std::pair&lt;<span class="type">const</span> ORB_SLAM2::KeyFrame*, g2o::Sim3&gt; &gt; &gt;’:</span><br><span class="line">ORB_SLAM2/src/LoopClosing.cc:<span class="number">438</span>:<span class="number">21</span>: required from here</span><br><span class="line">/usr/include/c++/<span class="number">9</span>/bits/stl_map.h:<span class="number">122</span>:<span class="number">71</span>: error: <span class="type">static</span> assertion failed: std::map must have the same value_type as its allocator</span><br><span class="line"><span class="number">122</span> | <span class="built_in">static_assert</span>(is_same&lt;<span class="keyword">typename</span> _Alloc::value_type, value_type&gt;::value,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 解决方案：</span><br><span class="line">修改LoopClosing.h文件中的<span class="number">49</span>和<span class="number">50</span>行：</span><br><span class="line">修改前：</span><br><span class="line"><span class="keyword">typedef</span> map&lt;KeyFrame*,g2o::Sim3,std::less&lt;KeyFrame*&gt;,</span><br><span class="line">        Eigen::aligned_allocator&lt;std::pair&lt;<span class="type">const</span> KeyFrame*, g2o::Sim3&gt; &gt; &gt; KeyFrameAndPose;</span><br><span class="line">修改后：</span><br><span class="line"><span class="keyword">typedef</span> map&lt;KeyFrame*,g2o::Sim3,std::less&lt;KeyFrame*&gt;,</span><br><span class="line">       Eigen::aligned_allocator&lt;std::pair&lt;KeyFrame *<span class="type">const</span>, g2o::Sim3&gt; &gt; &gt; KeyFrameAndPose;</span><br></pre></td></tr></table></figure>
<p>至此，安装成功，生成了相应的可执行文件。</p>
<h3 id="4-3-运行测试"><a href="#4-3-运行测试" class="headerlink" title="4.3 运行测试"></a>4.3 运行测试</h3><p>本处使用了TUM RGB-D 数据集进行测试，测试命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt Examples/RGB-D/TUM1.yaml ~/datasets/TUM/rgbd_dataset_freiburg1_desk ~/datasets/TUM/rgbd_dataset_freiburg1_desk/associate.txt </span><br></pre></td></tr></table></figure>
<p>运行界面如下图所示：</p>
<p><img src="/2024/01/26/orbslam2-env/orbslam2-success.png" alt="orbslam2" title="ORB-SLAM2成功运行"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>ORB-SLAM</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>SLAM</tag>
        <tag>C++</tag>
        <tag>ORB-SLAM2</tag>
        <tag>OpenCV</tag>
      </tags>
  </entry>
  <entry>
    <title>zsh、Oh-My-Zsh及相关设置</title>
    <url>/2024/01/26/zsh-setup/</url>
    <content><![CDATA[<p>注：<strong>本文主要参考</strong><a href="https://www.kwchang0831.dev/dev-env/ubuntu/oh-my-zsh">该文章</a>。</p>
<h1 id="1-安装并设置zsh"><a href="#1-安装并设置zsh" class="headerlink" title="1 安装并设置zsh"></a>1 安装并设置zsh</h1><p>安装命令：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install zsh</span><br></pre></td></tr></table></figure>
<p>设置zsh为默认shell：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">chsh -s $(<span class="built_in">which</span> zsh)</span><br></pre></td></tr></table></figure>
<h1 id="2-安装Oh-My-Zsh"><a href="#2-安装Oh-My-Zsh" class="headerlink" title="2 安装Oh My Zsh"></a>2 安装Oh My Zsh</h1><p>安装命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh -c <span class="string">&quot;<span class="subst">$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)</span>&quot;</span></span><br></pre></td></tr></table></figure>
<h1 id="3-安装主题PowerLevel10k"><a href="#3-安装主题PowerLevel10k" class="headerlink" title="3 安装主题PowerLevel10k"></a>3 安装主题PowerLevel10k</h1><p>安装命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/romkatv/powerlevel10k.git <span class="variable">$ZSH_CUSTOM</span>/themes/powerlevel10k</span><br></pre></td></tr></table></figure>
<p>进入.zshrc设置主题：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ZSH_THEME=<span class="string">&quot;powerlevel10k/powerlevel10k&quot;</span></span><br></pre></td></tr></table></figure>
<h1 id="4-安装插件"><a href="#4-安装插件" class="headerlink" title="4 安装插件"></a>4 安装插件</h1><h2 id="4-1-日常插件"><a href="#4-1-日常插件" class="headerlink" title="4.1 日常插件"></a>4.1 日常插件</h2><p>zsh-autosuggestions:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/zsh-users/zsh-autosuggestions <span class="variable">$&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;</span>/plugins/zsh-autosuggestions</span><br></pre></td></tr></table></figure>
<p>zsh-syntax-highlighting:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/zsh-users/zsh-syntax-highlighting.git <span class="variable">$&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;</span>/plugins/zsh-syntax-highlighting</span><br></pre></td></tr></table></figure>
<p>设置要启动的插件（Plugins）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">plugins=(</span><br><span class="line">         git</span><br><span class="line">         zsh-autosuggestions </span><br><span class="line">         zsh-syntax-highlighting</span><br><span class="line">         extract</span><br><span class="line">         sudo</span><br><span class="line">         tmux</span><br><span class="line"></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>值得注意的是，除了zsh-autosuggestions 与zsh-syntax-highlighting 需要单独安装外，上述的其他插件均是内嵌的，可直接添加至配置文件中直接使用。</p>
<h2 id="4-2-Tmux设置"><a href="#4-2-Tmux设置" class="headerlink" title="4.2 Tmux设置"></a>4.2 Tmux设置</h2><p><strong>该部分主要参考</strong><a href="https://louiszhai.github.io/2017/09/30/tmux/#导读">文章</a>。</p>
<p>安装命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install tmux</span><br></pre></td></tr></table></figure>
<p>tmux 的个性化设置需在主目录编辑.tmux.conf 文件，本人的文件设置如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> -g prefix C-z</span><br><span class="line">unbind C-b</span><br><span class="line"><span class="built_in">bind</span> C-a send-prefix</span><br><span class="line"></span><br><span class="line"><span class="built_in">bind</span> r source-file ~/.tmux.conf \; display-message <span class="string">&quot;Config reloaded..&quot;</span></span><br><span class="line"></span><br><span class="line">unbind <span class="string">&#x27;&quot;&#x27;</span></span><br><span class="line"><span class="built_in">bind</span> - splitw -v -c <span class="string">&#x27;#&#123;pane_current_path&#125;&#x27;</span></span><br><span class="line">unbind %</span><br><span class="line"><span class="built_in">bind</span> | splitw -h -c <span class="string">&#x27;#&#123;pane_current_path&#125;&#x27;</span></span><br><span class="line"></span><br><span class="line">set-option -g mouse on</span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -g base-index 0</span><br><span class="line"><span class="built_in">set</span> -g pane-base-index 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># setw -g utf8 on</span></span><br><span class="line"><span class="built_in">set</span> -g status-interval 1</span><br><span class="line"><span class="built_in">set</span> -g status-justify left</span><br><span class="line">setw -g monitor-activity on</span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -g status-bg black</span><br><span class="line"><span class="built_in">set</span> -g status-fg yellow</span><br><span class="line"><span class="built_in">set</span> -g status-left <span class="string">&quot;#[bg=#FF661D] ❐ #S &quot;</span></span><br><span class="line"><span class="built_in">set</span> -g status-right <span class="string">&quot;%H:%M:%S %d-%b&quot;</span></span><br><span class="line"><span class="built_in">set</span> -g status-left-length 300</span><br><span class="line"><span class="built_in">set</span> -g status-right-length 500</span><br><span class="line"><span class="built_in">set</span> -wg window-status-current-format <span class="string">&quot; #I:#W#F &quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -g message-style <span class="string">&quot;bg=#202529, fg=#91A8BA&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -g default-terminal <span class="string">&quot;screen-256color&quot;</span></span><br></pre></td></tr></table></figure>
<p>其他使用说明可参考上述链接。在oh my zsh 插件中添加了 tmux 后，可以使用如下快捷键：</p>
<p><img src="/2024/01/26/zsh-setup/hotkey.png" alt="hotkey" title="快捷键设置"></p>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>zsh</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows远程桌面控制ubuntu</title>
    <url>/2024/01/26/win-remote-ubuntu/</url>
    <content><![CDATA[<h2 id="1-Ubuntu设置与软件安装"><a href="#1-Ubuntu设置与软件安装" class="headerlink" title="1 Ubuntu设置与软件安装"></a>1 Ubuntu设置与软件安装</h2><p>首先打开Ubuntu设置，将Sharing-Remote Desktop打开，如下图所示：</p>
<span id="more"></span>
<p><img src="/2024/01/26/win-remote-ubuntu/ubuntu-setting.png" alt="ubuntu-setting" title="Ubuntu远程桌面设置"></p>
<p>然后安装xrdp：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install xrdp</span><br></pre></td></tr></table></figure>
<p>接下来的步骤网上有很多不同的做法，其中<a href="https://zhuanlan.zhihu.com/p/145614559">这篇文章</a>提到需要修改startwm.sh文件:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo vi /etc/xrdp/startwm.sh</span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/win-remote-ubuntu/startwm.png" alt="startwm.sh" title="修改startwm.sh文件"></p>
<p>将最后两行注释掉，即：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 注释掉这两行</span></span><br><span class="line"><span class="comment"># test -x /etc/X11/Xsession &amp;&amp; exec /etc/X11/Xsession</span></span><br><span class="line"><span class="comment"># exec /bin/sh /etc/X11/Xsession</span></span><br></pre></td></tr></table></figure>
<p>然后即可通过win10自带的远程桌面（或在Microsoft Store安装的远程桌面）进行连接：</p>
<p><img src="/2024/01/26/win-remote-ubuntu/remote-access.png" alt="remote" title="win10远程桌面"></p>
<p><img src="/2024/01/26/win-remote-ubuntu/image-20240129103319493.png" alt="远程桌面" title="远程桌面"></p>
<p>到目前为止，本人可以在Windows中正常远程控制Ubuntu，且可实现Ubuntu显示和远程控制<strong>同时在线</strong>，即通过远程控制操作Ubuntu，同时另一个屏幕可正常显示Ubuntu界面，且实时显示远程控制的操作。</p>
<h2 id="2-黑屏问题"><a href="#2-黑屏问题" class="headerlink" title="2 黑屏问题"></a>2 黑屏问题</h2><p>但是，在这之后我重新安装了NVIDIA显卡驱动、CUDA、cuDNN，然后就发现远程控制无法进入Ubuntu系统了，点击“连接”后出现一段时间的黑屏后会自动退出。在网上查询了很多案例，有各种五花八门的解决方案，如换一个桌面程序、使用dconf-editor更改配置文件、远程控制与Ubuntu本地无法同时登陆（必须有一方log out）等等，以下是个人尝试的结果：</p>
<ul>
<li>换桌面程序：由于本人安装的是桌面版Ubuntu，故系统自带桌面程序gnome，之前配置远程控制时就受到网上各种五花八门意见的影响，使用了xfce4桌面程序，后来发现完全没必要，而且就在不久前还可以正常远程连接，说明应该不是桌面程序的原因，因此就没有尝试更换桌面程序；</li>
<li>使用dconf-editor更改配置文件：在<a href="https://zhuanlan.zhihu.com/p/345738274">文章</a>中提到，进入dconf-editor后，依次进入“org-&gt;gnome-&gt;desktop-&gt;remote-access”，将 requre-encryption 设为 False，这也是本人在上个系统（Ubuntu20.04）中的操作，但是在本系统（Ubuntu22.04）中没有发现有“remote-access”，故该方法也没有成功；</li>
<li>本地端与远程端无法共存：这个比较简单，本人试着将Ubuntu本地端账户log out，或者使用不同账户进行远程控制，发现仍然失败；但<strong>需要注意的是</strong>，在本人解决远程黑屏问题之后，发现该问题确实是存在的，若本地端或远程端没有log out，那么另一端就无法正常登陆显示界面，这边记录一下；</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 修改startwm.sh文件，添加下面几行，仍旧没有解决问题</span></span><br><span class="line"><span class="built_in">unset</span> DBUS_SESSION_BUS_ADDRESS</span><br><span class="line"><span class="built_in">unset</span> XDG_RUNTIME_DIR</span><br><span class="line">. <span class="variable">$HOME</span>/.profile</span><br></pre></td></tr></table></figure>
<p>最后，不知道怎么想起来，尝试把startwm.sh文件中注释掉的两行取消注释，峰回路转、柳暗花明，竟然成了！！！于是，本人最终的startwm.sh文件内容如下所示：</p>
<p><img src="/2024/01/26/win-remote-ubuntu/startwm-1.png" alt="startwm" title="startwm.sh文件最终版"></p>
<h2 id="3-后记"><a href="#3-后记" class="headerlink" title="3 后记"></a>3 后记</h2><p>虽然不知道原因是什么，只知道目前这样子是可行的，但同时也发现无法像安装显卡驱动前那样本地端和远程端同时登陆了，必须要有一端log out，另一端才可正常工作，现做以下记录：</p>
<ul>
<li>使用远程连接桌面后，不能简单地关掉远程桌面：若只是简单地关掉远程桌面后，在本地端可以正常进入账号选择、密码输入界面，输完密码后会显示黑屏；此时，只有重新进入远程桌面，选择log out，本地端方可正常登录。</li>
<li>对于本地端Todesk等远程控制软件：经过本人测试，Todesk 软件若想正常工作需要本地端正常登录，即要求远程端log out，然后本地端log in；那么也就意味着本地端电脑需要连接显示器。</li>
</ul>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>remote access</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Towards semantic SLAM using a monocular camera</title>
    <url>/2024/01/30/civera2011/</url>
    <content><![CDATA[<p>Civera, J., D. Galvez-Lopez, L. Riazuelo, J. D. Tardos, and J. M. M. Montiel. “Towards Semantic SLAM Using a Monocular Camera.” In <em>2011 IEEE/RSJ International Conference on Intelligent Robots and Systems</em>, 1277–84. San Francisco, CA: IEEE, 2011. <a href="https://doi.org/10.1109/IROS.2011.6094648">https://doi.org/10.1109/IROS.2011.6094648</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文提出了一种语义SLAM 算法，本算法将传统<strong>无意义的特征点</strong>和<strong>物体</strong>融合进到地图中。</p>
<p>本算法融合了三个不同领域的SOTA：</p>
<ol>
<li>一个<strong>EKF 单目SLAM</strong> 提供相机位姿的在线实时估计，以及包含点特征的稀疏地图；</li>
<li>利用 <strong>Structure from Motion (SfM)</strong> 从稀疏图片中计算一个物体模型数据库；</li>
<li><strong>视觉识别</strong>来检测图片流中物体是否存在。</li>
</ol>
<span id="more"></span>
<h1 id="3-Notation-and-General-Overview"><a href="#3-Notation-and-General-Overview" class="headerlink" title="3 Notation and General Overview"></a>3 Notation and General Overview</h1><p>本算法分为两个支流：</p>
<ol>
<li>单目SLAM；</li>
<li>物体识别（通常要慢于SLAM支流）。</li>
</ol>
<p>本算法的整体视图如Fig. 1所示：</p>
<p><img src="/2024/01/30/civera2011/fig1.png" alt="fig1" title="figure 1"></p>
<p>单目SLAM 利用图片从<strong>k-m-1 到 k-m</strong>更新几何状态向量 x，而视觉识别结果会在<strong>步骤k</strong> 处得到，但是物体插入应当是相对于输入图片 $I_{k-m}$ 而言的。</p>
<p>作者提前对每一个想要识别的物体进行建模，这些模型包含<strong>外观和几何信息</strong>：</p>
<ul>
<li>外观信息是由<strong>SURF 描述子</strong>组成的；</li>
<li>几何信息是这些<strong>SURF 特征点的3D 位置</strong>信息。</li>
</ul>
<p>物体识别支流将数据库中的物体插入到SLAM 地图的步骤如下：</p>
<ol>
<li>算法搜寻图片 $I_{k-m}$ 中SURF 特征点与数据库中每一个物体之间的关联；</li>
<li>利用RANSAC 来计算每个物体的<strong>一致几何模型</strong>，来最大化关联数量；</li>
<li>如果存在足够的一致关联，那么这个特征点就被认为属于该物体，并插入SLAM 地图中；</li>
<li>被插入地图中的特征点会被持续跟踪与位置优化。</li>
</ol>
<h1 id="4-Object-Model"><a href="#4-Object-Model" class="headerlink" title="4 Object Model"></a>4 Object Model</h1><p>如上文所述，物体模型包含由SURF 特征描述子构成的<strong>外观信息</strong>和特征点位置<strong>几何信息</strong>，其构建示例如Fig. 2所示：</p>
<p><img src="/2024/01/30/civera2011/fig2.png" alt="fig2" title="figure 2"></p>
<p>图中的黄色圆圈代表用来进行识别的SURF 特征。</p>
<p>物体的构建是通过不同的faces 完成的，而构建模型的<strong>每一幅图片</strong>是一个face 的<strong>基础</strong>。作者用一个tuple F 来表示face，该tuple中包含了SURF 特征点的位置坐标和描述子，以及形成该face 的图片的位置信息和朝向信息。</p>
<h1 id="5-Object-Recognition"><a href="#5-Object-Recognition" class="headerlink" title="5 Object Recognition"></a>5 Object Recognition</h1><p>物体识别是通过计算图片 $I_{k-m}$ 与物体数据库中每个物体的关联来实现的：</p>
<ol>
<li>对于每个物体而言，计算图片 $I_{k-m}$ 与属于该物体的faces F之间的关联；</li>
<li>然后使用RANSAC 进行外点剔除；</li>
<li>最终至少包含5对关系才能确认为图片 $I_{k-m}$ 与物体 $I_F$ 之间建立了联系；</li>
<li>利用PnP 来估计相对位姿转换关系。</li>
</ol>
<h1 id="6-Monocular-SLAM"><a href="#6-Monocular-SLAM" class="headerlink" title="6 Monocular SLAM"></a>6 Monocular SLAM</h1><h2 id="6-1-Standard-Mode-EKF"><a href="#6-1-Standard-Mode-EKF" class="headerlink" title="6.1 Standard Mode EKF"></a>6.1 Standard Mode EKF</h2><p>估计参数建模为多维高斯变量x，包含相机的运动参数以及n 个特征点：</p>
<p><img src="/2024/01/30/civera2011/f1.png" alt="f1" title="formula 1"></p>
<p>其中，运动参数 $\mathbf{x}_{C_k}$ 包含相机的位置参数 $\mathbf{t}_{C_{k-m}}$ 和方向参数 $\mathbf{q}_{C_{k-m}}$ ，以及线速度和角速度。</p>
<h2 id="6-2-State-Augmentation-with-Past-Camera-Pose"><a href="#6-2-State-Augmentation-with-Past-Camera-Pose" class="headerlink" title="6.2 State Augmentation with Past Camera Pose"></a>6.2 State Augmentation with Past Camera Pose</h2><p>当<strong>物体识别支流</strong>在步骤k-m 开始时，SLAM的状态向量需要使用当前步骤的<strong>相机位姿进行增强</strong>：将当前步骤的位姿参数复制到状态向量中去，并传播相应的协方差矩阵。假设 k 步完成了物体识别与插入操作，那么k-1 步骤的状态向量表示如下：</p>
<p><img src="/2024/01/30/civera2011/f2.png" alt="f2" title="formula 2"></p>
<p>如果物体识别成功，则过去的相机位姿（本例中为步骤 k-m）被用来进行物体的<strong>延迟初始化</strong>。在此完成之后，用来增强的相机位姿不再需要，即可从状态向量中进行剔除。</p>
<h2 id="6-3-Object-Insertion"><a href="#6-3-Object-Insertion" class="headerlink" title="6.3 Object Insertion"></a>6.3 Object Insertion</h2><p><strong>物体识别支流的输出</strong>是一组物体参考坐标系中的特征点 $\mathbf{y}_F^O$ ，并由此计算face F 与k-m 时刻相机之间的位姿转换关系  $\mathbf{t}^{C_{k-m}}_F, \mathbf{q}^{C_{k-m}}_F$  ；基于此并利用6.2节中的k-m 步骤的增强相机位姿，实现物体特征点到SLAM 地图的插入操作：</p>
<p><img src="/2024/01/30/civera2011/f5.png" alt="f5" title="formula 5"></p>
<p>因为物体模型中点的3D 位置信息是已知的，因此该模型的所有点可以根据 $\mathbf{y}_F^W$ 结合欧式坐标添加进SLAM 地图中。</p>
<p>在完成插入操作后，物体点被跟踪并利用单目SLAM 算法进行位姿优化。</p>
<h1 id="8-Conclusions-and-Future-Works"><a href="#8-Conclusions-and-Future-Works" class="headerlink" title="8 Conclusions and Future Works"></a>8 Conclusions and Future Works</h1><p>本文提出的算法只使用相机作为传感器，是第一个将普通3D 物体<strong>实时加入几何SLAM 地图</strong>中的算法。</p>
<p>作者提到本算法将<strong>相机运动和3D 场景理解</strong>结合了起来，提供了部分标注的地图和相机位姿，方便后期机器人的任务开发（如抓住某个物体等）；此外，基于提前构建的物体3D 模型库，可以实现在观察到物体某一面之后即可将整个3D 物体添加进地图中去，完成对于未观测信息的补充。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Map-Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>C++</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 VSO_Visual Semantic Odometry</title>
    <url>/2024/01/30/vso/</url>
    <content><![CDATA[<p>Lianos, Konstantinos-Nektarios, Johannes L. Schönberger, Marc Pollefeys, and Torsten Sattler. “VSO: Visual Semantic Odometry.” In <em>Computer Vision – ECCV 2018</em>, edited by Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, 11208:246–63. Lecture Notes in Computer Science. Cham: Springer International Publishing, 2018. <a href="https://doi.org/10.1007/978-3-030-01225-0_15">https://doi.org/10.1007/978-3-030-01225-0_15</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者认为，VO 的核心问题在于<strong>数据关联DA</strong>。</p>
<p>通常来讲，有两种方法来减少VO 中的漂移：</p>
<ol>
<li>方法一：利用连续图片之间的<strong>短期关联</strong>来进行偏移矫正；</li>
<li>方法二：利用回环检测实现<strong>长期关联</strong>。</li>
</ol>
<p>传统的几何特征（点、线、面）在光照、视角变化下鲁棒性较差，不能在长距离上保持持续跟踪，而语义特征作为更高级的特征，语义特征在光照、视角、尺寸发生较大变化时仍然可以保持不变，使得<strong>中期关联</strong>成为可能，如Fig. 1所示。</p>
<span id="more"></span>
<p><img src="/2024/01/30/vso/fig1.png" alt="fig1" title="figure 1"></p>
<p>本文的贡献如下：</p>
<ul>
<li>构建一个新的<strong>损失函数</strong>来最小化<strong>语义特征的重投影误差</strong>，并使用<strong>EM 算法</strong>进行最小化，且可以适用于任何语义分割算法；</li>
<li>作者证明了将语义损失考虑进VO 中可以大幅减小平移漂移，且本算法可以<strong>直接集成</strong>到现有的VO 算法中，无论是直接法还是间接法；</li>
<li>作者进行了实验验证，表明了该算法在特定条件下的效果，并讨论了当前的限制。</li>
</ul>
<h1 id="3-Visual-Semantic-Odometry"><a href="#3-Visual-Semantic-Odometry" class="headerlink" title="3 Visual Semantic Odometry"></a>3 Visual Semantic Odometry</h1><p>与文章(Bowman 等, 2017)采用物体的离散DA不同，本文考虑与物体边界的<strong>连续距离</strong>来定义损失函数。</p>
<h2 id="3-1-Visual-Semantic-Odometry-Framework"><a href="#3-1-Visual-Semantic-Odometry-Framework" class="headerlink" title="3.1 Visual Semantic Odometry Framework"></a>3.1 Visual Semantic Odometry Framework</h2><p>传统里程计的目标函数是：</p>
<p><img src="/2024/01/30/vso/f1.png" alt="f1" title="formula 1"></p>
<p>作者定义的语义损失函数是：</p>
<p><img src="/2024/01/30/vso/f2.png" alt="f2" title="formula 2"></p>
<p>将传统的损失函数和语义损失函数结合起来便形成了本文算法的目标函数：</p>
<p><img src="/2024/01/30/vso/f3.png" alt="f3" title="formula 3"></p>
<h2 id="3-2-Semantic-Cost-Function"><a href="#3-2-Semantic-Cost-Function" class="headerlink" title="3.2 Semantic Cost Function"></a>3.2 Semantic Cost Function</h2><p>联系语义观测 $S_k$ ，相机位姿 $T_k$ 和3D 点 $P_i$ （包含标签 $Z_i$ 和位置 $X_i$ ）来定义<strong>观测似然模型</strong>：$p(S_k|T_k, X_i, Z_i=c)$ ，该似然函数应该随着 3D 点 $P_i$ 在图片中的投影位置 $\pi(T_k, X_i)$ 与标注为类别 c 的区域的最近<strong>距离成反比</strong>；基于此，作者利用了distance transform，如Fig. 2所示，作者首先由语义分割图片（a）为每一个分类 c 生成相应的二分值灰度图片（b），然后基于该灰度图片定义一个distance transform：</p>
<p><img src="/2024/01/30/vso/f4.png" alt="DT" title="DT"></p>
<p><img src="/2024/01/30/vso/fig2.png" alt="fig2" title="figure 2"></p>
<p>利用 $DT^{(c)}_k(p)$ 定义观测似然函数：</p>
<p><img src="/2024/01/30/vso/f4-1.png" alt="f4" title="formula 4"></p>
<p>其中，$\pi$ 表示投影函数；$\sigma$ 表示语义图片分类的<strong>不确定性</strong>。为了简明起见，作者省略了归一化因子。根据上式，可通过调整相机位姿与点的位置，使得投影点移动到正确标签的区域内，以此最大化该似然函数。</p>
<p>根据式（4）作者定义了语义损失项：</p>
<p><img src="/2024/01/30/vso/f5.png" alt="f5" title="formula 5"></p>
<p>其中，$w_i^{(c)}$ 表示点 $P_i$ 属于种类 c 的概率。从上式可以看出，$e_{sem}(k,i)$ 表示<strong>2D 距离的加权平均</strong>，每个投影点 $\pi(T_k, X_i)$ 到最近属于种类 c 的距离 $DT^{(c)}_k(\pi(T_k, X_i))$ 被相应的类别概率 $w_i^{(c)}$ 加权。</p>
<p>值得注意的是，对于点 $P_i$ 的标签概率向量 $w_i$ 是通过所有对到该点的观测计算得到的：</p>
<p><img src="/2024/01/30/vso/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$\mathcal{T}_i$ 表示观测到点 $P_i$ 的相机位姿集合；参数 $\alpha$ 为归一化参数。利用该准则可以通过累积语义观测来实现对标签概率向量 $w_i$ 的<strong>增量式细调</strong>。</p>
<h2 id="3-3-Optimization"><a href="#3-3-Optimization" class="headerlink" title="3.3 Optimization"></a>3.3 Optimization</h2><p>式（5）、（6）表明优化参数涉及3D 点位置坐标，相机位姿以及种类关联参数，作者使用EM 算法进行求解，步骤如下：</p>
<ul>
<li>E step：基于式（6）计算每个点的权重向量 $w_i$ ，此步骤点坐标和相机位姿保持固定；</li>
<li>M step：优化点坐标和相机位姿，此步骤权重参数保持固定。</li>
</ul>
<p>该优化框架中，将点 $P_i$ 的标签 $Z_i$ 视为<strong>隐变量</strong>。</p>
<p>作者提到，使用本文提出的语义约束，可以实现不变观测，但是<strong>缺乏结构信息</strong>；仅使用语义项对地图点和相机位姿进行优化会导致<strong>欠约束</strong>，因为等式（4）表示的似然函数在物体边界内部服从<strong>均匀分布</strong>，为解决该问题，对于 $E_{sem}$ 的优化操作如下：</p>
<ol>
<li>如式（3）所示，与传统的VO 进行<strong>联合优化</strong>；</li>
<li>利用<strong>多重点和语义约束</strong>来优化相机位姿；</li>
<li>仅提供语义约束的点得到固定，然后只优化与它们相关的相机位姿来减小漂移，该方法不仅可以限制优化参数的数量，而且会在点之间引入<strong>结构相关</strong>，从而约束位姿求解（如Fig. 3所示）；</li>
<li><strong>频繁</strong>使用语义优化可以减少一个点错误关联的概率，因为基于distance transform 梯度的优化可以保证该点足够靠近正确标签的区域，并不断拉向该区域。</li>
</ol>
<p><img src="/2024/01/30/vso/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="3-4-Obtaining-Semantic-Constraints-amp-System-Integration"><a href="#3-4-Obtaining-Semantic-Constraints-amp-System-Integration" class="headerlink" title="3.4 Obtaining Semantic Constraints &amp; System Integration"></a>3.4 Obtaining Semantic Constraints &amp; System Integration</h2><p>类似于传统VO 使用的关键帧active window (AW)，作者也定义了一个关键帧active semantic window (ASW)，一个关键帧从AW 中剔除后会被添加进ASW，本算法在尽可能覆盖更多的轨迹的同时，也会限制ASW 中关键帧的数量。且，ASW 中关键帧的位姿<strong>不再进行优化</strong>，因为这些关键帧通常与目前的帧缺少光度/几何约束。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Pose-Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>C++</tag>
        <tag>Pose Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 X-View_Graph-Based Semantic Multi-View Localization</title>
    <url>/2024/01/30/x-view/</url>
    <content><![CDATA[<p>Gawel, Abel, Carlo Del Don, Roland Siegwart, Juan Nieto, and Cesar Cadena. “X-View: Graph-Based Semantic Multi-View Localization.” <em>IEEE Robotics and Automation Letters (RA-L)</em> 3, no. 3 (2018): 1687–94.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文做出的贡献：</p>
<ol>
<li>提出一个新颖的<strong>语义拓扑图表示</strong>方法；</li>
<li>引进了一个<strong>基于随机游走的图描述子</strong>，可以有效地使用既定的匹配方法进行高效匹配；</li>
<li>用于全局定位的语义分割完整pipeline；</li>
<li>开源<em>X-View</em> 算法；</li>
<li>公开数据集的测试结果。</li>
</ol>
<span id="more"></span>
<p><img src="/2024/01/30/x-view/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-X-View"><a href="#3-X-View" class="headerlink" title="3 X-View"></a>3 X-View</h1><p>X-View 利用从输入语义数据提取得到的图，并使用图描述子进行图匹配，本文提出的全局定位算法架构如Fig. 2所示，本系统被设计为可以用于任何给定的语义信息作为输入的里程估计系统，为了简化表示，Fig. 2中只列举了语义分割图片作为输入。</p>
<p><img src="/2024/01/30/x-view/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-System-input"><a href="#3-1-System-input" class="headerlink" title="3.1 System input"></a>3.1 System input</h2><p>将语义分割或实例分割图片作为系统输入，且假定外部里程系统的估计，以及一个参考语义图 $G_{db}$ 。</p>
<h2 id="3-2-Graph-extraction-and-assembly"><a href="#3-2-Graph-extraction-and-assembly" class="headerlink" title="3.2 Graph extraction and assembly"></a>3.2 Graph extraction and assembly</h2><p>将一系列语义分割图片 $I_q$ 转化为一个质询图 $G_q$ ，从连接区域中提取斑点 blobs，例如每幅图中具有相同语义标签 $l_j$ 的区域。为了应对语义分割的噪声（如标签中的洞、断连的边、边缘错误的标签等），作者使用 dilating 和 eroding 对每个班点的边界进行柔化，且设定最小像素数阈值为4以排除过小及次要物体的影响。斑点的中心位置 $p_j$ 被提取出来，与标签一起存储为顶点 $v_j = \{l_j, p_j\}$ 。</p>
<p>顶点之间的无向边可以是图像空间或3D 空间的，当考虑图像空间中的边时，认为图片在时间序列中根据几幅连续的输入图片流来生成图（利用连续图片获取3D距离信息），在3D 空间中不需要考虑这方面。</p>
<p>利用深度通道数据或者深度估计来组成3D空间结构，使用图片斑点的3D位置来计算欧氏距离，图提取与组合的过程如Fig. 3所示，当对多幅连续图片的图进行组合时，将距离较近且拥有相同语义标签的重复顶点融合为一个顶点，该顶点位置选择为初次观测到的位置。</p>
<p><img src="/2024/01/30/x-view/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="3-3-Descriptors"><a href="#3-3-Descriptors" class="headerlink" title="3.3 Descriptors"></a>3.3 Descriptors</h2><p>子图匹配是一个 NP-complete 难题，且为了实现机器人的实时定位问题，作者提出了为图中每个节点建立随机游走描述子并进行匹配，这样做的优势是根据给定的静态或者增长的参考图，匹配时间分别是常数或者线性变化的。每个顶点的描述子是一个 $n \times m$ 的矩阵，包含 <em>n</em> 个深度为 <em>m</em> 的随机游走，每个随机游走开始于源顶点 $v_j$ ，并存储访问过顶点的分类标签，随机游走描述子提取过程如Fig. 4所示。</p>
<p><img src="/2024/01/30/x-view/fig4.png" alt="fig4" title="figure 4"></p>
<h2 id="3-4-Descriptor-Matching"><a href="#3-4-Descriptor-Matching" class="headerlink" title="3.4 Descriptor Matching"></a>3.4 Descriptor Matching</h2><p>在质询图 $G_q$ 与 参考图 $G_{db}$ 完成之后，通过计算相应的图描述子之间的similarity score来建立两图顶点之间的联系：</p>
<ul>
<li>相似性是通过对质询图与参考图中顶点的语义描述子的每一行进行匹配计算获取的；</li>
<li>两个描述子拥有的相同随机游走数量决定了similarity score（分布在0与1之间）；</li>
<li>然后选取得分最高的前 k 个匹配顶点对来计算质询图在参考图中的定位信息。</li>
</ul>
<h2 id="3-5-Localization-Back-End"><a href="#3-5-Localization-Back-End" class="headerlink" title="3.5 Localization Back-End"></a>3.5 Localization Back-End</h2><p>一共有三种类型的约束：</p>
<ol>
<li>来自语义描述子匹配的约束；</li>
<li>联系定位图中连续位姿的机器人估计位姿约束；</li>
<li>每一个robot-vertex观测的转换信息编码得到的robot-vertex约束。</li>
</ol>
<p>作者根据以上三个约束参数计算最大后验估计 (MAP)来获取定位信息。</p>
<h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h1><p>在两个不同的合成户外数据集进行评估，包括<strong>前视——后视</strong>视角变化、<strong>前视——空视</strong>视角变化，以及一个真实世界户外数据集，包含<strong>前视——后视</strong>视角变化。</p>
<h2 id="4-1-Datasets"><a href="#4-1-Datasets" class="headerlink" title="4.1 Datasets"></a>4.1 Datasets</h2><p>SYNTHIA 数据集是在城市环境模拟生成的，采集汽车有8个摄像头，四个方向各有两个，相邻图片间的距离在0~1m。作者使用photo-realistic Arisim 生成了空对地观测的数据集，包含空中向地面观测与地面前向观测两种数据，两种数据之间只有在 z 轴存在偏差，其余均相同，相邻图片间的距离总是1m。真实户外场景的数据集是通过Google StreetView 获取的，类似于SYNTHIA 数据集，只使用前视与后视摄像头，相邻图片间的距离接近10m。</p>
<p><img src="/2024/01/30/x-view/fig5.png" alt="fig5" title="figure 5"></p>
<h2 id="4-3-Localization-performance"><a href="#4-3-Localization-performance" class="headerlink" title="4.3 Localization performance"></a>4.3 Localization performance</h2><p>设置两组实验：</p>
<ol>
<li>在SYNTHIA 数据集上测试不同的参数设置，如随机游走参数、质询图片数量、动态分类物体、图边缘构建技术等；</li>
<li>在SYNTHIA、Airsim 以及 StreetView 上进行对比分析实验。</li>
</ol>
<p>与基于外观匹配的算法进行对比，使用了BoW、NetVLAD 算法。</p>
<h2 id="4-4-Results"><a href="#4-4-Results" class="headerlink" title="4.4 Results"></a>4.4 Results</h2><p>Fig. 6是对SYNTHIA 数据集进行的参数对比试验结果。6(a) 中 n 代表随机游走的个数， m 代表随机游走的深度。</p>
<p><img src="/2024/01/30/x-view/fig6.png" alt="fig6" title="figure 6"></p>
<p>PR评估曲线以及不同定位误差下的成功率结果如Fig. 7所示。</p>
<p><img src="/2024/01/30/x-view/fig7.png" alt="fig7" title="figure 7"></p>
<p>系统组件的时间消耗如Table 1所示。</p>
<p><img src="/2024/01/30/x-view/t1.png" alt="t1" title="table 1"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Topology</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>C++</tag>
        <tag>Topology</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Detect-SLAM_Making Object Detection and SLAM Mutually Beneficial</title>
    <url>/2024/01/30/detect-slam/</url>
    <content><![CDATA[<p>Zhong, Fangwei, Sheng Wang, Ziqi Zhang, China Chen, and Yizhou Wang. “Detect-SLAM: Making Object Detection and SLAM Mutually Beneficial.” In <em>2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, 1001–10, 2018. <a href="https://doi.org/10.1109/WACV.2018.00115">https://doi.org/10.1109/WACV.2018.00115</a>.</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文提出的Detect-SLAM 较其他SOTA 方法的优势：</p>
<ol>
<li>通过利用物体检测器完成对移动物体上的<strong>不可靠特征点</strong>进行剔除，极大地提高了SLAM 算法在动态环境中的<strong>准确性与鲁棒性</strong>；</li>
<li>在线构建一个<strong>实例级语义地图</strong>；</li>
<li>通过利用物体语义地图来<strong>提高物体检测器性能</strong>，使得其在挑战性环境中可以更有效地识别出物体。</li>
</ol>
<span id="more"></span>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>Detect-SLAM 是第一个将SLAM 和 基于DNN 的检测器结合起来同时完成三种视觉任务的工作：提高SLAM 在动态环境中的鲁棒性，构建语义地图，以及增强物体检测性能。</p>
<p><img src="/2024/01/30/detect-slam/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-Detect-SLAM"><a href="#3-Detect-SLAM" class="headerlink" title="3 Detect-SLAM"></a>3 Detect-SLAM</h1><p>本架构是基于ORB-SLAM2 算法的，在其基础上包含三个新步骤：</p>
<ul>
<li>Moving objects removal：将动态物体上的特征点进行<strong>剔除</strong>；</li>
<li>Object Mapping：对关键帧中的静态物体进行<strong>重建</strong>，物体语义地图包含带有物体ID 的稠密点云；</li>
<li>SLAM-enhanced Detector：利用物体语义地图作为先验知识来<strong>增强</strong>探测器在挑战环境中的性能。</li>
</ul>
<h2 id="3-1-Moving-Objects-Removal"><a href="#3-1-Moving-Objects-Removal" class="headerlink" title="3.1 Moving Objects Removal"></a>3.1 Moving Objects Removal</h2><p>动态物体剔除如Fig. 3所示，值得注意的是，此处的动态物体指的是属于可移动种类中的物体，并不能保证其一定是移动的，即<strong>潜在动态物体</strong>。然而，利用DNN 进行动态物体检测速度较慢，SSD 只能实现3 FPS的速度。为解决该问题，作者采用两个策略进行应对：</p>
<ol>
<li>只在<strong>关键帧</strong>中进行动态物体检测，然后在local map 中更新特征点的<strong>移动概率</strong>来加速tracking 支流；</li>
<li>在tracking 支流中利用<strong>特征匹配</strong>和<strong>匹配点扩张</strong>来传递移动概率，从而在相机位姿估计前有效移除动态物体上的特征点。</li>
</ol>
<p>作者将物体的移动概率分为4个层次，如Fig. 2所示，使用<strong>高置信度点</strong>在<strong>匹配点扩张</strong>中将移动概率传递给周围<strong>未匹配</strong>的特征点，在每个点都得到相应的移动概率后，去除动态特征点并使用RANSAC 滤除其他外点，然后进行位姿估计。</p>
<p><img src="/2024/01/30/detect-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h3 id="3-1-1-Updating-Moving-Probability"><a href="#3-1-1-Updating-Moving-Probability" class="headerlink" title="3.1.1 Updating Moving Probability"></a>3.1.1 Updating Moving Probability</h3><p><img src="/2024/01/30/detect-slam/fig3.png" alt="fig3" title="figure 3"></p>
<p>关键帧的物体检测完成后会被插入到local map 中并更新地图中特征点的移动概率：</p>
<p><img src="/2024/01/30/detect-slam/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$P_{t-1}(X^i)$ 表示3D 点 $X^i$ 在上个关键帧更新后的移动概率，若点 $X^i$ 是新添加的点，则令 $P_{t-1}(X^i)=0.5$ ；与点 $X^i$ 匹配的关键帧中的点 $x_i$ 若位于动态物体的bbox 内，则令 $S_t(x^i) = 1$ ，其他情况下令其为0。</p>
<h3 id="3-1-2-Moving-Probability-Propagation"><a href="#3-1-2-Moving-Probability-Propagation" class="headerlink" title="3.1.2 Moving Probability Propagation"></a>3.1.2 Moving Probability Propagation</h3><p>移动概率传递在帧间通过两种方式确定：<strong>特征匹配</strong>和<strong>匹配点扩张</strong>，如Fig. 4所示。</p>
<p><img src="/2024/01/30/detect-slam/fig4.png" alt="fig4" title="figure 4"></p>
<p>对于<strong>特征匹配</strong>的情况，若一个特征点同时与上一帧特征点和地图中的3D 点匹配上时，应该<strong>以地图中3D 点的移动概率为准</strong>。将未匹配的特征点移动概率初始化为0.5。特征匹配的情况总结为下式：</p>
<p><img src="/2024/01/30/detect-slam/f2.png" alt="f2" title="formula 2"></p>
<p>对于匹配点扩张的情况，是利用<strong>高置信度的特征点</strong>（包括高静态可能和高动态可能特征点）对周围未匹配特征点的移动概率进行确认，确认过程如下所示：</p>
<p><img src="/2024/01/30/detect-slam/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$\chi_t$ 为高置信度特征点；$P_{init}$ 为初始化概率值，为0.5；$\lambda(d)$ 为距离因子，若d 小于半径阈值，则令 $\lambda(d) = Ce^{-d/r}$ 。</p>
<h2 id="3-2-Mapping-Objects"><a href="#3-2-Mapping-Objects" class="headerlink" title="3.2 Mapping Objects"></a>3.2 Mapping Objects</h2><h3 id="3-2-1-Predicting-Region-ID"><a href="#3-2-1-Predicting-Region-ID" class="headerlink" title="3.2.1 Predicting Region ID"></a>3.2.1 Predicting Region ID</h3><p>物体ID 的预测是基于一个<strong>几何假设</strong>：地图中物体投影到关键帧中的区域与帧中物体检测区域<strong>存在重叠</strong>，且区域属于同一个物体。利用IOU 来表示两个区域的重叠程度：</p>
<p><img src="/2024/01/30/detect-slam/f4.png" alt="f4" title="formula 4"></p>
<p>若IOU 大于0.5，则进一步估计<strong>深度似然</strong>：</p>
<p><img src="/2024/01/30/detect-slam/f5.png" alt="f5" title="formula 5"></p>
<p>其中，Err 是重叠区域内观测与投影深度的MSE：</p>
<p><img src="/2024/01/30/detect-slam/f6.png" alt="f6" title="formula 6"></p>
<p>若深度似然大于设定阈值，则将地图物体的ID 赋予给检测区域，否则，分配一个新的ID 给该检测区域。</p>
<h3 id="3-2-2-Cutting-Background"><a href="#3-2-2-Cutting-Background" class="headerlink" title="3.2.2 Cutting Background"></a>3.2.2 Cutting Background</h3><p>将投影与观测重叠区域的点作为前景seed，将bbox 外的点作为背景seed，利用Grab-Cut 算法得到物体的分割掩码。</p>
<p><img src="/2024/01/30/detect-slam/fig5.png" alt="fig5" title="figure 5"></p>
<h3 id="3-2-3-Recobstruction"><a href="#3-2-3-Recobstruction" class="headerlink" title="3.2.3 Recobstruction"></a>3.2.3 Recobstruction</h3><p>利用物体掩码，创建物体点云并剔除噪声点，最终将物体点云转换至世界坐标系中并添加至物体语义地图。</p>
<h2 id="3-3-SLAM-enhanced-Detector"><a href="#3-3-SLAM-enhanced-Detector" class="headerlink" title="3.3 SLAM-enhanced Detector"></a>3.3 SLAM-enhanced Detector</h2><p>利用物体语义地图和相机位姿来增强物体检测器的性能。</p>
<h3 id="3-3-1-Region-Proposal"><a href="#3-3-1-Region-Proposal" class="headerlink" title="3.3.1 Region Proposal"></a>3.3.1 Region Proposal</h3><p>利用当前估计的相机位姿将语义地图中的3D 物体投影至2D 平面，将具有相同物体ID 的<strong>像素进行聚类</strong>来确定可能含有物体的区域。</p>
<h3 id="3-3-2-Region-Filter"><a href="#3-3-2-Region-Filter" class="headerlink" title="3.3.2 Region Filter"></a>3.3.2 Region Filter</h3><p>去除掉较小尺寸（20<em>20像素）的候选区域，并估计观测深度与投影深度之间的<em>*似然</em></em>来检测遮挡的候选目标。</p>
<h3 id="3-3-3-Hard-Example-Mining"><a href="#3-3-3-Hard-Example-Mining" class="headerlink" title="3.3.3 Hard Example Mining"></a>3.3.3 Hard Example Mining</h3><p>前人的工作证明了选择使用<strong>困难的数据</strong>来训练或者细调DNN 网络可以极大促进检测效果，于是，作者使用SLAM-enhanced 物体检测器来挖掘困难的数据（Fig. 7所示）来<strong>强化训练集</strong>，然后用这些强化的数据集来细调SSD 网络。</p>
<p><img src="/2024/01/30/detect-slam/fig7.png" alt="fig7" title="figure 7"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 DynaSLAM_Tracking, Mapping, and Inpainting in Dynamic Scenes</title>
    <url>/2024/01/30/dynaslam/</url>
    <content><![CDATA[<p>Bescos, Berta, Jose M. Facil, Javier Civera, and Jose Neira. “DynaSLAM: Tracking, Mapping, and Inpainting in Dynamic Scenes.” <em>IEEE Robotics and Automation Letters</em> 3, no. 4 (October 2018): 4076–83. <a href="https://doi.org/10.1109/LRA.2018.2860039">https://doi.org/10.1109/LRA.2018.2860039</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文提出的DynaSLAM 是在ORB-SLAM2 基础上增加一个前端模块，来处理环境中的动态物体。对于单目和双目相机，使用CNN 产生物体的像素级语义分割结果，剔除掉先验动态物体中的特征点；对于RGB-D 相机，结合多视角几何模型和CNN 来检测动态物体，从图片中移除动态物体并进行场景恢复。</p>
<span id="more"></span>
<p><img src="/2024/01/30/dynaslam/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-System-Description"><a href="#3-System-Description" class="headerlink" title="3 System Description"></a>3 System Description</h1><p>本系统架构如Fig. 2所示，在处理RGB-D 图像时，本系统使用<strong>多视角几何</strong>在两方面提高动态物体分割的效果：</p>
<ol>
<li><strong>细调</strong>Mask R-CNN 获取的先验动态物体分割区域；</li>
<li>利用多视角几何判断<strong>先验静态物体</strong>是否是动态的。</li>
</ol>
<p>在处理单目或双目图像时，CNN 获取的语义分割中，属于先验动态物体的特征点<strong>直接被舍弃</strong>，不参与跟踪或制图。</p>
<p><img src="/2024/01/30/dynaslam/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-Segmentation-of-Potential-Dynamic-Content-Using-a-CNN"><a href="#3-1-Segmentation-of-Potential-Dynamic-Content-Using-a-CNN" class="headerlink" title="3.1 Segmentation of Potential Dynamic Content Using a CNN"></a>3.1 Segmentation of Potential Dynamic Content Using a CNN</h2><p>本系统使用Mask R-CNN 实现像素级语义分割，利用MS COCO 数据集进行训练，识别出图片中的先验动态物体。</p>
<h2 id="3-2-Low-Cost-Tracking"><a href="#3-2-Low-Cost-Tracking" class="headerlink" title="3.2 Low-Cost Tracking"></a>3.2 Low-Cost Tracking</h2><p>利用语义分割获取先验动态物体后，利用图片中的静态区域进行相机位姿估计。</p>
<h2 id="3-3-Segmentation-of-Dynamic-Content-Using-Mask-R-CNN-and-Multi-View-Geometry"><a href="#3-3-Segmentation-of-Dynamic-Content-Using-Mask-R-CNN-and-Multi-View-Geometry" class="headerlink" title="3.3 Segmentation of Dynamic Content Using Mask R-CNN and Multi-View Geometry"></a>3.3 Segmentation of Dynamic Content Using Mask R-CNN and Multi-View Geometry</h2><p>由于<strong>先验静态物体</strong>也可能是移动的，所以需要结合<strong>多视角几何</strong>实现更精确的动态物体检测。多视角几何检验过程如Fig. 3所示，将之前关键帧中的每个关键点 $x$ 投影到当前帧，得到关键点 $x’$ 以及投影深度 $z_{proj}$ ，然后计算 $x, x’$ 之间的视差角 $\alpha$ ，如果该角度大于30°，则该点可能被遮挡，应当忽略该点；然后从当前帧的深度图中获取 $x’$ 的深度 $z’$ ，比较 $\Delta z = z_{proj} - z’$ 是否超过阈值，若超过阈值则判定该点为<strong>动态特征点</strong>。</p>
<p><img src="/2024/01/30/dynaslam/fig3.png" alt="fig3" title="figure 3"></p>
<p>对于位于动态物体<strong>边缘</strong>上的特征点，在深度图中如果该点周边块的<strong>方差较高</strong>，则判定该点为<strong>静态点</strong>。为了区分所有属于动态物体的像素，本系统在深度图中动态特征点周围的区域进行<strong>生成操作</strong>，得到相应的动态区域掩码，如Fig. 4（a）所示。Fig. 4展示了结合语义信息与多视角几何信息（<strong>两种信息互补</strong>），得到最终的动态物体检测效果。</p>
<p><img src="/2024/01/30/dynaslam/fig4.png" alt="fig4" title="figure 4"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Semantic SLAM Based on Object Detection and Improved Octomap</title>
    <url>/2024/01/30/zhang2018/</url>
    <content><![CDATA[<p>Zhang, Liang, Leqi Wei, Peiyi Shen, Wei Wei, Guangming Zhu, and Juan Song. “Semantic SLAM Based on Object Detection and Improved Octomap.” <em>IEEE Access</em> 6 (2018): 75545–59. <a href="https://doi.org/10.1109/ACCESS.2018.2873617">https://doi.org/10.1109/ACCESS.2018.2873617</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>基于ORB-SLAM2，利用YOLO 实现<strong>物体检测</strong>，剔除掉<strong>先验动态物体</strong>上的特征点，提高精度；并建立<strong>物体级语义八叉树地图</strong>，且优化了制图的速度。</p>
<p>本文的贡献：</p>
<ul>
<li>本系统可以检测80-200 种物体种类，而现有的语义制图系统只能检测不超过20个种类；</li>
<li>本系统不需要先验3D 模型就可以实现对环境中物体的3D 建模；</li>
<li>本系统利用的是物体级模型信息，而不是像素级的。</li>
</ul>
<span id="more"></span>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3 System Overview"></a>3 System Overview</h1><h2 id="3-1-SLAM-Analysis"><a href="#3-1-SLAM-Analysis" class="headerlink" title="3.1 SLAM Analysis"></a>3.1 SLAM Analysis</h2><p>大多数基于特征的SLAM 系统基于一个<strong>强假设</strong>：移动物体上的特征点数量要远小于静态物体特征点数量。</p>
<p><strong>八叉树地图Octomap</strong> 不仅可以存储RGB 和位置信息，还可以保存<strong>语义信息</strong>，且利用<strong>概率模型</strong>来构建更为精准的地图。</p>
<h2 id="3-2-ORB-SLAM-Analysis"><a href="#3-2-ORB-SLAM-Analysis" class="headerlink" title="3.2 ORB-SLAM Analysis"></a>3.2 ORB-SLAM Analysis</h2><p>ORB-SLAM2 的架构如Fig. 1所示，包含三个并行处理的线程：</p>
<ul>
<li>Tracking：负责实时定位每一帧图片对应的<strong>位姿</strong>，并决定哪些图片作为<strong>关键帧</strong>。与前一帧进行特征匹配并利用BA 进行位姿优化，如果跟踪失败，利用Bag of Word 进行全局<strong>重定位</strong>，将地图中的特征点进行<strong>重投影</strong>并进行位姿优化。</li>
<li>Local Mapping：在获取一个新的关键帧之后，该线程对新地图点进行<strong>三角化</strong>，利用BA 对关键帧和地图点进行<strong>位姿优化</strong>，并对冗余的关键帧和低质量的地图点进行剔除。</li>
<li>Loop Closing：对关键帧进行<strong>回环检测</strong>，若检测成功，计算相似度转换作为环路<strong>累积漂移</strong>，然后进行对齐、融合与位姿优化。</li>
</ul>
<p><img src="/2024/01/30/zhang2018/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-3-Overview-of-Semantic-SLAM-system"><a href="#3-3-Overview-of-Semantic-SLAM-system" class="headerlink" title="3.3 Overview of Semantic SLAM system"></a>3.3 Overview of Semantic SLAM system</h2><p>本文提出的语义SLAM 系统架构如Fig. 2所示，本系统是基于ORB-SLAM2 构建的，ORB-SLAM2 负责相机定位，并利用每一个RGB-D 图片帧进行制图。</p>
<p>tracking 线程使用<strong>关键帧进行跟踪</strong>，来减小移动物体的影响；local mapping 线程添加<strong>少量</strong>的关键帧来创造语义信息，因为语义信息的提取满足实时的要求。</p>
<p>本系统使用YOLO 网络对关键帧进行物体检测，并使用<strong>CRF</strong> (Conditional Random Field) 进行<strong>物体正则化</strong>来对YOLO 检测出的物体置信度进行<strong>矫正优化</strong>，该过程使用MS-COCO 数据集的统计数据计算得到的物体间<strong>约束</strong>。</p>
<p><img src="/2024/01/30/zhang2018/fig2.png" alt="fig2" title="figure 2"></p>
<p>当获取了每个物体的<strong>准确标签</strong>后，利用滤波器对特征进行筛选，并利用投影的点云来创建<strong>临时的物体模型</strong>；在此基础上，利用<strong>数据关联</strong>决定创建新的模型或是与地图中现有的模型进行融合。</p>
<p>最后，Map Generation 利用存储在物体中的点云生成八叉树地图，并使用多线程和Fast Line Rasterization 算法进行加速。</p>
<h2 id="3-4-Relationship-between-Keyframes-and-Objects"><a href="#3-4-Relationship-between-Keyframes-and-Objects" class="headerlink" title="3.4 Relationship between Keyframes and Objects"></a>3.4 Relationship between Keyframes and Objects</h2><p>借鉴ORB-SLAM2 系统中<strong>关键帧和地图点</strong>之间的关系来创建本系统<strong>关键帧和物体</strong>之间的联系。</p>
<p>本系统中，每个<strong>物体 $O_i$</strong> 需包含：</p>
<ul>
<li>物体中点云的坐标；</li>
<li>固定数量的类别标签，以及通过递归贝叶斯更新recursive Bayesian update 计算的置信度；</li>
<li>观测到该物体的关键帧；</li>
<li>物体中点云的Kd-tree 结构（用于快速查找）；</li>
<li>物体所属的类别标签；</li>
<li>物体被观测的数量。</li>
</ul>
<p>每个<strong>关键帧 $K_i$</strong> 需包含：</p>
<ul>
<li>用于物体检测的相应RGB 图片；</li>
<li>用于产生点云的相应深度图片；</li>
<li>本关键帧中观测到的物体。</li>
</ul>
<h1 id="4-Semantic-Mapping"><a href="#4-Semantic-Mapping" class="headerlink" title="4 Semantic Mapping"></a>4 Semantic Mapping</h1><h2 id="4-1-Improved-SLAM"><a href="#4-1-Improved-SLAM" class="headerlink" title="4.1 Improved SLAM"></a>4.1 Improved SLAM</h2><p>ORB-SLAM2 中，tracking 线程的步骤：</p>
<ol>
<li>提取<strong>ORB 特征</strong>；</li>
<li>ORB 特征与<strong>reference 帧</strong>进行<strong>特征匹配</strong>，初步计算相机位姿并返回匹配地图点（通过搜索<strong>相关的关键帧</strong>来获取）的数量；</li>
<li>利用匹配地图点进行<strong>位姿优化</strong>；</li>
<li>决定哪些帧作为<strong>关键帧</strong>。</li>
</ol>
<p>为了减小动态物体的影响，对跟踪线程的第二步进行更改：由之前的reference 帧改为与<strong>关键帧进行特征匹配</strong>，这是因为<strong>旧的关键帧不包含动态物体的特征点</strong>。</p>
<p>跟踪线程的第三步改为：与第二步的结果比较<strong>匹配内点数量</strong>，来判断当前帧是否跟踪失败。</p>
<h2 id="4-2-Object-Detection"><a href="#4-2-Object-Detection" class="headerlink" title="4.2 Object Detection"></a>4.2 Object Detection</h2><p>利用在COCO 数据集上训练的Tiny YOLO 网络进行物体检测。</p>
<h2 id="4-3-Object-Regularization"><a href="#4-3-Object-Regularization" class="headerlink" title="4.3 Object Regularization"></a>4.3 Object Regularization</h2><p>常规的物体检测等网络没有考虑<strong>上下文信息（场景信息）</strong>，使用CRF 可为语义提取过程添加上下文信息约束，CRF 擅长对分类器的<strong>类别得分</strong>与图片的<strong>局部信息</strong>进行建模，可以视为一个<strong>最大后验概率问题</strong>。定义<strong>unary potentials</strong> 来对像素或图像块的所属类别进行概率建模，定义<strong>pairwise potentials</strong> 来对像素间或图像块间的关系进行建模。</p>
<p>作者构建了一个<strong>基于物体的概率稠密CRF 算法</strong>，较基于像素的方法大大减少了计算复杂度，相应的Gibbs 能量方程如下所示：</p>
<p><img src="/2024/01/30/zhang2018/f1-2.png" alt="f1" title="f1-2"></p>
<p>与文章(Runz 和 Agapito, 2017)相似，$x$ 表示类别标签；$i, j$ 取值1 到 $k$ ，其中 $k$ 表示地图中物体的数量；$Z$ 是归一化参数。本物体级 CRF 的目标是最小化 $E(x)$ ，相应的unary potentials 和 pairwise potentials 表示为：</p>
<p><img src="/2024/01/30/zhang2018/f3-4.png" alt="f3" title="f3-4"></p>
<p>其中，$\mu$ 函数被称为<strong>标签兼容性函数</strong>，用来描述相邻位置两个不同标签同时出现的可能性；$f_{i, j}$ 是第 i 个物体和第 j 个物体的约束。</p>
<p><img src="/2024/01/30/zhang2018/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$p_{i, j}$ 表示两个物体出现在同一视野中的概率，通过对COCO 数据集进行统计分析获取，如Fig. 4所示，对角线数字表示该物体在数据集中出现的次数。</p>
<p><img src="/2024/01/30/zhang2018/fig4.png" alt="fig4" title="figure 4"></p>
<p>由此，本系统实现了同时利用YOLO 和 CRF 实现对物体分类的<strong>置信度</strong>确定。</p>
<h2 id="4-4-Temporary-Objects-Generation"><a href="#4-4-Temporary-Objects-Generation" class="headerlink" title="4.4 Temporary Objects Generation"></a>4.4 Temporary Objects Generation</h2><p>在确定物体的类别标签后，使用feature filter 根据物体类别实现对特征点和地图点的筛选。作者将属于<strong>先验动态物体</strong>的ORB 特征、地图点、DBoW 特征进行剔除，只保留先验静态物体的特征。</p>
<p><img src="/2024/01/30/zhang2018/fig5.png" alt="fig5" title="figure 5"></p>
<p>在动态物体特征剔除之后，生成包含物体尺寸、类别、置信度的得分以及点云的<strong>临时物体模型</strong>，并对噪声点进行剔除。</p>
<p><img src="/2024/01/30/zhang2018/fig6.png" alt="fig6" title="figure 6"></p>
<h2 id="4-5-Data-Association"><a href="#4-5-Data-Association" class="headerlink" title="4.5 Data Association"></a>4.5 Data Association</h2><p>本系统中的DA 是用来判定检测物体是新观测到的还是地图中已存在的。</p>
<p>首先，为每个临时物体模型寻找<strong>候选匹配模型</strong>。可轻易找出与当前关键帧相关联的历史关键帧，以此确定候选匹配模型，该过程如Fig. 7所示。</p>
<p><img src="/2024/01/30/zhang2018/fig7.png" alt="fig7" title="figure 7"></p>
<p>然后，在候选匹配模型中选取<strong>最相似的模型</strong>。本过程在临时模型和候选模型的点云间进行nearest neighbor search ，计算匹配点对之间的<strong>欧氏距离</strong>；该过程利用<strong>k-d 树</strong>来加速匹配过程；选取匹配数量最多且超过一定阈值的候选匹配模型作为成功匹配的模型，若没有成功匹配，则视为<strong>新模型</strong>添加进地图中。</p>
<h2 id="4-6-Object-Model-Update"><a href="#4-6-Object-Model-Update" class="headerlink" title="4.6 Object Model Update"></a>4.6 Object Model Update</h2><p>若模型成功匹配，则<strong>点云</strong>与<strong>类别置信度</strong>需要进行融合。利用recursive Bayesian update 来更新相应的概率分布：</p>
<p><img src="/2024/01/30/zhang2018/f9.png" alt="f9" title="formula 9"></p>
<h2 id="4-7-Map-Generation"><a href="#4-7-Map-Generation" class="headerlink" title="4.7 Map Generation"></a>4.7 Map Generation</h2><p><img src="/2024/01/30/zhang2018/fig9.png" alt="fig9" title="figure 9"></p>
<p>在制图中，移动物体以及测距误差会造成很多误差，八叉树地图使用概率模型来解决该问题。八叉树地图的每个叶子节点存储被占用或空闲的概率，当新的3D 点被插入时会更新概率：</p>
<p><img src="/2024/01/30/zhang2018/f10-11.png" alt="f10-11" title="f10-11"></p>
<p>其中，$n$ 表示叶子节点；$z_t$ 表示观测；$P(n|z_t)$ 表示在给定观测 $z_t$ 时体素 $n$ 被占用的概率。</p>
<p>此外，除了占用概率，体素中还会存储<strong>固定数量的类别标签</strong>以及<strong>置信度得分</strong>。</p>
<p>八叉树地图绘制过程中会消耗大量的时间来计算空的体素，作者利用优化来加速该过程，如Fig. 10所示。</p>
<p><img src="/2024/01/30/zhang2018/fig10.png" alt="fig10" title="figure 10"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>C++</tag>
      </tags>
  </entry>
</search>
