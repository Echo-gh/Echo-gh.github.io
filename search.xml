<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>论文记录 Co-fusion_Real-time segmentation, tracking and fusion of multiple objects</title>
    <url>/2024/01/27/Co-fusion/</url>
    <content><![CDATA[<p>Runz, Martin, and Lourdes Agapito. “Co-Fusion: Real-Time Segmentation, Tracking and Fusion of Multiple Objects.” In <em>2017 IEEE International Conference on Robotics and Automation (ICRA)</em>, 4471–78. Singapore, Singapore: IEEE, 2017. <a href="https://doi.org/10.1109/ICRA.2017.7989518">https://doi.org/10.1109/ICRA.2017.7989518</a>.</p>
<h1 id="3-Overview-of-our-Method"><a href="#3-Overview-of-our-Method" class="headerlink" title="3 Overview of our Method"></a>3 Overview of our Method</h1><p>Co-Fusion 是一个可以<strong>实时处理</strong>每一帧输入图片的RGB-D SLAM 系统，本系统为场景中每一个分割的物体<strong>存储模型</strong>，而且可以<strong>独立跟踪</strong>它们的运动，每个模型是由<strong>一组3D 点</strong>构成的。本系统维护两组物体模型：当前在视野中可见的<strong>active 模型</strong>，以及曾经观测到的模型，但是目前不在视野中，记为<strong>inactive 模型</strong>。本系统的框架如Fig. 2所示，在初始化阶段，场景只包含一个active 模型——<strong>背景</strong>，初始化完成后，按照Fig .2的流程处理每一帧图片。</p>
<span id="more"></span>
<p><img src="/2024/01/27/Co-fusion/overview.png" alt="overview" title="Overview"></p>
<p><strong>tracking</strong> 与 <strong>fusion</strong> 步骤是在GPU 上完成的，而<strong>segmentation</strong> 步骤是在CPU 上完成的。</p>
<h2 id="3-1-Tracking"><a href="#3-1-Tracking" class="headerlink" title="3.1 Tracking"></a><strong>3.1 Tracking</strong></h2><p>在当前帧中跟踪每一个active 模型的6DOF 位姿，通过最小化每个模型独立的目标方程来实现，该目标方程包含：</p>
<ul>
<li><strong>几何误差</strong>：基于稠密的iterative closest point (ICP) 对齐；</li>
<li><strong>光度误差</strong>：基于当前帧和存储的3D 模型之间的颜色差异。</li>
</ul>
<h2 id="3-2-Segmentation"><a href="#3-2-Segmentation" class="headerlink" title="3.2 Segmentation"></a><strong>3.2 Segmentation</strong></h2><p>该阶段将当前帧中的每一个像素与某个active 模型/物体联系起来，有两种手段来实现该过程：motion 以及 semantic labels。</p>
<h3 id="3-2-1-Motion-segmentation"><a href="#3-2-1-Motion-segmentation" class="headerlink" title="3.2.1 Motion segmentation"></a>3.2.1 Motion segmentation</h3><p>将运动分割构建为一个使用全连接的Conditional Random Field (CRF) 解决的<strong>分类问题</strong>，可在CPU 上实时处理。当将一个像素与一个运动模型联系起来时，unary potentials 编码一个几何 <strong>ICP 损失函数</strong>。</p>
<h3 id="3-2-2-Multi-class-image-segmentation"><a href="#3-2-2-Multi-class-image-segmentation" class="headerlink" title="3.2.2 Multi-class image segmentation"></a>3.2.2 Multi-class image segmentation</h3><p>利用基于<strong>深度学习</strong>的方法实现像素级的语义分割，作为动作分割的备选方案。</p>
<h2 id="3-3-Fusion"><a href="#3-3-Fusion" class="headerlink" title="3.3 Fusion"></a>3.3 Fusion</h2><p>本系统使用surfel-based 融合方法，利用新估计的6DOF 位姿将属于某个模型的点更新至其active 模型。其中，每个模型是由一个sufel 列表构成的，$\mathcal{M}_m^s \in (\mathbf{p}\in \mathbb{R}^3, \mathbf{n}\in \mathbb{R}^3, \mathbf{c}\in \mathbb{N}^3, w \in \mathbb{R}, r \in \mathbb{R}, \mathbf{t}\in \mathbb{R}^2)$ ，分别表示位置、法向量、颜色、权重、半径以及两个时间戳。</p>
<p>为了解决<strong>动态物体</strong>的跟踪问题，本系统使用 $\mathcal{T}_t = \{\mathbf{T}_{tm}()\}$ 来表示每个active 模型 $\mathcal{M}_m$ 在时间 t 相对于全局参考坐标系的位姿转换几何，即 $\mathbf{T}_{tm}$ 表示时间 t 时模型 $\mathcal{M}_m$ 的全局位姿。特别地，作者使用 $\mathbf{T}_{tb}$ 来表示<strong>背景模型</strong>的位姿转换。</p>
<h1 id="5-Tracking-Active-Models"><a href="#5-Tracking-Active-Models" class="headerlink" title="5 Tracking Active Models"></a>5 Tracking Active Models</h1><p>对于时间t 的图像帧中的每一个active 模型  $\mathcal{M}_m$ ，系统通过配准<strong>当前的深度图</strong>和<strong>前一帧的深度图</strong>（通过将存储的3D 模型利用t-1 的估计位姿进行投影而获取）来跟踪其全局位姿 $\mathbf{T}_{tm}$ ，对每一个active 模型进行<strong>独立优化和跟踪</strong>。</p>
<h2 id="5-1-Energy"><a href="#5-1-Energy" class="headerlink" title="5.1 Energy"></a>5.1 Energy</h2><p>误差项包含<strong>ICP 几何误差</strong>和<strong>光度误差</strong>，其中光度误差是由预测的图片（将之前帧中存储的3D 模型投影而获取）与当前图片的<strong>颜色差异</strong>构成，</p>
<p><img src="/2024/01/27/Co-fusion/formula1.png" alt="formula1"></p>
<h2 id="5-2-Geometry-Term"><a href="#5-2-Geometry-Term" class="headerlink" title="5.2 Geometry Term"></a>5.2 Geometry Term</h2><p>ICP 几何误差定义为以下两者之间的误差：</p>
<ul>
<li>当前帧深度图的<strong>逆向投影</strong>3D 点；</li>
<li>前一帧t-1 <strong>预测</strong>的深度图。</li>
</ul>
<p><img src="/2024/01/27/Co-fusion/formula2.png" alt="formula2"></p>
<p>其中，$\mathbf{v}_t^i$ 是当前帧深度图 $\mathcal{D}_t$ 中第i 个点的反向映射3D 点；$\mathbf{v}^i, \mathbf{n}^i$ 分别是t-1 时刻<strong>预测的深度图</strong>中模型m 第i 个点的反向映射点以及其法向量；$\mathbf{T}_m$ 是将前一帧与当前帧的模型m 对齐的<strong>位姿转换</strong>。</p>
<h2 id="5-3-Photometric-Color-Term"><a href="#5-3-Photometric-Color-Term" class="headerlink" title="5.3 Photometric Color Term"></a>5.3 Photometric Color Term</h2><p>在给定(1)当前深度图、(2)每个active 模型的3D 几何估计，以及(3)将每个模型与前一帧对齐的位姿转换关系，即可将当前场景合成为一个与前一帧对齐的虚拟投影，进而，跟踪问题就变为了当前帧与合成的虚拟投影之间的<strong>光度配准</strong>问题：</p>
<p><img src="/2024/01/27/Co-fusion/formula3.png" alt="formula3"></p>
<p>其中，$\mathbf{T}_m$ 是将前一帧与当前帧的模型m 对齐的<strong>位姿转换</strong>；$\mathbf{I}_{t-1}()$ 表示提供模型在前一帧上顶点的颜色参数。</p>
<p>为了鲁棒性和效率，本优化使用一个<strong>4层的空间金字塔</strong>来集成到一个<strong>由粗到细</strong>的方法中，并在GPU 中完成运算。</p>
<h1 id="6-Motion-Segmentation"><a href="#6-Motion-Segmentation" class="headerlink" title="6 Motion Segmentation"></a>6 Motion Segmentation</h1><p>在跟踪步骤之后，在t 时刻有 $M_t$ 个新的位姿转换 $\{\mathbf{T}_{tm}\}$ ，来描述每个active 模型相对于全局坐标系的绝对位姿；接下来作者将帧t 的运动分割问题构建为一个<strong>分类标记问题</strong>，而标签为 $M_t$ 个位姿转换 $\{\mathbf{T}_{tm}\}$ ，作者将 $M_t+1$ 种可能分配到<strong>每一个像素</strong>中，即 $\mathcal{l} \in \mathcal{L}_t = \{1, …, |M_t|+1\}$ ，除了 $M_t$ 个位姿转换 $\{\mathbf{T}_{tm}\}$外还包含一个<strong>外点标签</strong> $\mathcal{l}_{|M_t|+1}$ 。</p>
<p>为了可以在CPU上实施完成分割步骤，系统首先将当前帧分割为SLIC 超像素，并在超像素级别上进行分类标记，超像素的位置、颜色与深度由从属的所有像素均值得到。代价函数如下所示：</p>
<p><img src="/2024/01/27/Co-fusion/formula4.png" alt="formula4"></p>
<p>其中，i 和 j 图片中超像素的索引（从1到S）。</p>
<h2 id="6-1-The-Unary-Potentials"><a href="#6-1-The-Unary-Potentials" class="headerlink" title="6.1 The Unary Potentials"></a>6.1 The Unary Potentials</h2><p>对于 $\psi_u(x_i)$ 表示为超像素 $s_i$ 分配标签为 $x_i$ 的代价，对于运动分割模式，该代价为<strong>ICP 几何对齐损失函数</strong>（式 2）。</p>
<h2 id="6-2-The-Pairwise-Potentials"><a href="#6-2-The-Pairwise-Potentials" class="headerlink" title="6.2 The Pairwise Potentials"></a>6.2 The Pairwise Potentials</h2><p>对于 $\psi_p(x_i, x_j)$ 可表示为：</p>
<p><img src="/2024/01/27/Co-fusion/formula5.png" alt="formula5"></p>
<p>其中，$\mu(x_i, x_j)$ 惩罚<strong>临近像素标签不同</strong>的情况；$k_m(f_i, f_j)$ 测量像素外观之间的<strong>相似度</strong>，代表的含义是：两个超像素的<strong>特征向量</strong>之间的距离较小时应具有相同的标签，所谓的特征向量 $f_i$  包含2D 位置、RGB 颜色以及深度值。</p>
<h2 id="6-3-Post-processing"><a href="#6-3-Post-processing" class="headerlink" title="6.3 Post-processing"></a>6.3 Post-processing</h2><p>在分割之后，采用一系列<strong>后处理步骤</strong>来获取更为鲁棒的结果：</p>
<ol>
<li>对具有相似位姿转换关系的模型进行<strong>融合操作</strong>；</li>
<li>抑制相同标签中除最大之外的所有区域来保证不连接区域的<strong>独立建模</strong>；</li>
<li>小于一定阈值的区域被<strong>舍弃</strong>。</li>
</ol>
<h2 id="6-4-Addition-of-New-Models"><a href="#6-4-Addition-of-New-Models" class="headerlink" title="6.4 Addition of New Models"></a>6.4 Addition of New Models</h2><p>一个区域内的外点数量若大于总像素数的3%，则判定该物体为一个新物体；若该新物体的部分几何结构已存在于地图中，会对重复的构建进行剔除。</p>
<p>如果一个物体消失在视野中，并在一定帧内不再出现，则将该模型添加进inactive 列表中。</p>
<h1 id="7-Object-Instance-Segmentation"><a href="#7-Object-Instance-Segmentation" class="headerlink" title="7 Object Instance Segmentation"></a>7 Object Instance Segmentation</h1><p>使用实例分割网络SharpMask 进行物体语义信息的获取。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 3DS-SLAM_A 3D Object Detection based Semantic SLAM towards Dynamic Indoor Environments</title>
    <url>/2024/03/01/3ds-slam/</url>
    <content><![CDATA[<p>Krishna, Ghanta Sai, Kundrapu Supriya, and Sabur Baidya. “3DS-SLAM: A 3D Object Detection Based Semantic SLAM towards Dynamic Indoor Environments.” arXiv, October 10, 2023. <a href="https://doi.org/10.48550/arXiv.2310.06385">https://doi.org/10.48550/arXiv.2310.06385</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ul>
<li>将一个<strong>轻量级3D HTx 物体检测架构</strong>集成到vSLAM 中，使得系统可以获取动态环境中的<strong>语义-空间信息</strong>；</li>
<li>提出一个新颖的端到端架构，集成了HTx 和 HDBSCAN 来有效解决<strong>语义和几何约束</strong>，实现对整体的优化；</li>
<li>实验证明了3DS-SLAM 超越了现有的算法，增强了系统在动态环境中的位姿<strong>准确性与稳定性</strong>。</li>
</ul>
<span id="more"></span>
<h1 id="3-3DS-SLAM-The-Approach"><a href="#3-3DS-SLAM-The-Approach" class="headerlink" title="3 3DS-SLAM: The Approach"></a>3 3DS-SLAM: The Approach</h1><h2 id="3-1-System-Architecture"><a href="#3-1-System-Architecture" class="headerlink" title="3.1 System Architecture"></a>3.1 System Architecture</h2><p>3DS-SLAM 是在ORB-SLAM2 的基础上进一步开发实现的，系统整体架构如Fig. 1所示：</p>
<p><img src="/2024/03/01/3ds-slam/fig1.png" alt="fig1" title="figure 1"></p>
<p>相较于ORB-SLAM2 ，本系统额外增加了两个线程：<strong>3D 物体检测线程</strong>以及<strong>动态特征滤波线程</strong>。系统框架如Fig. 2所示，3D 物体检测线程使用一个<strong>轻量级的HTx 架构</strong>，动态特征滤波线程利用<strong>基于几何深度的HDBSCAN 聚类</strong>来识别动态点。</p>
<p><img src="/2024/03/01/3ds-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-2-Hybrid-Transformer-Light-Weight-3D-Object-Detector"><a href="#3-2-Hybrid-Transformer-Light-Weight-3D-Object-Detector" class="headerlink" title="3.2 Hybrid Transformer: Light-Weight 3D Object Detector"></a>3.2 Hybrid Transformer: Light-Weight 3D Object Detector</h2><p>在vSLAM 中，相机捕获的图片一般只包含物体的<strong>局部信息</strong>，这使得利用局部物体信息进行预测的算法变得很必要。本文使用的HTx 架构以<strong>3D 点云</strong>作为输入，预测物体的状态，包括<strong>深度、朝向以及位置信息</strong>；该方法与现有的transformer 架构在<strong>数据层面</strong>有很大的不同，包括点云预处理以及同时使用<strong>点特征</strong>和<strong>体素特征</strong>来进行局部物体的定位。由于3D 点云的计算量要远大于图片，因此，作者对点云进行了大量的<strong>预处理</strong>（Voxel Downsampling, RANSAC-Ground Filtering, Radius based Outlier Removal 等) 来进行有效压缩；而且，本HTx 架构将实时性作为优先考虑，因此不使用点云的颜色和法向量信息进行物体检测。</p>
<p>本系统所使用的3D 物体检测算法是基于3DETR 进行改进的，以增强其与vSLAM 的兼容性，特别是强化了<strong>局部物体检测层</strong>，以解决vSLAM 中由于部分观测、相机旋转以及其他环境因素导致的检测失败。同时为局部感知物体定位和类别感知物体定位设计一个损失函数过于复杂，所以作者设计了两个独立的损失函数。对于每个3D bbox，其参数表示为 $\hat{b} =[\hat{c},\hat{d},\hat{a},\hat{s}]$ ，其包含以下元素：</p>
<ol>
<li><strong>几何元素</strong> $\hat{c},\hat{d}\in[0,1]^3$ 定义了bbox 的中心与尺寸，$\hat{a} = [\hat{a}_c, \hat{a}_r]$ 表示角度与角度残差；</li>
<li><strong>语义项</strong> $\hat{s} = [0, 1]^{K+1}$ 表示物体所属类别的概率分布。</li>
</ol>
<p>作者使用<strong>L1 回归损失</strong>来表示中心与尺寸的损失函数，使用<strong>Huber 回归损失</strong>表示角度残差损失函数，使用<strong>交叉熵</strong>表示角度和语义分类损失：</p>
<p><img src="/2024/03/01/3ds-slam/f1.png" alt="f1" title="formula 1"></p>
<p>对于局部感知物体定位，作者通过表示点在3D bbox 框内的<strong>相对位置</strong>，为每个点定义一个<strong>物体内部局部定位</strong>，每个点坐标为 $(x^p, y^p, z^p)$ ，则其物体内部的局部坐标表示为 $(x^f, y^f, z^f)$ ：</p>
<p><img src="/2024/03/01/3ds-slam/f2.png" alt="f2" title="formula 2"></p>
<p>其中，$(x^c, y^c, z^c)$ 表示bbox 的中心坐标，bbox 的尺寸与朝向参数为 $(h, w, l, \theta)$ 。</p>
<p>为了估计 $(x^f, y^f, z^f)$ ，定义每个点的<strong>交叉熵损失函数</strong>：</p>
<p><img src="/2024/03/01/3ds-slam/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$\tilde{u}^f$ 表示预测的局部坐标 $(x^f, y^f, z^f)$ ；$u^f$ 表示真实的坐标。</p>
<h2 id="3-3-HDBSCAN-Clustering-and-Dynamic-Feature-Filter"><a href="#3-3-HDBSCAN-Clustering-and-Dynamic-Feature-Filter" class="headerlink" title="3.3 HDBSCAN Clustering and Dynamic Feature Filter"></a>3.3 HDBSCAN Clustering and Dynamic Feature Filter</h2><p>物体检测算法获取的bbox 通常会包含<strong>大量的背景点</strong>，特别是对于非刚性物体情况更为明显。作者以人为对象，阐述一下如何对bbox 内的外点进行剔除。人类表面的特征点通常有着较好的<strong>深度连续性</strong>，且与背景有着较大的深度差别；因此，当人类bbox 占据视野中的绝大部分时，作者对HDBSCAN 密度聚类算法进行优化，来区分bbox 内的前景点与背景点：将具有较低深度参数的点进行<strong>聚类</strong>，作为前景动态点，可增强HDBSCAN 的鲁棒性，且可有效应对人类被其他物体<strong>部分遮挡</strong>的情况。</p>
<p>HDBSCAN 算法处理3D <strong>空间点 K</strong> 和<strong>深度图 D</strong>，利用core distance $k(i)$  定义点密度， $k(i)$ 表示一个点到k-th 最近邻域的欧式距离。为了区分低密度点（高 $k(i)$ ），作者定义了一个 mutual reachable distance：</p>
<p><img src="/2024/03/01/3ds-slam/f4.png" alt="f4" title="formula 4"></p>
<p>本算法与同类算法的精度比较：</p>
<p><img src="/2024/03/01/3ds-slam/t1.png" alt="t1" title="table 1"></p>
<p>时间消耗对比：</p>
<p><img src="/2024/03/01/3ds-slam/fig3.png" alt="fig3" title="figure 3"></p>
<p>可以看出3DS-SLAM 的实时性能相当不错，这是因为其进行了以下操作：</p>
<ol>
<li>对语义线程和几何线程进行<strong>并行处理</strong>；</li>
<li>对点云进行<strong>预处理</strong>，消除掉不必要的数据。</li>
</ol>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu系统安装显卡驱动、CUDA、cuDNN</title>
    <url>/2024/01/26/Ubuntu-dl-setup/</url>
    <content><![CDATA[<p><strong>本文主要参考</strong><a href="https://zhuanlan.zhihu.com/p/643954422">该文章</a>。</p>
<h2 id="1-安装显卡驱动"><a href="#1-安装显卡驱动" class="headerlink" title="1 安装显卡驱动"></a>1 安装显卡驱动</h2><h3 id="1-1-前期准备"><a href="#1-1-前期准备" class="headerlink" title="1.1 前期准备"></a>1.1 前期准备</h3><p>根据显卡型号在<a href="https://www.nvidia.com/Download/Find.aspx?lang=en-us#">Nvidia 官网</a>下载相应的驱动程序，然后安装必备软件：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 更新源</span></span><br><span class="line">sudo apt-get update</span><br><span class="line"><span class="comment">## 安装必备软件</span></span><br><span class="line">sudo apt-get install g++</span><br><span class="line">sudo apt-get install gcc</span><br><span class="line">sudo apt-get install make</span><br></pre></td></tr></table></figure>
<h3 id="1-2-禁用默认驱动"><a href="#1-2-禁用默认驱动" class="headerlink" title="1.2 禁用默认驱动"></a>1.2 禁用默认驱动</h3><p>在安装NVIDIA驱动前需要禁止系统自带的显卡驱动 nouveau：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 编辑blacklist.conf 文件</span></span><br><span class="line">sudo vi /etc/modprobe.d/blacklist.conf</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在文件末尾添加以下内容并保存</span></span><br><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新initramfs，然后重启电脑</span></span><br><span class="line">sudo update-initramfs -u</span><br><span class="line">sudo reboot now</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查是否关闭nouveau，若没有输出，则说明已成功关闭</span></span><br><span class="line">lsmod | grep nouveau</span><br></pre></td></tr></table></figure>
<h3 id="1-3-安装驱动"><a href="#1-3-安装驱动" class="headerlink" title="1.3 安装驱动"></a>1.3 安装驱动</h3><p>进入tty 模式，并关闭图形进程：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo service gdm3 stop</span><br></pre></td></tr></table></figure>
<p>开始安装驱动：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 赋予执行权限</span></span><br><span class="line">sudo <span class="built_in">chmod</span> 777 NVIDIA-Linux-x86_64-535.54.03.run</span><br><span class="line"><span class="comment">## 安装：不安装OpenGL,安装时关闭X服务  </span></span><br><span class="line">sudo ./NVIDIA-Linux-x86_64-535.54.03.run –no-opengl-files -no-x-check</span><br><span class="line"><span class="comment">## Install Nvidia&#x27;s 32-bit compatibility libraries?</span></span><br><span class="line"><span class="comment">## 选择 &quot;No&quot;</span></span><br><span class="line"><span class="comment">## Would you like to run the nvidia-xconfig utility to automatically update your X configuration file so that the NVIDIA X driver dill be used dhen you restart X? Any pre-existing X configuration file will be backed up.</span></span><br><span class="line"><span class="comment">## 选择 &quot;Yes&quot;</span></span><br></pre></td></tr></table></figure>
<p>成功安装之后，会进入图形界面，此时使用命令nvidia-smi 检查驱动是否安装成功，若出现下图界面，证明驱动安装成功。</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/nvidia-smi.png" alt="nvidia-smi" title="nvidia-smi显示界面"></p>
<h3 id="1-4-显卡驱动失效问题记录"><a href="#1-4-显卡驱动失效问题记录" class="headerlink" title="1.4 显卡驱动失效问题记录"></a>1.4 显卡驱动失效问题记录</h3><p>一次重启电脑后，发现Ubuntu提示某个文件发生错误，是否需要反馈给Ubuntu，本人当时没在意点了“否”，在后续操作过程中发现nvidia-smi命令报错，显示找不到显卡驱动：</p>
<blockquote>
<p>NVIDIA-SMI has failed because it couldn’t communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.</p>
</blockquote>
<p>但使用nvcc -V 命令可以正常显示CUDA 版本。由于之前也遇到过类似驱动失效的问题，所以选择重装显卡驱动，按照之前的步骤检查nouveau、关闭图形界面、安装驱动……然后就一路报错了……</p>
<blockquote>
<p>NVIDIA-SMI has failed because it couldn‘t communicate with the NVIDIA driver.</p>
</blockquote>
<p>网上找解决方案，这篇<a href="https://blog.csdn.net/wjinjie/article/details/108997692">文章</a>提到一个解决方案，使用dkms：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 首先，查看显卡驱动版本</span></span><br><span class="line"><span class="built_in">ls</span> /usr/src | grep nvidia</span><br><span class="line">nvidia-535.146.02</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 然后，安装dkms，并修复显卡驱动</span></span><br><span class="line">sudo apt-get install dkms</span><br><span class="line">sudo dkms install -m nvidia -v 535.146.02</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 输入nvidia-smi命令，一切恢复正常</span></span><br></pre></td></tr></table></figure>
<p>然后搜了一下dkms的作用，wikipedia 中介绍如下：</p>
<blockquote>
<p>动态内核模块支持 （Dynamic Kernel Module Support，DKMS）是用来生成Linux的内核模块的一个框架，其源代码一般不在Linux内核源代码树。 当新的内核安装时，DKMS 支持的内核设备驱动程序 到时会自动重建。 DKMS 可以用在两个方向：如果一个新的内核版本安装，自动编译所有的模块，或安装新的模块（驱动程序）在现有的系统版本上，而不需要任何的手动编译或预编译软件包需要。例如，这使得新的显卡可以使用在旧的Linux系统上。</p>
</blockquote>
<p>唔……很奇怪，我应该没有更新Ubuntu的内核，不晓得为什么会出现这个情况，先记录一下吧，当前本人的Ubuntu内核版本为：</p>
<blockquote>
<p>Linux echo-dell 6.5.0-14-generic #14~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Nov 20 18:15:30 UTC 2 x86_64 x86_64 x86_64 GNU/Linux</p>
</blockquote>
<h2 id="2-安装CUDA"><a href="#2-安装CUDA" class="headerlink" title="2 安装CUDA"></a>2 安装CUDA</h2><h3 id="2-1-下载与安装"><a href="#2-1-下载与安装" class="headerlink" title="2.1 下载与安装"></a>2.1 下载与安装</h3><p>可同时安装不同的CUDA 版本，根据不同环境需求选择使用不同版本，本处以CUDA-11.8为例进行安装说明。</p>
<p>在<a href="https://developer.nvidia.com/cuda-toolkit-archive">NVIDIA官网</a>下载对应版本，推荐使用runfile (local) 进行安装：</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/CUDA.png" alt="cuda" title="CUDA安装命令"></p>
<p>根据官网给出的下载和安装命令执行即可：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run</span><br><span class="line">sudo sh cuda_11.8.0_520.61.05_linux.run</span><br></pre></td></tr></table></figure>
<h3 id="2-2-环境变量配置"><a href="#2-2-环境变量配置" class="headerlink" title="2.2 环境变量配置"></a>2.2 环境变量配置</h3><p>安装完成后，打开账户的配置文件，进行以下修改：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## CUDA ENV</span></span><br><span class="line"><span class="built_in">export</span> CUDA_HOME=/usr/local/cuda</span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="variable">$&#123;LD_LIBRARY_PATH&#125;</span>:<span class="variable">$&#123;CUDA_HOME&#125;</span>/lib64</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$&#123;CUDA_HOME&#125;</span>/bin:<span class="variable">$&#123;PATH&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新配置文件设置</span></span><br><span class="line"><span class="built_in">source</span> ~/.zshrc</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证安装是否成功</span></span><br><span class="line">nvcc -V</span><br></pre></td></tr></table></figure>
<p>若出现下图界面，则证明CUDA 安装成功:</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/nvcc.png" alt="cuda" title="检验CUDA安装成功"></p>
<h3 id="2-3-多版本切换"><a href="#2-3-多版本切换" class="headerlink" title="2.3 多版本切换"></a>2.3 多版本切换</h3><p>CUDA 安装位置的文件如下图所示：</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/CUDA-version.png" alt="cuda-version" title="CUDA版本切换"></p>
<p>根据所需版本，切换cuda 的软链接即可：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo <span class="built_in">ln</span> -snf /usr/local/cuda-11.8 /usr/local/cuda</span><br></pre></td></tr></table></figure>
<h2 id="3-安装cuDNN"><a href="#3-安装cuDNN" class="headerlink" title="3 安装cuDNN"></a>3 安装cuDNN</h2><p>注：此处可参考<a href="https://docs.nvidia.com/deeplearning/cudnn/install-guide/index.html#verify">NVIDIA官网教程</a>进行安装。</p>
<h3 id="3-1-前期准备与下载"><a href="#3-1-前期准备与下载" class="headerlink" title="3.1 前期准备与下载"></a>3.1 前期准备与下载</h3><p>首先安装依赖包：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install zlib1g</span><br></pre></td></tr></table></figure>
<p>然后在<a href="https://developer.nvidia.com/rdp/cudnn-download">NVIDIA官网</a>下载相应版本的安装包，注意，下载cuDNN 需要注册NVIDIA账号。选取合适的版本进行下载：</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/cudnn.png" alt="cudnn" title="cuDNN安装包"></p>
<h3 id="3-2-安装"><a href="#3-2-安装" class="headerlink" title="3.2 安装"></a>3.2 安装</h3><p>解压并安装：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i cudnn-local-repo-ubuntu2204-8.9.3.28_1.0-1_amd64.deb</span><br></pre></td></tr></table></figure>
<p>按照提示导入CUDA GPG key：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo <span class="built_in">cp</span> /var/cudnn-local-repo-ubuntu2204-8.9.3.28/cudnn-local-*-keyring.gpg /usr/share/keyrings/</span><br></pre></td></tr></table></figure>
<p>更新源并安装：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line"><span class="comment">## 1. Install the runtime library.</span></span><br><span class="line"><span class="comment">## sudo apt-get install libcudnn8=8.x.x.x-1+cudaX.Y</span></span><br><span class="line">sudo apt-get install libcudnn8=8.9.3.28-1+cuda11.8</span><br><span class="line"><span class="comment">## 2. Install the developer library.</span></span><br><span class="line"><span class="comment">## sudo apt-get install libcudnn8-dev=8.x.x.x-1+cudaX.Y</span></span><br><span class="line">sudo apt-get install libcudnn8-dev=8.9.3.28-1+cuda11.8</span><br><span class="line"><span class="comment">## 3. Install the code samples.</span></span><br><span class="line"><span class="comment">## sudo apt-get install libcudnn8-samples=8.x.x.x-1+cudaX.Y</span></span><br><span class="line">sudo apt-get install libcudnn8-samples=8.9.3.28-1+cuda11.8</span><br></pre></td></tr></table></figure>
<h3 id="3-3-测试"><a href="#3-3-测试" class="headerlink" title="3.3 测试"></a>3.3 测试</h3><p>输入检查命令，出现下图证明安装初步成功。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo dpkg -l | grep cudnn</span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/Ubuntu-dl-setup/cudnn-dpkg.png" alt="cudnn-version" title="cuDNN安装成功"></p>
<p>按照官网教程，进行代码测试：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将cuDNN samples 拷贝至主目录</span></span><br><span class="line"><span class="built_in">cp</span> -r /usr/src/cudnn_samples_v8/ <span class="variable">$HOME</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 进入samples 文件夹，并编译</span></span><br><span class="line"><span class="built_in">cd</span>  <span class="variable">$HOME</span>/cudnn_samples_v8/mnistCUDNN</span><br><span class="line">make clean &amp;&amp; make</span><br></pre></td></tr></table></figure>
<p>此时可能出现如下报错：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">fatal error: FreeImage.h: No such file <span class="keyword">or</span> directory</span><br><span class="line">    <span class="number">1</span> | <span class="meta">#<span class="keyword">include</span> <span class="string">&quot;FreeImage.h&quot;</span></span></span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/Ubuntu-dl-setup/cudnn-error.png" alt="cudnn-error" title="cuDNN报错"></p>
<p>可安装相应包进行解决：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install libfreeimage3 libfreeimage-dev</span><br></pre></td></tr></table></figure>
<p>安装之后重新编译，并运行生成文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 重新编译</span></span><br><span class="line">make clean &amp;&amp; make</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行生成文件</span></span><br><span class="line">./mnistCUDNN</span><br></pre></td></tr></table></figure>
<p>若出现如下结果，则证明cuDNN 安装成功：</p>
<p><img src="/2024/01/26/Ubuntu-dl-setup/cudnn-success.png" alt="cudnn-success" title="cuDNN安装成功"></p>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Deep Learning</tag>
        <tag>Nvidia</tag>
        <tag>CUDA</tag>
        <tag>cuDNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu系统安装</title>
    <url>/2024/01/26/Ubuntu-setup/</url>
    <content><![CDATA[<h1 id="1-系统盘制作"><a href="#1-系统盘制作" class="headerlink" title="1 系统盘制作"></a>1 系统盘制作</h1><p>推荐使用 Rufus 这个软件进行制作：</p>
<span id="more"></span>
<p><img src="/2024/01/26/Ubuntu-setup/rufus.png" alt="rufus" title="rufus设置界面"></p>
<h1 id="2-进入BIOS更改启动顺序"><a href="#2-进入BIOS更改启动顺序" class="headerlink" title="2 进入BIOS更改启动顺序"></a>2 进入BIOS更改启动顺序</h1><p>将制作好的U盘插入电脑，启动电脑进入BIOS，设置第一启动项为U盘启动，保存BIOS设置并重新启动电脑，启动后选择”Try or install Ubuntu”选项，注意，此时可能会出现电脑黑屏的情况，但是屏幕是点亮的、鼠标有灯效，证明确实进入了安装系统，但是由于Ubuntu显卡驱动的问题，此时需要进行额外设置：</p>
<ol>
<li><p>重新启动电脑，在光标选择”Try or install Ubuntu”选项后，不要点击Enter键，而是点击”e”键进入命令行编辑模型；</p>
</li>
<li><p>删除”quite splash”后的”—-“，并添加”nomodeset”（依照不同显卡进行不同显卡驱动选项的添加，对于Nvidia显卡，添加nomodeset）；</p>
</li>
<li><p>然后，点击”F10”开始安装，此时电脑屏幕会正常。</p>
</li>
</ol>
<p>值得注意的是，装机成功重启后可能也会出现黑屏的情况（本人没有遇到），此时应在开机后点击”e”键，同样找到”quite splash” 并在后面添加”nomodeset”，按”F10”启动系统 ，进去系统之后编辑”/etc/default/grub”这个文件，具体操作在此不做赘述，可参考<a href="https://blog.csdn.net/qq_32285693/article/details/118900765">该文章</a>。</p>
<h1 id="3-设置硬盘分区"><a href="#3-设置硬盘分区" class="headerlink" title="3 设置硬盘分区"></a>3 设置硬盘分区</h1><p>本人想将之前的双系统进行清空，但又不想使用默认设置，故选择”something else”：</p>
<ol>
<li>清空之前双系统的所有磁盘空间，将硬盘全置为free；</li>
<li>根据个人需求设置硬盘分区，以下是重点部分，本人设置情况如下所示（<strong>注意分区类型</strong>）：</li>
</ol>
<p><img src="/2024/01/26/Ubuntu-setup/disk_set.png" alt="磁盘分区设置" title="磁盘分区设置"></p>
<p>具体空间分配可根据个人使用习惯和硬盘大小进行设置，值得注意的是，<strong>EFI 分区一定是第一个设置的！！！</strong></p>
<h1 id="4-Ubuntu设置"><a href="#4-Ubuntu设置" class="headerlink" title="4 Ubuntu设置"></a>4 Ubuntu设置</h1><p>进入Ubuntu系统后的设置不再赘述。</p>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 BEV-Locator_An End-to-end Visual Semantic Localization Network Using Multi-View Images</title>
    <url>/2024/03/06/bev-locator/</url>
    <content><![CDATA[<p>Zhang, Zhihuang, Meng Xu, Wenqiang Zhou, Tao Peng, Liang Li, and Stefan Poslad. “BEV-Locator: An End-to-End Visual Semantic Localization Network Using Multi-View Images.” arXiv, November 27, 2022. <a href="http://arxiv.org/abs/2211.14927">http://arxiv.org/abs/2211.14927</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者认为，本文提出的BEV-Locator 是第一个利用<strong>端到端学习框架</strong>解决<strong>视觉语义定位问题</strong>的工作，主要贡献如下：</p>
<ul>
<li>提出一个新颖的<strong>端到端视觉语义定位架构</strong>，利用多视角图片和语义环境实现自身位姿的精确估计，<strong>数据驱动</strong>的方法避免了几何优化策略设计及参数细调；</li>
<li>利用transformer 架构解决语义地图元素与相机图片之间的<strong>跨模态匹配问题</strong>；</li>
<li>通过一个统一的<strong>BEV 特征空间</strong>，利用环视图片来增强感知能力；同时，作者验证了视觉语义定位问题可作为基于BEV 特征大模型的子任务进行解决的可行性；</li>
<li>通过大范围数据及验证，作者证明了本方法的优越性。</li>
</ul>
<span id="more"></span>
<h1 id="3-Methods"><a href="#3-Methods" class="headerlink" title="3 Methods"></a>3 Methods</h1><p>本方法的基本框架如Fig. 2所示，主要包括以下组件：Visual BEV encoder module，Semantic Map encoder module，Cross-model transformer module，以及一个Pose decoder module：</p>
<ul>
<li>环视图片的<strong>BEV 特征</strong>被Visual BEV Encoder 转换为一个<strong>栅格化表示</strong> rasterized representation；</li>
<li>语义地图被Semantic Map Encoder 实例化编码为几个<strong>压缩向量</strong>（也叫做map queries）；</li>
<li>基于BEV 特征和 map queries，cross-model transformer 模块通过<strong>自注意力和交叉注意力</strong>计算汽车的queried-out 位姿信息；</li>
<li>基于queried-out 位姿信息，pose decoder 模块根据地图特征和环视图片之间的<strong>最优匹配</strong>计算出相机的位姿信息。</li>
</ul>
<p><img src="/2024/03/06/bev-locator/fig2.png" alt="fig2" title="figure 2"></p>
<p><img src="/2024/03/06/bev-locator/a1.png" alt="a1" title="algorithm 1"></p>
<p><img src="/2024/03/06/bev-locator/a2.png" alt="a2" title="algorithm 2"></p>
<p><img src="/2024/03/06/bev-locator/a4.png" alt="a4" title="algorithm 4"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Probabilistic data association for semantic SLAM</title>
    <url>/2024/02/18/bowman2017/</url>
    <content><![CDATA[<p>Bowman, Sean L., Nikolay Atanasov, Kostas Daniilidis, and George J. Pappas. “Probabilistic Data Association for Semantic SLAM.” In <em>2017 IEEE International Conference on Robotics and Automation (ICRA)</em>, 1722–29. Singapore, Singapore: IEEE, 2017. <a href="https://doi.org/10.1109/ICRA.2017.7989203">https://doi.org/10.1109/ICRA.2017.7989203</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文做出的贡献：</p>
<ul>
<li>第一个将惯性、几何以及语义观测信息<strong>紧耦合至一个优化框架</strong>的定位算法；</li>
<li>将联合metric-semantic SLAM 问题分解为<strong>连续（位姿）</strong>和<strong>离散（数据关联DA和语义标签）</strong>优化子问题；</li>
<li>在包含<strong>光照变化的杂乱场景中</strong>利用里程计和视觉测量信息进行<strong>室内外真实场景</strong>的长轨迹测试实验。</li>
</ul>
<span id="more"></span>
<h1 id="2-Probabilistic-Data-Association-in-SLAM"><a href="#2-Probabilistic-Data-Association-in-SLAM" class="headerlink" title="2 Probabilistic Data Association in SLAM"></a>2 Probabilistic Data Association in SLAM</h1><p>经典的SLAM问题可表述如下：移动传感器穿越一个未知环境，环境中包含M 个静态路标集合 $\mathcal{L} = \{l_m\}^M_{m=1}$ ，给定一组传感器测量数据 $\mathcal{Z} = \{z_k\}^K_{k=1}$ ，任务是估计地标位置 $\mathcal{L}$ 以及传感器的位姿序列 $\mathcal{X} = \{x_t\}^T_{t=1}$ 。</p>
<p>大部分现有的工作都聚焦于估计  $\mathcal{L}$ 和  $\mathcal{X}$ ，很少强调数据关联 $\mathcal{D}$ <strong>实际上是未知的</strong>。这里的数据关联 $\mathcal{D} = \{(\alpha_k, \beta_k)\}^K_{k=1}$ 指的是在传感器位姿 $x_{\alpha_k}$ 情况下对路标 $l_{\beta_k}$ 的观测数据为 $z_k$ ，即在相机位姿 $x_{\alpha_k}$ 下，观测数据 $z_k$ 和路标 $l_{\beta_k}$ 之间的关联。</p>
<p>SLAM问题的求解可表述为：在给定观测数据 $\mathcal{Z}$ 时，求解  $\mathcal{L}$ ，$\mathcal{X}$ 和 $\mathcal{D}$ 的最大似然估计：</p>
<p><img src="/2024/02/18/bowman2017/f1.png" alt="f1" title="f1"></p>
<p><strong>求解方法一：</strong></p>
<p>给定先验估计 $\mathcal{X}^0,\mathcal{L}^0$ ，求得数据关联的最大似然估计 $\hat{\mathcal{D}}$ ，然后利用 $\hat{\mathcal{D}}$ 计算路标与传感器状态  $\mathcal{L}$ ，$\mathcal{X}$ 的最大似然估计。</p>
<p><img src="/2024/02/18/bowman2017/f2.png" alt="f2" title="f2"></p>
<p>上述方法的缺点在于，错误的数据匹配会严重影响估计结果；而如果为了避免错误匹配而舍弃模糊测量，在后续位姿优化后也无法利用这些模糊测量。</p>
<p><strong>求解方法二：坐标下降法coordinate descent</strong></p>
<p>为了避免方法一简单的一步处理，本方法使用迭代计算：</p>
<p><img src="/2024/02/18/bowman2017/f3.png" alt="f3" title="f3"></p>
<p>本方法可以在状态估计得到优化后返回来重新估计数据关联；然而，本方法不能解决模糊测量的问题，因为仍然需要一个<strong>硬数据关联</strong>。</p>
<p>为解决模糊测量导致的数据关联问题，当估计数据关联时，作者不是简单地选取物体属于某个特定的类别 $\hat{\mathcal{D}}=p(\mathcal{D}|\mathcal{X}, \mathcal{L}, \mathcal{Z})$ ，而是<strong>考虑观测数据与路标之间所有可能的数据关联</strong> $D$ 。给定一个初始估计 $\mathcal{X}^i, \mathcal{L}^i$ ,使用 $\mathcal{D}$ 中的全部可能可得到一个更优的估计值，作者采用了expectation maximization (EM) 方法来最大化 expected measurement likelihood 进行计算：</p>
<p><img src="/2024/02/18/bowman2017/f4.png" alt="f4" title="f4"></p>
<p>其中， $\mathbb{D}$ 是 $\mathcal{D}$ 的所有取值空间。本EM 方程优势在于不需要硬数据关联 hard decisions on data association，而是平均所有可能的数据关联情况。</p>
<p>为了与坐标下降法进行比较，上式可重写为：</p>
<p><img src="/2024/02/18/bowman2017/f5.png" alt="f5" title="f5"></p>
<p>其中，$w_{kj}^i$ 是一个量化了<strong>软数据关联</strong>的权重参数，表示K 个观测和M 个路标之间所有关联对应的权重参数，$w_{kj}^i = \sum_{\mathcal{D}\in\mathbb{D}(k,j)}p(\mathcal{D}|\mathcal{X}^i, \mathcal{L}^i, \mathcal{Z})$ ，其中，$\mathbb{D}(k,j) = \{\mathcal{D}\in\mathbb{D} | \beta_k = j\}\subseteq \mathbb{D}$ 是观测 $k$ 分配给地标 $j$ 的所有数据关联组合。</p>
<p>上式的求解同样采用<strong>EM 方法</strong>：</p>
<ul>
<li>E 步骤：以权重参数 $w_{kj}^i$ 的形式来估计<strong>数据关联分布</strong> $p(\mathcal{D}|\mathcal{X}^i, \mathcal{L}^i, \mathcal{Z})$ ；</li>
<li>M 步骤：根据E 步骤计算得到的分布来最大化 expected measurement log likelihood。</li>
</ul>
<h1 id="3-Semantic-SLAM"><a href="#3-Semantic-SLAM" class="headerlink" title="3 Semantic SLAM"></a>3 Semantic SLAM</h1><p>作者设定地标的状态 $l$ 包含位置信息 $l^p \in \mathbb{R}^3$ 和类别标签 $l^c$ 。为了估计<strong>地标状态</strong> $\mathcal{L}$ <strong>（位姿与种类）和传感器运动轨迹</strong> $\mathcal{X}$ ，作者使用三种信息源：惯性、几何点特征和物体语义观测。</p>
<p><img src="/2024/02/18/bowman2017/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-1-Inertial-information"><a href="#3-1-Inertial-information" class="headerlink" title="3.1 Inertial information"></a>3.1 Inertial information</h2><p>作者每隔n 帧选取一张单目相机采集到的图像作为关键帧，第t 帧对应的传感器状态表示为 $x_t$  ，包含6自由度的位姿、速度以及IMU 偏差值。在相邻两个关键帧之间还有一组IMU 测量数据 $\mathcal{I}$ ，包含线性加速度和旋转角速度。</p>
<h2 id="3-2-Geometric-information"><a href="#3-2-Geometric-information" class="headerlink" title="3.2 Geometric information"></a>3.2 Geometric information</h2><p>作者从每一个关键帧中提取ORB 特征点，并与后续的关键帧进行匹配。</p>
<h2 id="3-3-Semantic-information"><a href="#3-3-Semantic-information" class="headerlink" title="3.3 Semantic information"></a>3.3 Semantic information</h2><p>从第t 个关键帧中检测到一组物体 $\mathcal{S}_t$ ，其中每个物体表示为 $\mathbf{s}_k = (s_k^c, s_k^s, s_k^b) \in \mathcal{S}_t$ ，三个参数分别表示所属的物体种类、量化的检测置信度得分，以及bounding box。本文的物体检测使用deformable parts model (DPM) 算法完成，该算法可以在CPU上实时运行。</p>
<p>如果语义观测 $\mathbf{s}_k$ 的数据关联 $\mathcal{D} = \{(\alpha_k, \beta_k)\}$ 已知，<strong>语义观测似然</strong>可以分解为：</p>
<script type="math/tex; mode=display">p(\mathbf{s}_k|x_{\alpha_k},l_{\beta_k})=p(s_k^c|l_{\beta_k}^c)p(s_k^s|l_{\beta_k}^c,s_k^c)p(s_k^b|x_{\alpha_k},l_{\beta_k}^p)</script><p>其中，类别估计概率分布 $p(s_k^c|l_{\beta_k}^c)$ 对应于物体检测的<strong>混淆矩阵</strong>，其与得分分布（置信度）$p(s_k^s|l_{\beta_k}^c,s_k^c)$ 可以经过离线学习获取。而bounding-box 似然 $p(s_k^b|x_{\alpha_k},l_{\beta_k}^p)$ 被假设服从正态分布，其均值为物体中心在成像平面上的透视投影 (???)，协方差与探测的bounding-box 尺寸成比例关系。</p>
<blockquote>
<p>作者将<strong>语义SLAM 问题</strong>表述如下：</p>
<p>给定惯性测量 $\mathcal{I} = \{\mathcal{I}_t\}^T_{t=1}$ ，几何测量 $\mathcal{Y} = \{\mathcal{Y}_t\}^T_{t=1}$ 和语义测量 $\mathcal{S} = \{\mathcal{S}_t\}^T_{t=1}$ ，估计传感器的状态轨迹 $\mathcal{X}$ 以及环境中物体的位置和类别 $\mathcal{L}$ 。</p>
</blockquote>
<p>作者使用惯性和视觉几何测量信息来跟踪<strong>传感器的局部运动轨迹</strong>，不恢复几何结构。而语义测量信息用来构建包含物体信息的地图用以<strong>回环检测</strong>，不仅鲁棒性更好、精确度更高，而且相较于保留全部几何结构信息的SLAM 方法更为高效。</p>
<h1 id="4-Semantic-SLAM-using-EM"><a href="#4-Semantic-SLAM-using-EM" class="headerlink" title="4 Semantic SLAM using EM"></a>4 Semantic SLAM using EM</h1><blockquote>
<p>在优化过程中，作者除了将<strong>数据关联</strong>视为一个隐变量外，也将离散的<strong>路标类别</strong>视为一个隐变量，从而对离散变量和连续变量完成干净且高效的区分。</p>
</blockquote>
<p>几何特征点的数据关联由现有的特征跟踪算法提供，因此，本文提到的隐变量为语义观测信息的数据关联 $\mathcal{D}$  和地标所属类别 $l_{1:M}^c$  。</p>
<p>利用EM 求解语义SLAM 过程如下：</p>
<p><img src="/2024/02/18/bowman2017/f6.png" alt="f6" title="f6"></p>
<p>其中，$\mathbb{D}_t$ 是在时间戳 t 时刻观测数据中所有可能数据关联的组合，$\mathbb{D}_t(i,j)\subseteq \mathbb{D}_t$ 是所有可能的数据关联中观测 $i$ 分配给地标 $j$ 的组合。</p>
<h2 id="Object-classes-and-data-association-E-step"><a href="#Object-classes-and-data-association-E-step" class="headerlink" title="Object classes and data association (E step)"></a>Object classes and data association (E step)</h2><p>一旦每一个<strong>测量-地标匹配对</strong>的权重参数 $w_{kj}^{t,(i)}$ 得到之后，即可用于传感器状态和地标位置的连续优化中。此外，物体类别的最大似然 $l^c$ 也可以通过计算得到的 k 值进行恢复：</p>
<p><img src="/2024/02/18/bowman2017/f7.png" alt="f7" title="f7"></p>
<h2 id="Pose-graph-optimization-M-step"><a href="#Pose-graph-optimization-M-step" class="headerlink" title="Pose graph optimization (M step)"></a>Pose graph optimization (M step)</h2><p>作者使用位姿图进行优化。图包含一组顶点 $\mathcal{V}$ ，每个顶点对应一个优化变量；顶点之间还包含一组因子 $\mathcal{F}$ ，对应损失函数。优化方程如下所示：</p>
<p><img src="/2024/02/18/bowman2017/f8.png" alt="f8" title="f8"></p>
<p>前人的工作使用硬数据关联 a hard data association，在传感器位姿节点和地标节点之间定义一个单独的因子factor；而本文考虑使用soft semantic data association multiple factors，在节点间使用多个因子进行关联：Semantic Factors, Geometric Factors and Inertial Factors。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Semantic Monocular SLAM for Highly Dynamic Environments</title>
    <url>/2024/02/02/brasch2018/</url>
    <content><![CDATA[<p>Brasch, Nikolas, Aljaz Bozic, Joe Lallemand, and Federico Tombari. “Semantic Monocular SLAM for Highly Dynamic Environments.” In <em>2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 393–400. Madrid: IEEE, 2018. <a href="https://doi.org/10.1109/IROS.2018.8593828">https://doi.org/10.1109/IROS.2018.8593828</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者通过结合<strong>基于特征法</strong>与<strong>直接法</strong>，来实现动态环境中的鲁棒性。</p>
<p>本文的贡献：</p>
<ul>
<li>利用语义分割获取场景的语义信息，可检测<strong>潜在动态物体</strong>，并在后续跟踪中避免使用潜在动态物体上的特征点；</li>
<li>提出一个<strong>概率模型</strong>，考虑观测到某个地图点的<strong>所有帧的语义信息</strong>来更新该地图点的语义类别；</li>
<li>除了使用语义信息，还是用<strong>临时动作信息temporal dynamic information</strong> 来判断地图点是否是静态的；</li>
<li>为了实现实时性能，作者设计了一个<strong>高效的在线概率更新方法</strong>；</li>
<li>作者在合成和实际场景中测试了本算法，证明了本算法在高动态场景中可以获取更稳定的结果。</li>
</ul>
<span id="more"></span>
<p><img src="/2024/02/02/brasch2018/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-Probabilistic-Semantic-SLAM"><a href="#3-Probabilistic-Semantic-SLAM" class="headerlink" title="3 Probabilistic Semantic SLAM"></a>3 Probabilistic Semantic SLAM</h1><p>本系统是基于ORB-SLAM 开发的，整体架构如Fig. 2所示，</p>
<p><img src="/2024/02/02/brasch2018/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-Pose-estimation-and-mapping"><a href="#3-1-Pose-estimation-and-mapping" class="headerlink" title="3.1 Pose estimation and mapping"></a>3.1 Pose estimation and mapping</h2><p>鉴于ORB 特征点法和直接法的特点，作者会<strong>优先选择使用ORB 特征点法</strong>；而在特征点不充足的情况下（低纹理特征场景），使用直接法。包含重投影误差 $E_R$ 和光度误差 $E_P$ 的<strong>综合误差项</strong>如下所示：</p>
<p><img src="/2024/02/02/brasch2018/f1.png" alt="f1" title="formula 1"></p>
<p>作者使用带有鲁棒Huber 核的高斯牛顿法来求解非线性最小二乘问题，并使用逆协方差来代表<strong>观测的不确定度</strong>；对于每个新的观测，使用下式进行<strong>协方差传递更新</strong>：</p>
<p><img src="/2024/02/02/brasch2018/f5.png" alt="f5" title="formula 5"></p>
<h2 id="3-2-Probabilistic-outlier-rejection"><a href="#3-2-Probabilistic-outlier-rejection" class="headerlink" title="3.2 Probabilistic outlier rejection"></a>3.2 Probabilistic outlier rejection</h2><p>在更新地图点的位置参数时，由于一些观测较其他观测更为可靠，因此，使用一个概率模型将<strong>观测的方差</strong>作为权重的方法要比所有观测使用相同权重的方法表现更好。作者使用参数 $\phi$ 表示地图点的<strong>内点概率</strong>（静态点的可能），这样，地图点除了位置参数外，还包含深度 d，内点概率 $\phi$ ，以及语义类别 c。</p>
<p>当地图点被观测到时，利用三角化计算深度估计值 $d_i$ ，以及估计的方差 $\tau_i^2$ 。作者用 $CNN(c_k | I_i) \in [0, 1]$ 表示CNN 网络输出结果，表示当前图片 $I_i$ 中该特征点属于类别 $c_k$ 的概率。特征点的<strong>深度观测似然概率</strong>如下所示：</p>
<p><img src="/2024/02/02/brasch2018/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$\alpha_i$ 表示<strong>匹配准确度</strong>；$\bar{x} = (1 - x)$ 。上式的内在机理为：若当前特征点正确匹配，且地图点是静态的，那么 $\alpha_i, \phi$ 接近于1，则深度估计 $d_i$ 近似服从<strong>高斯分布</strong> $\mathcal{N}(\mu, \sigma^2)$ ；若当前匹配错误，或者特征点是动态的，则当前观测被认为服从均匀分布 $\mathcal{U}(a, b)$ ，在平均深度的估计中不提供任何有用的信息。与深度估计类似，作者将<strong>地图点的语义信息</strong>建模为网络输出 $CNN(c_k | I_i)$ 和错误匹配的均匀分布的混合：</p>
<p><img src="/2024/02/02/brasch2018/f7.png" alt="f7" title="formula 7"></p>
<p>这使得地图点可以高效地在线更新，以及在动态和静态之间进行<strong>平滑转换</strong>。</p>
<p>对于特征点的<strong>内点概率与语义类别的关系</strong>，可用Beta 分布来建模：</p>
<p><img src="/2024/02/02/brasch2018/f8.png" alt="f8" title="formula 8"></p>
<p>其中，$A_k, B_k$ 是针对每种类别 k 的固定常数，表示种类 k 属于静态或动态的概率，如，对于汽车，$A_k$ 较低、$B_k$ 较高，意味着汽车具有较高的动态概率；此外，$A_k, B_k$ 还可以作为调整语义观测与深度观测之间的权重参数：较高的$A_k, B_k$ 会给予语义先验较运动先验在内点概率估计中更高的权重。</p>
<p>Fig. 3给出了深度信息、内点概率和语义类别联合概率模型的依赖图：</p>
<p><img src="/2024/02/02/brasch2018/fig3.png" alt="fig3" title="figure 3"></p>
<p>近似推导可得到一个<strong>结合三项的后验概率</strong>：</p>
<p><img src="/2024/02/02/brasch2018/f9.png" alt="f9" title="formula 9"></p>
<p>其中，第一项将深度视为高斯分布；第二项为内点概率服从关于深度观测信息的Beta 分布；第三项为内点概率服从关于语义类别的Beta 分布；$D = \{d_1, …, d_N\}$ 表示深度观测信息；$S = \{s_1, …, s_N\}$ 表示语义观测信息，其中，$s_i = (CNN(c_1|I_i), …, CNN(c_K|I_i))$ 为CNN 网络在K 个类别上的<strong>概率密度</strong>。</p>
<p>当前<strong>内点概率</strong>可通过下式计算得到：</p>
<p><img src="/2024/02/02/brasch2018/f15.png" alt="f15" title="formula 15"></p>
<h2 id="3-3-Real-time-semantic-segmentation"><a href="#3-3-Real-time-semantic-segmentation" class="headerlink" title="3.3 Real-time semantic segmentation"></a>3.3 Real-time semantic segmentation</h2><p>高动态场景中，图片内容会快速发生变化，因此需要在每一帧中提取新的关键点来保持足够多的数量进行跟踪，为了让每个新的地图点得到一致的语义观测，作者对<strong>所有帧</strong>进行语义分割。作者使用<strong>ICNet</strong> 进行语义分割。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 A Monocular-Visual SLAM System with Semantic and Optical-Flow Fusion for Indoor Dynamic Environments</title>
    <url>/2024/03/01/chen2022b/</url>
    <content><![CDATA[<p>Chen, Weifeng, Guangtao Shang, Kai Hu, Chengjun Zhou, Xiyang Wang, Guisheng Fang, and Aihong Ji. “A Monocular-Visual SLAM System with Semantic and Optical-Flow Fusion for Indoor Dynamic Environments.” <em>Micromachines</em> 13, no. 11 (November 17, 2022): 2006. <a href="https://doi.org/10.3390/mi13112006">https://doi.org/10.3390/mi13112006</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文做出的贡献：</p>
<ol>
<li>提出一个基于ORB-SLAM2 的新颖的<strong>单目视觉SLAM 系统</strong>，可在<strong>动态场景</strong>中实现更精确的定位与建图；</li>
<li>对Mask R-CNN 网络进行修改，使其可以在室内动态环境中<strong>更准确</strong>地分割出先验高动态物体；</li>
<li>结合<strong>光流法</strong>和<strong>几何方法</strong>来识别出高动态物体。</li>
</ol>
<span id="more"></span>
<h1 id="3-System-Overview-and-Approach"><a href="#3-System-Overview-and-Approach" class="headerlink" title="3 System Overview and Approach"></a>3 System Overview and Approach</h1><h2 id="3-1-System-Overview"><a href="#3-1-System-Overview" class="headerlink" title="3.1 System Overview"></a>3.1 System Overview</h2><p>系统架构如Fig. 2 所示，建图与回环检测线程与ORB-SLAM2 一致，作者在跟踪线程前增加了一个动态物体检测线程，该线程包含三步：先验动态物体的分割，光流特征跟踪，以及动态特征消除。</p>
<p><img src="/2024/03/01/chen2022b/fig2.png" alt="fig2" title="figure 2"></p>
<p>具体来讲，动态特征剔除过程包括以下三个步骤：</p>
<ol>
<li>Mask R-CNN 网络处理RGB 图片，识别出<strong>先验动态物体</strong>并<strong>移除所属的特征点</strong>，该步骤<strong>同时提取ORB 特征点</strong>；</li>
<li>使用<strong>运动一致性检验</strong>来识别动态特征点：使用光流跟踪来求解基础矩阵，利用对极几何识别出动态特征点，建立动态特征点集合；</li>
<li><strong>动态特征剔除模块</strong>根据识别出的动态特征点判断图像中的物体是否是动态的，然后剔除掉<strong>属于动态物体的ORB 特征</strong>。</li>
</ol>
<h2 id="3-3-Dynamic-Object-Removal"><a href="#3-3-Dynamic-Object-Removal" class="headerlink" title="3.3 Dynamic-Object Removal"></a>3.3 Dynamic-Object Removal</h2><p>动态特征识别过程如Fig. 5所示</p>
<p><img src="/2024/03/01/chen2022b/fig5.png" alt="fig5" title="figure 5"></p>
<h3 id="3-3-1-Optical-Flow-Tracking"><a href="#3-3-1-Optical-Flow-Tracking" class="headerlink" title="3.3.1 Optical-Flow Tracking"></a>3.3.1 Optical-Flow Tracking</h3><p>移除掉属于先验动态物体上的特征点后，对<strong>剩余ORB 特征点</strong>进行光流跟踪，作者使用<strong>LK 光流法</strong>。图像中物体的运动状态可通过对所有像素的光流向量进行分析得到，然而，对于稀疏光流来说，获取到的光流向量不足以完成以上分析，因此，这里作者只用光流法实现对<strong>特征点的跟踪</strong>。光流法如Fig. 6所示，其中 $I_{t1}, I_{t2}, I_{t3}$ 为不同时刻下像素的灰度值。利用LK 光流实现对特征点的跟踪之后，即可解算相邻帧之间的<strong>基础矩阵</strong>。</p>
<p><img src="/2024/03/01/chen2022b/fig6.png" alt="fig6" title="figure 6"></p>
<h3 id="3-3-2-Geometric-Constraints"><a href="#3-3-2-Geometric-Constraints" class="headerlink" title="3.3.2 Geometric Constraints"></a>3.3.2 Geometric Constraints</h3><p>在获取了基础矩阵之后，结合特征点坐标即可根据<strong>对极几何理论</strong>构建极线，若特征点与极线之间的距离大于一定阈值，则判定该特征点属于动态特征点。理想状态下，两个匹配点会服从以下约束：</p>
<p><img src="/2024/03/01/chen2022b/f14.png" alt="f14" title="formula 14"></p>
<p>其中，F 是基础矩阵；$x_i$ 为特征点在两帧图片上的齐次坐标。</p>
<p>考虑到噪声与畸变的影响，式14表示的理想状态基本无法实现，因此求取特征点距离极线的距离作为判断参数：</p>
<p><img src="/2024/03/01/chen2022b/f15.png" alt="f15" title="formula 15"></p>
<p>若距离 D超过给定的阈值，则判定该点不满足对极几何约束。本文中，作者<strong>将不符合对极几何约束的特征点均视为动态特征点</strong>。</p>
<p>Fig. 7展示了本文提出的动态特征检测算法的效果，图1、2中，人搬着箱子移动，通过本文方法判定箱子上的特征点也属于动态特征点，并进行剔除；图3、4箱子被放置在地上之后，其上的特征点又被正确识别为静态特征点。</p>
<p><img src="/2024/03/01/chen2022b/fig7.png" alt="fig7" title="figure 7"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 CFP-SLAM_A Real-time Visual SLAM Based on Coarse-to-Fine Probability in Dynamic Environments</title>
    <url>/2024/03/01/cfp-slam/</url>
    <content><![CDATA[<p>Hu, Xinggang, Yunzhou Zhang, Zhenzhong Cao, Rong Ma, Yanmin Wu, Zhiqiang Deng, and Wenkai Sun. “CFP-SLAM: A Real-Time Visual SLAM Based on Coarse-to-Fine Probability in Dynamic Environments.” In <em>2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 4399–4406. Kyoto, Japan: IEEE, 2022. <a href="https://doi.org/10.1109/IROS47612.2022.9981826">https://doi.org/10.1109/IROS47612.2022.9981826</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者认为当前面向动态场景的语义SLAM 方法虽然可以有效消除动态物体对位姿解算的影响，但很难实现<strong>精度与时间的平衡</strong>，难以实现实时处理，而且这些算法在<strong>低动态场景</strong>中的效果普遍不好。针对这些问题，作者提出了<strong>CFP-SLAM</strong>。</p>
<p>本文的贡献如下：</p>
<ul>
<li>当使用<strong>DBSCAN 聚类算法</strong>来区分bbox 内的前景点和背景点，基于EKF 和 Hungarian 算法来<strong>补偿物体漏检测</strong>；</li>
<li>基于<strong>YOLOv5</strong> 和<strong>几何约束</strong>将物体的运动属性分为高动态和低动态，为后续的处理提供先验信息，以此来提高SLAM 系统的<strong>鲁棒性与适应能力</strong>；</li>
<li>提出一种基于物体静态概率的<strong>关键点静态概率</strong>两阶段计算方法，使用DBSCAN 聚类算法、对极几何约束以及投影约束来解决由非刚性物体局部运动造成的<strong>静态点误检测问题</strong>。</li>
</ul>
<span id="more"></span>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3 System Overview"></a>3 System Overview</h1><h2 id="3-2-System-Architecture"><a href="#3-2-System-Architecture" class="headerlink" title="3.2 System Architecture"></a>3.2 System Architecture</h2><p>CFP-SLAM 系统架构如Fig. 1所示，该系统是基于ORB-SLAM2 开发实现的。</p>
<p><img src="/2024/03/01/cfp-slam/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="4-Specific-Implementation"><a href="#4-Specific-Implementation" class="headerlink" title="4 Specific Implementation"></a>4 Specific Implementation</h1><h2 id="4-1-Missed-Detection-Compensation-Algorithm"><a href="#4-1-Missed-Detection-Compensation-Algorithm" class="headerlink" title="4.1 Missed Detection Compensation Algorithm"></a>4.1 Missed Detection Compensation Algorithm</h2><p>稳定准确的语义信息至关重要，漏检物体会带来以下影响：</p>
<ol>
<li>影响后续基于<strong>语义先验信息</strong>处理动态物体的方法的正常运行；</li>
<li>在动态场景中，动态物体的突然出现会导致邻接帧之间<strong>错误匹配</strong>的特征点数量猛然提高，可能会导致SLAM 算法<strong>跟踪失败</strong>。</li>
</ol>
<p>本系统使用<strong>EKF 和 Hungarian 算法</strong>来补偿潜在动态物体的<strong>漏检测</strong>：利用EKF 来<strong>预测</strong>当前帧中的潜在动态物体bbox，Hungarian 算法将EKF 预测的bbox 与YOLOv5 的检测结果<strong>进行关联</strong>，若预测框没有找到一个关联的检测框，则认为出现了漏检测，采用EKF 预测的框作为漏检测的框。然后，EKF 和Hungarian 算法进一步被用于帧间bbox 内的<strong>数据关联</strong>。</p>
<h2 id="4-2-Static-Probability-of-Objects"><a href="#4-2-Static-Probability-of-Objects" class="headerlink" title="4.2 Static Probability of Objects"></a>4.2 Static Probability of Objects</h2><p>本系统提取并跟踪邻接帧之间的<strong>光流点对</strong>，并使用潜在动态物体bbox <strong>外部的光流点对</strong>计算<strong>基础矩阵</strong>；然后计算光流点对的polar error，并使用卡方分布计算<strong>物体的静态概率</strong>；作者设定阈值为0.9，根据物体的静态概率分为高动态物体和低动态物体，然后将潜在动态物体bbox 内的所有特征点的静态概率<strong>初始化</strong>为物体的静态概率，图片中其余特征点的静态概率设置为1。</p>
<h2 id="4-3-Static-Probability-of-Keypoints-in-the-First-Stage"><a href="#4-3-Static-Probability-of-Keypoints-in-the-First-Stage" class="headerlink" title="4.3 Static Probability of Keypoints in the First Stage"></a>4.3 Static Probability of Keypoints in the First Stage</h2><h2 id="4-3-1-DBSCAN-Density-Clustering-Algorithm"><a href="#4-3-1-DBSCAN-Density-Clustering-Algorithm" class="headerlink" title="4.3.1 DBSCAN Density Clustering Algorithm"></a>4.3.1 DBSCAN Density Clustering Algorithm</h2><p>物体检测算法无法提供物体<strong>精确的掩码</strong>，特别是对于非刚性物体（如人类），检测框中会有很多的<strong>背景点</strong>。作者认为人表面上特征点的<strong>深度连续性</strong>很好，因此，作者使用<strong>DBSCAN 聚类算法</strong>来区分bbox 内的前景点和背景点，根据集群组的数量，作者将几组具有较小深度的集群组视为前景，该策略使得DBSCAN 聚类算法有着较强的<strong>鲁棒性</strong>。在识别出潜在动态物体bbox 内的前景点和背景点后，作者使用一个soft strategy 来进一步估计<strong>背景点的静态概率</strong>：</p>
<p><img src="/2024/03/01/cfp-slam/f1.png" alt="f1" title="formula 1"></p>
<p>当前阶段特征点的静态概率还没有进行严格计算，属于比较粗糙的状态，此时的相机位姿估计容易受到动态点的影响，所以作者将此时<strong>高动态物体</strong>bbox 内<strong>前景点</strong>的静态概率设为0。</p>
<h3 id="4-3-2-First-Stage-Pose-Optimization"><a href="#4-3-2-First-Stage-Pose-Optimization" class="headerlink" title="4.3.2 First Stage Pose Optimization"></a>4.3.2 First Stage Pose Optimization</h3><p>更新<strong>特征点的静态概率</strong>：</p>
<p><img src="/2024/03/01/cfp-slam/f2.png" alt="f2" title="formula 2"></p>
<p>SLAM 系统初始化时，将地图点的静态概率初始化为对应特征点的静态概率；在初始化之后，帧间的<strong>位姿转换矩阵</strong>在第一阶段获取到，然后特征点的静态概率可进一步基于<strong>投影约束</strong>和<strong>对极约束</strong>实现精确计算。</p>
<h2 id="4-4-Static-Probability-of-Keypoints-in-the-Second-Stage"><a href="#4-4-Static-Probability-of-Keypoints-in-the-Second-Stage" class="headerlink" title="4.4 Static Probability of Keypoints in the Second Stage"></a>4.4 Static Probability of Keypoints in the Second Stage</h2><h3 id="4-4-1-Static-Probability-Based-on-the-Projection-Constraints"><a href="#4-4-1-Static-Probability-Based-on-the-Projection-Constraints" class="headerlink" title="4.4.1 Static Probability Based on the Projection Constraints"></a>4.4.1 Static Probability Based on the Projection Constraints</h3><p>利用初始位姿将上一帧的特征点投影至当前帧中，计算投影点至匹配点之间的<strong>欧氏距离</strong>，记为<strong>投影偏差距离</strong> $d_i^T$ ；统计当前帧中动态物体bbox 外所有特征点的投影偏差距离并按从小到大进行排序，以0.8处的偏差距离作为<strong>自适应阈值</strong> $d_{TH}^T$ ，同时记录最小值 $d_{min}^T$ ，作者使用Sigmoid 函数形式来计算bbox 内匹配点的<strong>静态概率</strong>：</p>
<p><img src="/2024/03/01/cfp-slam/f5.png" alt="f5" title="formula 5"></p>
<p>其中，参数 m 表示Sigmoid 函数的间隔。</p>
<p>对于一对匹配点，投影约束的满足不仅与<strong>静态环境的假设</strong>有关，也与<strong>位姿转换矩阵的求解</strong>正确与否有关。因此，作者计算位姿转换矩阵的<strong>统计置信度和计算置信度</strong>：</p>
<p><img src="/2024/03/01/cfp-slam/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$N_{BA}$ 为参与上帧位姿求解的<strong>内点数量</strong>；$Th_{BA}$ 为参与位姿解算的<strong>最小内点数量要求</strong>；$N_T$ 为<strong>所有采样点的数量</strong>；$\sum d_i^T$ 表示小于阈值的所有<strong>投影偏差距离之和</strong>。</p>
<h3 id="4-4-2-Static-Probability-Based-on-the-Epipolar-Constraints"><a href="#4-4-2-Static-Probability-Based-on-the-Epipolar-Constraints" class="headerlink" title="4.4.2 Static Probability Based on the Epipolar Constraints"></a>4.4.2 Static Probability Based on the Epipolar Constraints</h3><p>基于第一阶段的位姿估计，可以得到<strong>更为准确</strong>的基础矩阵：</p>
<p><img src="/2024/03/01/cfp-slam/f8.png" alt="f8" title="formula 8"></p>
<p>值得注意的是，上式的计算过程要求相机的平移应该要<strong>足够大</strong>，否则无法计算基础矩阵。由此，可以计算polar error：</p>
<p><img src="/2024/03/01/cfp-slam/f10.png" alt="f10" title="formula 10"></p>
<p>与上文类似的，可以基于对极约束计算相应的<strong>静态概率及置信度</strong> $K_i^{Fk}, C_s^{Fk}, C_c^{Fk}$ ；若相机的平移未超过设定的阈值 $t_{Th}$ ，则有：</p>
<p><img src="/2024/03/01/cfp-slam/f11.png" alt="f11" title="formula 11"></p>
<h3 id="4-4-3-Second-Stage-Pose-Optimization"><a href="#4-4-3-Second-Stage-Pose-Optimization" class="headerlink" title="4.4.3 Second Stage Pose Optimization"></a>4.4.3 Second Stage Pose Optimization</h3><p>在使用投影约束和对极几何约束后，更新当前帧与上一帧匹配点的静态概率；根据动态物体的移动概率，进行如下的<strong>特征点静态概率更新</strong>：</p>
<p>若物体静态概率<strong>低于阈值</strong>，则特征点静态概率更新为：</p>
<p><img src="/2024/03/01/cfp-slam/f12.png" alt="f12" title="formula 12"></p>
<p>若物体静态概率<strong>高于阈值</strong>，则特征点静态概率更新为：</p>
<p><img src="/2024/03/01/cfp-slam/f13.png" alt="f13" title="formula 13"></p>
<p>最后利用低动态特征点和地图点实现对相机位姿的<strong>第二次优化</strong>。</p>
<h1 id="5-Experiments-and-Results"><a href="#5-Experiments-and-Results" class="headerlink" title="5 Experiments and Results"></a>5 Experiments and Results</h1><p><img src="/2024/03/01/cfp-slam/t1.png" alt="t1" title="table 1"></p>
<p><img src="/2024/03/01/cfp-slam/t5.png" alt="t5" title="table 5"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>C++ 将整型参数传递至argv[]中</title>
    <url>/2024/03/12/cpp-notes-01/</url>
    <content><![CDATA[<p>本文参考<a href="https://blog.csdn.net/Blunt_Du/article/details/111621039">文章1</a>、<a href="https://blog.csdn.net/MM_man/article/details/103475684">文章2</a>。</p>
<p>对于C++程序，主程序<code>int main(int argc, char *argv[])</code>中，参数含义如下所示：</p>
<ul>
<li><code>argc</code>代表向main函数传入参数的数量；</li>
<li><code>argv[]</code>代表传入的字符串构成的数组，默认<code>argv[0]</code>表示程序的名称，后续的<code>argv[1]</code>、<code>argv[2]</code>…表示传入的字符串数量。</li>
</ul>
<p>由于<code>argv[]</code>是字符串数组，若想传入整型参数则需要进行额外操作：</p>
<ul>
<li>包含头文件 <code>#include &lt;stdlib.h&gt;</code>;</li>
<li><code>int i = atoi(argv[1]);</code>：将字符型<code>argv[1]</code>转换为整型并赋值给 <code>i</code>。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;opencv2/opencv.hpp&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span> *argv[])</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">int</span> rows = <span class="built_in">atoi</span>(argv[<span class="number">1</span>]);</span><br><span class="line">    <span class="type">int</span> cols = <span class="built_in">atoi</span>(argv[<span class="number">2</span>]);</span><br><span class="line">    ...</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>随记</category>
        <category>C++</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>C++</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Towards semantic SLAM using a monocular camera</title>
    <url>/2024/01/30/civera2011/</url>
    <content><![CDATA[<p>Civera, J., D. Galvez-Lopez, L. Riazuelo, J. D. Tardos, and J. M. M. Montiel. “Towards Semantic SLAM Using a Monocular Camera.” In <em>2011 IEEE/RSJ International Conference on Intelligent Robots and Systems</em>, 1277–84. San Francisco, CA: IEEE, 2011. <a href="https://doi.org/10.1109/IROS.2011.6094648">https://doi.org/10.1109/IROS.2011.6094648</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文提出了一种语义SLAM 算法，本算法将传统<strong>无意义的特征点</strong>和<strong>物体</strong>融合进到地图中。</p>
<p>本算法融合了三个不同领域的SOTA：</p>
<ol>
<li>一个<strong>EKF 单目SLAM</strong> 提供相机位姿的在线实时估计，以及包含点特征的稀疏地图；</li>
<li>利用 <strong>Structure from Motion (SfM)</strong> 从稀疏图片中计算一个物体模型数据库；</li>
<li><strong>视觉识别</strong>来检测图片流中物体是否存在。</li>
</ol>
<span id="more"></span>
<h1 id="3-Notation-and-General-Overview"><a href="#3-Notation-and-General-Overview" class="headerlink" title="3 Notation and General Overview"></a>3 Notation and General Overview</h1><p>本算法分为两个支流：</p>
<ol>
<li>单目SLAM；</li>
<li>物体识别（通常要慢于SLAM支流）。</li>
</ol>
<p>本算法的整体视图如Fig. 1所示：</p>
<p><img src="/2024/01/30/civera2011/fig1.png" alt="fig1" title="figure 1"></p>
<p>单目SLAM 利用图片从<strong>k-m-1 到 k-m</strong>更新几何状态向量 x，而视觉识别结果会在<strong>步骤k</strong> 处得到，但是物体插入应当是相对于输入图片 $I_{k-m}$ 而言的。</p>
<p>作者提前对每一个想要识别的物体进行建模，这些模型包含<strong>外观和几何信息</strong>：</p>
<ul>
<li>外观信息是由<strong>SURF 描述子</strong>组成的；</li>
<li>几何信息是这些<strong>SURF 特征点的3D 位置</strong>信息。</li>
</ul>
<p>物体识别支流将数据库中的物体插入到SLAM 地图的步骤如下：</p>
<ol>
<li>算法搜寻图片 $I_{k-m}$ 中SURF 特征点与数据库中每一个物体之间的关联；</li>
<li>利用RANSAC 来计算每个物体的<strong>一致几何模型</strong>，来最大化关联数量；</li>
<li>如果存在足够的一致关联，那么这个特征点就被认为属于该物体，并插入SLAM 地图中；</li>
<li>被插入地图中的特征点会被持续跟踪与位置优化。</li>
</ol>
<h1 id="4-Object-Model"><a href="#4-Object-Model" class="headerlink" title="4 Object Model"></a>4 Object Model</h1><p>如上文所述，物体模型包含由SURF 特征描述子构成的<strong>外观信息</strong>和特征点位置<strong>几何信息</strong>，其构建示例如Fig. 2所示：</p>
<p><img src="/2024/01/30/civera2011/fig2.png" alt="fig2" title="figure 2"></p>
<p>图中的黄色圆圈代表用来进行识别的SURF 特征。</p>
<p>物体的构建是通过不同的faces 完成的，而构建模型的<strong>每一幅图片</strong>是一个face 的<strong>基础</strong>。作者用一个tuple F 来表示face，该tuple中包含了SURF 特征点的位置坐标和描述子，以及形成该face 的图片的位置信息和朝向信息。</p>
<h1 id="5-Object-Recognition"><a href="#5-Object-Recognition" class="headerlink" title="5 Object Recognition"></a>5 Object Recognition</h1><p>物体识别是通过计算图片 $I_{k-m}$ 与物体数据库中每个物体的关联来实现的：</p>
<ol>
<li>对于每个物体而言，计算图片 $I_{k-m}$ 与属于该物体的faces F之间的关联；</li>
<li>然后使用RANSAC 进行外点剔除；</li>
<li>最终至少包含5对关系才能确认为图片 $I_{k-m}$ 与物体 $I_F$ 之间建立了联系；</li>
<li>利用PnP 来估计相对位姿转换关系。</li>
</ol>
<h1 id="6-Monocular-SLAM"><a href="#6-Monocular-SLAM" class="headerlink" title="6 Monocular SLAM"></a>6 Monocular SLAM</h1><h2 id="6-1-Standard-Mode-EKF"><a href="#6-1-Standard-Mode-EKF" class="headerlink" title="6.1 Standard Mode EKF"></a>6.1 Standard Mode EKF</h2><p>估计参数建模为多维高斯变量x，包含相机的运动参数以及n 个特征点：</p>
<p><img src="/2024/01/30/civera2011/f1.png" alt="f1" title="formula 1"></p>
<p>其中，运动参数 $\mathbf{x}_{C_k}$ 包含相机的位置参数 $\mathbf{t}_{C_{k-m}}$ 和方向参数 $\mathbf{q}_{C_{k-m}}$ ，以及线速度和角速度。</p>
<h2 id="6-2-State-Augmentation-with-Past-Camera-Pose"><a href="#6-2-State-Augmentation-with-Past-Camera-Pose" class="headerlink" title="6.2 State Augmentation with Past Camera Pose"></a>6.2 State Augmentation with Past Camera Pose</h2><p>当<strong>物体识别支流</strong>在步骤k-m 开始时，SLAM的状态向量需要使用当前步骤的<strong>相机位姿进行增强</strong>：将当前步骤的位姿参数复制到状态向量中去，并传播相应的协方差矩阵。假设 k 步完成了物体识别与插入操作，那么k-1 步骤的状态向量表示如下：</p>
<p><img src="/2024/01/30/civera2011/f2.png" alt="f2" title="formula 2"></p>
<p>如果物体识别成功，则过去的相机位姿（本例中为步骤 k-m）被用来进行物体的<strong>延迟初始化</strong>。在此完成之后，用来增强的相机位姿不再需要，即可从状态向量中进行剔除。</p>
<h2 id="6-3-Object-Insertion"><a href="#6-3-Object-Insertion" class="headerlink" title="6.3 Object Insertion"></a>6.3 Object Insertion</h2><p><strong>物体识别支流的输出</strong>是一组物体参考坐标系中的特征点 $\mathbf{y}_F^O$ ，并由此计算face F 与k-m 时刻相机之间的位姿转换关系  $\mathbf{t}^{C_{k-m}}_F, \mathbf{q}^{C_{k-m}}_F$  ；基于此并利用6.2节中的k-m 步骤的增强相机位姿，实现物体特征点到SLAM 地图的插入操作：</p>
<p><img src="/2024/01/30/civera2011/f5.png" alt="f5" title="formula 5"></p>
<p>因为物体模型中点的3D 位置信息是已知的，因此该模型的所有点可以根据 $\mathbf{y}_F^W$ 结合欧式坐标添加进SLAM 地图中。</p>
<p>在完成插入操作后，物体点被跟踪并利用单目SLAM 算法进行位姿优化。</p>
<h1 id="8-Conclusions-and-Future-Works"><a href="#8-Conclusions-and-Future-Works" class="headerlink" title="8 Conclusions and Future Works"></a>8 Conclusions and Future Works</h1><p>本文提出的算法只使用相机作为传感器，是第一个将普通3D 物体<strong>实时加入几何SLAM 地图</strong>中的算法。</p>
<p>作者提到本算法将<strong>相机运动和3D 场景理解</strong>结合了起来，提供了部分标注的地图和相机位姿，方便后期机器人的任务开发（如抓住某个物体等）；此外，基于提前构建的物体3D 模型库，可以实现在观察到物体某一面之后即可将整个3D 物体添加进地图中去，完成对于未观测信息的补充。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 CubeSLAM_Monocular 3-D Object SLAM</title>
    <url>/2024/01/31/cubeslam/</url>
    <content><![CDATA[<p>Yang, Shichao, and Sebastian Scherer. “CubeSLAM: Monocular 3-D Object SLAM.” <em>IEEE Transactions on Robotics</em> 35, no. 4 (August 2019): 925–38. <a href="https://doi.org/10.1109/TRO.2019.2909168">https://doi.org/10.1109/TRO.2019.2909168</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文作者提出了一个可以同时应用于<strong>静态与动态场景</strong>中，将2D、3D 物体检测与SLAM 位姿估计相结合的系统，如Fig. 1所示。前提假设：物体立方体经投影后会与2D bbox 相吻合；在此假设下，当给定检测到的2D 物体时，作者利用vanishing point (VP) 产生3D 立方体 proposals，然后通过<strong>多视角BA</strong> ，利用点和相机对立方体进行进一步的优化。</p>
<span id="more"></span>
<p><img src="/2024/01/31/cubeslam/fig1.png" alt="fig1" title="figure 1"></p>
<p>物体在本系统中的作用有两个方面：</p>
<ol>
<li>在BA 过程中提供<strong>几何和尺度约束</strong>；</li>
<li>为难以进行三角测量的点提供<strong>深度初始化信息</strong>。</li>
</ol>
<p>在动态场景中，本系统不是简单地将动态观测物体视为外点，而是基于<strong>动态点观测</strong>与<strong>运动模型约束</strong>，联合优化相机与物体的轨迹。</p>
<p>本文做出的贡献总结如下：</p>
<ul>
<li>不使用先验物体模型的高效、准确以及鲁棒的<strong>单张图片3D 物体检测方法</strong>；</li>
<li>提出了一种在相机、物体与点之间使用新颖的观测手段的<strong>object SLAM 方法</strong>，在许多数据集上实现了更好的位姿估计；</li>
<li>实验结果证明<strong>物体检测</strong>与<strong>SLAM</strong> 两者相互促进；</li>
<li>一个在动态场景中<strong>利用移动物体来提高位姿估计</strong>的方法。</li>
</ul>
<h1 id="3-Single-Image-3-D-Object-Detection"><a href="#3-Single-Image-3-D-Object-Detection" class="headerlink" title="3 Single Image 3-D Object Detection"></a>3 Single Image 3-D Object Detection</h1><h2 id="3-1-3D-Box-Proposal-Generation"><a href="#3-1-3D-Box-Proposal-Generation" class="headerlink" title="3.1 3D Box Proposal Generation"></a>3.1 3D Box Proposal Generation</h2><h3 id="3-1-1-3D-Box-Proposal-Generation"><a href="#3-1-1-3D-Box-Proposal-Generation" class="headerlink" title="3.1.1 3D Box Proposal Generation"></a>3.1.1 3D Box Proposal Generation</h3><p>一个通用的3D 立方体有9自由度：位置 $\mathbf{t} = [t_x, t_y, t_z]$ ，旋转矩阵 R，以及三维尺度 $\mathbf{d} = [d_x, d_y, d_z]$ 。前提假设：立方体的角在图像上的投影与2D bbox 紧密重合。在该假设下，2D bbox只能提供<strong>4个约束</strong>（4条边），无法构建9自由度的立方体。已有文献利用预先提供或预测的尺度信息和物体朝向信息作为额外的约束，作者选择使用<strong>消失点 VP</strong> 来减少回归参数，以适用于通用物体。</p>
<p>消失点 VP 是平行线在<strong>透视图像</strong>上的交点，3D 立方体有3个正交轴，因此经过投影可形成3个VP，投影过程依赖于相对于相机坐标系的<strong>旋转矩阵 R</strong> 和<strong>相机内参 K</strong>：</p>
<p><img src="/2024/01/31/cubeslam/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$R_{col(i)}$ 表示R 的第 i 列。</p>
<h3 id="3-1-2-Get-2D-Corners-From-the-VP"><a href="#3-1-2-Get-2D-Corners-From-the-VP" class="headerlink" title="3.1.2 Get 2D Corners From the VP"></a>3.1.2 Get 2D Corners From the VP</h3><p>对于一个立方体，单一视角下最多可同时观测到其3个面，基于观测到的面数量，作者分为三组情况，如Fig. 2所示。</p>
<p><img src="/2024/01/31/cubeslam/fig2.png" alt="fig2" title="figure 2"></p>
<p>作者以Fig. 2（a）的情况说明，如何基于VP 获取立方体的8个2D 角点：假设VP 和角点 $p_1$ 已知或已被估计，则有 $p_2 = (VP_1, p_1) \times (B, C)$ ， $p_4 = (VP_2, p_1) \times (A, D)$ ， $p_3 = (VP_1, p_4) \times (VP_2, p_2)$ ，其余2D 角点均可通过类似方法获取，其中 $\times$ 表示两直线的交点。</p>
<h3 id="3-1-3-Get-3D-Box-Pose-From-2D-Corners"><a href="#3-1-3-Get-3D-Box-Pose-From-2D-Corners" class="headerlink" title="3.1.3 Get 3D Box Pose From 2D Corners"></a>3.1.3 Get 3D Box Pose From 2D Corners</h3><p>在获取了2D 角点之后，需要进一步获取立方体的3D 角点，作者分两种情况进行讨论：</p>
<p><strong>情况一 任意位姿的物体：</strong>作者使用PnP solver 来求解通用立方体的3D 位置和维度（缺少尺度参数，因为是单目相机）。在物体坐标系（原点设在物体中心）中，3D 立方体的8个顶点坐标可表示为 $[\pm d_x, \pm d_y, \pm d_z]/2$ ，则2D 角点 $p_1$ 可由相应的3D 角点通过下式获取：</p>
<p><img src="/2024/01/31/cubeslam/f2.png" alt="f2" title="formula 2"></p>
<p>其中，$\pi$ 表示相机投影方程。则式2可提供两个约束（2D 坐标分别代表两个约束方程），那么，选取4个邻接角点（如1，2，4，7）即可得到全约束方程（除了尺度参数）。</p>
<p><strong>情况二 位于地面上的物体：</strong>对于位于地面上的物体，可以大幅简化情况一的求解过程并获取<strong>尺度参数</strong>。在地平面上构建世界坐标系，则物体的roll/pitch 角均为0，这里不再使用复杂的PnP solver，而是直接将8个2D 角点逆投影至3D 地平面，然后计算其他垂直角，从而组成一个3D 立方体；该方法<strong>高效且有解析表达式</strong>，如位于3D 地平面上的角点5，使用 $[\mathbf{n}, m]$ 来表示（相机坐标系的法向量和距离），则3D 角点 $P_5$ 可通过求<strong>逆投影线</strong> $K^{-1}p_5$ 与地平面的交点获取：</p>
<p><img src="/2024/01/31/cubeslam/f3.png" alt="f3" title="formula 3"></p>
<p>尺度参数由投影过程中<strong>相机的高度</strong>来决定。</p>
<h3 id="3-1-4-Sample-VP-and-Summary"><a href="#3-1-4-Sample-VP-and-Summary" class="headerlink" title="3.1.4 Sample VP and Summary"></a>3.1.4 Sample VP and Summary</h3><p>经过上述分析，3D 立方体的估计问题变为了如何获取三个VPs 以及一个 2D 角点。从式1可得，VPs 通过物体旋转矩阵获取，尽管可通过深度学习来预测物体的旋转矩阵，但是为了<strong>更好的泛化性</strong>，作者选择<strong>手动采样旋转矩阵</strong>，并对它们进行<strong>评分</strong>。本文只考虑位于地面上的物体，因此只需要相机的roll/pitch 角和物体的yaw 角，对于多视角视频数据，作者使用SLAM 来估计相机位姿，因此使得采样空间大幅减小且更为准确。</p>
<h2 id="3-2-Proposal-Scoring"><a href="#3-2-Proposal-Scoring" class="headerlink" title="3.2 Proposal Scoring"></a>3.2 Proposal Scoring</h2><p>在获取了许多立方体proposals 后，作者定义损失函数来进行评分，如Fig. 3所示。</p>
<p><img src="/2024/01/31/cubeslam/fig3.png" alt="fig3" title="figure 3"></p>
<p>作者提出了不同的损失函数，如语义分割、edge distance、HOG 特征等，最终，作者使用了将立方体和图片边特征进行对齐的快速有效的损失函数，该方法对于具有清晰边缘的物体效果较好，但是由于VP 和鲁棒边滤波的约束，使得其对于自行车、马桶等效果也不差。</p>
<p>将图片记为 $I$ ，立方体表示为 $O = \{R, \mathbf{t}, \mathbf{d}\}$ ，损失函数表示为：</p>
<p><img src="/2024/01/31/cubeslam/f4.png" alt="f4" title="formula 4"></p>
<p>其中，$\phi_{dist},\phi_{angle},\phi_{shape}$ 代表三个损失项，$w_1=0.8, w_2=1.5$ 表示权重参数。</p>
<p><strong>3.2.1 Distance Error</strong> $\phi_{dist} = (O, I)$</p>
<p>作者认为，立方体的2D 边应该与实际图片中的边相吻合；首先检测Canny 边，并基于此构建一个distance transform map；然后对立方体的每个可见边（Fig. 2（a）中的蓝色实线边）均匀采样10个点，归纳所有的距离地图值并除以2D bbox 的对角线。</p>
<p><strong>3.2.2 Angle Alignment Error</strong> $\phi_{angle}(O, I)$</p>
<p>distance error 容易受到噪声影响，如物体表面的纹理会被识别成边，因此，作者也检测长线段（Fig. 3 中的绿线），并测量它们的角度是否与VP 对齐；首先基于点线关系将这些线与三个VPs 中的一个相关联，对于每个VP，可以找到最外侧的两个线段（分别具有最大、最小的坡度slope），分别记为 $<l_{i-ms}, l_{i-mt}>,<l_{i-ns}, l_{i-nt}>$。其中，$<a, b>$ 表示两顶点 a，b 组成的线段的坡度角。最终，角度对齐误差表示为：</a,></l_{i-ns},></l_{i-ms},></p>
<p><img src="/2024/01/31/cubeslam/f5.png" alt="f5" title="formula 5"></p>
<p><strong>3.2.3 Shape Error</strong> $\phi_{shape}(O)$</p>
<p>同一组2D 角点也许会生成相当不同的3D 立方体，因此，作者对具有较大长宽比的立方体增加了一个惩罚项：</p>
<p><img src="/2024/01/31/cubeslam/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$s = max(d_x/d_y, d_y/d_x)$ 表示物体长宽比；阈值 $\sigma$ 设为1；上式表示若物体长宽比为1，则该惩罚项为0。</p>
<h1 id="4-Object-SLAM"><a href="#4-Object-SLAM" class="headerlink" title="4 Object SLAM"></a>4 Object SLAM</h1><p>作者将单图片3D 物体检测扩展至object SLAM中对物体和相机位姿进行联合优化，系统基于ORB-SLAM2 搭建的，包括前端相机跟踪和后端BA，作者主要对BA 部分进行修改以包含物体、特征点和相机位姿；值得注意的是，本节内容只使用静态物体。</p>
<h2 id="4-1-BA-Formulation"><a href="#4-1-BA-Formulation" class="headerlink" title="4.1 BA Formulation"></a>4.1 BA Formulation</h2><p>作者提到，除了使用物体进行约束外，还是用了特征点，这是因为<strong>仅靠物体难以完整约束相机位姿</strong>。相机位姿、3D 立方体和特征点集合分别表示为：$C = \{C_i\}, O = \{O_j\}, P = \{P_k\}$ ，则BA 可表示为以下的非线性最小二乘优化问题：</p>
<p><img src="/2024/01/31/cubeslam/f7.png" alt="f7" title="formula 7"></p>
<h2 id="4-2-Measurement-Errors"><a href="#4-2-Measurement-Errors" class="headerlink" title="4.2 Measurement Errors"></a>4.2 Measurement Errors</h2><h3 id="4-2-1-Camera-Object-Measurement"><a href="#4-2-1-Camera-Object-Measurement" class="headerlink" title="4.2.1 Camera-Object Measurement"></a>4.2.1 Camera-Object Measurement</h3><p>作者提供两种观测误差：</p>
<p><strong>3D Measurements</strong></p>
<p>当3D 物体检测较准确时（如使用RGB-D 相机时）使用3D 观测误差。从单张图片检测到的<strong>物体位姿</strong> $O_m = (T_{om}, \mathbf{d}_m)$ 作为<strong>相机坐标系下</strong>的物体观测信息，然后将物体地标转换至相机坐标系以计算观测误差：</p>
<p><img src="/2024/01/31/cubeslam/f8.png" alt="f8" title="formula 8"></p>
<p>其中，log 将SE3 误差映射至 6 自由度的切向量空间，因此 $e_{co-3D} \in \mathbb{R}^9$ 。</p>
<p>值得注意的是，由于没有先验物体模型，本文使用的基于图片的立方体检测不能区分物体的前后方向，因此，对于式8，需要沿着物体z 轴旋转 $0, \pm90^o, 180^o$ 寻找最小的误差值。</p>
<p><strong>2D Measurements</strong></p>
<p>对于2D 观测，作者将物体地标投影至图片坐标系中以获取2D bbox，如Fig. 4（b）中的红色长方形表示，然后与2D 物体检测bbox （蓝色长方形）进行比较。具体来讲，将3D 立方体的8个角点投影至图像坐标系中，寻找xy 坐标轴的极值点组成一个长方形。</p>
<p><img src="/2024/01/31/cubeslam/f9.png" alt="f9" title="formula 9"></p>
<p>其中，$c, s$ 分别表示2D bbox 的中心坐标和长宽尺寸；因此，4维矩形误差表示为：</p>
<p><img src="/2024/01/31/cubeslam/f10.png" alt="f10" title="formula 10"></p>
<p>该观测误差较式8表示的3D 观测误差有着小得多的不确定性，因为2D 物体检测相较于3D 检测<strong>通常更准确</strong>；但同时也丢失了不少信息，因为许多不同的3D 立方体可以投影为相同的2D 矩形，因此需要更多观测信息来约束相机位姿和立方体。</p>
<p>作者表示，由于<strong>复杂的检测过程</strong>，对误差协方差矩阵 $\sum$ 或者Hessian 矩阵 W 的建模和估计相较于特征点来说要更难，因此，作者对高语义置信度和几何相似物体赋予<strong>更高的权重</strong>，假设立方体与相机之间距离为 d，物体检测概率为 p，则对于KITTI 数据集设定以下权重参数：$w = p \times max(70-d, 0)/50$ ，其中，70m是截断距离，该参数对不同的数据集会有不同的取值。</p>
<h3 id="4-2-2-Object-Point-Measurement"><a href="#4-2-2-Object-Point-Measurement" class="headerlink" title="4.2.2 Object-Point Measurement"></a>4.2.2 Object-Point Measurement</h3><p>特征点与物体可以相互提供约束，如果点 P 属于某个物体，如Fig. 4（b）所示，则其应该位于3D 立方体内部。作者首先将特征点转换至立方体坐标系中，然后与立方体尺寸进行比较来获取3D 误差：</p>
<p><img src="/2024/01/31/cubeslam/f11.png" alt="f11" title="formula 11"></p>
<h3 id="4-2-3-Camera-Point-Measurement"><a href="#4-2-3-Camera-Point-Measurement" class="headerlink" title="4.2.3 Camera-Point Measurement"></a>4.2.3 Camera-Point Measurement</h3><p>使用标准的3D 点重投影误差作为相机-特征点观测误差：</p>
<p><img src="/2024/01/31/cubeslam/f12.png" alt="f12" title="formula 12"></p>
<p>其中，$z_m$ 是3D 点P 的观测像素坐标。</p>
<p><img src="/2024/01/31/cubeslam/fig4.png" alt="fig4" title="figure 4"></p>
<h2 id="4-3-Data-Association"><a href="#4-3-Data-Association" class="headerlink" title="4.3 Data Association"></a>4.3 Data Association</h2><p>作者提出了一种<strong>基于特征点匹配的物体关联</strong>，数据关联的步骤如下：</p>
<ol>
<li>首先，如果特征点在至少两帧图片中被观测到位于2D 物体检测框中，且它们距离3D 的立方体中心小于1m 时，将这些<strong>特征点与对应的物体关联起来</strong>；</li>
<li>如果两个物体在不同帧中拥有最多数量的共同特征点，且点的数量超过一定阈值（本文设定为10）时，判定两个物体为同一物体并进行数据关联。</li>
<li>需要注意的是，当物体几乎没有与其他帧相关联的特征点时，将该物体<strong>判定为动态物体</strong>，如Fig. 5中的青色框汽车。</li>
</ol>
<p><img src="/2024/01/31/cubeslam/fig5.png" alt="fig5" title="figure 5"></p>
<h1 id="5-Dynamic-SLAM"><a href="#5-Dynamic-SLAM" class="headerlink" title="5 Dynamic SLAM"></a>5 Dynamic SLAM</h1><p>为了减少未知参量让问题可解，作者使用了两个假设：<strong>刚体假设</strong>与<strong>物理运动模型假设</strong>。</p>
<ul>
<li>刚体假设是为了保证特征点在关联物体上的位置保持不变，这使得作者可以使用标准的3D <strong>地图点重投影误差</strong>来优化位置参数；</li>
<li>最简单的运动模型就是<strong>匀速运动模型</strong>，至于一些特定物体，如自行车，会额外受限于nonholonomic wheel model。</li>
</ul>
<h2 id="5-1-Notations"><a href="#5-1-Notations" class="headerlink" title="5.1 Notations"></a>5.1 Notations</h2><p>作者使用 $^jO^i$ 表示动态物体 $O^i$ 在观测帧 j 中的位姿；对于动态物体 $O^i$ 上的动态特征点 $P^k$ 表示为 $^iP^k$ （物体坐标系），基于刚体假设，其在物体上的位置是固定的。</p>
<h2 id="5-2-SLAM-Optimization"><a href="#5-2-SLAM-Optimization" class="headerlink" title="5.2 SLAM Optimization"></a>5.2 SLAM Optimization</h2><p>动态物体估计的因子图如Fig. 6所示，其中绿色方框是观测量，包含：camera-object 因子（式10），object-velocity 因子（式14），以及point-camera-object 因子（式15）。</p>
<p><img src="/2024/01/31/cubeslam/fig6.png" alt="fig6" title="figure 6"></p>
<h3 id="5-2-1-Object-Motion-Model"><a href="#5-2-1-Object-Motion-Model" class="headerlink" title="5.2.1 Object Motion Model"></a>5.2.1 Object Motion Model</h3><p>3D 物体的通用运动方式可表示为位姿转换矩阵 $T \in SE(3)$ ，作者采用一个更严格的非完整轮模型 nonholonomic wheel model，将汽车运动表示为线速度 $v$ 和转向角 $\phi$ 。假设汽车运动在平面上，则三维向量即可表示完整状态 $T_0 = [R(\theta), [t_x, t_y, o]’]$ ，则根据速度预测的状态表示为：</p>
<p><img src="/2024/01/31/cubeslam/f13.png" alt="f13" title="formula 13"></p>
<p>其中，L 表示汽车前后轮中心的距离。最终的<strong>运动模型误差</strong>表示为：</p>
<p><img src="/2024/01/31/cubeslam/f14.png" alt="f14" title="formula 14"></p>
<h3 id="5-2-2-Dynamic-Point-Observation"><a href="#5-2-2-Dynamic-Point-Observation" class="headerlink" title="5.2.2 Dynamic Point Observation"></a>5.2.2 Dynamic Point Observation</h3><p>对于动态物体上的特征点，首先将其从物体坐标系转换至世界坐标系，再根据相机位姿投影至像素平面，重投影误差表示为：</p>
<p><img src="/2024/01/31/cubeslam/f15.png" alt="f15" title="formula 15"></p>
<p>其中，$T^j_c$ 表示相机位姿；$z_{kj}$ 表示观测到的像素坐标。</p>
<h2 id="5-3-Dynamic-Data-Association"><a href="#5-3-Dynamic-Data-Association" class="headerlink" title="5.3 Dynamic Data Association"></a>5.3 Dynamic Data Association</h2><p>4.3节使用的静态物体数据关联不适用于动态物体，跟踪特征点的<strong>经典方法</strong>是预测投影位置，然后在附近搜索<strong>描述子匹配</strong>的特征点，然后检查<strong>对极几何约束</strong>验证是否匹配正确；但单眼相机动态场景中，很难准确预测物体与特征点的运动，且当物体运动不准确时难以应用对极几何进行计算。因此，作者使用2D <strong>KLT 稀疏光流算法</strong>进行动态物体特征点的跟踪，该算法并不需要3D 点位置信息。在使用像素追踪之后，考虑物体的运动利用三角测量计算动态特征的3D 位置。</p>
<p>假设两帧图片的投影矩阵为 $M_1, M_2$ ；同一3D 点在两帧对应的位置分别是 $P_1, P_2$ ；对应像素位置分别为 $z_1, z_2$ ；物体在两帧间的运动转换矩阵为 $\Delta T$ 。则有 $P_2 = \Delta T P_1$ ，基于此，有：</p>
<p><img src="/2024/01/31/cubeslam/f16.png" alt="f16" title="formula 16"></p>
<p>若将 $M_2 \Delta T$ 视为<strong>补偿物体运动的相机位姿</strong>，则式16可视为<strong>标准的两视角三角化问题</strong>，可使用SVD 方法进行求解。</p>
<p>值得注意的是，作者提到当<strong>像素位移过大</strong>时KLT 方法会跟踪失败，因此，对于动态物体跟踪作者直接使用<strong>视觉物体跟踪算法</strong>：跟踪物体2D bbox，从前一帧预测其在当前帧的大概位置，然后从当前帧的2D bbox 中选取具有最大重叠比例的bbox 实现跟踪。</p>
<h1 id="8-Experiments-Object-SLAM"><a href="#8-Experiments-Object-SLAM" class="headerlink" title="8 Experiments-Object SLAM"></a>8 Experiments-Object SLAM</h1><h2 id="8-1-TUM-RGBD-and-ICL-NUIM-Dataset"><a href="#8-1-TUM-RGBD-and-ICL-NUIM-Dataset" class="headerlink" title="8.1 TUM RGBD and ICL-NUIM Dataset"></a>8.1 TUM RGBD and ICL-NUIM Dataset</h2><p>为了更好地评估单目位姿偏移，作者在使用及结果对比时<strong>关掉了ORB SLAM 的回环检测模块</strong>。</p>
<p>Fig. 9所示的是在TUM fr3_cabinet 数据集上的实验结果，由于该场景特征点太少，导致现有的单目SLAM 算法均无法完成实验。而本文提出的算法使用object-camera 观测作为额外约束，实现了较好的定位效果。Fig. 9左边下图中估计的立方体存在较大的误差，但经过多视角优化后，3D立方体可以与真实点云实现良好的契合。实验结果如Table 2所示。</p>
<p><img src="/2024/01/31/cubeslam/fig9.png" alt="fig9" title="figure 9"></p>
<p><img src="/2024/01/31/cubeslam/t2.png" alt="t2" title="table 2"></p>
<h2 id="8-2-Collected-Chair-Dataset"><a href="#8-2-Collected-Chair-Dataset" class="headerlink" title="8.2 Collected Chair Dataset"></a>8.2 Collected Chair Dataset</h2><p><img src="/2024/01/31/cubeslam/fig10.png" alt="fig10" title="figure 10"></p>
<h2 id="8-4-Dynamic-Object"><a href="#8-4-Dynamic-Object" class="headerlink" title="8.4 Dynamic Object"></a>8.4 Dynamic Object</h2><p><img src="/2024/01/31/cubeslam/fig12.png" alt="fig12" title="figure 12"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Pose Optimization</tag>
        <tag>Object SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Semantic Segmentation-Based Lane-Level Localization Using Around View Monitoring System</title>
    <url>/2024/02/29/deng2019/</url>
    <content><![CDATA[<p>Deng, Liuyuan, Ming Yang, Bing Hu, Tianyi Li, Hao Li, and Chunxiang Wang. “Semantic Segmentation-Based Lane-Level Localization Using Around View Monitoring System.” <em>IEEE Sensors Journal</em> 19, no. 21 (November 1, 2019): 10077–86. <a href="https://doi.org/10.1109/JSEN.2019.2929135">https://doi.org/10.1109/JSEN.2019.2929135</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>一般来讲，对于使用视觉相机进行定位的方法是：通过从图片中提取特征，并与一个先验地图进行匹配从而得到汽车位置；许多方法是使用前置单目相机或立体相机来采集数据，但是在交通繁忙的场景中，其field of view (FOV) 容易被其他车辆干扰导致效果不佳。本文提出的方法采用<strong>Around View Monitoring (AVM)</strong> 系统来减小环境干扰的影响。</p>
<p>AVM 系统中鱼眼相机采集到的图片通常会进行去畸变、合成一张顶视图片（鸟瞰视角图片，如下图所示），若要通过检测车道线等标志进行车辆定位，经过处理后的图片覆盖范围有限，仅能获取当前车道线内的路标，难以区分道路上不同车道线之间的区别；故本文使用鱼眼相机采集到的原始图片进行车道线级别的定位。</p>
<span id="more"></span>
<p><img src="/2024/02/29/deng2019/fig1.png" alt="fig1" title="figure 1"></p>
<p>对于鱼眼相机图片的使用存在两个问题：</p>
<ol>
<li><strong>车辆周围存在动态物体的干扰</strong>：动态物体的干扰会导致定位的不稳定，为解决此问题，本文采用基于CNN 的语义分割来提取道路特征（如道路边界、标志等），同时利用语义场景理解，将动态物体进行剔除；</li>
<li><strong>道路边界可以提供横向位置信息，但是当距离车辆比较远时会有比较大的不确定度</strong>：本文提出了<strong>Coarse-Scale Localization (CSL)</strong> 方法与<strong>Fine-Scale Localization (FSL)</strong> 方法相结合来实现精准定位，CSL 利用道路边界来估计一个粗略的位置，将该粗略位置作为FSL的初始值进行高精度定位。由于边界点具有区别度很大的不确定性，CSL 采用weight Iterative Closest Point （ICP) 来提高匹配准确度，并给出置信度。FSL 通过将车辆附近的路标与先验地图进行匹配，将该匹配结果息与运动信息相结合得到最终的车辆位置信息。</li>
</ol>
<p>本文做出的主要贡献：</p>
<ol>
<li>对AVM 系统的鱼眼相机采集到的图片进行语义分割实现对道路特征的提取：<ul>
<li>利用语义环境感知来识别出正确的道路边界点，剔除动态物体的干扰；</li>
<li>检测线形标志（车道线等）与非线形标志（箭头等）来提高横向与纵向的定位精度。</li>
</ul>
</li>
<li>提出了 CSL 与 FSL 方法，充分利用具有不同精确度的道路特征来实现高精度定位；</li>
<li>仅使用AVM系统、GPS、proprioceptive sensors (IMU 、里程计) 以及先验地图来实现城市环境车辆的分米级定位。</li>
</ol>
<h1 id="3-System-Framework"><a href="#3-System-Framework" class="headerlink" title="3 System Framework"></a>3 System Framework</h1><p>系统包括四大模块：detection, map manager, CSL and FSL。</p>
<p><img src="/2024/02/29/deng2019/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-detection-module"><a href="#3-1-detection-module" class="headerlink" title="3-1 detection module"></a>3-1 detection module</h2><p>该模块检测道路边界与路标，并将检测到的图像像素转换为2D点：使用路标的轮廓点进行表示，使得不同的路标可以用统一的方式进行表示，有着更小的契合误差。</p>
<h2 id="3-2-map-manager"><a href="#3-2-map-manager" class="headerlink" title="3-2 map manager"></a>3-2 map manager</h2><p>该模块包括先验<strong>道路边界地图与路标地图</strong>，GPS提供一个粗略的位置信息，根据地图中的道路结构进行约束，沿着车辆行驶方向对齐到距离最近的车道线中间线上的点。</p>
<h2 id="3-3-CSL"><a href="#3-3-CSL" class="headerlink" title="3-3 CSL"></a>3-3 CSL</h2><p>该模块通过匹配道路边界线来提供一个粗略的位置信息，目标是输出一个横向定位精度小于半个车道宽度的位置信息作为FSL 的初始值。<strong>置信度计算与weighted ICP 是同时、独立进行的</strong>，如果置信度值低于设定的阈值，则判定匹配结果无效。</p>
<h2 id="3-4-FSL"><a href="#3-4-FSL" class="headerlink" title="3-4 FSL"></a>3-4 FSL</h2><p>将检测到的2D道路标志点进行累积形成一个本地道路标志地图，然后将该地图与先验路标地图使用 ICP 进行匹配，得到车辆的位置信息。由于本地地图匹配结果通常包含<strong>未知的时间相关性</strong>，本文使用<strong>Split Covariance Intersection Filter (Split CIF)</strong> ——可被视为Kalman滤波器的泛化，擅长处理具有未知相关性的数据——来融合匹配结果与动作数据。</p>
<h2 id="4-Semantic-Segmentation-Based-Road-Boundary-and-Road-Marking-Detection"><a href="#4-Semantic-Segmentation-Based-Road-Boundary-and-Road-Marking-Detection" class="headerlink" title="4 Semantic Segmentation-Based Road Boundary and Road Marking Detection"></a>4 Semantic Segmentation-Based Road Boundary and Road Marking Detection</h2><h2 id="4-1-Semantic-Segmentation-on-Raw-Fisheye-Images-From-AVM"><a href="#4-1-Semantic-Segmentation-on-Raw-Fisheye-Images-From-AVM" class="headerlink" title="4-1 Semantic Segmentation on Raw Fisheye Images From AVM"></a>4-1 Semantic Segmentation on Raw Fisheye Images From AVM</h2><p><img src="/2024/02/29/deng2019/f1.png" alt="f1" title="formula 1"></p>
<p>I是输入图片，$\theta$ 是CNN 模型参数，L是语义分割种类数，本文设置为18，包括free space，静态与动态物体。</p>
<h2 id="4-2-Road-Boundary-and-Road-Marking-Detection"><a href="#4-2-Road-Boundary-and-Road-Marking-Detection" class="headerlink" title="4-2 Road Boundary and Road Marking Detection"></a>4-2 Road Boundary and Road Marking Detection</h2><p>本文利用像素级语义信息的优势来区分真实与虚假的道路边界。</p>
<p>将18个类别分为3个大类：free space (F), static objects (S), dynamic objects (D)。<strong>定义F与S的边界作为真正的道路边界，将F与D的边界作为虚假的边界</strong>。</p>
<p><img src="/2024/02/29/deng2019/fig4.png" alt="fig4" title="figure 4"></p>
<p>本文采用自下而上、列独立的搜索策略，如上图所示，搜索过程可并行处理，而且可以在GPU上高速运行。针对每一张图片，<strong>边界像素的数量等于图片的宽度（包括真边界与假边界）</strong>。真实边界点通过逆透视变换法 <strong>(Inverse Perspective Mapping, IPM)</strong> 转换至2D VCS (Vehicle Coordinate System) 边界点。</p>
<p><img src="/2024/02/29/deng2019/fig5.png" alt="fig5" title="figure 5"></p>
<p>路标的提取过程如上图所示，首先将鱼眼相机的语义分割图片经过IPM变换为顶视图片，然后根据像素级语义分割顶视图片计算路标的边缘：</p>
<ul>
<li>首先将语义分割图片转化为灰度图；</li>
<li>然后利用一系列形态操作子morphological operators (Opening, Closing, Gradient) 对灰度图片去除噪点、获取路标边缘；</li>
<li>将图像坐标系中的像素转换为2D VCS。</li>
</ul>
<h1 id="5-Coarse-Scale-Localization-Method"><a href="#5-Coarse-Scale-Localization-Method" class="headerlink" title="5 Coarse-Scale Localization Method"></a>5 Coarse-Scale Localization Method</h1><h2 id="5-1-Map-Matching-Based-on-Weighted-ICP"><a href="#5-1-Map-Matching-Based-on-Weighted-ICP" class="headerlink" title="5-1 Map Matching Based on Weighted ICP"></a>5-1 Map Matching Based on Weighted ICP</h2><p>通过IPM从鱼眼相机图片中获取的道路边界点有着不同的测量不确定度，该不确定度与相机和道路边界点之间的距离有关。因此，ICP 中的点对需要根据距离来赋予不同的权重。本文采用<strong>weighted ICP 进行地图匹配</strong>。</p>
<p><img src="/2024/02/29/deng2019/f2.png" alt="f2" title="formula 2"></p>
<p><img src="/2024/02/29/deng2019/f3.png" alt="f3" title="formula 3"></p>
<p>权重$w_i$ 只与相机和边界点之间的距离$d_i$ 相关，当$d_i$足够小时权重设为0，即忽略该边界点。</p>
<h2 id="5-2-Confidence-Computation"><a href="#5-2-Confidence-Computation" class="headerlink" title="5-2 Confidence Computation"></a>5-2 Confidence Computation</h2><p>当出现极端情况时CSL 不会输出位置信息，如当所有边界点都被车辆遮挡，或者边界点距离采样相机太远时。<strong>CSL 计算置信度来预估当前场景是否可靠</strong>。置信度取决于真实边界点的数量以及这些边界点与相机之间的距离。</p>
<p><img src="/2024/02/29/deng2019/f4.png" alt="f4" title="formula 4"></p>
<h1 id="6-Fine-Scale-Localization-Method"><a href="#6-Fine-Scale-Localization-Method" class="headerlink" title="6 Fine-Scale Localization Method"></a>6 Fine-Scale Localization Method</h1><h2 id="6-2-Fine-Position-Estimation"><a href="#6-2-Fine-Position-Estimation" class="headerlink" title="6-2 Fine Position Estimation"></a>6-2 Fine Position Estimation</h2><p>利用低成本IMU与里程计，积累短距离内采集到的路标边缘点来生成本地路标地图，比起只使用一帧图片中采集到的路标边缘点会提供更高的稳定性；与此同时，过多的采样点也会导致计算量激增，所以本文采取了一系列的措施来限制本地路标地图中的采样点数量。</p>
<p>本地地图与先验地图通过ICP 进行匹配，计算ICP 的协方差，然后使用Split CIF 将匹配结果与运动数据相结合得到最终的定位结果。</p>
<h1 id="7-Experimental-Results"><a href="#7-Experimental-Results" class="headerlink" title="7 Experimental Results"></a>7 Experimental Results</h1><h2 id="7-1-Road-Boundary-and-Road-Marking-Detection-Results"><a href="#7-1-Road-Boundary-and-Road-Marking-Detection-Results" class="headerlink" title="7-1 Road Boundary and Road Marking Detection Results"></a>7-1 Road Boundary and Road Marking Detection Results</h2><p>基于CNN 的鱼眼相机语义分割网络采用作者前作(Deng 等, 2020)中的架构，将真实采集到的鱼眼相机图片与转化得到的鱼眼相机图片共同训练多任务学习架构网络。</p>
<p><img src="/2024/02/29/deng2019/fig7.png" alt="fig7" title="figure 7"></p>
<p>根据上图道路边界检测结果，本文提出的方法可以很好地检测到边界点，而且可以区分正确与错误的边界点。</p>
<p><img src="/2024/02/29/deng2019/fig8.png" alt="fig8" title="figure 8"></p>
<p>上图是利用语义分割对路标点进行提取，路标点很少受到遮挡，这是因为我们只关注邻近区域内的路标点。</p>
<h2 id="7-2-The-Results-of-Coarse-Scale-Localization"><a href="#7-2-The-Results-of-Coarse-Scale-Localization" class="headerlink" title="7-2 The Results of Coarse-Scale Localization"></a>7-2 The Results of Coarse-Scale Localization</h2><p><img src="/2024/02/29/deng2019/fig11.png" alt="fig11" title="figure 11"></p>
<p>上图是标准ICP 与weighted ICP 解算结果对比，表明weighted ICP 的优势。</p>
<p><img src="/2024/02/29/deng2019/fig12.png" alt="fig12" title="figure 12"></p>
<p>上图是不同车道线时CSL 的输出结果，可以发现随着车道线数量的增加，<strong>横向误差逐渐增大、置信度逐渐下降，原因是道路边界与相机之间的距离变大，导致检测误差、系统误差增大，以及边界点的权重降低</strong>。</p>
<p><img src="/2024/02/29/deng2019/fig13.png" alt="fig13" title="figure 13"></p>
<p>上图是当道路边界被遮挡时的横向误差与置信度，图（a）右侧道路边界被大量遮挡，导致横向误差增大、置信度降低（因为真实边界点数量的减少）；图（b）左右两侧的道路边界几乎被完全遮挡，虽然匹配的横向误差不大，但是置信度过低表明结果不可靠。</p>
<p><img src="/2024/02/29/deng2019/fig14.png" alt="fig14" title="figure 14"></p>
<p>上图是在2.5km的测试路段CSL 的实验结果，算法使用单帧图片的数据进行解算，不使用跟踪：</p>
<ul>
<li>Root Mean Square Error (RMSE) 是0.26m，Max Absolute Error (MAE) 是1.55m；</li>
<li>当置信度较低时说明边界线被大量遮挡（不考虑处于交叉路口的情况）；</li>
<li>第2222帧到2774帧图片采集区域为三车道线区域（其余区域为4车道线）内置信度较高、横向误差较小，与理论分析相契合；</li>
<li>当置信度低于阈值时，关闭CSL 模块，阈值是通过在大量不同道路场景下的测试来确定的。</li>
</ul>
<h2 id="7-3-The-Results-of-Fine-Scale-Localization"><a href="#7-3-The-Results-of-Fine-Scale-Localization" class="headerlink" title="7-3 The Results of Fine-Scale Localization"></a>7-3 The Results of Fine-Scale Localization</h2><p><img src="/2024/02/29/deng2019/fig15.png" alt="fig15" title="figure 15"></p>
<p>对采集到的原始数据进行下采样，最终选择最小距离分辨率为0.08m，在保留地图细节的同时尺寸更小。</p>
<p><img src="/2024/02/29/deng2019/fig16.png" alt="fig16" title="figure 16"></p>
<p>注意上图中绿色点为本地生成的路标点，距离车辆越近点越稠密，这是因为采用了6-2节提到的dropping策略：<strong>越靠近车辆当前位置附近的点会有更小的轨迹推断误差dead reckoning error</strong>。</p>
<ul>
<li>图（a）是FSL 的初始化过程，使用CSL 的结果作为初始值，然后使用dead reckoning。<strong>一旦本地路标地图距离达到30m就开始进行地图匹配以矫正位置信息</strong>；</li>
<li>图（b）、（c）分别是道路段、人行横道段的结果，箭头与斑马线会提供良好的横向与径向距离信息；</li>
<li>图（d）显示了路口处的结果，在<strong>此处仅使用dead reckoning进行位置推断</strong>。</li>
</ul>
<p><img src="/2024/02/29/deng2019/t1.png" alt="t1" title="table 1"></p>
<p><img src="/2024/02/29/deng2019/fig17.png" alt="fig17" title="figure 17"></p>
<p>上图为FSL 的实验结果，观察可以发现：</p>
<ul>
<li>横向误差大部分情况下都小于0.1m，除了在路口阶段缺少路标的情况；</li>
<li>径向误差比横向误差大得多，原因之一是径向距离信息（停止线、斑马线等）远少于横向距离信息（车道线等），另一个原因是径向距离更容易受到颠簸或俯仰角改变的影响bumps and pitch changes。</li>
</ul>
<p>本文方法是基于平面道路或者有着较小俯仰角变化的假设，利用IPM 来获取路标位置的；所以当出现颠簸，或者汽车刹车时，IPM 的假设基础不再满足，定位误差（特别是径向误差）会增大。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Detect-SLAM_Making Object Detection and SLAM Mutually Beneficial</title>
    <url>/2024/01/30/detect-slam/</url>
    <content><![CDATA[<p>Zhong, Fangwei, Sheng Wang, Ziqi Zhang, China Chen, and Yizhou Wang. “Detect-SLAM: Making Object Detection and SLAM Mutually Beneficial.” In <em>2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, 1001–10, 2018. <a href="https://doi.org/10.1109/WACV.2018.00115">https://doi.org/10.1109/WACV.2018.00115</a>.</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文提出的Detect-SLAM 较其他SOTA 方法的优势：</p>
<ol>
<li>通过利用物体检测器完成对移动物体上的<strong>不可靠特征点</strong>进行剔除，极大地提高了SLAM 算法在动态环境中的<strong>准确性与鲁棒性</strong>；</li>
<li>在线构建一个<strong>实例级语义地图</strong>；</li>
<li>通过利用物体语义地图来<strong>提高物体检测器性能</strong>，使得其在挑战性环境中可以更有效地识别出物体。</li>
</ol>
<span id="more"></span>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>Detect-SLAM 是第一个将SLAM 和 基于DNN 的检测器结合起来同时完成三种视觉任务的工作：提高SLAM 在动态环境中的鲁棒性，构建语义地图，以及增强物体检测性能。</p>
<p><img src="/2024/01/30/detect-slam/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-Detect-SLAM"><a href="#3-Detect-SLAM" class="headerlink" title="3 Detect-SLAM"></a>3 Detect-SLAM</h1><p>本架构是基于ORB-SLAM2 算法的，在其基础上包含三个新步骤：</p>
<ul>
<li>Moving objects removal：将动态物体上的特征点进行<strong>剔除</strong>；</li>
<li>Object Mapping：对关键帧中的静态物体进行<strong>重建</strong>，物体语义地图包含带有物体ID 的稠密点云；</li>
<li>SLAM-enhanced Detector：利用物体语义地图作为先验知识来<strong>增强</strong>探测器在挑战环境中的性能。</li>
</ul>
<h2 id="3-1-Moving-Objects-Removal"><a href="#3-1-Moving-Objects-Removal" class="headerlink" title="3.1 Moving Objects Removal"></a>3.1 Moving Objects Removal</h2><p>动态物体剔除如Fig. 3所示，值得注意的是，此处的动态物体指的是属于可移动种类中的物体，并不能保证其一定是移动的，即<strong>潜在动态物体</strong>。然而，利用DNN 进行动态物体检测速度较慢，SSD 只能实现3 FPS的速度。为解决该问题，作者采用两个策略进行应对：</p>
<ol>
<li>只在<strong>关键帧</strong>中进行动态物体检测，然后在local map 中更新特征点的<strong>移动概率</strong>来加速tracking 支流；</li>
<li>在tracking 支流中利用<strong>特征匹配</strong>和<strong>匹配点扩张</strong>来传递移动概率，从而在相机位姿估计前有效移除动态物体上的特征点。</li>
</ol>
<p>作者将物体的移动概率分为4个层次，如Fig. 2所示，使用<strong>高置信度点</strong>在<strong>匹配点扩张</strong>中将移动概率传递给周围<strong>未匹配</strong>的特征点，在每个点都得到相应的移动概率后，去除动态特征点并使用RANSAC 滤除其他外点，然后进行位姿估计。</p>
<p><img src="/2024/01/30/detect-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h3 id="3-1-1-Updating-Moving-Probability"><a href="#3-1-1-Updating-Moving-Probability" class="headerlink" title="3.1.1 Updating Moving Probability"></a>3.1.1 Updating Moving Probability</h3><p><img src="/2024/01/30/detect-slam/fig3.png" alt="fig3" title="figure 3"></p>
<p>关键帧的物体检测完成后会被插入到local map 中并更新地图中特征点的移动概率：</p>
<p><img src="/2024/01/30/detect-slam/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$P_{t-1}(X^i)$ 表示3D 点 $X^i$ 在上个关键帧更新后的移动概率，若点 $X^i$ 是新添加的点，则令 $P_{t-1}(X^i)=0.5$ ；与点 $X^i$ 匹配的关键帧中的点 $x_i$ 若位于动态物体的bbox 内，则令 $S_t(x^i) = 1$ ，其他情况下令其为0。</p>
<h3 id="3-1-2-Moving-Probability-Propagation"><a href="#3-1-2-Moving-Probability-Propagation" class="headerlink" title="3.1.2 Moving Probability Propagation"></a>3.1.2 Moving Probability Propagation</h3><p>移动概率传递在帧间通过两种方式确定：<strong>特征匹配</strong>和<strong>匹配点扩张</strong>，如Fig. 4所示。</p>
<p><img src="/2024/01/30/detect-slam/fig4.png" alt="fig4" title="figure 4"></p>
<p>对于<strong>特征匹配</strong>的情况，若一个特征点同时与上一帧特征点和地图中的3D 点匹配上时，应该<strong>以地图中3D 点的移动概率为准</strong>。将未匹配的特征点移动概率初始化为0.5。特征匹配的情况总结为下式：</p>
<p><img src="/2024/01/30/detect-slam/f2.png" alt="f2" title="formula 2"></p>
<p>对于匹配点扩张的情况，是利用<strong>高置信度的特征点</strong>（包括高静态可能和高动态可能特征点）对周围未匹配特征点的移动概率进行确认，确认过程如下所示：</p>
<p><img src="/2024/01/30/detect-slam/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$\chi_t$ 为高置信度特征点；$P_{init}$ 为初始化概率值，为0.5；$\lambda(d)$ 为距离因子，若d 小于半径阈值，则令 $\lambda(d) = Ce^{-d/r}$ 。</p>
<h2 id="3-2-Mapping-Objects"><a href="#3-2-Mapping-Objects" class="headerlink" title="3.2 Mapping Objects"></a>3.2 Mapping Objects</h2><h3 id="3-2-1-Predicting-Region-ID"><a href="#3-2-1-Predicting-Region-ID" class="headerlink" title="3.2.1 Predicting Region ID"></a>3.2.1 Predicting Region ID</h3><p>物体ID 的预测是基于一个<strong>几何假设</strong>：地图中物体投影到关键帧中的区域与帧中物体检测区域<strong>存在重叠</strong>，且区域属于同一个物体。利用IOU 来表示两个区域的重叠程度：</p>
<p><img src="/2024/01/30/detect-slam/f4.png" alt="f4" title="formula 4"></p>
<p>若IOU 大于0.5，则进一步估计<strong>深度似然</strong>：</p>
<p><img src="/2024/01/30/detect-slam/f5.png" alt="f5" title="formula 5"></p>
<p>其中，Err 是重叠区域内观测与投影深度的MSE：</p>
<p><img src="/2024/01/30/detect-slam/f6.png" alt="f6" title="formula 6"></p>
<p>若深度似然大于设定阈值，则将地图物体的ID 赋予给检测区域，否则，分配一个新的ID 给该检测区域。</p>
<h3 id="3-2-2-Cutting-Background"><a href="#3-2-2-Cutting-Background" class="headerlink" title="3.2.2 Cutting Background"></a>3.2.2 Cutting Background</h3><p>将投影与观测重叠区域的点作为前景seed，将bbox 外的点作为背景seed，利用Grab-Cut 算法得到物体的分割掩码。</p>
<p><img src="/2024/01/30/detect-slam/fig5.png" alt="fig5" title="figure 5"></p>
<h3 id="3-2-3-Recobstruction"><a href="#3-2-3-Recobstruction" class="headerlink" title="3.2.3 Recobstruction"></a>3.2.3 Recobstruction</h3><p>利用物体掩码，创建物体点云并剔除噪声点，最终将物体点云转换至世界坐标系中并添加至物体语义地图。</p>
<h2 id="3-3-SLAM-enhanced-Detector"><a href="#3-3-SLAM-enhanced-Detector" class="headerlink" title="3.3 SLAM-enhanced Detector"></a>3.3 SLAM-enhanced Detector</h2><p>利用物体语义地图和相机位姿来增强物体检测器的性能。</p>
<h3 id="3-3-1-Region-Proposal"><a href="#3-3-1-Region-Proposal" class="headerlink" title="3.3.1 Region Proposal"></a>3.3.1 Region Proposal</h3><p>利用当前估计的相机位姿将语义地图中的3D 物体投影至2D 平面，将具有相同物体ID 的<strong>像素进行聚类</strong>来确定可能含有物体的区域。</p>
<h3 id="3-3-2-Region-Filter"><a href="#3-3-2-Region-Filter" class="headerlink" title="3.3.2 Region Filter"></a>3.3.2 Region Filter</h3><p>去除掉较小尺寸（20<em>20像素）的候选区域，并估计观测深度与投影深度之间的<em>*似然</em></em>来检测遮挡的候选目标。</p>
<h3 id="3-3-3-Hard-Example-Mining"><a href="#3-3-3-Hard-Example-Mining" class="headerlink" title="3.3.3 Hard Example Mining"></a>3.3.3 Hard Example Mining</h3><p>前人的工作证明了选择使用<strong>困难的数据</strong>来训练或者细调DNN 网络可以极大促进检测效果，于是，作者使用SLAM-enhanced 物体检测器来挖掘困难的数据（Fig. 7所示）来<strong>强化训练集</strong>，然后用这些强化的数据集来细调SSD 网络。</p>
<p><img src="/2024/01/30/detect-slam/fig7.png" alt="fig7" title="figure 7"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 DM-SLAM_A Feature-Based SLAM System for Rigid Dynamic Scenes</title>
    <url>/2024/02/22/dm-slam/</url>
    <content><![CDATA[<p>Cheng, Junhao, Zhi Wang, Hongyan Zhou, Li Li, and Jian Yao. “DM-SLAM: A Feature-Based SLAM System for Rigid Dynamic Scenes.” <em>ISPRS International Journal of Geo-Information</em> 9, no. 4 (April 2020): 202. <a href="https://doi.org/10.3390/ijgi9040202">https://doi.org/10.3390/ijgi9040202</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文做出的贡献：</p>
<ul>
<li>提出一个完整的视觉SLAM 系统——DM-SLAM，该系统结合<strong>实例分割网络</strong>和<strong>光流信息</strong>，可在高动态环境中消除动态物体对位姿估计的影响，且本系统适用于<strong>单目、双目和RGB-D 相机</strong>；</li>
<li>针对RGB-D/双目和单目相机，分别提出<strong>两种高效提取动态点</strong>的策略；</li>
<li>在公开数据集上进行测试，证明了本算法的有效性。</li>
</ul>
<span id="more"></span>
<h1 id="3-System-Introduction"><a href="#3-System-Introduction" class="headerlink" title="3 System Introduction"></a>3 System Introduction</h1><h2 id="3-2-Proposed-Method"><a href="#3-2-Proposed-Method" class="headerlink" title="3.2 Proposed Method"></a>3.2 Proposed Method</h2><p>DM-SLAM 包含四个模块：语义分割、自运动估计、动态点检测，以及基于特征的SLAM 架构。</p>
<h3 id="3-2-1-Overview-of-the-Proposed-Approach"><a href="#3-2-1-Overview-of-the-Proposed-Approach" class="headerlink" title="3.2.1 Overview of the Proposed Approach"></a>3.2.1 Overview of the Proposed Approach</h3><p>本方法的整体架构如Fig. 2所示：</p>
<p><img src="/2024/02/22/dm-slam/fig2.png" alt="fig2" title="figure 2"></p>
<p>Fig. 3展示了本方法的细节，Mask R-CNN 处理过的图片包含像素级的语义信息，本系统<strong>暂时舍弃</strong>属于<strong>先验动态物体</strong>的特征点，利用剩余的特征点进行<strong>粗略的自运动估计</strong>；因为更少的特征点参与位姿解算，而且没有使用局部BA ，所以当前模块获取的初始位姿是<strong>不准确</strong>的。</p>
<p>然后，针对双目/RGB-D 和单目相机采用<strong>不同的</strong>动态点检测策略：</p>
<ul>
<li>针对单目相机，使用<strong>对极几何约束</strong>实现对动态点的检测：计算匹配点距离极线的距离，然后进行判断；</li>
<li>对于双目/RGB-D 相机，可利用三角化或深度图获取特征点的深度信息，将前一帧上的特征点重投影到当前帧，计算<strong>重投影偏差</strong>。</li>
</ul>
<p>之后，利用检测到的动态特征点判断所属物体是否属于动态物体。</p>
<p><img src="/2024/02/22/dm-slam/fig3.png" alt="fig3" title="figure 3"></p>
<h3 id="3-2-3-Ego-Motion-Estimation"><a href="#3-2-3-Ego-Motion-Estimation" class="headerlink" title="3.2.3 Ego-Motion Estimation"></a>3.2.3 Ego-Motion Estimation</h3><p>在暂时剔除掉先验动态特征点之后，本系统采用一个<strong>轻量级的跟踪算法</strong>进行位姿估计，该算法与ORB-SLAM2 不同，不进行局部BA 和新关键帧筛选，只与历史帧的特征点进行匹配，并将关联的地图点投影到当前帧，利用<strong>最小化重投影误差</strong>来获取粗略的初始位姿。</p>
<h3 id="3-2-4-Dynamic-Feature-Points-Extraction"><a href="#3-2-4-Dynamic-Feature-Points-Extraction" class="headerlink" title="3.2.4 Dynamic Feature Points Extraction"></a>3.2.4 Dynamic Feature Points Extraction</h3><p>针对RGB-D/双目相机，作者首先利用<strong>LK 光流</strong>算法获取两帧之间的匹配特征点，然后利用初始位姿估计将前一帧的特征点投影至当前帧，由于初始位姿估计的精度问题，必然会存在一定的<strong>重投影偏差向量</strong>，如Fig. 5、Fig. 6所示。</p>
<p><img src="/2024/02/22/dm-slam/fig5.png" alt="fig5" title="figure 5"></p>
<p><img src="/2024/02/22/dm-slam/fig6.png" alt="fig6" title="figure 6"></p>
<p>可以看到，即便初始位姿估计精度较差，但仍然可以从重投影偏差向量识别出静态点与动态点。基于此，作者使用静态区域（去除掉先验动态物体区域）的特征点，经加权平均求得<strong>阈值</strong>：</p>
<p><img src="/2024/02/22/dm-slam/f6.png" alt="f6" title="formula 6"></p>
<p><img src="/2024/02/22/dm-slam/f7.png" alt="f7" title="formula 7"></p>
<p>其中，$\epsilon$ 表示偏差向量的模值；$\theta$ 表示角度。</p>
<p>根据计算出的阈值，判断先验动态物体区域内的特征点是否属于动态点：</p>
<p><img src="/2024/02/22/dm-slam/f8.png" alt="f8" title="formula 8"></p>
<p>对于单目相机，简单地使用<strong>对极约束</strong>来提取动态点：</p>
<p><img src="/2024/02/22/dm-slam/f9.png" alt="f9" title="formula 9"></p>
<p>其中，$q_i, p_i$ 分别是当前帧和历史帧的匹配特征点；F 是基础矩阵。对当前帧的每一点计算其<strong>与极线之间的距离</strong>：</p>
<p><img src="/2024/02/22/dm-slam/f10.png" alt="f10" title="formula 10"></p>
<p>利用该距离判断是否属于动态点。</p>
<p>在得到动态点之后，根据物体掩码内的动态点数量判断该物体是否是动态的。</p>
<p>作者比较了本方法与DS-SLAM、DynaSLAM 的优势：</p>
<ul>
<li>DS-SLAM 只将人类作为典型的动态物体代表，而本方法将20种物体作为潜在动态物体；且DS-SLAM 不能处理立体相机数据；</li>
<li>DynaSLAM 直接将潜在动态物体判定为动态的，与现实不符；且该方法只适用于RGB-D 数据采用多视角几何进行动态点检测。</li>
</ul>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Multimodal Semantic SLAM with Probabilistic Data Association</title>
    <url>/2024/02/28/doherty2019/</url>
    <content><![CDATA[<p>Doherty, Kevin, Dehann Fourie, and John Leonard. “Multimodal Semantic SLAM with Probabilistic Data Association.” In <em>2019 International Conference on Robotics and Automation (ICRA)</em>, 2419–25. Montreal, QC, Canada: IEEE, 2019. <a href="https://doi.org/10.1109/ICRA.2019.8794244">https://doi.org/10.1109/ICRA.2019.8794244</a>.</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p><strong>语义SLAM</strong> 可被分解为：</p>
<ul>
<li>一个<strong>离散</strong>的推理问题：决定物体种类与观测地标之间的联系（data association, DA)</li>
<li>一个<strong>连续</strong>的推理问题：获取机器人的位姿和地标的位置。</li>
</ul>
<p>在模糊的DA 情况下，语义SLAM 通常不是一个高斯推理过程，现有的工作多是基于潜在的假设或者使用多重可能假设进行求解的。而作者提出了一种将DA 假设表示为一种<strong>等效非高斯模型的多模态形式</strong>的解算方法。</p>
<span id="more"></span>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>如图1所示，即便那些可以用高斯分布表示的观测模型在<strong>DA 和地标种类模糊</strong>的情况下也可以使用一个<strong>非高斯观测模型</strong>来表示。</p>
<p><img src="/2024/02/28/doherty2019/fig1.png" alt="fig1" title="figure 1"></p>
<p>本文做出的贡献：</p>
<ol>
<li>提出一个<strong>nonparametric belief propagation</strong> 方法，为带有模糊DA 的语义SLAM 进行<strong>后验推理</strong>；</li>
<li>描述了<strong>multimodal semantic factors</strong> ，使得我们可以将<strong>不确定性</strong>包含到DA 和语义中，作为因子图中的<strong>非高斯因子</strong>，从而利用mm-iSAM (multimodal incremental smoothing and mapping) 进行位姿与地标的<strong>连续优化</strong>；</li>
<li>在仿真与真实数据集中进行实验，证明了该方法在DA 和地标种类模糊方面的鲁棒性。</li>
</ol>
<h1 id="3-Semantic-SLAM-with-Ambiguous-Data-Association"><a href="#3-Semantic-SLAM-with-Ambiguous-Data-Association" class="headerlink" title="3 Semantic SLAM with Ambiguous Data Association"></a>3 Semantic SLAM with Ambiguous Data Association</h1><h2 id="3-1-Semantic-SLAM-with-Known-Data-Association"><a href="#3-1-Semantic-SLAM-with-Known-Data-Association" class="headerlink" title="3.1 Semantic SLAM with Known Data Association"></a>3.1 Semantic SLAM with Known Data Association</h2><p>本文使用最大后验估计理论进行求解：</p>
<p><img src="/2024/02/28/doherty2019/f2.png" alt="f2" title="formula 2"></p>
<h2 id="3-2-Probabilistic-Data-Association"><a href="#3-2-Probabilistic-Data-Association" class="headerlink" title="3.2 Probabilistic Data Association"></a>3.2 Probabilistic Data Association</h2><p>为解决带有模糊DA 的语义SLAM 问题，作者分两步<strong>交替计算</strong>DA 概率与位姿、地标位置：</p>
<p>第一步，边缘化位姿与地标来计算DA 概率：</p>
<p><img src="/2024/02/28/doherty2019/f7.png" alt="f7" title="formula 7"></p>
<p>其中，$\eta_{\mathcal{D}}$ 为归一化常数；$\Theta = \{\mathcal{X}, \mathcal{L}\}$ 分别表示<strong>相机位姿与地标</strong>。</p>
<p>第二步，边缘化DA 来获取位姿与地标的分布：</p>
<p><img src="/2024/02/28/doherty2019/f8.png" alt="f8" title="formula 8"></p>
<p>作者只在接收到观测之后进行一次迭代的DA 概率计算，从而减少了计算负担。</p>
<h1 id="4-Multimodal-Semantic-SLAM"><a href="#4-Multimodal-Semantic-SLAM" class="headerlink" title="4 Multimodal Semantic SLAM"></a>4 Multimodal Semantic SLAM</h1><h2 id="4-1-Multimodal-iSAM"><a href="#4-1-Multimodal-iSAM" class="headerlink" title="4.1 Multimodal iSAM"></a>4.1 Multimodal iSAM</h2><p>SLAM 中的因子图表示为：</p>
<p><img src="/2024/02/28/doherty2019/f10.png" alt="f10" title="formula 10"></p>
<p>其中，$\varphi$ 表示<strong>观测因子</strong>，$\psi$ 表示<strong>先验因子</strong>。这里，因子图是一个间接的图模型，位姿与地标是被观测因子与先验因子连接起来的<strong>隐变量</strong>。每个变量的边缘分布可以使用<strong>belief propagation</strong> 进行解算，该方法在变量服从高斯分布时可以产生方便的分析形式。</p>
<p>multimodal iSAM 利用<strong>nonparametric belief propagation</strong> 来容纳非高斯变量，该方法在没有高斯假设的情况下，使用Gibbs 采样和kernel density estimation 组合方法来近似所有连续状态变量的<strong>置信度</strong>。对于随机变量X：</p>
<p><img src="/2024/02/28/doherty2019/f11.png" alt="f11" title="formula 11"></p>
<p>其中，$\mathcal{N}$ 是多元高斯核 multivariate Gaussian kernel，每个核的中心位于一个采样 $x^{[n]}$ ， $w^{[n]}$ 是相应核的权重（采用均匀分布），而 $\sum^{[n]}$ 是相应高斯核的带宽，通过leave-one-out 交叉验证来获取。</p>
<p>该方法的一个好处在于，在后验估计中不再需要直接表示多种潜在可能的modes，这种<strong>隐含表示</strong>将推理的复杂度从假设数量中<strong>解耦</strong>了出来，因为在近似边缘化中的计算只依赖于采样的<strong>固定数量</strong>。结果就是，具有非常低概率的modes 在近似边缘分布中不太可能会出现，但是这些modes 也不会直接被舍弃，会继续存在于因子图中，这样那些后面变得更有可能的modes 可以得到<strong>恢复</strong>。</p>
<h2 id="3-2-Multimodal-Semantic-Factors"><a href="#3-2-Multimodal-Semantic-Factors" class="headerlink" title="3.2 Multimodal Semantic Factors"></a>3.2 Multimodal Semantic Factors</h2><p>设定一个语义观测模型因子为</p>
<p>$p(y_t^k | x_t, l_j) = p(y_t^{k,c} | l_j^c)p(y_t^{k,r} | x_t, l_j)p(y_t^{k,b} | x_t, l_j)$</p>
<p>其中，$y_t^{k,c}$ 表示路标<strong>类别</strong>，通过物体检测网络获取；$y_t^{k,r}$  表示到路标的<strong>距离</strong>；$y_t^{k,b}$ 表示物体的<strong>方位</strong>。</p>
<p>假设 $p(y_t^{k,r} | x_t, l_j), p(y_t^{k,b} | x_t, l_j)$ 服从高斯分布，均值与方差分别为 $y_t^{k,r}$  ，$y_t^{k,b}$  和 $\sigma_t^{2,k,r}, \sigma_t^{2,k,b}$ 。</p>
<p>假设一个统一的先验DA：</p>
<p><img src="/2024/02/28/doherty2019/f12.png" alt="f12" title="formula 12"></p>
<p>其中，$\mathbb{D}\{d_t^k = j\} = \{\mathcal{D}_t \in \mathbb{D}_t | d_t^k = j\}$ 表示在时间t 所有可能的数据关联集合，其中观测 k 与地标 j 相关联。</p>
<p>在给定数据关联 $d_t^k$  情况下边缘化位姿估计、地标位置和种类，来计算每个观测 $y_t^k$  的似然：</p>
<p><img src="/2024/02/28/doherty2019/f13.png" alt="f13" title="formula 13"></p>
<p>上式中，使用<strong>采样</strong>来逼近位姿分布上的积分计算，对于DA 计算，作者采纳了一个最大似然模型来简化地标位置的积分计算，作者发现该方法在高斯分布模型上效果很好，对于非高斯模型可以使用基于采样的逼近策略来近似。</p>
<p>对于属于集合 $\mathcal{J} \subseteq\mathcal{L}$ 中的所有地标 $l_j$ ，给定 $\hat{p}(d_t^k = j)$ ，一个multimodal 语义因子将位姿 $x_t$ 与 $\mathcal{J}$  中的每一个候选联系起来：</p>
<p><img src="/2024/02/28/doherty2019/f14.png" alt="f14" title="formula 14"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 DP-SLAM_A visual SLAM with moving probability towards dynamic environments</title>
    <url>/2024/02/24/dp-slam/</url>
    <content><![CDATA[<p>Li, Ao, Jikai Wang, Meng Xu, and Zonghai Chen. “DP-SLAM: A Visual SLAM with Moving Probability towards Dynamic Environments.” <em>Information Sciences</em> 556 (May 1, 2021): 128–42. <a href="https://doi.org/10.1016/j.ins.2020.12.019">https://doi.org/10.1016/j.ins.2020.12.019</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ol>
<li>作者提出了一种通过实时传递每个关键点<strong>移动概率</strong>的动态物体检测方法，该移动概率传递方法克服了<strong>几何约束和语义信息的偏差</strong>，可提高vSLAM 的准确性与鲁棒性；</li>
<li>利用静态信息<strong>补全</strong>遮挡的背景区域，获取没有动态物体的合成RGB 图片以及相应的深度图片，有益于虚拟现实等应用；</li>
<li>作者将该移动物体检测方法集成至ORB-SLAM2 系统中，利用公开数据集进行测试，证明了本方法对SLAM <strong>精度与鲁棒性</strong>的提高。</li>
</ol>
<span id="more"></span>
<h1 id="3-DP-SLAM"><a href="#3-DP-SLAM" class="headerlink" title="3 DP-SLAM"></a>3 DP-SLAM</h1><h2 id="3-1-The-approach-overview"><a href="#3-1-The-approach-overview" class="headerlink" title="3.1 The approach overview"></a>3.1 The approach overview</h2><p>本文提出的移动物体检测算法流程图如Fig. 1所示：</p>
<p><img src="/2024/02/24/dp-slam/fig1.png" alt="fig1" title="figure 1"></p>
<p>本方法结合几何模型和语义分割进行动态物体检测，几何模型和语义分割的结果转化为<strong>观测概率</strong>，前一帧中关键点的移动概率被视为<strong>先验概率</strong>，基于<strong>贝叶斯理论</strong>，每个关键点的移动概率可通过观测概率和先验概率进行更新。</p>
<p><img src="/2024/02/24/dp-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-2-Semantic-segmentation"><a href="#3-2-Semantic-segmentation" class="headerlink" title="3.2 Semantic segmentation"></a>3.2 Semantic segmentation</h2><p>本方法使用Mask R-CNN 网络获取<strong>像素级语义信息</strong>，将场景中的<strong>先验动态物体</strong>（人、自行车、汽车等）掩码生成到一张图片中；由于CNN 网络的精度限制，导致部分关键点分类错误的情况，特别是在物体<strong>轮廓边界</strong>附近，为了实现更精确的分类结果，作者使用<strong>二项式逻辑回归binomial logistic regression 模型</strong>来计算每个关键点的<strong>语义分割动态概率</strong>，如Fig. 3所示，掩码内具有低得不正常的移动概率（0.75）的关键点更有可能是识别错误的，应该是位于静态背景中的关键点。</p>
<p><img src="/2024/02/24/dp-slam/fig3.png" alt="fig3" title="figure 3"></p>
<p>用来估计关键点 $p_i$ <strong>语义分割动态概率</strong>的二项式逻辑回归模型如下所示：</p>
<p><img src="/2024/02/24/dp-slam/f0.png" alt="f0" title="formula 0"></p>
<p>其中，$\alpha$ 为影响因子来平滑即时检测结果，本文中被设定为0.1；$dist(p_i, z_t)$ 为关键点 $p_i$ 与语义分割掩码边界之间的距离。上式的含义在于，位于先验动态物体掩码内的关键点<strong>距离边界越近</strong>，那么该关键点的<strong>语义分割动态概率越小</strong>，即该点被错误分类的可能性越高。</p>
<h2 id="3-3-Epipolar-geometry-constraint"><a href="#3-3-Epipolar-geometry-constraint" class="headerlink" title="3.3 Epipolar geometry constraint"></a>3.3 Epipolar geometry constraint</h2><p>Mask R-CNN 网络是由COCO 数据集训练的，只能对<strong>预定义的种类</strong>进行识别，不能对其他物体进行检测，因此，作者额外使用<strong>对极几何约束</strong>来检测关键点的移动概率。如Fig. 4所示，计算当前帧<strong>匹配点与极线</strong>之间的距离判断该点的移动概率。</p>
<p><img src="/2024/02/24/dp-slam/fig4.png" alt="fig4" title="figure 4"></p>
<p>利用对极几何约束计算特征点的移动概率过程如算法1所示，具体步骤如下所示：</p>
<ol>
<li>首先，通过计算当前帧的<strong>光流金字塔</strong>来获取匹配的关键点，剔除掉那些距离图片边缘过近或匹配点对之间像素差异过大的点；</li>
<li>然后，利用RANSAC 算法获取<strong>基础矩阵</strong>，在此基础上计算关键点距离极线的距离，若大于一定阈值（0.75）则判定为外点。</li>
</ol>
<p><img src="/2024/02/24/dp-slam/a1.png" alt="a1" title="algorithm 1"></p>
<p>由于动态物体的存在，图片中每个关键点不会严格位于对应的极线上，距离越大，移动概率就越大；因此，作者假设关键点与其极线的距离满足<strong>高斯分布</strong>：</p>
<p><img src="/2024/02/24/dp-slam/f1.png" alt="f1" title="formula 1"></p>
<h2 id="3-4-Iteratively-moving-probability-update"><a href="#3-4-Iteratively-moving-probability-update" class="headerlink" title="3.4 Iteratively moving probability update"></a>3.4 Iteratively moving probability update</h2><p>持续跟踪动态物体会<strong>极大提高</strong>定位表现，因此，作者提出一种<strong>移动概率传递算法</strong>，从多帧图片中结合几何模型和语义分割信息进行动态物体检测。</p>
<p>定义关键点 $p_i$ 在时间 t 的<strong>运动状态</strong>为 $D_t(p_i)$ ，若该关键点被判定为动态点，则 $D_t(p_i)=1$ ，否则为0。结合来自<strong>语义模型和语义分割的移动概率</strong> $P(D_t(p_i) | c_{p_i}^t), P(D_t(p_i) | s_{p_i}^t)$ ：</p>
<p><img src="/2024/02/24/dp-slam/f2.png" alt="f2" title="formula 2"></p>
<p>其中，</p>
<p><img src="/2024/02/24/dp-slam/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$N_c, N_s$ 分别表示几何模型、语义分割识别出当前帧中<strong>动态点的数量</strong>。</p>
<p>在这个过程中，作者假设动态概率传递模型拥有<strong>马尔可夫特性</strong>，则基于贝叶斯理论的关键点<strong>移动概率传递模型</strong>表示为：</p>
<p><img src="/2024/02/24/dp-slam/f4.png" alt="f4" title="formula 4"></p>
<p>观测概率表示为：</p>
<p><img src="/2024/02/24/dp-slam/f5.png" alt="f5" title="formula 5"></p>
<p>其中，</p>
<p><img src="/2024/02/24/dp-slam/f6.png" alt="f6" title="formula 6"></p>
<p>最后，将关键点移动概率 $P(D_t(p_i)|c_{p_i}^t, s_{p_i}^t) &gt; 0.5$ 的关键点视为<strong>外点</strong>，在后续的跟踪制图进程中不再使用。</p>
<p><img src="/2024/02/24/dp-slam/a2.png" alt="a2" title="algorithm 2"></p>
<h2 id="3-5-Background-inpainting"><a href="#3-5-Background-inpainting" class="headerlink" title="3.5 Background inpainting"></a>3.5 Background inpainting</h2><p>在移除掉动态物体后，根据之前的<strong>静态观测</strong>来补全遮挡的背景区域，生成一个不包含动态物体的静态图片，该静态图片包含环境中的静态结构，有益于后续的<strong>回环检测和制图</strong>。</p>
<p>作者将本算法与其他类似的方法进行比较，结果如Fig. 10所示，证明了本算法的优势。</p>
<p><img src="/2024/02/24/dp-slam/fig10.png" alt="fig10" title="figure 10"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 DS-SLAM_A Semantic Visual SLAM towards Dynamic Environments</title>
    <url>/2024/02/04/ds-slam/</url>
    <content><![CDATA[<p>Yu, Chao, Zuxin Liu, Xin-Jun Liu, Fugui Xie, Yi Yang, Qi Wei, and Qiao Fei. “DS-SLAM: A Semantic Visual SLAM towards Dynamic Environments.” In <em>2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 1168–74. Madrid: IEEE, 2018. <a href="https://doi.org/10.1109/IROS.2018.8593691">https://doi.org/10.1109/IROS.2018.8593691</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ol>
<li>提出一个基于ORB-SLAM2 应用于动态环境中的语义SLAM 系统——DS-SLAM，在公开数据集上进行测试，证明了本系统在动态环境中的<strong>精度与鲁棒性</strong>；</li>
<li>添加一个<strong>实时语义分割网络线程</strong>，结合<strong>语义分割和运动一致性检验</strong>来滤除场景中的动态区域；</li>
<li>创建了一个构建稠密3D 语义八叉树地图的线程。</li>
</ol>
<span id="more"></span>
<h1 id="3-System-Introduction"><a href="#3-System-Introduction" class="headerlink" title="3 System Introduction"></a>3 System Introduction</h1><h2 id="3-1-Framework-of-DS-SLAM"><a href="#3-1-Framework-of-DS-SLAM" class="headerlink" title="3.1 Framework of DS-SLAM"></a>3.1 Framework of DS-SLAM</h2><p>本章介绍五个部分的内容：</p>
<ol>
<li>DS-SLAM 的整体架构；</li>
<li>本算法中使用的实时语义分割方法；</li>
<li>特征点运动一致性检验；</li>
<li>外点剔除方法：结合语义分割和运动一致性来滤除动态物体；</li>
<li>构建语义八叉树地图的方法。</li>
</ol>
<h2 id="3-1-Framework-of-DS-SLAM-1"><a href="#3-1-Framework-of-DS-SLAM-1" class="headerlink" title="3.1 Framework of DS-SLAM"></a>3.1 Framework of DS-SLAM</h2><p>由于ORB-SLAM2 在实际应用中的优良表现，作者使用ORB-SLAM2 作为本算法的基本架构，来提供一个基于特征的全局SLAM 算法架构。系统整体架构如Fig. 1所示，DS-SLAM 运行5个线程：<strong>跟踪</strong>，<strong>语义分割</strong>，<strong>局部制图</strong>，<strong>回环检测</strong>和<strong>稠密建图</strong>。</p>
<p><img src="/2024/02/04/ds-slam/fig1.png" alt="fig1" title="figure 1"></p>
<p>系统框架如Fig. 2所示，整体流程如下所示：</p>
<ol>
<li>RGB 图片同时被<strong>跟踪线程</strong>和<strong>语义分割线程</strong>处理；</li>
<li>跟踪线程首先提取ORB 特征点，然后粗略检验特征点的<strong>运动一致性，</strong>并保存<strong>潜在的外点；</strong></li>
<li>待像素级语义分割结果生成后，基于<strong>语义分割结果</strong>和上步检测到的<strong>潜在外点</strong>，识别出<strong>动态物体</strong>，并剔除位于<strong>动态物体内的ORB 外点</strong>；</li>
<li>最后，利用剩余的稳定特征点来计算转换矩阵。</li>
</ol>
<p><img src="/2024/02/04/ds-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-3-Moving-Consistency-Check"><a href="#3-3-Moving-Consistency-Check" class="headerlink" title="3.3 Moving Consistency Check"></a>3.3 Moving Consistency Check</h2><p>本系统认为，<strong>如果一个分割物体内的一些点被识别为动态的，那么就判定该物体是动态物体</strong>。</p>
<p><strong>运动一致性检验</strong>步骤如下：</p>
<ol>
<li>首先，计算<strong>光流金字塔</strong>来获取当前帧中的匹配特征点；</li>
<li>然后，如果特征点匹配对太靠近图片边缘，或者两帧图片以特征点中心的3*3 图像块的像素差异过大，那么该匹配对也会被舍弃；</li>
<li>接着，利用<strong>RANSAC</strong> 策略计算<strong>基础矩阵</strong>，在此基础上计算当前帧中的<strong>极线</strong>；</li>
<li>最后，根据匹配点对与极线之间的<strong>距离</strong>来判断该匹配点对是否是动态的。</li>
</ol>
<p>基础矩阵将上一帧的特征点投影至当前帧上，得到对应的<strong>搜索区域</strong>，即<strong>极线</strong>。假设相邻两帧的匹配点对齐次坐标与像素坐标为：</p>
<p><img src="/2024/02/04/ds-slam/f1.png" alt="f1" title="formula 1"></p>
<p>然后，利用下式计算对应的<strong>极线</strong>：</p>
<p><img src="/2024/02/04/ds-slam/f2.png" alt="f2" title="formula 2"></p>
<p>其中，$[X,Y,Z]^T$ 表示极线向量；F 表示基础矩阵。在此基础上，计算当前帧匹配点与极线之间的距离：</p>
<p><img src="/2024/02/04/ds-slam/f3.png" alt="f3" title="formula 3"></p>
<p>运动一致性检验算法整体流程如算法1所示：</p>
<p><img src="/2024/02/04/ds-slam/a1.png" alt="a1" title="algorithm 1"></p>
<h2 id="3-4-Outliers-Rejection"><a href="#3-4-Outliers-Rejection" class="headerlink" title="3.4 Outliers Rejection"></a>3.4 Outliers Rejection</h2><p>利用几何方法难以确定动态区域的轮廓，因此DS-SLAM 结合语义分割结果来得到动态物体的轮廓：利用运动一致性检验得到<strong>动态特征点集合</strong>，然后判断物体轮廓内动态特征点的数量<strong>是否超过阈值</strong>，若超过阈值，则判定该物体属于<strong>动态物体</strong>，然后剔除掉属于该物体的<strong>所有特征点</strong>。</p>
<p>作者的实验证明，本系统ORB 特征提取时间+移动一致性检验的时间，与语义分割线程所需的时间基本一致。</p>
<p><img src="/2024/02/04/ds-slam/t1.png" alt="t1" title="table 4"></p>
<p>由于人类活动会在大部分真实场景中影响机器人的定位，所以作者以人类作为一个典型示例，展示DS-SLAM 的效果，<strong>但理论上DS-SLAM 可以应用于多种不同的动态物体检测。</strong></p>
<p>作者判断动态物体的策略为：如果语义分割结果中没有人类，则使用<strong>所有的ORB 特征点直接</strong>进行匹配与位姿解算；如果有人类被检测到，则结合运动一致性来检验动态物体。</p>
<h1 id="4-Experimental-Results"><a href="#4-Experimental-Results" class="headerlink" title="4 Experimental Results"></a>4 Experimental Results</h1><p><img src="/2024/02/04/ds-slam/t3.png" alt="t3" title="table 3"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 DynaTM-SLAM_Fast filtering of dynamic feature points and object-based localization in dynamic indoor environments</title>
    <url>/2024/03/01/dynatm-slam/</url>
    <content><![CDATA[<p>Zhong, Meiling, Chuyuan Hong, Zhaoqian Jia, Chunyu Wang, and Zhiguo Wang. “DynaTM-SLAM: Fast Filtering of Dynamic Feature Points and Object-Based Localization in Dynamic Indoor Environments.” <em>Robotics and Autonomous Systems</em> 174 (April 2024): 104634. <a href="https://doi.org/10.1016/j.robot.2024.104634">https://doi.org/10.1016/j.robot.2024.104634</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>针对dynamic SLAM，作者提到DS-SLAM 和 DynaSLAM 是两个代表性作品：</p>
<ul>
<li>DS-SLAM 通过<strong>语义分割</strong>和<strong>运动一致性</strong>检验来减少动态物体的影响，但是其效果因高度依赖<strong>阈值设定</strong>而受限，因为该方法是根据物体内动态特征点所占比例来判断该物体是否是动态的。</li>
<li>DynaSLAM 使用<strong>语义分割</strong>和<strong>多视图几何理论</strong>来检测动态物体，该方法利用一个区域成长算法 region growing algorithm ，从孤立的外点来判断整个区域是否是动态的，即便该区域只有一个特征点被判别为外点，那么整个区域就被判定为是动态的；这就导致该算法对外点检测的表现要求极高。</li>
</ul>
<p>且上述两个算法均是使用语义分割网络来获取语义信息，其<strong>复杂度较高、运算速度较慢</strong>，而且均使用了<strong>先验动态物体假设</strong>，具有局限性。</p>
<span id="more"></span>
<p>本文作者基于ORB-SLAM2 算法，提出一种<strong>在线滤除动态物体</strong>的算法，并使用更多的<strong>静态物体信息</strong>来优化相机位姿：在RGB 图片上进行物体检测之后，使用滑动窗口约束的<strong>模板匹配方法</strong>来检测动态物体，而不需要预先假定某类物体的运动状态；此外，还为静态物体建立数据关联，为系统增加额外的<strong>物体级语义约束</strong>。</p>
<p>本文的主要贡献如下：</p>
<ol>
<li>在ORB-SLAM2 中增加一个<strong>物体检测线程</strong>，在RGB 图片上进行<strong>物体检测</strong>，并使用<strong>物体匹配</strong>和<strong>滑动窗口</strong>来滤除真正的动态特征点，并从静态物体上提取语义信息。该算法可<strong>精确识别</strong>物体的当前运动状态，甚至可以识别出物体<strong>极小幅度的运动</strong>，如人的轻微晃动等；且<strong>时间复杂度较低</strong>。</li>
<li>提出一种使用静态<strong>物体的语义约束来优化相机位姿</strong>，该算法利用静态物体的语义和几何信息来建立物体级的数据关联，从而构建一个在线<strong>物体数据库</strong>。</li>
</ol>
<h1 id="3-System-overview"><a href="#3-System-overview" class="headerlink" title="3 System overview"></a>3 System overview</h1><p>DynaTM-SLAM 的整体架构如Fig. 1 所示，对每一张输入图片进行ORB 特征提取和物体检测，YOLOv7 可实现高精度高帧率（30FPS）的检测效果；然后使用<strong>滑动窗口机制</strong>和<strong>模板匹配算法</strong>进行动态特征点剔除；在跟踪相机位姿阶段，基于<strong>相似物体间的中心距离</strong>构建额外的语义约束进行相机位姿优化。</p>
<p><img src="/2024/03/01/dynatm-slam/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-1-Sliding-Window-amp-Template-Matching"><a href="#3-1-Sliding-Window-amp-Template-Matching" class="headerlink" title="3.1 Sliding Window &amp; Template Matching"></a>3.1 Sliding Window &amp; Template Matching</h2><p>检测物体运动状态有两种常用的方法：</p>
<ol>
<li>方法一：建立一个相对参考坐标系来观察物体的运动状态；</li>
<li>方法二：比较不同时间图片在同一区域内的相似度，若相似度高，则判定为静态的。</li>
</ol>
<p>方法一对相对参考坐标系的选取要求较高，若选取的坐标系本身是动态的，那么检测会失败；因此，作者使用<strong>基于方法二的模板匹配算法</strong>来估计物体的运动状态。</p>
<p>本文作者使用<strong>标准相关系数匹配</strong>来减小计算结果中光照的影响：</p>
<p><img src="/2024/03/01/dynatm-slam/f1.png" alt="f1" title="formula 1"></p>
<p>其中，I 和 T 分别表示源图片和目标图片；$(x, y)$ 表示像素坐标。</p>
<p>利用<strong>滑动窗口机制</strong>，DynaTM-SLAM 通过模板匹配来比较固定时间间隔的两张图片，来检测物体的运动状态；窗口内存储固定数量的连续图片。该过程如Fig. 2所示：图片j 中的检测框box1 经<strong>位姿转换矩阵</strong> $T_{ij}$ 投影至图片i 中，得到检测框box2；然后对box1 和 box2 进行<strong>模板匹配</strong>，如果匹配数值超过一个预定义的阈值，则判定box1 是静态的。</p>
<p><img src="/2024/03/01/dynatm-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-2-Data-Association"><a href="#3-2-Data-Association" class="headerlink" title="3.2 Data Association"></a>3.2 Data Association</h2><p>数据关联指的是<strong>当前帧</strong>检测到的物体与<strong>地图中现存</strong>物体的匹配与关联。利用RGB 图片获取的物体检测bbox，结合深度图，可以得到对应的物体3D 点云；物体检测会包含部分背景信息，作者使用<strong>基于密度的聚类方法</strong>来剔除背景点，然后在于数据库中的物体进行匹配；聚类的可视化过程如Fig. 3所示：</p>
<p><img src="/2024/03/01/dynatm-slam/fig3.png" alt="fig3" title="figure 3"></p>
<p>本算法构建一个物体数据库并进行实时数据关联，物体中心的计算如下所示：</p>
<p><img src="/2024/03/01/dynatm-slam/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$r_i = (x_i, y_i, z_i), i = 1, 2, …, n$ 表示每个粒子的坐标；$m_i$ 表示每个粒子的质量。</p>
<p>在实际应用中，作者假设每个粒子的质量为1，则上式演变为：</p>
<p><img src="/2024/03/01/dynatm-slam/f4.png" alt="f4" title="formula 4"></p>
<h2 id="3-3-Bundle-Adjustment-with-Static-Objects"><a href="#3-3-Bundle-Adjustment-with-Static-Objects" class="headerlink" title="3.3 Bundle Adjustment with Static Objects"></a>3.3 Bundle Adjustment with Static Objects</h2><p>DynaTM-SLAM 使用因子图进行BA 优化，如Fig. 4所示：</p>
<p><img src="/2024/03/01/dynatm-slam/fig4.png" alt="fig4" title="figure 4"></p>
<p>地图点重投影误差表示为：</p>
<p><img src="/2024/03/01/dynatm-slam/f6.png" alt="f6" title="formula 6"></p>
<p>物体中心距离误差表示为：</p>
<p><img src="/2024/03/01/dynatm-slam/f7.png" alt="f7" title="formula 7"></p>
<p>其中，$^cp’$ 表示当前帧构建物体的中心坐标；$^cp$ 表示数据库中同一物体的中心坐标；$T_{cw}, T_{cw}^\ast$ 分别是当前帧的初始位姿估计、优化后的位姿估计。</p>
<h1 id="4-Experimental-results"><a href="#4-Experimental-results" class="headerlink" title="4 Experimental results"></a>4 Experimental results</h1><h2 id="4-1-Ablation-Study"><a href="#4-1-Ablation-Study" class="headerlink" title="4.1 Ablation Study"></a>4.1 Ablation Study</h2><p>只包含动态特征点滤除的方法记为 DynaTM-SLAM（A），只包含静态物体中心约束的方法记为 DynaTM-SLAM（B），两种方法的结合记为DynaTM-SLAM（A+B）。实验结果如Table 1所示，其中，前四个为高动态场景，本文提出的方法有巨大精度提升；后两个为低动态场景，本文提出的方法有略微提升，这是因为ORB-SLAM2 本身在这种环境中表现就很不错，但本方法加上了物体级约束后，会有轻微的提升效果。</p>
<p><img src="/2024/03/01/dynatm-slam/t1.png" alt="t1" title="table 1"></p>
<h2 id="4-2-Comparison-with-other-advanced-SLAM-algorithms"><a href="#4-2-Comparison-with-other-advanced-SLAM-algorithms" class="headerlink" title="4.2 Comparison with other advanced SLAM algorithms"></a>4.2 Comparison with other advanced SLAM algorithms</h2><p>Table 2，3，4展示了本方法与其他dynamic SLAM 方法的效果对比。</p>
<p><img src="/2024/03/01/dynatm-slam/t2.png" alt="t2" title="table 2"></p>
<p><img src="/2024/03/01/dynatm-slam/t3.png" alt="t3" title="table 3"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Pose Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 DynaSLAM_Tracking, Mapping, and Inpainting in Dynamic Scenes</title>
    <url>/2024/01/30/dynaslam/</url>
    <content><![CDATA[<p>Bescos, Berta, Jose M. Facil, Javier Civera, and Jose Neira. “DynaSLAM: Tracking, Mapping, and Inpainting in Dynamic Scenes.” <em>IEEE Robotics and Automation Letters</em> 3, no. 4 (October 2018): 4076–83. <a href="https://doi.org/10.1109/LRA.2018.2860039">https://doi.org/10.1109/LRA.2018.2860039</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文提出的DynaSLAM 是在ORB-SLAM2 基础上增加一个<strong>前端模块</strong>，来处理环境中的动态物体。对于单目和双目相机，使用CNN 产生物体的像素级<strong>语义分割结果</strong>，剔除掉先验动态物体中的特征点；对于RGB-D 相机，结合多视图几何模型和CNN 来检测动态物体，从图片中移除动态物体并进行场景恢复。</p>
<span id="more"></span>
<p><img src="/2024/01/30/dynaslam/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-System-Description"><a href="#3-System-Description" class="headerlink" title="3 System Description"></a>3 System Description</h1><p>本系统架构如Fig. 2所示，对于RGB-D 相机，本系统使用<strong>多视图几何</strong>在两方面提高动态物体分割的效果：</p>
<ol>
<li>对Mask R-CNN 获取的<strong>先验动态物体分割区域</strong>进行<strong>细调</strong>；</li>
<li>利用多视图几何判断<strong>先验静态物体</strong>是否是动态的。</li>
</ol>
<p><strong>值得注意的是，</strong>对于单目或双目相机，CNN 获取的语义分割中，属于先验动态物体的特征点<strong>直接被舍弃</strong>，不参与跟踪或制图。</p>
<p><img src="/2024/01/30/dynaslam/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-Segmentation-of-Potential-Dynamic-Content-Using-a-CNN"><a href="#3-1-Segmentation-of-Potential-Dynamic-Content-Using-a-CNN" class="headerlink" title="3.1 Segmentation of Potential Dynamic Content Using a CNN"></a>3.1 Segmentation of Potential Dynamic Content Using a CNN</h2><p>本系统使用Mask R-CNN 实现像素级语义分割，利用MS COCO 数据集进行训练，识别出图片中的<strong>先验动态物体</strong>。</p>
<h2 id="3-2-Low-Cost-Tracking"><a href="#3-2-Low-Cost-Tracking" class="headerlink" title="3.2 Low-Cost Tracking"></a>3.2 Low-Cost Tracking</h2><p>利用语义分割获取先验动态物体后，利用图片中的静态区域进行相机位姿估计，利用<strong>地图点重投影误差</strong>来优化相机位姿。此外，作者认为分割轮廓线通常是<strong>高梯度区域</strong>，因此，在进行位姿估计时也不考虑轮廓线区域内的特征点。</p>
<h2 id="3-3-Segmentation-of-Dynamic-Content-Using-Mask-R-CNN-and-Multi-View-Geometry"><a href="#3-3-Segmentation-of-Dynamic-Content-Using-Mask-R-CNN-and-Multi-View-Geometry" class="headerlink" title="3.3 Segmentation of Dynamic Content Using Mask R-CNN and Multi-View Geometry"></a>3.3 Segmentation of Dynamic Content Using Mask R-CNN and Multi-View Geometry</h2><p>由于<strong>先验静态物体</strong>也可能是移动的，所以需要结合<strong>多视图几何</strong>实现更精确的动态物体检测。对于每个输入帧，作者选取5个与之拥有<strong>最高重叠度</strong>的关键帧，此处考虑两帧之间的距离和旋转。然后，将这些关键帧中的所有特征点 $x$ 投影至当前帧中，得到关键点 $x’$ 以及<strong>投影深度</strong> $z_{proj}$ 。然后计算 $x, x’$ 之间的<strong>视差角</strong> $\alpha$ ，如果该角度大于30°，则该点可能被遮挡，应当<strong>忽略该点</strong>；然后从当前帧的深度图中获取 $x’$ 的深度 $z’$ ，比较 $\Delta z = z_{proj} - z’$ 是否超过阈值，若超过阈值则判定该点为<strong>动态特征点</strong>。</p>
<p><img src="/2024/01/30/dynaslam/fig3.png" alt="fig3" title="figure 3"></p>
<p>对于位于动态物体<strong>边缘</strong>上的特征点，在深度图中如果该点周边块的<strong>方差较高</strong>，则判定该点为<strong>静态点</strong>。为了区分所有属于动态物体的像素，在深度图中，对动态特征点周围的区域进行<strong>生成操作</strong>，得到相应的动态区域掩码。</p>
<p>作者分析了CNN 方法和几何方法的优劣势：</p>
<ul>
<li><strong>几何方法</strong>：可以检测出任意物体的动态属性；但主要问题在于其<strong>初始化</strong>需要多视角观测，且存在<strong>检测不准确</strong>，如Fig.4（a）所示，没有识别出桌子后面移动的人，作者认为有两种原因导致检测失败：<ol>
<li>RGB-D 相机测量远距离物体深度信息的准确度不足；</li>
<li>可靠的特征点基本位于图片的定义区域（也是近景区域？？）</li>
</ol>
</li>
<li><strong>CNN 方法</strong>：仅需单帧图片即可，无需初始化；但只能检测出先验动态物体，无法识别出移动的先验静态物体，如Fig.4（b）中人手上的书本未被检测出来。</li>
</ul>
<p>Fig. 4展示了结合语义信息与多视图几何信息（<strong>两种信息互补</strong>）得到最终的动态物体检测效果。同时，作者提到，如果一个物体同时被两种方法检测到属于动态的，则采用几何方法获取的掩码区域为准；若只被深度学习检测到，则使用语义信息代表的分割掩码区域（如Fig.4所示）。</p>
<p>即，本方法对特征点的动态属性判断使用的是<strong>“或“操作</strong>：两种动态判定方法中只要满足一种，即判定该点属于动态特征点。</p>
<p><img src="/2024/01/30/dynaslam/fig4.png" alt="fig4" title="figure 4"></p>
<h2 id="3-4-Tracking-and-Mapping"><a href="#3-4-Tracking-and-Mapping" class="headerlink" title="3.4 Tracking and Mapping"></a>3.4 Tracking and Mapping</h2><p>本阶段的系统输入为RGB 和深度图，以及相应的分割掩码，此处使用静态ORB 特征点，不包括分割线附近区域的特征点。</p>
<h2 id="3-5-Background-Inpainting"><a href="#3-5-Background-Inpainting" class="headerlink" title="3.5 Background Inpainting"></a>3.5 Background Inpainting</h2><p>使用其他视角的静态区域来修复当前帧中的动态掩码区域，作者认为这对于VR、AR等应用具有重要意义。</p>
<h1 id="4-Experimental-Results"><a href="#4-Experimental-Results" class="headerlink" title="4 Experimental Results"></a>4 Experimental Results</h1><p><img src="/2024/01/30/dynaslam/t1.png" alt="t1" title="table 1"></p>
<p>其中，N 表示只使用CNN 获取先验动态物体进行动态物体剔除；G 表示只使用基于深度差异的多视图几何方法；N+G 表示两种方法结合；N+G+BI 代表Fig.6所示的方法，该方法中，背景修复 (background inpainting, BI) 是在跟踪和制图前完成，该方法是为了验证修复完动态物体区域后，在<strong>静态世界假设</strong>下进行传统SLAM 计算的精度。</p>
<p><img src="/2024/01/30/dynaslam/fig6.png" alt="fig6" title="figure 6"></p>
<p>Table 1的结果说明N+G 方法效果最好；作者分析G 方法效果较差的原因在于：需要运动（多视角观测）进行<strong>初始化</strong>，因此，该方法的检测效果具有<strong>延迟性</strong>，需要经过一段时间后才会准确，而在此期间的估计误差较大。</p>
<p><img src="/2024/01/30/dynaslam/t2.png" alt="t2" title="table 2"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 EAO-SLAM_Monocular Semi-Dense Object SLAM Based on Ensemble Data Association</title>
    <url>/2024/03/08/eao-slam/</url>
    <content><![CDATA[<p>Wu, Yanmin, Yunzhou Zhang, Delong Zhu, Yonghui Feng, Sonya Coleman, and Dermot Kerr. “EAO-SLAM: Monocular Semi-Dense Object SLAM Based on Ensemble Data Association.” In <em>2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 4966–73. Las Vegas, NV, USA: IEEE, 2020. <a href="https://doi.org/10.1109/IROS45743.2020.9341757">https://doi.org/10.1109/IROS45743.2020.9341757</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者认为object SLAM 当前面对两种问题：</p>
<ol>
<li>在面对包含多个物体实例的复杂场景中，现有的<strong>DA 方法</strong>不够鲁棒或准确。</li>
<li><strong>物体位姿估计</strong>不够准确，特别是对于单目object SLAM。</li>
</ol>
<p>为此，作者提出了一个单目object SLAM 算法——EAO-SLAM，来解决DA 和位姿估计问题。</p>
<span id="more"></span>
<p>本文的贡献如下：</p>
<ol>
<li>提出一种<strong>集合数据关联策略</strong>，可有效聚合不同的观测信息来提高DA 精度；</li>
<li>提出一种基于iForest (isolation forest) 的<strong>物体位姿估计架构</strong>，该架构对外点具有鲁棒性，可准确实现对物体位置、姿态及尺寸的估计；</li>
<li>基于提出的方法，应用EAO-SLAM 构建<strong>轻量级面向物体的语义地图</strong>；</li>
<li>在公开数据集和真实场景中进行测试，证明了本方法的有效性，代码开源：<a href="https://github.com/yanmin-wu/EAO-SLAM。">https://github.com/yanmin-wu/EAO-SLAM。</a></li>
</ol>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3 System Overview"></a>3 System Overview</h1><p>系统整体架构如Fig.2所示，该算法是基于ORB-SLAM2 进一步开发的：</p>
<ol>
<li><strong>集合数据关联</strong>是在跟踪线程完成的，结合了<strong>bbox</strong>、<strong>语义标签</strong>和<strong>点云信息</strong>；</li>
<li>在DA 的基础上，使用iForest 来<strong>消除外点</strong>，并为联合优化提供准确的<strong>初始位姿估计</strong>；</li>
<li>然后对物体位姿与尺度、相机位姿进行<strong>联合优化</strong>，并构建一个<strong>轻量级物体语义地图</strong>；</li>
<li>最后，将该语义地图与半稠密地图进行结合，生成<strong>半稠密语义地图</strong>。</li>
</ol>
<p><img src="/2024/03/08/eao-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h1 id="4-Ensemble-Data-Association"><a href="#4-Ensemble-Data-Association" class="headerlink" title="4 Ensemble Data Association"></a>4 Ensemble Data Association</h1><p>本文的标注：</p>
<p><img src="/2024/03/08/eao-slam/fig2-1.png" alt="fig2-1" title="figure 2-1"></p>
<h2 id="4-1-Nonparametric-Test"><a href="#4-1-Nonparametric-Test" class="headerlink" title="4.1 Nonparametric Test"></a>4.1 Nonparametric Test</h2><p>非参数测试是用来处理物体点云的（Fig.3 a中的红绿点），根据作者的实验研究（6.1节），这些点云<strong>不服从高斯分布</strong>。理论上讲，若P 和 Q 属于同一个物体，那么它们应该服从相同的分布，即 $f_P = f_Q$ ，接下来作者进行验证。</p>
<p><img src="/2024/03/08/eao-slam/fig3.png" alt="fig3" title="figure 3"></p>
<p>作者使用Mann-Whitney statistics：$W = min(W_P,W_Q)$ ，其中 $W_P,W_Q$ 定义如下：</p>
<p><img src="/2024/03/08/eao-slam/f1.png" alt="f1" title="formula 1"></p>
<p>最终，作者证明如果两组点云 P、Q的Mann-Whitney statistics W满足式4，则证明这两组点云属于同一个物体，即标明DA 成功：</p>
<p><img src="/2024/03/08/eao-slam/f4.png" alt="f4" title="formula 4"></p>
<h2 id="4-2-Single-sample-and-Double-sample-T-test"><a href="#4-2-Single-sample-and-Double-sample-T-test" class="headerlink" title="4.2 Single-sample and Double-sample T-test"></a>4.2 Single-sample and Double-sample T-test</h2><p><strong>单采样t-test</strong> 用于处理不同帧中观测到的物体中心（Fig.3 b 中的星形），被证明服从高斯分布（6.1节）。作者假设 $C, \mathbf{c}$ 来自同一物体，定义t：</p>
<p><img src="/2024/03/08/eao-slam/f5.png" alt="f5" title="formula 5"></p>
<p>作者阐述，如果t 满足式6，则可证明假设成立，即 $C, \mathbf{c}$ 来自同一物体。</p>
<p><img src="/2024/03/08/eao-slam/f6.png" alt="f6" title="formula 6"></p>
<p>由于上述严格的DA 策略，以及较差的观测视角等情况，有可能将现有的物体判定为新物体，为此，作者提出了<strong>双采样t-test</strong>，通过对历史中心距离进行测试来判定是否需要融合两个物体（Fig.3 c）。为 $C_1, C_2$ 构建t：</p>
<p><img src="/2024/03/08/eao-slam/f7.png" alt="f7" title="formula 7"></p>
<p>同样的，如果t 满足式6，则证明属于同一物体，进行融合。</p>
<h1 id="5-Object-SLAM"><a href="#5-Object-SLAM" class="headerlink" title="5 Object SLAM"></a>5 Object SLAM</h1><p>本节内容的标注：</p>
<p><img src="/2024/03/08/eao-slam/fig2-2.png" alt="fig2-2" title="figure 2-2"></p>
<h2 id="5-1-Object-Representation"><a href="#5-1-Object-Representation" class="headerlink" title="5.1 Object Representation"></a>5.1 Object Representation</h2><p>作者使用<strong>立方体</strong>和<strong>二次曲面</strong>来表示物体：</p>
<ul>
<li>对于常规形状的物体，如书、键盘、椅子等，作者使用立方体来表示（使用顶点 $P_O$ 进行编码）；</li>
<li>对于没有具体方向的非常规物体，如球、瓶子等，作者使用二次曲面来表示（使用半轴 $Q_O$ 进行编码）。</li>
</ul>
<p>立方体和二次曲面在<strong>世界坐标系</strong>中的表示为：</p>
<p><img src="/2024/03/08/eao-slam/f9.png" alt="f9" title="formula 9"></p>
<p>作者假设物体被放置在与地面平行的状态，即 $\theta_r = \theta_p = 0$ ，则对于立方体，只需要估计 $[\theta_y, \mathbf{t}, \mathbf{s}]$ ；对于二次曲面，只需要估计  $[ \mathbf{t}, \mathbf{s}]$ 。</p>
<h2 id="5-2-Estimate-t-and-s"><a href="#5-2-Estimate-t-and-s" class="headerlink" title="5.2 Estimate t and s"></a>5.2 Estimate <em>t</em> and <em>s</em></h2><p>基于iForest 的外点鲁棒的物体中心和尺度估计算法如下所示：</p>
<p><img src="/2024/03/08/eao-slam/a1.png" alt="a1" title="algorithm 1"></p>
<p>该算法的关键点在于：将数据点递归分为一系列孤立的数据点，将<strong>最容易被孤立出来</strong>的点视为外点。如Fig.4所示，d中的黄点经过4步被孤立出来，所以其步长为4，而e中的绿点步长为8，那么黄点<strong>更有可能是外点</strong>。</p>
<p><img src="/2024/03/08/eao-slam/fig4.png" alt="fig4" title="figure 4"></p>
<p>经过算法1，使用剩下的点计算 $\mathbf{t},\mathbf{s}$ ，基于 $\mathbf{s}$ 可在物体坐标系中初始化构建立方体和二次曲面，如Fig.4 a-c所示。 $\mathbf{s}$ 会与后续的物体和相机位姿得到进一步优化。</p>
<h2 id="5-3-Estimate-yaw-angle"><a href="#5-3-Estimate-yaw-angle" class="headerlink" title="5.3 Estimate yaw angle"></a>5.3 Estimate yaw angle</h2><p>$\theta_y$ 的优化分为两步：首先找到一个良好的初始化值，然后基于初始化进行数次优化。位姿初始化过程如算法2所示：</p>
<p><img src="/2024/03/08/eao-slam/a2.png" alt="a2" title="algorithm 2"></p>
<p>LSD 线段是从t 张连续图片中获取，位于bbox 内部的线段被分配给对应物体，如Fig.5 a所示。物体的初始姿态被假定为与全局坐标系保持一致，即 $\theta_0 =0$ （Fig.5 b），然后在 $[-\pi/2, \pi/2]$ 中均匀采样30个角度，针对每个采样角度，通过计算LSD 线段 $Z_{lsd}$ 和2D 投影边（由上步得到的立方体进行投影得到）之间的<strong>累积角度误差</strong>来得到该采样角度的得分，最终选取最高得分的角度作为初始值参与后续的联合优化。</p>
<p><img src="/2024/03/08/eao-slam/fig5.png" alt="fig5" title="figure 5"></p>
<h2 id="5-4-Joint-Optimization"><a href="#5-4-Joint-Optimization" class="headerlink" title="5.4 Joint Optimization"></a>5.4 Joint Optimization</h2><p>利用初始估计进行联合优化：</p>
<p><img src="/2024/03/08/eao-slam/f15.png" alt="f15" title="formula 15"></p>
<p>其中，$e(\theta)$ 为物体姿态误差，如Fig.5 e-g所示；$e(s)$ 定义为立方体投影边与最近的平行LSD 线段之间的距离；$e(p)$ 为传统SLAM 的重投影误差项。</p>
<h1 id="6-Experimental-Results"><a href="#6-Experimental-Results" class="headerlink" title="6 Experimental Results"></a>6 Experimental Results</h1><p>作者的实验包含以下部分：</p>
<ul>
<li>Distributions of Different Statistics</li>
<li>Ensemble Data Association Experiments</li>
<li>Qualitative Assessment of Object Pose Estimation</li>
<li>Object-Oriented Map Building</li>
</ul>
<p>本文算法着力于对物体位姿与尺寸进行估计，并构建半稠密物体级语义地图，没有对相机位姿解算效果进行介绍。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Pose Optimization</tag>
        <tag>Object SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>gcc、g++版本管理</title>
    <url>/2024/01/26/gcc-g/</url>
    <content><![CDATA[<p>本文主要参考<a href="https://zhuanlan.zhihu.com/p/261001751">文章</a>。</p>
<h2 id="1-版本查看"><a href="#1-版本查看" class="headerlink" title="1 版本查看"></a>1 版本查看</h2><p>对系统中的现有gcc、g++版本进行查看，安装所需版本：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 查看当前版本</span></span><br><span class="line">gcc -v</span><br><span class="line">g++ -v</span><br><span class="line"></span><br><span class="line"><span class="comment">## 查看系统已安装版本</span></span><br><span class="line"><span class="built_in">ls</span> /usr/bin/gcc*</span><br><span class="line"><span class="built_in">ls</span> /usr/bin/g++*</span><br><span class="line"></span><br><span class="line"><span class="comment">## 安装新版本</span></span><br><span class="line">sudo apt install gcc-11</span><br><span class="line">sudo apt install g++-11</span><br></pre></td></tr></table></figure>
<h2 id="2-版本切换"><a href="#2-版本切换" class="headerlink" title="2 版本切换"></a>2 版本切换</h2><p>首先，将已有版本添加到update-alternatives中：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 最后的数值代表该版本的权重参数，越大优先级越高</span></span><br><span class="line"><span class="comment"># gcc</span></span><br><span class="line">sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-9 100</span><br><span class="line">sudo update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-11 70</span><br><span class="line"></span><br><span class="line"><span class="comment"># g++</span></span><br><span class="line">sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 100</span><br><span class="line">sudo update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-11 70</span><br></pre></td></tr></table></figure>
<p>若想删除某个版本的管理：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo update-alternatives --remove gcc /usr/bin/gcc-11</span><br></pre></td></tr></table></figure>
<p>手动切换版本：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo update-alternatives --config gcc</span><br><span class="line">sudo update-alternatives --config g++</span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/gcc-g/g++.png" alt="g++" title="g++ version"></p>
<p>如上图所示，输入相应的id 即可实现不同版本之间的切换。</p>
<p>切换完之后，查看版本是否切换成功：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 查看当前版本</span></span><br><span class="line">gcc -v</span><br><span class="line">g++ -v</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>g++</tag>
        <tag>gcc</tag>
      </tags>
  </entry>
  <entry>
    <title>利用对极几何约束来检测动态特征点</title>
    <url>/2024/11/01/epipolar-constraints/</url>
    <content><![CDATA[<h1 id="方法介绍"><a href="#方法介绍" class="headerlink" title="方法介绍"></a>方法介绍</h1><p>对极几何约束常被用于检测特征点的运动状态，其原理示意图如Fig. 1所示：<br><span id="more"></span><br><img src="/2024/11/01/epipolar-constraints/fig1.png" alt="fig1" title="Fig. 1"><br>$X$ 是一地标点；$O_L$ ，$O_C$ 分别是两个相机的光心；$p_1$， $p_2$ 分别是 $X$ 在两个相机成像平面上的投影；$O_L$ ，$O_C$ 连线与成像平面的交点为极点 $e_1$，  $e_2$ ；$O_L$ ，$O_C$，$X$ 形成的平面为极平面；极平面与成像平面的相交直线为极线 $L_c$。理想情况下，对于静态地标点 $X$ ，其投影点 $p_2$ 应该位于极线 $L_c$ 上，然而，现实中存在各种观测误差，导致 $p_2$ 与极线 $L_c$ 存在一定的距离 $d_{epi}$ ，对于静态地标点，该距离 $d_{epi}$ 应该较小，而动态地标点对应的距离 $d_{epi}$ 会很大，现有的动态特征检测方法多根据距离 $d_{epi}$ 与预设的阈值进行比较来判定该特征的运动状态。</p>
<p>然而，对极几何约束存在缺陷，当路标点 $X$ 在极平面上移动时，其投影点仍会位于极线或距离极线很近，那么，此时就无法使用对极几何约束来准确判断特征点的运动状态，这种情况称为degenerate motion，如Fig. 2所示。这种情况最常见于自动驾驶场景，动态车辆与自身的运动方向相同，此时难以利用对极几何约束实现对动态特征点的检测。<br><img src="/2024/11/01/epipolar-constraints/fig2.png" alt="fig2" title="Fig. 2"></p>
<h1 id="论文方法-amp-问题"><a href="#论文方法-amp-问题" class="headerlink" title="论文方法 &amp; 问题"></a>论文方法 &amp; 问题</h1><p>Kundu 等人[Kundu 等, 2009]提出了一种方法来应对此种情况，称为 <em>Flow Vector Bound (FVB)</em> ，现对该方法进行介绍，以及对其中发现的问题进行描述。</p>
<p>作者假设两个相机之间只存在一个平移 $\mathbf{t}=[t_1,t_2,t_3]^T$，以第一个相机的坐标系作为世界坐标系，则静态路标点 $X=[x,y,z]^T$ 与第一个成像平面上的投影 $p_1$ 分别为：</p>
<script type="math/tex; mode=display">\displaylines{p_1 = \frac{1}{z}KX \\
X = zK^{-1}p_1 \tag{1}}</script><p>作者进而根据两个相机之间的平移转换，得到 $p_2$ 的坐标：</p>
<script type="math/tex; mode=display">p_2 = K[I|\mathbf{t}]X = p_1+\frac{K\mathbf{t}}{z} \tag{2}</script><p>然后，作者根据上式得到同一特征点在两幅成像平面上的坐标差为 $K\mathbf{t}/z$ ，作者通过设定距离阈值来判断特征点的运动状态。</p>
<p>但是，我在推导中发现上式存在问题；而且，根据上式，由于图中的任一特征点具有相同的 $K\mathbf{t}$ ，那么所有的特征点应该具有相同的光流方向，无非是光流尺度不同罢了。然而，根据实际情况发现并不如此，如Fig. 3所示。可以发现，位于不同区域的特征点具有不同的光流方向，后文会继续推导该方向是由特征点在图片中的不同象限决定的。<br><img src="/2024/11/01/epipolar-constraints/fig3.png" alt="fig3" title="Fig. 3"></p>
<h1 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h1><p>现在回到对 $p_2$ 的计算上去。$[I|\mathbf{t}]X$ 是将地标点 $X$ 从世界坐标系（相机1的坐标系）转换至相机2坐标系下，得到的坐标应为:</p>
<script type="math/tex; mode=display">X_{C2} = X + \mathbf{t} = [x+t_1, y+t_2, z+t_3]^T \tag{3}</script><p>然后，利用内参 $K$ 将其投影至成像坐标系中，在该过程中论文中的推导出现了问题，根据论文中的结果，反推其运算过程应为：</p>
<script type="math/tex; mode=display">p_2 = \frac{1}{z}K[I|\mathbf{t}]X = \frac{1}{z}K(X+\mathbf{t}) = \frac{1}{z}K(zK^{-1}p_1+\mathbf{t}) = p_1+\frac{K\mathbf{t}}{z} \tag{4}</script><p>上述推导过程出错的地方在于，在相机2坐标系下，地标点 $X$ 的深度信息不再是 $z$ ，而应该是 $z+t_3$ 。则上式的正确推导过程为：</p>
<script type="math/tex; mode=display">\displaylines{p_2 = \frac{1}{z+t_3}K[I|\mathbf{t}]X = \frac{1}{z+t_3}K(X+\mathbf{t}) \\
    = \frac{1}{z+t_3}K(zK^{-1}p_1+\mathbf{t}) = \frac{1}{z+t_3}(zp_1 + K\mathbf{t}) \tag{5}}</script><p>那么，根据上式可以进一步得到特征点在两幅图片中的光流：</p>
<script type="math/tex; mode=display">\displaylines{p_2-p_1 = [u_2,v_2]^T - [u_1,v_1]^T = [\Delta u, \Delta v]^T = \frac{1}{z+t_3}(K\mathbf{t} - t_3p_1) \\
K\mathbf{t} = [f_x t_1+c_x t_3, f_y t_2 + c_y t_3, t_3]^T = [M,N,t_3]^T \\
\Delta u = \frac{M - t_3 u_1}{z+t_3} \\
\Delta v = \frac{N - t_3 v_1}{z+t_3} \tag{6}}</script><p>上文也提到过，由于 $K\mathbf{t}$ 对于图中所有特征点是一致的，所以用 $M$, $N$ 分别表示前两个常数元素。值得注意的是，式（6）中的 $z+t_3$ 应该是一个正数，因为针孔相机模型只能映射位于相机前方的物体。</p>
<p>根据式（6）的结果可以看出，对于不同的特征点，其光流方向会根据其坐标位置而不同，即表现为 $\Delta u$, $\Delta v$ 会有正负。那么，进一步分析，不同位置特征点的光流方向具体表现为什么呢？根据Fig. 3 可以得到如Fig. 4 所示的区域光流方向分布情况，对于相机的纯移动情况，两幅成像平面中的极点相同，即 $e_1= e_2=$ Focus of Expansion (FOE)；且静态特征点的光流方向为沿着极点向外辐射移动。</p>
<p><img src="/2024/11/01/epipolar-constraints/fig4.png" alt="fig4" title="Fig. 4"></p>
<p>Fig. 4 是针对相机发生前向移动的情况，此时 $\mathbf{t} = [0,0,t_3]^T$ ，即相机只沿着z 轴进行前向移动；值得注意的是，$T_{C_2W} = [I|\mathbf{t}]$ 是从世界坐标系（相机1坐标系）转换为相机2坐标系，此处的 $t_3 &lt; 0$ 。由此，可对 $\Delta u, \Delta v$ 进一步更新：</p>
<script type="math/tex; mode=display">\displaylines{\Delta u = \frac{M - t_3 u_1}{z+t_3} = \frac{t_3 (c_x - u_1)}{z+t_3} \\
\Delta v = \frac{N - t_3 v_1}{z+t_3} = \frac{t_3 (c_y - v_1)}{z+t_3} \tag{7}}</script><p>以象限①内的特征点为例，该象限内特征点 $u_1 &gt; c_x, v_1 &lt; c_y$ ，则 $\Delta u &gt; 0, \Delta v &lt; 0$ ，与实际光流方向契合。由此证明了式（6）的正确。</p>
<h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1] Kundu, Abhijit, K. Madhava Krishna, and Jayanthi Sivaswamy. “Moving object detection by multi-view geometric techniques from a single camera mounted robot.” 2009 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 2009.</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>HEXO报错Error_Cannot find module &#39;./languages/vim&#39; 问题记录</title>
    <url>/2024/05/16/hexo-error/</url>
    <content><![CDATA[<h1 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h1><p>昨天在使用hexo准备新建博文时，突然报错：</p>
<blockquote>
<p>Error: Cannot find module ‘./languages/vim’</p>
</blockquote>
<p>首先是看到这个名字以为是跟前天把windows上的Vim卸载了有关（但心里还是十分怀疑这个相关性的）。先在Google搜索了该报错语句之后，竟然没有找到有相同报错内容的问题，于是乎死马当活马医，先重装Vim再说，果不其然，没用。</p>
<p>然后查看是否跟安装的主题有关，在themes下的languages中看到了vi.kml，基于Linux下vi与vim的关系继续盲目尝试（心里还是十分怀疑这个相关性的），copy一下该文件并改名为vim.kml，果然还是没用……</p>
<p>最后只好继续在网页大海中遨游，尝试了多种方法之后终于利用该<a href="https://github.com/hexojs/hexo/issues/1922#issuecomment-268710384">网页</a>中提供的方法解决了该问题，虽然最终也没有搞清楚问题出在哪，但，能用就行。在此记录一下该解决方法：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">npm uninstall hexo-cli -g</span></span><br><span class="line"><span class="meta prompt_">$ </span><span class="language-bash">npm install hexo-cli -g</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>随记</category>
        <category>问题记录</category>
      </categories>
      <tags>
        <tag>HEXO</tag>
        <tag>Node</tag>
        <tag>Blog</tag>
      </tags>
  </entry>
  <entry>
    <title>使用ffmpeg将图片转为视频</title>
    <url>/2024/07/12/ffmpeg-imgs2video/</url>
    <content><![CDATA[<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h1><p>近期采了一组数据（包含GNSS、IMU、LiDAR、相机），想要将其中一段图片转为视频放到PPT中结合轨迹图进行讲解，使用了ffmpeg将图片流转换为了视频文件，在此记录一下大致过程，方便后续使用时进行查阅。</p>
<span id="more"></span>
<p>首先介绍一下FFmpeg（<a href="https://ffmpeg.org/">官网</a>）：</p>
<blockquote>
<p>FFmpeg 是一个开放原始码的自由软体，可以执行音讯和视讯多种格式的录影、转档、串流功能，包含了libavcodec——这是一个用于多个专案中音讯和视讯的解码器函式库，以及libavformat——一个音讯与视讯格式转换函式库。         —Wikipedia</p>
</blockquote>
<h1 id="2-转换过程"><a href="#2-转换过程" class="headerlink" title="2 转换过程"></a>2 转换过程</h1><h2 id="2-1-文件名更改-amp-转换视频"><a href="#2-1-文件名更改-amp-转换视频" class="headerlink" title="2.1 文件名更改 &amp; 转换视频"></a>2.1 文件名更改 &amp; 转换视频</h2><p>在使用ffmpeg将图片转为视频时首先需要对图片文件名进行预处理，将图片按照顺序使用数字来作为其文件名，可在终端中使用以下命令来批量修改文件夹中的文件名：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">declare</span> -i num=0;</span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> `<span class="built_in">ls</span>`; <span class="keyword">do</span> <span class="built_in">mv</span> -f <span class="variable">$name</span> `<span class="built_in">echo</span> <span class="variable">$num</span><span class="string">&quot;.jpg&quot;</span>`;num=num+1;<span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>然后，使用以下命令将图片转换为视频：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ffmpeg -f image2 -i %d.jpg video.mp4</span><br></pre></td></tr></table></figure>
<p><strong>注：</strong>上述两个命令中需要根据图片的实际格式对“jpg”进行更改。</p>
<h2 id="2-2-ffmpeg参数"><a href="#2-2-ffmpeg参数" class="headerlink" title="2.2 ffmpeg参数"></a>2.2 ffmpeg参数</h2><p>首先介绍一下上述命令中的参数：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ffmpeg -f image2 -i %d.jpg video.mp4</span><br></pre></td></tr></table></figure>
<ul>
<li>-f：指定输入/输出文件格式，包括视频（MP4、AVI等）、音频（MP3、WAV等）、图片（PNG、JPEG等），此处后面为image2表示输入为图片流。<strong>注：</strong>-f参数应在-i参数前使用。</li>
<li>-i：指定输入文件。</li>
</ul>
<p>ffmpeg还有各种参数可以进行自定义设置，现挑几个常用的参数进行说明。</p>
<h3 id="r-帧率"><a href="#r-帧率" class="headerlink" title="-r 帧率"></a>-r 帧率</h3><p>ffmpeg默认帧率设置为25帧，可通过下述命令来修改帧率：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ffmpeg -r 40 -f image2 -i %d.jpg video.mp4</span><br></pre></td></tr></table></figure>
<p>值得注意的是，<code>-r 40</code> 需要放在 <code>-i %d.jpg</code> 前方可生效。</p>
<h3 id="s-vf-分辨率"><a href="#s-vf-分辨率" class="headerlink" title="-s -vf 分辨率"></a>-s -vf 分辨率</h3><p>如果想将视频分辨率调整为某分辨率，可直接使用-s参数进行修改：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ffmpeg -r 40 -f image2 -i %d.jpg -s 640x480 video.mp4</span><br></pre></td></tr></table></figure>
<p>但是如果原始图片分辨率不是4:3的话，会对原始图片进行拉伸、压缩操作，导致视频效果不好。这种情况下可以考虑-vf参数，固定高度或宽度，进行比例缩放：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ffmpeg -r 40 -f image2 -i %d.jpg -vf scale=-1:480 video.mp4</span><br></pre></td></tr></table></figure>
<p>其中，-1表示比例缩放。</p>
<h3 id="b-v-码率"><a href="#b-v-码率" class="headerlink" title="-b:v 码率"></a>-b:v 码率</h3><p>首先简要介绍一下视频码率：</p>
<blockquote>
<p><strong>视频码率或者码率</strong>是指视频信息每秒传输的比特数目。码率的单位通常为：</p>
<ul>
<li>kbps或者千比特每秒</li>
<li>mbps或兆比特每秒</li>
</ul>
</blockquote>
<p>根据概念可知，码率会影响视频的质量，即高码率一般对应着较高的视频质量。但两者之间并不存在绝对的对应关系，因为一方面码率对视频质量的增加存在一个瓶颈，视频质量一旦达到某一点，无论如何增加码率也无法进一步提升视频质量；另一方面，视频质量也是由多方因素决定的，如分辨率、帧率、编解码格式等。</p>
<p>ffmpeg修改码率的命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ffmpeg -r 40 -f image2 -i %d.jpg -b:v 4M video.mp4</span><br></pre></td></tr></table></figure>
<h3 id="c-v-编码格式"><a href="#c-v-编码格式" class="headerlink" title="-c:v 编码格式"></a>-c:v 编码格式</h3><p>默认使用的是h264，如想要切换为h265，可使用如下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ffmpeg -r 40 -f image2 -i %d.jpg -c:v libx265 video.mp4</span><br></pre></td></tr></table></figure>
<p>注：使用该参数时可能需要安装编码器依赖，否则会报错。</p>
<h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ol>
<li><a href="https://blog.csdn.net/moshuilangting/article/details/120194895">ubuntu下ffmpeg图片转视频</a></li>
<li><a href="https://blog.csdn.net/xindoo/article/details/121451318">使用ffmpeg将图片合成为视频(附完整参数介绍)</a></li>
<li><a href="https://livevideostack.cn/news/bitrate-vs-resolution-video-streaming-compression/">码率vs.分辨率，哪一个更重要？</a></li>
<li><a href="https://ffmpeg.org/ffmpeg.html#Stream-selection">ffmpeg Documentation</a></li>
</ol>
]]></content>
      <categories>
        <category>随记</category>
        <category>notes</category>
      </categories>
      <tags>
        <tag>ffmpeg</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Analysis and modeling GPS NLOS effect in highly urbanized area</title>
    <url>/2024/03/25/hsu2017/</url>
    <content><![CDATA[<p>Hsu, Li-Ta. “Analysis and Modeling GPS NLOS Effect in Highly Urbanized Area.” <em>GPS Solutions</em> 22, no. 1 (November 4, 2017): 7. <a href="https://doi.org/10.1007/s10291-017-0667-9">https://doi.org/10.1007/s10291-017-0667-9</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>传统的<strong>基于测距的NLOS探测矫正方法</strong>难以适用于低成本接收器的原因在于：</p>
<ol>
<li>利用ray-tracing 的计算成本太高；</li>
<li>3D 建筑模型难以实时获取到。</li>
</ol>
<p>本文的创新与贡献在于提出一个应用于低成本设备中用于基于假设定位 hypothesis-based positioning 的NLOS 模型，而不需要使用ray-tracing。</p>
<span id="more"></span>
<p>在分析阶段，利用给定的<strong>接收机位置真值和3D 建筑模型</strong>进行NLOS 识别，也就是说，如果ray-tracing 模拟表明建筑模型挡到了信号的直线传播，但是接收机仍然接收到了该卫星的信号，那么就认定该信号为NLOS 信号。为了获取伪距NLOS 信号延迟，利用差分GPS 对其他误差进行消除；在城市环境中采集24h 的GPS 原始数据来恢复<strong>不同载噪比、不同高度角</strong>的NLOS 信号伪距误差。作者发现，NLOS 信号造成的伪距误差<strong>与卫星的高度角高度相关</strong>，而不是载噪比。最终，作者构建了一个关于<strong>卫星高度角和接收机与反射建筑物之间距离</strong>的方程来作为<strong>NLOS 延迟模型</strong>。</p>
<p><img src="/2024/03/25/hsu2017/fig2.png" alt="fig2" title="figure 2"></p>
<h1 id="2-Estimation-of-NLOS-delays-in-the-pseudorange-domain"><a href="#2-Estimation-of-NLOS-delays-in-the-pseudorange-domain" class="headerlink" title="2 Estimation of NLOS delays in the pseudorange domain"></a>2 Estimation of NLOS delays in the pseudorange domain</h1><p>伪距测量的误差项包括电离层延迟、对流层延迟、卫星钟差、轨道误差、接收机钟差以及我们的目标NLOS，其他误差需要在NLOS 分析前进行消除，本文分别使用 DGPS 和 最小二乘法（LSE）来处理前部和后部误差。</p>
<h2 id="2-1-Differential-GPS-correction"><a href="#2-1-Differential-GPS-correction" class="headerlink" title="2.1 Differential GPS correction"></a>2.1 Differential GPS correction</h2><p>参考站的多路径、NLOS 效应可以忽略，因为参考站一般选取安置在开阔环境中，且使用带有扼流圈的接收天线。理论上来讲，若流动站和参考站之间的距离小于100 km时，DGPS 矫正参数可以消除卫星钟差、轨道误差、电离层对流层延迟。</p>
<h2 id="2-2-Receiver-clock-bias-and-thermal-noise"><a href="#2-2-Receiver-clock-bias-and-thermal-noise" class="headerlink" title="2.2 Receiver clock bias and thermal noise"></a>2.2 Receiver clock bias and thermal noise</h2><p>接收机钟差的消除是通过将其视为未知数，利用多个伪距观测连同位置参数一同解算来获取的。为了更好地计算接收机钟差可以将位置参数从求解方程中进行剔除，作者利用香港的精细地形图（分辨率20 cm）等手段来获取接收机的<strong>精确静态位置</strong>，从而实现对位置参数的消除，进而利用伪距观测来计算接收机钟差。为了进一步提高对接收机钟差的解算精度，作者利用<strong>载噪比和ray-tracing 方法</strong>对NLOS 信号进行简单地识别，只利用LOS 信号对接收机钟差进行解算。</p>
<p>接收机热噪声误差被忽略掉，因为其远小于NLOS 带来的影响。</p>
<p>最终，<strong>NLOS 信号带来的延迟</strong>被定义为：</p>
<p><img src="/2024/03/25/hsu2017/f4.png" alt="f4" title="formula 4"></p>
<p>其中，$\rho$ 表示NLOS 伪距测量值；$R^{rcv}$  表示卫星与接收机之间的LOS 距离；$\rho^{Coor}$  表示DGPS 矫正参数。</p>
<h1 id="3-NLOS-data-analysis"><a href="#3-NLOS-data-analysis" class="headerlink" title="3 NLOS data analysis"></a>3 NLOS data analysis</h1><p>一个level of detai (LOD) 1 的3D 建筑模型从香港地政总署获取，在香港九龙的不同地方采集到了两份数据，数据一与数据二分别包含24 h 以及 30 min 的GPS 伪距原始观测量，两份数据的天空视图如Fig. 4所示。</p>
<p><img src="/2024/03/25/hsu2017/fig4.png" alt="fig4" title="figure 4"></p>
<h2 id="3-1-NLOS-data-1：case-of-24-h"><a href="#3-1-NLOS-data-1：case-of-24-h" class="headerlink" title="3.1 NLOS data 1：case of 24 h"></a>3.1 NLOS data 1：case of 24 h</h2><p>24 h 的观测数据中一共有128,054个NLOS 观测数据，Fig. 5展示了NLOS 信号伪距延迟直方图。</p>
<p><img src="/2024/03/25/hsu2017/fig5.png" alt="fig5" title="figure 5"></p>
<p>作者对该NLOS 误差概率分布利用Gamma 分布进行拟合：</p>
<p><img src="/2024/03/25/hsu2017/f5.png" alt="f5" title="formula 5"></p>
<p>根据传统的定权模型，信号强度表明了伪距测量的质量，Fig. 6展示了<strong>NLOS 延迟与载噪比</strong>之间的关系，观察该图可以发现，随着载噪比的增加，NLOS 误差会缓慢下降，其标准差也有相同的趋势。这表明利用信号载噪比对GPS 观测进行定权确实可在一定程度上提高定位结果。</p>
<p><img src="/2024/03/25/hsu2017/fig6.png" alt="fig6" title="figure 6"></p>
<p>在传统的<strong>基于高度角的定权模型</strong>中，一些误差源（如大气误差、多路径误差等）与卫星高度角相关，这些误差通常随着高度角降低而单调增加。Fig. 7展示了NLOS 延迟与高度角之间的对应关系，由图可直观看出NLOS 延迟与高度角有明显的负相关关系，这说明传统的高度角定权模型也可用于消除NLOS 效应。</p>
<p><img src="/2024/03/25/hsu2017/fig7.png" alt="fig7" title="figure 7"></p>
<p>Fig .8展示了使用相同接收机接收到的LOS/NLOS 信号的不同表现。对于LOS 信号，高度角越高，载噪比均值越大、标准差越小，这说明对于LOS信号而言，高度角越高质量越稳定。然而，NLOS 信号却不同，随着高度角的增加，载噪比均值与标准差均没有明显的改善，这是因为NLOS 信号是反射信号，<strong>反射表面的材料</strong>会严重影响信号的强度。比较Fig .8 的上下两图，发现也许可以利用<strong>信号在不同高度角下的载噪比变化</strong>来区分LOS/NLOS 信号。</p>
<p><img src="/2024/03/25/hsu2017/fig8.png" alt="fig8" title="figure 8"></p>
<h2 id="3-2-NLOS-data-2：case-of-30-min"><a href="#3-2-NLOS-data-2：case-of-30-min" class="headerlink" title="3.2 NLOS data 2：case of 30 min"></a>3.2 NLOS data 2：case of 30 min</h2><p><img src="/2024/03/25/hsu2017/fig9.png" alt="fig9" title="figure 9"></p>
<p><img src="/2024/03/25/hsu2017/fig10.png" alt="fig10" title="figure 10"></p>
<p>如Fig. 9，Fig. 10 所示，NLOS 伪距延迟并不会随着载噪比的增加而有明显的下降，但是会随着高度角的增加而明显下降。由于NLOS 信号可以一直存在于环境中，其载噪比也可以很强，所以<strong>无法利用载噪比来判断LOS/NLOS 信号</strong>。上节提到的载噪比与NLOS 延迟之间存在一定的相关关系（Fig. 6），这是因为高度角与载噪比之间的关系造成的：高度角增加，载噪比也会增加。</p>
<p>综上所述，作者认为<strong>高度角是NLOS 伪距误差的主导因素</strong>。</p>
<h1 id="4-Modeling-of-NLOS"><a href="#4-Modeling-of-NLOS" class="headerlink" title="4 Modeling of NLOS"></a>4 Modeling of NLOS</h1><p>作者考虑现代城市环境中，建筑表面多使用玻璃作为外表材料，因此GPS 信号的反射遵循反射定律，如Fig. 11所示。</p>
<p><img src="/2024/03/25/hsu2017/fig11.png" alt="fig11" title="figure 11"></p>
<p>由于卫星距离接收机的距离远大于LOS 与反射信号之间的距离，因此可以认为蓝色的LOS 线不仅与绿色线平行，而且距离相等。因此，<strong>NLOS 延迟 $\gamma$</strong> 可表示为：</p>
<p><img src="/2024/03/25/hsu2017/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$\alpha$  表示接收机与反射信号的遮挡物之间的距离；$\theta_{ele}$ 表示卫星高度角。根据反射定理可以得到 $\theta_{ele}=\theta_2=\theta_1=\theta_0$ ，最终，<strong>NLOS 延迟 $\gamma$</strong> 可表示为：</p>
<p><img src="/2024/03/25/hsu2017/f9.png" alt="f9" title="formula 9"></p>
<p>上式中，对与接收机而言只有距离参数 $\alpha$ 是未知的，可通过2D 地图或者LiDAR 进行获取。</p>
<p>Fig. 12展示了<strong>卫星高度角以及距离参数</strong> $\alpha$ 对NLOS 模型误差的影响。</p>
<p><img src="/2024/03/25/hsu2017/fig12.png" alt="fig12" title="figure 12"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>GNSS</category>
        <category>NLOS</category>
      </categories>
      <tags>
        <tag>GNSS</tag>
        <tag>NLOS</tag>
      </tags>
  </entry>
  <entry>
    <title>Waypoint Inertial Explorer 9.0 使用教程</title>
    <url>/2024/05/15/ie-tutorial/</url>
    <content><![CDATA[<h1 id="1-简介"><a href="#1-简介" class="headerlink" title="1 简介"></a>1 简介</h1><p>最近采完数据之后需要计算参考解，于是便使用组内购买的Waypoint Inertial Explorer 9.0 进行紧组合后处理，因为之前没有使用过该软件，所以向组内用过的师弟请教，在此记录一下使用过程，以免自己后续遗忘操作步骤时再去麻烦师弟。</p>
<p>首先介绍一下IE这个软件：</p>
<blockquote>
<p>Inertial Explorer 是 NovAtel 公司 Waypoint 产品组研发的强大的、可配置度高的事后处理软件，用于处理所有可用的 GNSS、INS 数据，提供高精度组合导航信息，包括位置、速度和姿态信息。针对精度和稳定性要求比较高，不需要实时定位导航信息的应用，可以通过 GNSS 和 INS 原始数据后处理的方式，提高组合导航解算精度和稳定性。</p>
</blockquote>
<p>具体的软件介绍可以参考<a href="https://novatel.com/products/waypoint-post-processing-software/inertial-explorer">Novatel官网</a>，官网中也有不同版本IE的<a href="https://novatel.com/support/waypoint-software/inertial-explorer">用户手册</a>，但是当前只找到了英文版本，且有200多页，应该比较适合当作一个参考文件。</p>
<span id="more"></span>
<h1 id="2-使用教程"><a href="#2-使用教程" class="headerlink" title="2 使用教程"></a>2 使用教程</h1><h2 id="2-1-软件内容介绍"><a href="#2-1-软件内容介绍" class="headerlink" title="2.1 软件内容介绍"></a>2.1 软件内容介绍</h2><p>安装完IE之后，在开始页面会有以下8个软件，详细用处可以参考用户手册，用来计算参考解时主要使用以下几个软件：</p>
<ul>
<li>Inertial Explorer：主体软件，可以在该软件页面中打开其他软件；</li>
<li>GNSS Data Converter：将GNSS原始数据转换为IE兼容的 .gpb 文件；</li>
<li>Local License Manager：激活软件；</li>
<li>Download Service Data：用以下载参考站数据。</li>
</ul>
<p><img src="/2024/05/15/ie-tutorial/image-20240515220136911.png" alt="image-20240515220136911" title="fig 1"></p>
<h2 id="2-2-激活"><a href="#2-2-激活" class="headerlink" title="2.2 激活"></a>2.2 激活</h2><p>该软件在我们组购买时已经不需要密码狗了，而是使用<strong>密钥</strong>进行验证。验证过程在Local License Manager中完成，如Fig 2所示，在 Activate License ①中输入激活码，②中随意输入名字，然后点击 Activate 即可激活，值得注意的是，可能需要美国IP 才可顺利激活成功。</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240515224436241.png" alt="image-20240515224436241" title="fig 2"></p>
<p>激活之后即可正常使用其他软件了。在使用完IE之后记得<strong>下线激活状态</strong>，否则会影响他人使用。下线过程如Fig 3所示，在 Local License - Inertial Explorer 点击Return即可下线（下方应选择<strong>Online</strong>）。</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240528175121693.png" alt="image-20240528175121693" title="figure 3"></p>
<h2 id="2-3-参考站数据下载"><a href="#2-3-参考站数据下载" class="headerlink" title="2.3 参考站数据下载"></a>2.3 参考站数据下载</h2><p>在软件激活状态可进入 Download Service Data 软件来下载参考站的数据，如Fig 4所示，可以通过①列表添加，②根据位置选取。然后修改保存路径、选取时间范围下载即可，通过该软件下载可以得到IE软件兼容的 .gpb 文件。</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240515225258793.png" alt="image-20240515225258793" title="fig 4"></p>
<h2 id="2-4-采集数据格式转换"><a href="#2-4-采集数据格式转换" class="headerlink" title="2.4 采集数据格式转换"></a>2.4 采集数据格式转换</h2><p>为了生成IE可兼容的 .gpb 文件，需要利用 GNSS Data Converter 对<strong>GNSS、IMU观测数据</strong>进行格式转换：</p>
<ol>
<li>选取GNSS或IMU观测数据文件（<strong>注</strong>：IMU观测数据也是在该软件中进行格式转换的，而不是在IMU Data Converter中）；</li>
<li>选取文件之后点击Add添加按钮；</li>
<li>点击Options按钮进行设置；</li>
<li><strong>参考站选择Static，流动站选择Kinematic</strong>；</li>
<li>点击Convert按钮进行转换。</li>
</ol>
<p><img src="/2024/05/15/ie-tutorial/image-20240515230927985.png" alt="image-20240515230927985" title="fig 5"></p>
<p>在对GNSS进行数据转换时，会提示选择Navigation file，选择对应文件然后进行转换即可。</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240515230336819.png" alt="image-20240515230336819" title="fig 6"></p>
<p>下图是GNSS数据转换完成的界面：</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240515230451367.png" alt="image-20240515230451367" title="fig 7"></p>
<p>转换成功之后，GNSS观测数据文件会生成.gpb文件，IMU观测数据文件会生成.imr文件，用于下一步的文件添加。</p>
<h2 id="2-5-文件添加"><a href="#2-5-文件添加" class="headerlink" title="2.5 文件添加"></a>2.5 文件添加</h2><p>在完成文件格式转换之后，即可在Inertial Explorer软件中进行文件添加操作。打开IE，首先新建一个工程，建议使用<strong>全英文路径</strong>。</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516083728998.png" alt="image-20240516083728998" title="fig 8"></p>
<p>然后，通过①②③分别添加<strong>参考站</strong>、<strong>流动站</strong>和<strong>IMU观测数据</strong>：</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516084236803.png" alt="image-20240516084236803" title="fig 9"></p>
<h3 id="2-5-1-参考站数据添加"><a href="#2-5-1-参考站数据添加" class="headerlink" title="2.5.1 参考站数据添加"></a>2.5.1 参考站数据添加</h3><p>对于参考站数据添加，选取.gpb文件之后会弹出参数面板，如Fig 10所示。①是参考站的经纬度、椭球高，添加参考站文件后会自动填入参考站的<strong>平均观测坐标</strong>，精度可能不会很高；若对精度要求较高的话，也可通过其他方式解算该参考站的坐标并填写进去。②中选取WGS84即可；③选择L1相位中心。然后点击确定即可。</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516084545245.png" alt="image-20240516084545245" title="fig 10"></p>
<p>在①中还有其他<strong>参考站坐标导入选项</strong>，点击Coord. options，有如Fig 11所示的几个选项，可根据需要进行选择。</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516085222448.png" alt="image-20240516085222448" title="fig 11"></p>
<p>添加完参考站数据后，会在界面显示参考站的标识：</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516085412938.png" alt="image-20240516085412938" title="fig 12"></p>
<h3 id="2-5-2-流动站数据添加"><a href="#2-5-2-流动站数据添加" class="headerlink" title="2.5.2 流动站数据添加"></a>2.5.2 流动站数据添加</h3><p>对于流动站数据添加，选择.gpb文件之后同样会弹出参数面板，如Fig 13所示，在①中选择采集数据时使用的天线型号，如果没有特定型号也可选择Generic；②中默认选择L1相位中心。然后点击确定即可。</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516085802250.png" alt="image-20240516085802250" title="fig 13"></p>
<p>流动站数据添加之后，会在界面中显示轨迹路线：</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516090110422.png" alt="image-20240516090110422" title="fig 14"></p>
<h3 id="2-5-3-IMU数据添加"><a href="#2-5-3-IMU数据添加" class="headerlink" title="2.5.3 IMU数据添加"></a>2.5.3 IMU数据添加</h3><p>对于IMU数据添加，选取.imr文件之后会弹出文件添加提示，点击确定即可。</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516090316248.png" alt="image-20240516090316248" title="fig 15"></p>
<p>至此，完成了观测数据的添加工作。</p>
<h2 id="2-6-紧组合解算"><a href="#2-6-紧组合解算" class="headerlink" title="2.6 紧组合解算"></a>2.6 紧组合解算</h2><p>数据添加完之后，选取紧组合处理。</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516090843559.png" alt="image-20240516090843559" title="fig 16"></p>
<p>点击之后会弹出参数面板，如Fig 17所示，各个参数设置如下：</p>
<ol>
<li>根据是否有参考站数据选择差分计算或是PPP解算；</li>
<li>一般选择Both、Multi-pass；</li>
<li>根据实验情况选择合适的载体模型，这里选择的是车载；</li>
<li>设置GNSS天线在IMU坐标系下的杆臂；</li>
<li>设置载体坐标系到IMU坐标系的转换关系。</li>
</ol>
<p><img src="/2024/05/15/ie-tutorial/image-20240516092914130.png" alt="image-20240516092914130" title="fig 17"></p>
<p>对于⑥Advanced GNSS，点击会弹出GNSS数据的参数面板，如Fig 18所示，在①处可对处理时间进行选择；②处对信号进行<strong>初步的筛选</strong>，如限制高度角、载噪比等；③处可对不同卫星、基线、时间段等参数进行设置，本处设置选择<strong>剔除掉Beidou 1~5星</strong>。设置完之后即可点击确定。</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516093550676.png" alt="image-20240516093550676" title="fig 18"></p>
<p>对于⑦Advanced IMU，点击同样会弹出IMU数据的参数面板。</p>
<p>首先，在Alignment界面，在①处对IMU的<strong>对齐方式</strong>进行设置，该处要将两个对齐方式均设置为<strong>Static Alignment</strong>（注：可能是因为IMU在初始状态一般是静止的），注意，此处设置很重要，若没有正确设置在后期解算时会报错<strong>Alignment Error</strong>；在②处可对要处理的IMU数据根据时间进行选择。</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516094049308.png" alt="image-20240516094049308" title="fig 19"></p>
<p>然后，在States界面选择IMU的<strong>误差模型</strong>，此处需要根据所用的IMU型号选择对应的误差模型，若选项中没有所需的型号可通过自定义进行添加。然后点击确定按键即可。</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516094456008.png" alt="image-20240516094456008" title="fig 20"></p>
<p>至此，完成了解算前的设置，然后就可点击Process按键开始解算。可能会出现以下的警告，点击Continue即可。</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516094735171.png" alt="image-20240516094735171" title="fig 21"></p>
<p>解算过程的界面如Fig 22所示：</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516094816491.png" alt="image-20240516094816491" title="fig 22"></p>
<p>解算完之后的结果如Fig 23所示：</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516094934625.png" alt="image-20240516094934625" title="fig 23"></p>
<h2 id="2-7-紧组合二次解算"><a href="#2-7-紧组合二次解算" class="headerlink" title="2.7 紧组合二次解算"></a>2.7 紧组合二次解算</h2><p>在操作文档中还提到了<strong>二次解算</strong>，即在紧组合解的情况下选取额外的约束进行二次解算。本人试了二次解算对本次采集的数据提升效果不明显，但还是记录一下操作步骤。</p>
<p>在进行紧组合解算之后，再次点击紧组合求解按钮，上次已有的参数无需更改。在Advanced IMU中进入Constraints界面，如Fig 24所示，在①中勾选Apply vehicle constraints，然后点击②Compute Boresight，接着勾选③Rotate processed output to vehicle frame，最后点击确定、解算即可。</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516095352008.png" alt="image-20240516095352008" title="fig 24"></p>
<h2 id="2-8-文件保存"><a href="#2-8-文件保存" class="headerlink" title="2.8 文件保存"></a>2.8 文件保存</h2><p>在主界面选择Export Wizard对解算结果进行保存，操作如下所示：</p>
<ol>
<li>点击Export Wizard按钮；</li>
<li>选择结果保存文件路径；</li>
<li>选取保存内容，可进行自定义；</li>
<li>自定义界面，可根据需要选择保存的内容。</li>
</ol>
<p><img src="/2024/05/15/ie-tutorial/image-20240516101842709.png" alt="image-20240516101842709" title="fig 25"></p>
<p>选择保存文档后点击下一页，默认使用WGS84；继续点击下一页，如Fig 26所示，①中自定义保存数据的时间段；②中修改输出时间间隔或距离间隔；③中可将输出结果转换至以IMU坐标系下的某一点为基准。</p>
<p><img src="/2024/05/15/ie-tutorial/image-20240516102212282.png" alt="image-20240516102212282" title="fig 26"></p>
<p>然后点击下一页，可对保存结果进行预览，最终保存完整的结果。</p>
<h1 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h1><p>以上就是目前使用IE的过程记录，其中还有很多参数没有搞清楚，只是过了一遍操作过程罢了，后面会根据实际使用情况对本文内容进行更新。</p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Multi-source</category>
      </categories>
      <tags>
        <tag>Multi-source</tag>
        <tag>IE</tag>
        <tag>Tight couple</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Semantic SLAM Based on Improved DeepLabv3⁺ in Dynamic Scenarios</title>
    <url>/2024/02/29/hu2022/</url>
    <content><![CDATA[<p>Hu, Zhangfang, Jiang Zhao, Yuan Luo, and Junxiong Ou. “Semantic SLAM Based on Improved DeepLabv3+ in Dynamic Scenarios.” <em>IEEE Access</em> 10 (2022): 21160–68. <a href="https://doi.org/10.1109/ACCESS.2022.3154086">https://doi.org/10.1109/ACCESS.2022.3154086</a>.</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文提出动态场景中的语义SLAM 系统DeepLabv3+_SLAM 包含三个线程：<strong>ORB-SLAM3</strong>，<strong>语义分割</strong>线程以及<strong>几何</strong>线程。</p>
<span id="more"></span>
<h1 id="3-System-Description"><a href="#3-System-Description" class="headerlink" title="3 System Description"></a>3 System Description</h1><p>系统架构如Fig. 1所示，本系统是基于ORB-SLAM3 进一步开发的。DeepLabv3+ 模型会获取像素级的<strong>先验动态物体</strong>，同时，利用<strong>多视角几何线程</strong>区分图片中的动态与静态特征点；然后，将语义分割结果和多视角几何方法得到的信息相结合得到<strong>动态物体的边界线</strong>，从而剔除掉所有的动态物体。</p>
<p>注：个人认为，多视角几何线程的图示不是很准确，应该将多视角几何与新的蚁群策略合为一个框，或将新蚁群策略提到多视角结合方法之前。因为本算法是利用新蚁群算法来减少多视角几何方法中的候选点数量的，两者应该是处于共同的层级，甚至新蚁群策略应该在多视角几何方法之前。</p>
<p><img src="/2024/02/29/hu2022/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-1-Semantic-Segmentation-DeepLabv3"><a href="#3-1-Semantic-Segmentation-DeepLabv3" class="headerlink" title="3.1 Semantic Segmentation DeepLabv3+"></a>3.1 Semantic Segmentation DeepLabv3+</h2><p>DeepLabv3+ 的架构如Fig. 2所示，本文使用ResNest 网络作为backbone，可以取得更优秀的分割精度。</p>
<p><img src="/2024/02/29/hu2022/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-2-ASPP-Module"><a href="#3-2-ASPP-Module" class="headerlink" title="3.2 ASPP Module"></a>3.2 ASPP Module</h2><p>作者对ASPP 模块进行了更改：</p>
<p><img src="/2024/02/29/hu2022/fig4.png" alt="fig4" title="figure 4"></p>
<h2 id="3-3-Dynamic-Object-Detection-Based-on-Multi-view-Geometry"><a href="#3-3-Dynamic-Object-Detection-Based-on-Multi-view-Geometry" class="headerlink" title="3.3 Dynamic Object Detection Based on Multi-view Geometry"></a>3.3 Dynamic Object Detection Based on Multi-view Geometry</h2><p>作者使用基于<strong>多视角几何</strong>处理的动态物体分割算法，如Fig. 5所示，该算法考虑历史帧 (hf) 和当前帧 (cf) 对同一特征点的<strong>视角变化</strong>，若视角变化值大于一个给定阈值，则判定该特征点属于动态特征点；此外，还计算关键点在当前帧的<strong>深度值</strong> $d_{cf}$ ，及历史关键帧在当前帧的投影深度值 $d_{proj}$ ，若两者之差大于给定阈值，则判定该特征点为动态特征点。</p>
<p><img src="/2024/02/29/hu2022/fig5.png" alt="fig5" title="figure 5"></p>
<h2 id="3-4-Ant-Colony-Strategy"><a href="#3-4-Ant-Colony-Strategy" class="headerlink" title="3.4 Ant Colony Strategy"></a>3.4 Ant Colony Strategy</h2><p>蚁群算法是一种模拟蚂蚁的觅食行为的优化算法，算法包含两个主要步骤：state transfer，pheromone update。</p>
<p>状态转换方程：</p>
<p><img src="/2024/02/29/hu2022/f4.png" alt="f4" title="formula 4"></p>
<p>信息素更新方程：</p>
<p><img src="/2024/02/29/hu2022/f5.png" alt="f5" title="formula 5"></p>
<h2 id="3-5-New-Ant-Colony-Strategy"><a href="#3-5-New-Ant-Colony-Strategy" class="headerlink" title="3.5 New Ant Colony Strategy"></a>3.5 New Ant Colony Strategy</h2><p>利用<strong>多视角几何方法</strong>将历史帧投影至当前帧过程，会得到大量的投影特征点，为了判断是否属于静态特征点需要<strong>遍历</strong>所有的点，运算量过大。作者提出一种<strong>新的蚁群算法</strong>，通过<strong>最优路径</strong>来找到所有的动态特征点群。</p>
<p>蚁群算法的策略为，从起点到终点过程中，蚁群会绕过它们遇到的障碍物来寻找到达终点的一条<strong>最优路径</strong>。图片中的动态点或静态点都是<strong>成块分布</strong>的，而不是随机散布在整幅图片上；因此，当遇到一个动态特征点时，会在该动态特征点<strong>所在的块内</strong>进行搜索，直到整块特征点都被探索过或到达块的范围，然后继续搜索下一块动态特征点集合。</p>
<p>根据特征点在图片中的分布，作者设计了一条从 S 到 T 的轨迹 l，如Fig. 6所示，搜索策略是蚁群从特征点 $m_i$ 持续移动到下一个特侦点，直到抵达终点 T。针对每一个特征点 $m_i$ ，以<strong>自身为原点、半径为 R 进行动态点搜索</strong>，每找到一个新的特征点就<strong>拓宽带宽</strong> $\Delta h$ ，直至在该区域内没有新的动态特征点，便移至下一个位置。</p>
<p><img src="/2024/02/29/hu2022/fig6.png" alt="fig6" title="figure 6"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>IMU预积分总结与公式推导</title>
    <url>/2024/04/17/imu-preinteg/</url>
    <content><![CDATA[<p>注：本文是对北航邱笑晨博士总结的预积分公式推导过程进行记录，由于是个人简单记录，所以基本上内容、公式都是截取自该文章。</p>
<h1 id="1-IMU-测量模型（Sensor-Model）和运动学模型（Kinetic-Model）"><a href="#1-IMU-测量模型（Sensor-Model）和运动学模型（Kinetic-Model）" class="headerlink" title="1 IMU 测量模型（Sensor Model）和运动学模型（Kinetic Model）"></a>1 IMU 测量模型（Sensor Model）和运动学模型（Kinetic Model）</h1><p>陀螺测量模型：</p>
<p><img src="/2024/04/17/imu-preinteg/01.png" alt="01" title="01"></p>
<p>其中，$\mathbf{b}_g$ 是bias，$\mathbf{\eta}_g$ 是白噪声。该模型利用了static world assumption：考虑到MEMS IMU 的观测精度，以及SLAM 的运动场景较小，因此忽略地球自转（认为地球是static world），并假设运行区域水平面是个平面，重力矢量 $\mathbf{g}^w$ 的指向固定且模值恒定。</p>
<p>加速度计测量模型：</p>
<p><img src="/2024/04/17/imu-preinteg/02.png" alt="02" title="02"></p>
<p>其中，$\mathbf{b}_a$ 是bias，$\mathbf{\eta}_a$ 是白噪声。</p>
<span id="more"></span>
<p>运动模型的微分方程：</p>
<p><img src="/2024/04/17/imu-preinteg/03.png" alt="03" title="03"></p>
<p>注：观察旋转的微分形式，斜对称矩阵在右边，说明这是在body 参考系下的，参照<a href="https://fzheng.me/2016/11/20/imu_model_eq/">文章</a>。</p>
<p>运动方程的离散形式：</p>
<p><img src="/2024/04/17/imu-preinteg/04.png" alt="04" title="04"></p>
<p>将测量模型代入离散运动方程有：</p>
<p><img src="/2024/04/17/imu-preinteg/05.png" alt="05" title="05"></p>
<p>离散噪声和连续噪声的协方差有如下关系：</p>
<p><img src="/2024/04/17/imu-preinteg/06.png" alt="06" title="06"></p>
<p>进一步假设 $\Delta t$ 恒定（即采样频率固定），使用 $k = 0, 1, 2, …$ 表示采样的离散时刻，则<strong>离散运动方程</strong>可进一步简化为：</p>
<p><img src="/2024/04/17/imu-preinteg/07.png" alt="07" title="07"></p>
<h1 id="2-IMU-预积分"><a href="#2-IMU-预积分" class="headerlink" title="2 IMU 预积分"></a>2 IMU 预积分</h1><p>当IMU 参与的组合导航时，IMU 的采样频率一般高于其他传感器，所以会将一段时间内的IMU 观测数据进行积分作为整个观测，以参与同其他传感器的组合求解，如利用 $k=i$ 时刻与 $k=j-1$ 时刻内所有的IMU 观测，基于 $k=i$ 时刻的 $\mathbf{R}_i, \mathbf{v}_i, \mathbf{q}_i$ 直接更新 $k=j$ 时刻的  $\mathbf{R}_j, \mathbf{v}_j, \mathbf{q}_j$ :</p>
<p><img src="/2024/04/17/imu-preinteg/08.png" alt="08" title="08"></p>
<p>在每次优化迭代后，会得到新的 $\mathbf{R}_i, \mathbf{v}_i, \mathbf{q}_i$ ，则上式中的积分操作就需要重新计算一遍，导致运算量极大。而IMU 预积分的思路就是把每次优化迭代时不变的项提取出来，以减小每次重新积分的计算量；即剥离出与更新量无关的参数（在上式中，更新量为 $\mathbf{R}_i, \mathbf{v}_i, \mathbf{q}_i$ 和 bias），得到 $k=i,k=j$ 之间的相对变化量，这样在每次迭代更新后只需要重新计算与更新量相关的项即可。根据上式获取的<strong>预积分项</strong>为：</p>
<p><img src="/2024/04/17/imu-preinteg/09.png" alt="09" title="09"></p>
<p>此处可进一步参考<a href="https://zhuanlan.zhihu.com/p/438525032">文章</a>的讲解，如下图所示，绿色部分即为与更新量相关的项，蓝色部分表示与更新量无关的项，$\alpha$ 相当于上式中的 $\Delta \mathbf{p}_{ij}$ ，$\beta$ 相当于上式中的 $\Delta \mathbf{v}_{ij}$ 。</p>
<p><img src="/2024/04/17/imu-preinteg/10.png" alt="10" title="10"></p>
<h1 id="3-预积分测量值与测量噪声"><a href="#3-预积分测量值与测量噪声" class="headerlink" title="3 预积分测量值与测量噪声"></a>3 预积分测量值与测量噪声</h1><p>将噪声项 ($\eta^{gd}_k, \eta^{ad}_k$) 从预积分项中分离出来，获得 “预积分测量值 = 理想值 + 白噪声” 的形式。首先做一个假设：在与积分时间内bias保持不变，即 $\mathbf{b}_{i}^g = \mathbf{b}_{i+1}^g = … = \mathbf{b}_{j}^g, \mathbf{b}_{i}^a = \mathbf{b}_{i+1}^a = … = \mathbf{b}_{j}^a$ 。</p>
<h2 id="3-1-测量值"><a href="#3-1-测量值" class="headerlink" title="3.1 测量值"></a>3.1 测量值</h2><h3 id="3-1-1-旋转预积分项"><a href="#3-1-1-旋转预积分项" class="headerlink" title="3.1.1 旋转预积分项"></a>3.1.1 旋转预积分项</h3><p><img src="/2024/04/17/imu-preinteg/11.png" alt="11" title="11"></p>
<p>其中，① 处使用了旋转矩阵的右乘小量扰动；② 处使用了Adjoint 性质，对 “ABABAB…AB” 连乘进行转换顺序即可。</p>
<p>对于上式，令</p>
<p><img src="/2024/04/17/imu-preinteg/12.png" alt="12" title="12"></p>
<p>则有：</p>
<p><img src="/2024/04/17/imu-preinteg/13.png" alt="13" title="13"></p>
<p>从而获得了旋转预积分项 “预积分测量值 = 理想值 + 白噪声” 的形式，其中，$\Delta \tilde{\mathbf{R}}_{ij}$ 为预积分测量值，由陀螺仪测量值和对陀螺仪bias 的估计值计算得到；$\delta \overrightarrow{\phi}_{ij}$ 为测量噪声。</p>
<p>补充说明：</p>
<p><strong>Adjoint 性质</strong>表示如下：</p>
<p>$Exp(\phi)\cdot \mathbf{R} = \mathbf{R} \cdot Exp(\mathbf{R}^T \phi)$</p>
<p>证明过程如下所示：</p>
<p>首先，令 $Exp(\phi) = exp(\hat{\phi}) = exp(\theta \hat{\mathbf{a}})$ ，然后</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbf{R}^T Exp(\phi)\cdot \mathbf{R} &= \mathbf{R}^T [cos\theta \cdot \mathbf{I} + (1-cos\theta)\mathbf{a}\mathbf{a}^T + sin\theta \cdot \hat{\mathbf{a}}] \mathbf{R} \\
&= cos\theta \cdot \mathbf{R}^T \mathbf{R} + (1-cos\theta)\mathbf{R}^T\mathbf{a}\mathbf{a}^T\mathbf{R} + sin\theta \cdot \mathbf{R}^T\hat{\mathbf{a}}\mathbf{R} \\
&= cos\theta \cdot \mathbf{I} + (1-cos\theta)(\mathbf{R}^T\mathbf{a})(\mathbf{R}^T\mathbf{a})^T + sin\theta (\hat{\mathbf{R}^T\mathbf{a}}) \\
&= Exp(\mathbf{R}^T\mathbf{a})
\end{aligned}</script><p>其中，用到了 $\mathbf{R}\hat{\mathbf{a}}\mathbf{R}^T = \hat{(\mathbf{R}\mathbf{a})}$ 的性质，该性质的证明可参考<a href="https://fzheng.me/2017/12/10/Rvhat/">文章</a>。</p>
<h3 id="3-1-2-速度预积分项"><a href="#3-1-2-速度预积分项" class="headerlink" title="3.1.2 速度预积分项"></a>3.1.2 速度预积分项</h3><p>将上节获取的旋转预积分项代入 $\Delta \mathbf{v}_{ij}$ 中，可得：</p>
<p><img src="/2024/04/17/imu-preinteg/14.png" alt="14" title="14"></p>
<p>对于上式，令：</p>
<p><img src="/2024/04/17/imu-preinteg/15.png" alt="15" title="15"></p>
<p>则有：</p>
<p><img src="/2024/04/17/imu-preinteg/16.png" alt="16" title="16"></p>
<p>从而获得了速度预积分项 “预积分测量值 = 理想值 + 白噪声” 的形式，其中，$\Delta \tilde{\mathbf{v}}_{ij}$ 为预积分测量值，由IMU 测量值和对bias 的估计值计算得到；$\delta \mathbf{v}_{ij}$ 为测量噪声。</p>
<h3 id="3-1-3-位置预积分项"><a href="#3-1-3-位置预积分项" class="headerlink" title="3.1.3 位置预积分项"></a>3.1.3 位置预积分项</h3><p>将旋转预积分项、速度预积分项代入 $\Delta \mathbf{p}_{ij}$ 中，有：</p>
<p><img src="/2024/04/17/imu-preinteg/17.png" alt="17" title="17"></p>
<p>对于上式，令：</p>
<p><img src="/2024/04/17/imu-preinteg/18.png" alt="18" title="18"></p>
<p>则有：</p>
<p><img src="/2024/04/17/imu-preinteg/19.png" alt="19" title="19"></p>
<p>从而获得了位置预积分项 “预积分测量值 = 理想值 + 白噪声” 的形式，其中，$\Delta \tilde{\mathbf{p}}_{ij}$ 为预积分测量值，由IMU 测量值和对bias 的估计值计算得到；$\delta \mathbf{p}_{ij}$ 为测量噪声。</p>
<h3 id="3-1-4-总结"><a href="#3-1-4-总结" class="headerlink" title="3.1.4 总结"></a>3.1.4 总结</h3><p>对前三节获取的预积分项理想值与测量值之间的关系汇总如下：</p>
<p><img src="/2024/04/17/imu-preinteg/20.png" alt="20" title="20"></p>
<p>代入预积分项<strong>理想值</strong>的表达式：</p>
<p><img src="/2024/04/17/imu-preinteg/21.png" alt="21" title="21"></p>
<p>则有：</p>
<p><img src="/2024/04/17/imu-preinteg/22.png" alt="22" title="22"></p>
<p>上式即为最终的 “测量值=真实值 + 测量噪声” 的模式。</p>
<h2 id="3-2-测量噪声分析"><a href="#3-2-测量噪声分析" class="headerlink" title="3.2 测量噪声分析"></a>3.2 测量噪声分析</h2><p>将预积分测量噪声表示为：</p>
<p><img src="/2024/04/17/imu-preinteg/23.png" alt="23" title="23"></p>
<p>希望其满足高斯分布，则对 $\eta_{ij}^\Delta$ 进行分析。</p>
<h3 id="3-2-1-旋转预积分测量噪声"><a href="#3-2-1-旋转预积分测量噪声" class="headerlink" title="3.2.1 旋转预积分测量噪声"></a>3.2.1 旋转预积分测量噪声</h3><p><img src="/2024/04/17/imu-preinteg/24.png" alt="24" title="24"></p>
<p>其中，$\mathbf{J}_r^k = \mathbf{J}_r((\tilde{\omega}_k - \mathbf{b}_i^g)\Delta t)$ 。对上式两侧取对数：</p>
<p><img src="/2024/04/17/imu-preinteg/25.png" alt="25" title="25"></p>
<p>对上式进行处理：</p>
<p><img src="/2024/04/17/imu-preinteg/26.png" alt="26" title="26"></p>
<p>即：</p>
<p><img src="/2024/04/17/imu-preinteg/27.png" alt="27" title="27"></p>
<p>由于 $\Delta \tilde{\mathbf{R}}_{k+1,j}^T, \mathbf{J}_r^k, \Delta t$ 均是已知量，而 $\eta_k^{gd}$ 是零均值高斯噪声，因此，$\delta \overrightarrow{\phi}_{ij}$ 也是零均值高斯噪声。</p>
<p>在此基础上，进一步推导旋转预积分测量噪声的递推形式，对于 $\delta \overrightarrow{\phi}_{ij-1}\rightarrow \delta \overrightarrow{\phi}_{ij}$ ，有：</p>
<p><img src="/2024/04/17/imu-preinteg/28.png" alt="28" title="28"></p>
<h3 id="3-2-2-速度预积分测量噪声"><a href="#3-2-2-速度预积分测量噪声" class="headerlink" title="3.2.2 速度预积分测量噪声"></a>3.2.2 速度预积分测量噪声</h3><p>由于$\delta \overrightarrow{\phi}_{ij}$ 近似为零均值高斯噪声，且 $\eta_k^{ad}$ 也是零均值高斯噪声，则根据 $\delta \mathbf{v}_{ij}$ 表达式可知其也具有高斯分布的性质：</p>
<p><img src="/2024/04/17/imu-preinteg/29.png" alt="29" title="29"></p>
<p>在此基础上，进一步推导速度预积分测量噪声的递推形式，对于 $\delta \mathbf{v}_{ij-1}\rightarrow \delta \mathbf{v}_{ij}$ ，有：</p>
<p><img src="/2024/04/17/imu-preinteg/30.png" alt="30" title="30"></p>
<h3 id="3-2-3-位置预积分测量噪声"><a href="#3-2-3-位置预积分测量噪声" class="headerlink" title="3.2.3 位置预积分测量噪声"></a>3.2.3 位置预积分测量噪声</h3><p>类似 $\delta \mathbf{v}_{ij}$ ， $\delta \mathbf{p}_{ij}$ 也具有高斯分布的性质：</p>
<p><img src="/2024/04/17/imu-preinteg/31.png" alt="31" title="31"></p>
<p>在此基础上，进一步推导速度预积分测量噪声的递推形式，对于 $\delta \mathbf{p}_{ij-1}\rightarrow \delta \mathbf{p}_{ij}$ ，有：</p>
<p><img src="/2024/04/17/imu-preinteg/32.png" alt="32" title="32"></p>
<h3 id="3-2-4-总结"><a href="#3-2-4-总结" class="headerlink" title="3.2.4 总结"></a>3.2.4 总结</h3><p>综上所述，可以得到预积分测量噪声 $\eta_{ij}^\Delta$ 的递推形式：</p>
<p><img src="/2024/04/17/imu-preinteg/33.png" alt="33" title="33"></p>
<p>其中，$\eta_k^d = [(\eta_k^{gd})^T, (\eta_k^{ad})^T]^T$ 。若令：</p>
<p><img src="/2024/04/17/imu-preinteg/34.png" alt="34" title="34"></p>
<p>则有：</p>
<p><img src="/2024/04/17/imu-preinteg/35.png" alt="35" title="35"></p>
<p>相应地，预积分测量噪声的<strong>协方差矩阵的递推计算形式</strong>为：</p>
<p><img src="/2024/04/17/imu-preinteg/36.png" alt="36" title="36"></p>
<h2 id="3-3-bias-更新时的预积分测量值更新"><a href="#3-3-bias-更新时的预积分测量值更新" class="headerlink" title="3.3 bias 更新时的预积分测量值更新"></a>3.3 bias 更新时的预积分测量值更新</h2><p>前面的预积分计算是在积分区间bias 固定的假设下进行的，当bias 变化时为了减小预积分测量值计算的负担，<strong>利用线性化进行预积分项的一阶近似更新</strong>。对bias 更新过程进行如下定义：</p>
<p><img src="/2024/04/17/imu-preinteg/37.png" alt="37" title="37"></p>
<p>其中，$\overline{\mathbf{b}}$ 表示旧的bias，$\delta{\mathbf{b}}$ 表示bias变化量，$\hat{\mathbf{b}}$ 表示更新后的bias。预积分项的线性化更新过程如下所示：</p>
<p><img src="/2024/04/17/imu-preinteg/38.png" alt="38" title="38"></p>
<p>上式简写为：</p>
<p><img src="/2024/04/17/imu-preinteg/39.png" alt="39" title="39"></p>
<h3 id="3-3-1-旋转预积分bias更新偏导项求解"><a href="#3-3-1-旋转预积分bias更新偏导项求解" class="headerlink" title="3.3.1 旋转预积分bias更新偏导项求解"></a>3.3.1 旋转预积分bias更新偏导项求解</h3><p>求解过程如下所示：</p>
<p><img src="/2024/04/17/imu-preinteg/40.png" alt="40" title="40"></p>
<p>对上式进行同样的利用Adjoint性质对连乘顺序进行调整，即可得到：</p>
<p><img src="/2024/04/17/imu-preinteg/41.png" alt="41" title="41"></p>
<p>当 $\mathbf{c}_k$ 极小时，有 $\mathbf{J}_r(\mathbf{c}_k) \approx \mathbf{I}$ ，对上式进一步处理可得到：</p>
<p><img src="/2024/04/17/imu-preinteg/42.png" alt="42" title="42"></p>
<p>由此可以得到：</p>
<p><img src="/2024/04/17/imu-preinteg/43.png" alt="43" title="43"></p>
<p>其中，$\mathbf{J}_r^k = \mathbf{J}_r((\tilde{\omega}_k - \overline{\mathbf{b}}_i^g)\Delta t)$。</p>
<h3 id="3-3-2-速度预积分bias更新偏导项求解"><a href="#3-3-2-速度预积分bias更新偏导项求解" class="headerlink" title="3.3.2 速度预积分bias更新偏导项求解"></a>3.3.2 速度预积分bias更新偏导项求解</h3><p>与旋转预积分类似，可以得到两个偏导项为：</p>
<p><img src="/2024/04/17/imu-preinteg/44.png" alt="44" title="44"></p>
<h3 id="3-3-3-位置预积分bias更新偏导项求解"><a href="#3-3-3-位置预积分bias更新偏导项求解" class="headerlink" title="3.3.3 位置预积分bias更新偏导项求解"></a>3.3.3 位置预积分bias更新偏导项求解</h3><p>位置预积分的两个偏导项为：</p>
<p><img src="/2024/04/17/imu-preinteg/45.png" alt="45" title="45"></p>
<h1 id="4-残差及Jacobian"><a href="#4-残差及Jacobian" class="headerlink" title="4 残差及Jacobian"></a>4 残差及Jacobian</h1><h2 id="4-1-残差项"><a href="#4-1-残差项" class="headerlink" title="4.1 残差项"></a>4.1 残差项</h2><p>预积分项的<strong>理想值</strong>为：</p>
<p><img src="/2024/04/17/imu-preinteg/46.png" alt="46" title="46"></p>
<p>当状态参数更新时，形成的<strong>残差</strong>为：</p>
<p><img src="/2024/04/17/imu-preinteg/47.png" alt="47" title="47"></p>
<p>即，残差定义为 “残差 = 理想值 - 后验观测值”。</p>
<p>状态参数的<strong>更新过程</strong>表示为：</p>
<p><img src="/2024/04/17/imu-preinteg/48.png" alt="48" title="48"></p>
<p>在迭代优化中，待优化的<strong>状态参数</strong>为 $\mathbf{R}_i, \mathbf{p}_i, \mathbf{v}_i, \mathbf{R}_j, \mathbf{p}_j, \mathbf{v}_j, \delta  \mathbf{b}_i^g, \delta \mathbf{b}_i^a$ ，而迭代中求解增量方程得到的增量是：$\delta \overrightarrow{\phi}_i,\delta\mathbf{p}_i, \delta\mathbf{v}_i, \delta \overrightarrow{\phi}_j,\delta\mathbf{p}_j, \delta\mathbf{v}_j, \tilde{\delta \mathbf{b}_i^g}, \tilde{\delta \mathbf{b}_i^a}$ ，最后两项是<strong>bias增量的增量</strong>。</p>
<h2 id="4-2-Jacobian"><a href="#4-2-Jacobian" class="headerlink" title="4.2 Jacobian"></a>4.2 Jacobian</h2><p>求<strong>残差项</strong>（$\mathbf{r}_{\Delta \mathbf{R}_{ij}}, \mathbf{r}_{\Delta \mathbf{v}_{ij}}, \mathbf{r}_{\Delta \mathbf{p}_{ij}}$ ）关于<strong>各个状态参数</strong>的Jacobian。</p>
<h3 id="4-2-1-旋转残差项Jacobian"><a href="#4-2-1-旋转残差项Jacobian" class="headerlink" title="4.2.1 旋转残差项Jacobian"></a>4.2.1 旋转残差项Jacobian</h3><p><img src="/2024/04/17/imu-preinteg/49.png" alt="49" title="49"></p>
<p>首先是“0类”：</p>
<p><img src="/2024/04/17/imu-preinteg/50.png" alt="50" title="50"></p>
<p><strong>注：</strong>上式中应该是写错了，应是对状态参数进行Jacobian求解，而不是对状态参数的增量进行求解。</p>
<p>然后是“复杂类”：</p>
<p><img src="/2024/04/17/imu-preinteg/51.png" alt="51" title="51"></p>
<p>由此得到 $\mathbf{r}_{\Delta \mathbf{R}_{ij}}$ 关于 $\overrightarrow{\phi}_i$ 的Jacobian：</p>
<p><img src="/2024/04/17/imu-preinteg/52.png" alt="52" title="52"></p>
<p>同样地，可得到 $\mathbf{r}_{\Delta \mathbf{R}_{ij}}$ 关于 $\overrightarrow{\phi}_j, {\delta \mathbf{b}_i^g}$ 的Jacobian：</p>
<p><img src="/2024/04/17/imu-preinteg/53.png" alt="53" title="53"></p>
<p><img src="/2024/04/17/imu-preinteg/54.png" alt="54" title="54"></p>
<h3 id="4-2-2-速度残差项Jacobian"><a href="#4-2-2-速度残差项Jacobian" class="headerlink" title="4.2.2 速度残差项Jacobian"></a>4.2.2 速度残差项Jacobian</h3><p><img src="/2024/04/17/imu-preinteg/55.png" alt="55" title="55"></p>
<p>首先是“0类”：</p>
<p><img src="/2024/04/17/imu-preinteg/56.png" alt="56" title="56"></p>
<p>然后是“线性类”：</p>
<p><img src="/2024/04/17/imu-preinteg/57.png" alt="57" title="57"></p>
<p>最后是“复杂类”，$\mathbf{r}_{\Delta \mathbf{v}_{ij}}$ 关于 $\mathbf{v}_i, \mathbf{v}_j, \overrightarrow{\phi}_i$ 的Jacobian：</p>
<p><img src="/2024/04/17/imu-preinteg/58.png" alt="58" title="58"></p>
<p><img src="/2024/04/17/imu-preinteg/59.png" alt="59" title="59"></p>
<p><img src="/2024/04/17/imu-preinteg/60.png" alt="60" title="60"></p>
<h3 id="4-2-3-位置残差项Jacobian"><a href="#4-2-3-位置残差项Jacobian" class="headerlink" title="4.2.3 位置残差项Jacobian"></a>4.2.3 位置残差项Jacobian</h3><p><img src="/2024/04/17/imu-preinteg/61.png" alt="61" title="61"></p>
<p>首先是“0类”：</p>
<p><img src="/2024/04/17/imu-preinteg/62.png" alt="62" title="62"></p>
<p>然后是“线性类”：</p>
<p><img src="/2024/04/17/imu-preinteg/63.png" alt="63" title="63"></p>
<p>最后是“复杂类”，$\mathbf{r}_{\Delta \mathbf{p}_{ij}}$ 关于 $\mathbf{p}_i, \mathbf{p}_j, \mathbf{v}_i, \overrightarrow{\phi}_i$ 的Jacobian：</p>
<p><img src="/2024/04/17/imu-preinteg/64.png" alt="64" title="64"></p>
<p><img src="/2024/04/17/imu-preinteg/65.png" alt="65" title="65"></p>
<p><img src="/2024/04/17/imu-preinteg/66.png" alt="66" title="66"></p>
<p><img src="/2024/04/17/imu-preinteg/67.png" alt="67" title="67"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Multi-source</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>IMU</tag>
        <tag>Preintegration</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Towards Real-time Semantic RGB-D SLAM in Dynamic Environments</title>
    <url>/2024/02/27/ji2021/</url>
    <content><![CDATA[<p>Ji, Tete, Chen Wang, and Lihua Xie. “Towards Real-Time Semantic RGB-D SLAM in Dynamic Environments.” In <em>2021 IEEE International Conference on Robotics and Automation (ICRA)</em>, 11175–81. Xi’an, China: IEEE, 2021. <a href="https://doi.org/10.1109/ICRA48506.2021.9561743">https://doi.org/10.1109/ICRA48506.2021.9561743</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文做出的贡献：</p>
<ul>
<li>提出一个<strong>基于关键帧</strong>的语义RGB-D SLAM 系统，可以减少动态环境中移动物体的影响；</li>
<li>提出一个高效的<strong>几何模块</strong>，与语义SLAM 框架相结合来处理<strong>未知的移动物体</strong>；</li>
<li>通过实验证明本算法可在<strong>嵌入式系统中实时运行</strong>，同时可实现与SOTA 方法相当的精度。</li>
</ul>
<span id="more"></span>
<h1 id="3-Proposed-Method"><a href="#3-Proposed-Method" class="headerlink" title="3 Proposed Method"></a>3 Proposed Method</h1><p>本系统是基于ORB-SLAM2 算法的，系统框架如下图所示：</p>
<p><img src="/2024/02/27/ji2021/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-Semantic-Module"><a href="#3-1-Semantic-Module" class="headerlink" title="3.1 Semantic Module"></a>3.1 Semantic Module</h2><p>考虑到精度与速度的平衡，作者选用<strong>SegNet</strong> 作为语义分割的网络；为了进一步实现实时处理性能，作者<strong>只对关键帧</strong>进行语义分割，将潜在动态物体（如人、汽车、自行车等）上的特征点进行<strong>剔除</strong>；值得注意的是，作者<strong>并不进一步使用几何模块</strong>对潜在动态物体的<strong>真实运动情况</strong>进行确认，而是认为，对于长期一致的制图目的而言，即便这些潜在动态物体在某些时刻是静态的，但是长远考虑的话它们<strong>并不可靠</strong>，所以作者直接将这些潜在动态物体进行剔除。</p>
<h2 id="3-2-Geometry-Module"><a href="#3-2-Geometry-Module" class="headerlink" title="3.2 Geometry Module"></a>3.2 Geometry Module</h2><p>语义分割只能对训练时的标记种类进行识别，<strong>无法对未知类别</strong>的物体进行识别，因此，作者引入<strong>几何模块</strong>来对<strong>未知动态物体</strong>进行检测。</p>
<p>作者首先利用<strong>K-Means 算法</strong>将深度图中的点分割为N 个集群，3D 空间中<strong>距离较近的点</strong>被分为一组集群；作者假设每个集群属于一个物体表面，且同一集群内的点的<strong>运动状态一致</strong>。因为同一个物体可能被分为几个不同的集群，所以本算法中的物体<strong>可以不满足刚体假设</strong>。</p>
<p>对于每个集群 $c_j$ ，作者计算该集群内所有特征点 $\mathbf{u}_i’$ 与其在3D 空间中的关联点 $\mathbf{P}_i$ 之间的重投影误差，求得集群内的重投影误差均值 $r_j$ ：</p>
<p><img src="/2024/02/27/ji2021/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$\rho$ 是正则化方程（惩罚方程）。如果某个集群的平均重投影误差相较于其他集群更大，则<strong>标记该集群是动态的</strong>，并移除掉属于该集群的所有特征点。部分实验结果如Fig. 3所示，观察可得，在部分情况下语义分割会出现识别失败或错误的现象，而此时几何模块可以正常识别出动态物体。</p>
<p><img src="/2024/02/27/ji2021/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="3-3-Keyframe-and-Local-Map-Update"><a href="#3-3-Keyframe-and-Local-Map-Update" class="headerlink" title="3.3 Keyframe and Local Map Update"></a>3.3 Keyframe and Local Map Update</h2><p>由于一帧图片是使用<strong>关键帧</strong>和<strong>局部地图</strong>进行<strong>跟踪</strong>的，所以只需要确保关键帧和局部地图中只有静态特征点即可。当一个新的关键帧被选取后，利用语义分割识别出动态特征点，局部地图中也会<strong>同步移除</strong>相应的动态地图点，由此，可以保持一个只包含静态特征与地图点的数据库。</p>
<h2 id="3-4-Tracking"><a href="#3-4-Tracking" class="headerlink" title="3.4 Tracking"></a>3.4 Tracking</h2><p>本系统对每一帧新图片的跟踪采用类似于ORB-SLAM2 的<strong>两步法</strong>：</p>
<ol>
<li>首先，利用当前帧和与当前帧有最大重合区域的关键帧进行初始位姿估计，因为关键帧<strong>已经移除了潜在动态物体</strong>，所以初始位姿估计<strong>更可靠</strong>；</li>
<li>然后，几何模块利用初始位姿估计进行<strong>动态物体检测</strong>，并从当前帧中移除所有的动态特征点，在此基础上跟踪当前帧中所有观测到的地图点进行<strong>局部BA 优化</strong>，获取最终的位姿估计。</li>
</ol>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Loop Closure Detection Based on Object-level Spatial Layout and Semantic Consistency</title>
    <url>/2024/03/06/ji2023/</url>
    <content><![CDATA[<p>Ji, Xingwu, Peilin Liu, Haochen Niu, Xiang Chen, Rendong Ying, and Fei Wen. “Loop Closure Detection Based on Object-Level Spatial Layout and Semantic Consistency.” arXiv, April 14, 2023. <a href="http://arxiv.org/abs/2304.05146">http://arxiv.org/abs/2304.05146</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文做出的贡献：</p>
<ol>
<li>在3D 场景图中基于<strong>拓扑结构与几何匹配</strong>的回环检测方法，利用物体的信息和它们的邻域来建立<strong>3D 拓扑结构</strong>，保证几何布局以及语义属性的一致；</li>
<li>包含<strong>语义标签、bounding box IoU、物体颜色编码、物体级embedding</strong> 信息的物体级数据关联方法，基于该方法构建了<strong>3D 语义地图</strong>；</li>
<li>一个完整的视觉SLAM 系统，实验结果表证明了本算法的优越性。</li>
</ol>
<span id="more"></span>
<h1 id="3-Object-Level-Semantic-Mapping"><a href="#3-Object-Level-Semantic-Mapping" class="headerlink" title="3 Object-Level Semantic Mapping"></a>3 Object-Level Semantic Mapping</h1><p>本系统的架构如Fig. 2所示：</p>
<ol>
<li>基于<strong>双目里程计</strong>利用双目RGB 图片流获取初步的相对位姿；</li>
<li>利用一个<strong>物体提取网络</strong>来进行语义标记，并生成2D bounding boxes 和3D 立方体proposals；</li>
<li>将bounding boxes 输入<strong>instance-feature 学习网络</strong>来提取<strong>物体级的embeddings</strong>；</li>
<li>利用物体的语义等信息实现局部地图中的物体和全局地图中物体的<strong>数据关联</strong>，若全局地图中没有物体与检测到的物体实现匹配，则在全局地图中添加一个cuboid；</li>
<li>基于数据关联，利用物体位姿求解算法来将新检测到的物体放入全局地图的cuboid中，利用BA <strong>同时优化物体与相机的位姿</strong>。</li>
</ol>
<p><img src="/2024/03/06/ji2023/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-Object-Semantic-Information-Extraction"><a href="#3-1-Object-Semantic-Information-Extraction" class="headerlink" title="3.1 Object Semantic Information Extraction"></a>3.1 Object Semantic Information Extraction</h2><h3 id="3-1-1-Geometry-information"><a href="#3-1-1-Geometry-information" class="headerlink" title="3.1.1 Geometry information"></a>3.1.1 Geometry information</h3><p>几何信息包含：语义标签、物体尺寸、2D bounding boxes 以及 3D 立方体proposals。作者使用VisualDet3D 来提取2D bounding boxes 以及 3D 立方体，在所有物体均位于统一平面上的假设下，一个物体的3D cuboid 包含<strong>7自由度</strong>：3位置、3尺寸、yaw角，获取过程如Fig. 3所示。值得注意的是，作者在数据关联过程中忽略了那些体积过大或者距离相机过远的物体，以方便后续的3D 环境重建和回环检测。</p>
<p><img src="/2024/03/06/ji2023/fig3.png" alt="fig3" title="figure 3"></p>
<h3 id="3-1-2-Color-information"><a href="#3-1-2-Color-information" class="headerlink" title="3.1.2 Color information"></a>3.1.2 Color information</h3><p>为获取图片的颜色信息，作者将RGB 格式的图片转换为HSV (hue, saturation and value)格式，bounding box内的区域作为物体在图片中的区域，作者使用K-means++ 算法将物体的HSV 数值进行聚类，并分为K 个集群，对于每个集群选取其中心作为该集群的基本色彩，然后使用 <strong>K维的特征向量</strong>来表示集群色彩直方图的分布，以该向量作为物体的色彩信息。</p>
<h3 id="3-1-3-Embedding-information"><a href="#3-1-3-Embedding-information" class="headerlink" title="3.1.3 Embedding information"></a>3.1.3 Embedding information</h3><p>相似地，作者使用bounding boxes区域作为物体的图像块image patches，利用fast-reid 算法将patches 编码为向量表示。注：fast-reid 算法经VERI-Wild 数据集进行预训练。</p>
<p><img src="/2024/03/06/ji2023/fig4.png" alt="fig4" title="figure 4"></p>
<h2 id="3-2-Object-Data-Association"><a href="#3-2-Object-Data-Association" class="headerlink" title="3.2 Object Data Association"></a>3.2 Object Data Association</h2><p>假设在第 $c_i$ 帧照片上检测到N 个物体，则表示为 $D = \{d_i\}_N$  <em>,</em> $d_i = \{l_i, b_i, h_i, e_i, T_{co}\}$ ，其中，$l_i$  表示语义标签，$b_i$ 表示bounding box，$h_i$ 表示归一化的颜色直方图，$e_i$ 表示物体embedding，$T_{co}$ 表示物体<strong>在相机坐标系中的位姿</strong>。</p>
<p>相应地，全局地图中存在M 个物体地标 $O = \{o_k\}_{k=1}^M$ <em>，</em>每一个地标可表示为 $ o_k = \{\tilde{l}_k, \tilde{u}_k, \tilde{b}_k, H_k, E_k, T_{wo_k}\}$ ，其中，$\tilde{l}_k$ 表示语义标签，$H_k = \{\tilde{h}_j\}_{j=1}^n,E_k= \{\tilde{e}_j\}_{j=1}^n$ 分别是<strong>过去n 个匹配的颜色直方图和物体embedding 向量组</strong>，$T_{wo_k}$ 是<strong>物体在世界坐标系中的坐标</strong>，$\tilde{u}_k,\tilde{b}_k$  分别是cuboid 和 bounding box。<strong>bounding box 是由 cuboid 处理得到的</strong>：将cuboid 的8个顶点映射到图片中，获取x-y 坐标的最小值与最大值来形成一个矩形：</p>
<p><img src="/2024/03/06/ji2023/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$\pi(.)$  表示相机映射方程， R，t表示旋转与平移矩阵。</p>
<p>物体相似度计算如下所示：</p>
<p><img src="/2024/03/06/ji2023/f2.png" alt="f2" title="formula 2"></p>
<p>相似度计算涉及到物体的<strong>bounding boxes，颜色直方图以及物体embedding</strong>：</p>
<p><img src="/2024/03/06/ji2023/f3.png" alt="f3" title="formula 3"></p>
<p>其中，</p>
<p><img src="/2024/03/06/ji2023/f4.png" alt="f4" title="formula 4"></p>
<h2 id="3-3-Object-Pose-Refinement"><a href="#3-3-Object-Pose-Refinement" class="headerlink" title="3.3 Object Pose Refinement"></a>3.3 Object Pose Refinement</h2><p>实现数据关联之后，使用相机运动模型和物体空间定位来优化位姿。使用BA 联合优化物体与相机的位姿，形成一个<strong>非线性最小二乘优化问题</strong>：</p>
<p><img src="/2024/03/06/ji2023/f7.png" alt="f7" title="formula 7"></p>
<p>其中，$c_0, c_n$ 表示跟踪的图片帧数范围。作者采用VINS-Fusion 的<strong>滑动窗口优化机制</strong>。由此构建一个有着精确物体位姿的<strong>物体级语义地图</strong>。</p>
<h1 id="4-Loop-Closure-With-Semantic-Landmarks"><a href="#4-Loop-Closure-With-Semantic-Landmarks" class="headerlink" title="4 Loop Closure With Semantic Landmarks"></a>4 Loop Closure With Semantic Landmarks</h1><p>基于空间拓扑以及顶点属性，作者采用两步拓扑图匹配来确认局部图与全局图之间的联系：</p>
<ol>
<li>首先构建邻域拓扑结构，并基于<strong>空间布局一致性</strong>选取局部地图与全局地图的候选匹配；</li>
<li>计算<strong>语义相似度</strong>，选取相似度最高的匹配组。</li>
</ol>
<h2 id="4-1-Graph-Generation"><a href="#4-1-Graph-Generation" class="headerlink" title="4.1 Graph Generation"></a>4.1 Graph Generation</h2><p>顶点包含语义标签，颜色直方图向量，以及物体embedding 信息；顶点之间的边是由物体之间的空间距离决定的，作者选取K 个最近的邻接顶点进行连接。基于此，可分别根据局部地图与全局地图获取相应的图 $G_l, G_g$ 。</p>
<h2 id="4-2-Spatial-Layout-Matching"><a href="#4-2-Spatial-Layout-Matching" class="headerlink" title="4.2 Spatial Layout Matching"></a>4.2 Spatial Layout Matching</h2><p><img src="/2024/03/06/ji2023/fig5.png" alt="fig5" title="figure 5"></p>
<h2 id="4-3-Semantic-Properties-Verifying"><a href="#4-3-Semantic-Properties-Verifying" class="headerlink" title="4.3 Semantic Properties Verifying"></a>4.3 Semantic Properties Verifying</h2><p>经过空间布局匹配之后得到的候选匹配点需进一步计算语义属性的一致性：</p>
<p><img src="/2024/03/06/ji2023/f11.png" alt="f11" title="formula 11"></p>
<p>其中，$[d_x, d_y, d_z],t$ 表示顶点的尺寸和位置信息，$h, e$ 分别表示顶点的颜色和embedding 信息。最终，匹配点对的相似度需要大于一个阈值才可判定为匹配对。</p>
<h2 id="4-4-Pose-Graph-Optimization"><a href="#4-4-Pose-Graph-Optimization" class="headerlink" title="4.4 Pose Graph Optimization"></a>4.4 Pose Graph Optimization</h2><p>在回环检测中，当前帧的物体可能与之前多个帧中的物体匹配上了，此时选择第一个匹配帧作为回环检测对象，如Fig. 6所示。</p>
<p><img src="/2024/03/06/ji2023/fig6.png" alt="fig6" title="figure 6"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Topology</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Topology</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 LaneLoc_Lane marking based localization using highly accurate maps</title>
    <url>/2024/01/31/laneloc/</url>
    <content><![CDATA[<p>Schreiber, Markus, Carsten Knoppel, and Uwe Franke. “LaneLoc: Lane Marking Based Localization Using Highly Accurate Maps.” In <em>2013 IEEE Intelligent Vehicles Symposium (IV)</em>, 449–54. Gold Coast City, Australia: IEEE, 2013. <a href="https://doi.org/10.1109/IVS.2013.6629509">https://doi.org/10.1109/IVS.2013.6629509</a>.</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>作者利用双目立体相机系统和包含路沿与道路标志的先验高精度地图实现车辆定位。定位过程中，GNSS 位置只是用来进行初始化，后续的定位解算不需要GNSS。作者在长约50 km的郊区道路进行测试，最终的定位精度在分米级。</p>
<span id="more"></span>
<h1 id="2-Mapping"><a href="#2-Mapping" class="headerlink" title="2 Mapping"></a>2 Mapping</h1><p>制图过程如下所示：</p>
<p><img src="/2024/01/31/laneloc/fig2.png" alt="fig2" title="figure 2"></p>
<p>利用各个传感器采集到的数据制作鸟瞰视角地图，如下图所示：</p>
<p><img src="/2024/01/31/laneloc/fig4.png" alt="fig4" title="figure 4"></p>
<h1 id="3-Online-Localization"><a href="#3-Online-Localization" class="headerlink" title="3 Online Localization"></a>3 Online Localization</h1><p>作者的定位系统选择一个前向的立体相机系统以及一个IMU，如下图所示，其中GNSS 模块进行初始化。</p>
<p><img src="/2024/01/31/laneloc/fig6.png" alt="fig6" title="figure 6"></p>
<p>作者使用基于Kalman 滤波器的定位模型。</p>
<h2 id="3-1-Localization-Model"><a href="#3-1-Localization-Model" class="headerlink" title="3.1 Localization Model"></a>3.1 Localization Model</h2><p><strong>观测模型</strong>表示：所有测量点 $\vec{P}_{e,i}’$ 的<strong>预期位置</strong>（依赖于状态向量的方程${h}(\vec{x}_{veh})$ ）和所有<strong>观测值</strong> $\vec{P}_{m,i}$ （表示为 $\vec{y}$ ）在汽车坐标系下的<strong>残差</strong>：</p>
<p><img src="/2024/01/31/laneloc/fig7.png" alt="fig7" title="figure 7"></p>
<p><img src="/2024/01/31/laneloc/f6.png" alt="f6" title="formula 6"></p>
<p><strong>观测噪声</strong>的方差包含：</p>
<ul>
<li>地图数据噪声 $\sigma_{map}^2$ ;</li>
<li>back-projection 噪声 $\sigma_{cam}^2$ ，取决于相机高度和路的朝向。</li>
</ul>
<p><img src="/2024/01/31/laneloc/f7.png" alt="f7" title="formula 7"></p>
<p>作者在实验过程中分别假设：$\sigma_{map}^2 = 10 cm$ ，$\sigma_{cam}^2 = 3 px$ 。观测噪声表示为一个<strong>取决于残差的概率函数</strong>。</p>
<h2 id="3-2-Map-Matching"><a href="#3-2-Map-Matching" class="headerlink" title="3.2 Map Matching"></a>3.2 Map Matching</h2><p>用来定位的高精度地图包含表示<strong>路标和路沿的线段</strong>，而观测数据包含<strong>点</strong>；地图匹配的目标是实现<strong>测量点与线段之间的最佳可能匹配</strong>，必须满足横向和纵向匹配残差的最小化方可实现最佳匹配，因此，作者选择对地图上的线段<strong>采样为地图点进行匹配</strong>，如Fig. 8所示。</p>
<p><img src="/2024/01/31/laneloc/fig8.png" alt="fig8" title="figure 8"></p>
<h2 id="3-3-Measurement-Extraction"><a href="#3-3-Measurement-Extraction" class="headerlink" title="3.3 Measurement Extraction"></a>3.3 Measurement Extraction</h2><h3 id="3-3-1-Lane-Marking-Measurement"><a href="#3-3-1-Lane-Marking-Measurement" class="headerlink" title="3.3.1 Lane Marking Measurement"></a>3.3.1 Lane Marking Measurement</h3><p>作者使用一个<strong>定向匹配滤波器 oriented matched filter</strong>（在<strong>普通车道线检测系统</strong>中得到了成功应用）来检测车道线。</p>
<p>为了检测车道线，利用当前位姿估计将地图投影到图片中，并将<strong>搜索线</strong>置于预期的车道线周围，定向匹配滤波器会在这些搜索线中根据图片中测量的路标识别出一个<strong>low-high-low灰度值模式</strong>，利用立体相机提供的深度信息，这些被探测到的位置会投影至道路平面上来决定在汽车坐标系中所需的观测值（即残差 $\vec{r}’$ ）。</p>
<p>由于该投影过程对于汽车的俯仰角非常敏感，所以作者使用一个基于V-Diaparity 方法的<strong>俯仰角估计器</strong>来解决该问题。</p>
<p><img src="/2024/01/31/laneloc/fig10.png" alt="fig10" title="figure 10"></p>
<h3 id="3-3-2-Curb-Measurement"><a href="#3-3-2-Curb-Measurement" class="headerlink" title="3.3.2 Curb Measurement"></a>3.3.2 Curb Measurement</h3><p>作者使用一个基于强度图片 intensity image 和高度信息的<strong>基于分类器的识别</strong>来进行路沿探测，得到路沿在图片中的位置以及存在的概率参数。</p>
<p><img src="/2024/01/31/laneloc/fig11.png" alt="fig11" title="figure 11"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Semantic Mapping for View-Invariant Relocalization</title>
    <url>/2024/02/02/li2019/</url>
    <content><![CDATA[<p>Li, Jimmy, David Meger, and Gregory Dudek. “Semantic Mapping for View-Invariant Relocalization.” In <em>2019 International Conference on Robotics and Automation (ICRA)</em>, 7108–15. Montreal, QC, Canada: IEEE, 2019. <a href="https://doi.org/10.1109/ICRA.2019.8793624">https://doi.org/10.1109/ICRA.2019.8793624</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文使用了<strong>基于外观的几何特征</strong>与<strong>物体级语义特征</strong>的混合策略，主要贡献就是将传统视觉SLAM 和语义地标进行<strong>协同集成</strong>。</p>
<span id="more"></span>
<h1 id="3-Method"><a href="#3-Method" class="headerlink" title="3 Method"></a>3 Method</h1><h2 id="3-1-Problem-Statement"><a href="#3-1-Problem-Statement" class="headerlink" title="3.1 Problem Statement"></a>3.1 Problem Statement</h2><p>本文方法包含两个组件：</p>
<ol>
<li><strong>语义制图算法</strong>：跟踪物体帧间的3D 位姿变换，并产生包含物体的度量地图；</li>
<li><strong>重定位算法</strong>：给定同一场景的两个语义地图，进行对齐并产生位姿变换。</li>
</ol>
<h2 id="3-2-Semantic-Mapping"><a href="#3-2-Semantic-Mapping" class="headerlink" title="3.2 Semantic Mapping"></a>3.2 Semantic Mapping</h2><p>使用<strong>立方体</strong>来表示物体，包含9自由度：位置、朝向以及尺寸。</p>
<p>利用ORB-SLAM2 获取相机位姿，对物体检测算法处理过的图片中的物体进行三角化，通过<strong>基于采样的推理程序</strong>得到<strong>3D 立体框</strong>。为了简化3D 物体的几何推理，作者假定物体与场景布局是<strong>对齐的</strong>，场景布局包含三个正交轴。</p>
<p><img src="/2024/02/02/li2019/a1.png" alt="a1" title="algorithm 1"></p>
<h3 id="3-2-2-Data-Association"><a href="#3-2-2-Data-Association" class="headerlink" title="3.2.2 Data Association"></a>3.2.2 Data Association</h3><p>利用ORB-SLAM 获取关键帧的位姿后，将3D 物体地标<strong>投影至关键帧</strong>中得到相应 p 的bbox，关键帧中检测到的物体的bbox 记为 d，则定义p 与 d 之间的<strong>损失函数</strong>：</p>
<p><img src="/2024/02/02/li2019/f1.png" alt="f1" title="formula 1"></p>
<p>下标 $l, t, r, b$ 分别表示bbox 在像素坐标系中的左边、上边、右边、下边四条边。分母作用是<strong>归一化</strong>，防止较大物体的bbox 主导损失函数。</p>
<h3 id="3-2-3-Object-Pose-Update"><a href="#3-2-3-Object-Pose-Update" class="headerlink" title="3.2.3 Object Pose Update"></a>3.2.3 Object Pose Update</h3><p>物体3D 框的<strong>复杂几何性质</strong>以及<strong>模糊的投影过程</strong>，会造成强烈的<strong>非凸性搜索空间</strong>；作者提出一种高效的搜索策略：</p>
<ul>
<li>首先，在一个<strong>缩小的</strong>搜索空间中高效产生多个物体假设；</li>
<li>然后，利用它们在<strong>完整的</strong>搜索空间中快速探索多个局部最小值。</li>
</ul>
<p>将位于物体<strong>上表面的中心点</strong>X 投影至图像平面，构建服从高斯分布的概率模型：</p>
<p><img src="/2024/02/02/li2019/f2.png" alt="f2" title="formula 2"></p>
<p>其中，$f_k$ 表示将3D 点X 投影至图片关键帧k 中。计算后验分布：</p>
<p><img src="/2024/02/02/li2019/f3.png" alt="f3" title="formula 3"></p>
<p>得到一组3D 采样点，然后将其视为<strong>上表面的中心点</strong>来形成相应的3D bbox，将方向与场景布局进行<strong>对齐</strong>，尺寸选择该物体的<strong>平均尺寸</strong>，由此形成完整的3D bbox 估计集合 L，假设 $o\in L$ 对应的假设为 $H_o$ ，路标 $o$ 对应的物体假设 $h$ 的<strong>得分</strong>为：</p>
<p><img src="/2024/02/02/li2019/f4.png" alt="f4" title="formula 4"></p>
<p>其中，$K_o$ 表示有 o 关联检测的所有关键帧集合；$\delta_k$ 表示在关键帧 $k$ 中的关联检测；$f_k$ 将物体假设 h 投影至关键帧 $k$ 中；损失函数 $c$ 由式1描述；$\Gamma$ 表示两个物体 $h, h’$ <strong>在上下文环境中的相关性</strong>（如键盘和鼠标更有可能同时出现在一表面上）。</p>
<p>得到所有假设的得分之后，利用最高得分的假设（且必须比现有假设的得分更高）来更新每个路标。</p>
<h3 id="3-2-4-Contextual-Coherence"><a href="#3-2-4-Contextual-Coherence" class="headerlink" title="3.2.4 Contextual Coherence"></a>3.2.4 Contextual Coherence</h3><p>作者使用上下文约束来对物体的位姿估计进行<strong>正则化</strong>，以鼓励物体地标更符合典型的空间关系，作者之前的研究也证明了物体的<strong>共面性coplanarity</strong> 可视为对物体位姿估计的可靠约束。作者定义相关性函数 $\Gamma$ ：</p>
<p><img src="/2024/02/02/li2019/f5.png" alt="f5" title="formula 5"></p>
<p>其中，$BOTTOMDIST(h, h’)$ 给定两个3D bbox 的底面距离，$COPLANAR$ 指的是位于相同的表面上的两个物体。</p>
<h2 id="3-3-Relocalization"><a href="#3-3-Relocalization" class="headerlink" title="3.3 Relocalization"></a>3.3 Relocalization</h2><p>给定两个包含一组物体地标的语义地图 $L_1, L_2$ ，重定位过程可通过下述公式解决：</p>
<p><img src="/2024/02/02/li2019/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$\theta(o, L)$ 返回一组地标，地标位姿在坐标系 $o$ 中来表示；$\psi(s, L)$ 返回一组尺寸放大 $s$ 倍的地标；函数 $\Omega(L_1, L_2)$ 包含两个操作：</p>
<ol>
<li>利用Hungarian 算法计算两组地标的<strong>最佳匹配</strong>；</li>
<li>对匹配结果进行内点识别，最终返回<strong>内点数量</strong>。</li>
</ol>
<p>完成地图匹配之后即可得到两个相机轨迹之间的相对转换关系。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Topology</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Topology</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 RGB-D Object SLAM Using Quadrics for Indoor Environments</title>
    <url>/2024/03/05/liao2020/</url>
    <content><![CDATA[<p>Liao, Ziwei, Wei Wang, Xianyu Qi, and Xiaoyu Zhang. “RGB-D Object SLAM Using Quadrics for Indoor Environments.” <em>Sensors</em> 20, no. 18 (January 2020): 5150. <a href="https://doi.org/10.3390/s20185150">https://doi.org/10.3390/s20185150</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ul>
<li>提出了一种使用二次曲面作为物体模型的<strong>物体级SLAM 算法</strong>，并使用了两种RGB-D 二次曲面观测模型；</li>
<li>提出了一种从单张RGB-D 图片中提取出表示物体的完整椭球体的方法，该方法基于<strong>物体和其支撑平面</strong>之间的关系；</li>
<li>为二次曲面模型引进了一种<strong>非参数位姿图</strong> nonparametric pose graph，来解决后端的语义数据关联问题；</li>
<li>在两个公开数据集和自采数据集上，与两个SOTA 物体级SLAM 算法进行比较，评估了本算法的有效性。</li>
</ul>
<span id="more"></span>
<h1 id="3-Materials-and-Methods"><a href="#3-Materials-and-Methods" class="headerlink" title="3 Materials and Methods"></a>3 Materials and Methods</h1><h2 id="3-1-System-Overview"><a href="#3-1-System-Overview" class="headerlink" title="3.1 System Overview"></a>3.1 System Overview</h2><p>Fig. 1展示了本文方法的整体架构：</p>
<p><img src="/2024/03/05/liao2020/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-2-Quadrics-Model"><a href="#3-2-Quadrics-Model" class="headerlink" title="3.2 Quadrics Model"></a>3.2 Quadrics Model</h2><p>二次曲面包含多种类型，如椭球体、圆柱体等，考虑到室内人造物体多为闭合几何形状，作者使用椭球体作为物体的模型表示。</p>
<p>二次曲面可使用<strong>对偶形式</strong> $\mathbf{Q}^\ast$ 来表示，对于任意切平面 $\pi$ 满足以下关系：</p>
<p><img src="/2024/03/05/liao2020/f1.png" alt="f1" title="formula 1"></p>
<p>对偶二次曲面在相机观测 $\mathbf{P}$ 下得到曲线 $\mathbf{C}^\ast$ ，该过程如Fig. 2所示，并表示为下式：</p>
<p><img src="/2024/03/05/liao2020/f2.png" alt="f2" title="formula 2"></p>
<p>其中，$\mathbf{P} = \mathbf{K}[\mathbf{R}|\mathbf{t}]$ 。</p>
<p><img src="/2024/03/05/liao2020/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-3-RGB-D-Camera-Ellipsoid-Observation-Model"><a href="#3-3-RGB-D-Camera-Ellipsoid-Observation-Model" class="headerlink" title="3.3 RGB-D Camera-Ellipsoid Observation Model"></a>3.3 RGB-D Camera-Ellipsoid Observation Model</h2><p>使用式2对椭球体进行投影映射为图像平面上的<strong>椭圆</strong>，结合<strong>物体检测bbox</strong> 构建约束，则单次观测只能包含<strong>4自由度的约束</strong>（bbox 4条边），因此，为了解算具有9自由度的椭球体，需要<strong>多次观测</strong>，且观测之间要有足<strong>够的视差</strong>。考虑到移动机器人的运动模式，其在垂直方向和pitch 方向的视角变化受限，容易造成<strong>unobservable 问题</strong>。</p>
<p>为解决该问题，作者使用RGB-D 相机提供的<strong>深度信息</strong>：从深度信息中提取物体的支撑平面，由此引入<strong>物体-平面的支撑关系</strong>来帮助<strong>分割物体点云</strong>，并估计物体的朝向信息。为解决多视角变化的需求，作者推导了RGB-D <strong>完整观测模型</strong>，并使用单帧RGB-D 数据来估计完整的椭球体，以实现快速的初始化。针对物体遮挡导致的深度数据严重缺失问题，作者使用物体检测bbox 构建了<strong>部分约束模型</strong>，以实现对观测信息的最大化利用。</p>
<p>尽管RGB-D 观测可提供深度信息，但仍存在以下问题：1. 图像边缘造成的物体不完整；2. 物体间的遮挡；3. 光照、材料反射导致的深度信息不准确、不完整。这些问题会造成物体<strong>点云的缺失</strong>，对观测模型的构建造成影响。</p>
<p><img src="/2024/03/05/liao2020/fig3.png" alt="fig3" title="figure 3"></p>
<p>根据点云数据的完整度，作者提出了两种观测模型：<strong>部分约束模型</strong>和<strong>完整约束模型</strong>，如Fig. 4所示。</p>
<p>当点云质量较高时，使用完整约束模型从单帧RGB-D 图像中恢复完整的椭球体，该方法充分利用<strong>室内物体</strong>和其<strong>支撑平面</strong>之间的支撑关系来估计物体的位姿与形状；最终，该算法会为该椭球体建立一个约束立方体，立方体的6个面与该椭球体相切，从而获取至多9自由度的完整约束模型。</p>
<p>当出现严重遮挡或点云质量不足以构建一个完整的椭球体时，算法会使用部分约束模型，该方法充分利用bbox 产生的约束平面，并生成至多<strong>4个正切平面</strong>的约束。</p>
<p>此外，作者还提出了一种新颖的评估方式来<strong>判断立方体的质量</strong>，从而实现两种观测模型之间的<strong>灵活切换</strong>。</p>
<p><img src="/2024/03/05/liao2020/fig4.png" alt="fig4" title="figure 4"></p>
<p>作者将提出的<strong>观测模型</strong>和<strong>评估方式</strong>应用至物体级语义SLAM 架构：</p>
<ul>
<li>使用完整约束模型构建的椭球体来<strong>初始化新物体</strong>；</li>
<li>使用约束模型作为图优化中的<strong>损失函数</strong>，并解决<strong>数据关联</strong>问题。</li>
</ul>
<h2 id="3-4-Complete-Constraint-Model"><a href="#3-4-Complete-Constraint-Model" class="headerlink" title="3.4 Complete Constraint Model"></a>3.4 Complete Constraint Model</h2><p>完整约束模型充分利用人造物体的特性，以及它们与结构平面之间的关系；基于这些特性从单帧RGB-D 观测中恢复椭球体的完整参数，该过程如Fig. 5所示：</p>
<p><img src="/2024/03/05/liao2020/fig5.png" alt="fig5" title="figure 5"></p>
<p>完成对椭球体的估计后，使用所提出的<strong>评估方式</strong>对椭球体进行评估：如果超过预设阈值，则保留该椭球体；否则切换至<strong>部分观测模</strong>型来实现对观测信息的最大化利用，并保留<strong>约束置信度</strong>。</p>
<h3 id="3-4-1-Supporting-Planes-Segmentation"><a href="#3-4-1-Supporting-Planes-Segmentation" class="headerlink" title="3.4.1 Supporting Planes Segmentation"></a>3.4.1 Supporting Planes Segmentation</h3><p>典型的人造室内场景有许多特性：存在大量平面，平面之间平行或正交，物体一般有对应的支撑平面。作者利用这些特性来<strong>辅助分割点云</strong>，并为椭球体<strong>旋转参数</strong>提供<strong>先验约束</strong>。</p>
<p><img src="/2024/03/05/liao2020/fig6.png" alt="fig6" title="figure 6"></p>
<p>首先，基于作者之前的研究，使用<strong>RANSAC 方法</strong>从深度图像中<strong>提取平面</strong>，并记为平面集合 $S_0 = \{\pi_i\}$ ；这里作者使用了假设：物体的支撑平面与重力方向垂直，如地面、桌面等。然后，作者根据粗略估计的<strong>重力方向</strong> $\mathbf{n}_g$ 选择潜在的支撑平面：认为机器人的支撑平面是地面，且RGB-D 相机在机器人上是固定的，则可从相机与机器人之间的转换矩阵中得到重力方向的粗略估计。根据重力方向与平面法向量之间的夹角，筛选出<strong>潜在支撑平面集合</strong> $S = \{\pi_i\}$ 。</p>
<h3 id="3-4-2-Object-Point-Cloud-Segmentation"><a href="#3-4-2-Object-Point-Cloud-Segmentation" class="headerlink" title="3.4.2 Object Point Cloud Segmentation"></a>3.4.2 Object Point Cloud Segmentation</h3><p>基于物体检测bbox，将深度图进行<strong>逆投影</strong>可以获取物体粗略的点云集合，然后，作者使用一个轻量级的分割方法获取更为精确的物体点云：使用<strong>Euclidean filtering 方法</strong>滤除属于支撑平面的点云，保留属于物体的点云 $\mathcal{C}_i$ 。</p>
<h3 id="3-4-3-Object-Orientation-Estimation"><a href="#3-4-3-Object-Orientation-Estimation" class="headerlink" title="3.4.3 Object Orientation Estimation"></a>3.4.3 Object Orientation Estimation</h3><p>作者提出一种基于物体和其支撑平面之间关系的<strong>物体朝向估计方法</strong>，该方法基于两个假设：1. 物体的主轴之一与支撑平面的法向量平行；2. 对于有着主朝向方向的人造物体，物体表面的法向量直方图可以反应这种方向趋势。</p>
<h4 id="A-Definition-of-object-coordinate-system"><a href="#A-Definition-of-object-coordinate-system" class="headerlink" title="A Definition of object coordinate system"></a>A Definition of object coordinate system</h4><p>系统坐标系如Fig. 4（a）所示，Z 轴沿着重力方向向上，X 轴和 Y 轴表示物体两个正交的主方向。作者提到，由于椭球体是对称的，若沿着Z 轴每隔90度旋转四次，会得到四组旋转矩阵来表示椭球体的相同旋转。</p>
<h4 id="B-Z-axis-estimation-based-on-the-supporting-plane"><a href="#B-Z-axis-estimation-based-on-the-supporting-plane" class="headerlink" title="B Z-axis estimation based on the supporting plane"></a>B Z-axis estimation based on the supporting plane</h4><p>室内场景下，物体的支撑平面基本是地面和桌面，这种情况下，物体和对应的支撑平面形成一种垂直关系，因此物体的Z 轴与支撑平面的法向量平行。在点云分割过程中，获取了物体支撑平面 $\pi_s$ ，据此可得到物体的Z 轴 $\mathbf{n}_s$ 。</p>
<h4 id="C-Y-axis-estimation-based-on-the-histogram-of-normal-vectors"><a href="#C-Y-axis-estimation-based-on-the-histogram-of-normal-vectors" class="headerlink" title="C Y-axis estimation based on the histogram of normal vectors"></a>C Y-axis estimation based on the histogram of normal vectors</h4><p>如Fig. 7所示，大部分人造物体拥有互相垂直的主方向，如显示器、沙发等，对于这些物体，可以使用旋转作为<strong>角度约束信息</strong>；对于对称性物体，如水杯、瓶子等，方向信息对于这些物体没有意义。因此，作者根据物体<strong>语义标签</strong>来判断是否使用主方向来约束该物体。</p>
<p><img src="/2024/03/05/liao2020/fig7.png" alt="fig7" title="figure 7"></p>
<p>对于有主方向的物体，作者取物体点云<strong>法向量集合中yaw 角直方图</strong>的峰值来作为该物体的Y 轴。首先，作者在支撑平面上搭建一个极坐标系统，取平面上一个单位向量 $\mathbf{n}_r$ 作为坐标轴。具体算法如下所示：</p>
<ol>
<li><p>对于属于物体点云中的所有点 $\mathbf{P} \in \mathcal{C}_i$ 执行2-4步，通过将法向量投影至支撑平面来计算yaw 角；</p>
</li>
<li><p>根据局部深度信息来计算该点的法向量 $\mathbf{n}_p$ 。在提取平面的过程中，已经计算了整个深度图的法向量，所以可以直接获取该向量；</p>
</li>
<li><p>将法向量 $\mathbf{n}_p$ 投影至支撑平面 $\pi_s$ 来获取yaw 角。假设支撑平面的法向量为 $\mathbf{n}_s$ ，计算法向量 $\mathbf{n}_p$ 在平面上的投影向量 $\mathbf{n}_{ps}$ ：</p>
<p><img src="/2024/03/05/liao2020/f6.png" alt="f6" title="formula 6"></p>
</li>
<li><p>计算yaw 角 $\theta = acos(\mathbf{n}_{ps}, \mathbf{n}_r)$ ，将 $\theta$ 归一化至 $[0, \pi)$ 范围，并将 $\theta$ 添加至集合 Y中；</p>
</li>
<li><p>在获取所有点的法向量后，获取集合 Y 中的最大值，作为最终的 $\theta$ 值；</p>
</li>
<li><p>根据 $\theta$ 值计算相应的法向量形式  $\mathbf{n}_x$ —— <strong>沿着平面法向量 $\mathbf{n}_s$ 将极坐标轴 $\mathbf{n}_r$ 旋转 $\theta$ 度</strong>：首先根据Rodriguez 公式计算旋转矩阵 $\mathbf{R}_{\theta}$ ，进而得到法向量  $\mathbf{n}_x$ 。</p>
</li>
</ol>
<p><img src="/2024/03/05/liao2020/f8.png" alt="f8" title="formula 8"></p>
<p><img src="/2024/03/05/liao2020/f9.png" alt="f9" title="formula 9"></p>
<p>此外，基于该角度在集合直方图中的比例计算该<strong>置信度</strong> $P_{rot}$ ：</p>
<p><img src="/2024/03/05/liao2020/f10.png" alt="f10" title="formula 10"></p>
<h3 id="3-4-4-Ellipsoid-Generation"><a href="#3-4-4-Ellipsoid-Generation" class="headerlink" title="3.4.4 Ellipsoid Generation"></a>3.4.4 Ellipsoid Generation</h3><p>根据物体的旋转矩阵 $\mathbf{R}_c$ 和物体点云，可以构建出点云的最小bounding 立方体。确定点云在三个主轴的半轴长 $\mathbf{s} = [a, b, c]^T$ ，取各轴的中心作为立方体的中心点 $\mathbf{t}_c$ ，进而得到该立方体的位姿：</p>
<script type="math/tex; mode=display">
\mathbf{T}_c = \begin{bmatrix}
\mathbf{R}_c     & \mathbf{t}_c \\
\mathbf{0}     & 1
\end{bmatrix}</script><p>对于任意立方体，均有其对应的<strong>内切椭球体</strong>。根据立方体主轴长度生成对角矩阵 $\mathbf{D} = diag(\mathbf{s})$ ，则该立方体的内切椭球体表示为：</p>
<p><img src="/2024/03/05/liao2020/f11.png" alt="f11" title="formula 11"></p>
<p>由此，实现了从单帧RGB-D 提取了物体立方体模型。</p>
<h3 id="3-4-5-Confidence-Evaluation-of-the-Complete-Constraint-Model"><a href="#3-4-5-Confidence-Evaluation-of-the-Complete-Constraint-Model" class="headerlink" title="3.4.5 Confidence Evaluation of the Complete Constraint Model"></a>3.4.5 Confidence Evaluation of the Complete Constraint Model</h3><p>由于物体检测得到的bbox 是可靠的物体约束，所以，作者基于bbox 和完整约束模型构建的立方体来计算该模型的置信度。</p>
<p>首先，计算椭球体外形匹配概率 $P_{shape}$ ，根据式2将椭球体投影至图像平面中的椭圆形，在此基础上生成受限矩形；然后与bbox 计算IoU：</p>
<p><img src="/2024/03/05/liao2020/f12.png" alt="f12" title="formula 12"></p>
<p>其中，$\beta(\mathbf{C}^\ast)$ 表示获取椭圆的外包矩形。</p>
<p>在此基础上，基于<strong>贝叶斯理论</strong>，结合物体检测概率 $P_{det}$ 、主方向估计概率 $P_{rot}$ ，得到该椭球体<strong>约束模型的置信度</strong>：</p>
<p><img src="/2024/03/05/liao2020/f13.png" alt="f13" title="formula 13"></p>
<p>其中，$\alpha$ 为置信度阈值，判断当前模型是否可靠，若不可靠则选择使用<strong>部分约束模型</strong>。</p>
<h3 id="3-4-6-Generate-Constraint-Planes"><a href="#3-4-6-Generate-Constraint-Planes" class="headerlink" title="3.4.6 Generate Constraint Planes"></a>3.4.6 Generate Constraint Planes</h3><h4 id="A-Acquisition-of-tangent-plane"><a href="#A-Acquisition-of-tangent-plane" class="headerlink" title="A Acquisition of tangent plane"></a>A Acquisition of tangent plane</h4><p>为了灵活适应遮挡情况，作者将椭球体约束分割为<strong>多重离散约束</strong>。立方体的6个面均满足式1的正切约束，且满足<strong>角度约束</strong>：面的法向量与椭球体的某个主轴共线。因此，一个完整的约束平面包含两部分约束：<strong>正切约束</strong>和<strong>角度约束</strong>。整体的约束方程如下所示：</p>
<p><img src="/2024/03/05/liao2020/f15.png" alt="f15" title="formula 15"></p>
<p>其中，$Sem(\pi, l)$ 用来判断物体是否有主方向，或切平面是否垂直于Z 轴。基于式15构建的完整约束包含6个约束平面和对应的角度约束。</p>
<h4 id="B-Occlusion-of-the-image-edges"><a href="#B-Occlusion-of-the-image-edges" class="headerlink" title="B Occlusion of the image edges"></a>B Occlusion of the image edges</h4><p>因为使用了正切平面的形式来表示约束，所以可以判断每个正切平面的有效性，并移除由于遮挡造成的无效约束。每个正切平面包含受限矩形的4个顶点，作者通过判断位于<strong>图像边界内的投影顶点数量</strong>来决定该正切平面是否有效。</p>
<h2 id="3-5-Partial-Constraint-Model"><a href="#3-5-Partial-Constraint-Model" class="headerlink" title="3.5 Partial Constraint Model"></a>3.5 Partial Constraint Model</h2><p>对于完整约束模型和部分约束模型，作者均采用<strong>约束平面</strong>来表示，以实现在图优化期间更好的收敛表现。</p>
<h3 id="3-5-1-Acquisition-of-constraint-planes"><a href="#3-5-1-Acquisition-of-constraint-planes" class="headerlink" title="3.5.1 Acquisition of constraint planes"></a>3.5.1 Acquisition of constraint planes</h3><p>物体检测bbox 的每条边可通过逆投影形成一个平面：</p>
<p><img src="/2024/03/05/liao2020/f18.png" alt="f18" title="formula 18"></p>
<p>其中，$\mathbf{P}$ 表示相机位姿；l 表示bbox 中的一个边。</p>
<h3 id="3-5-2-Occlusion-of-the-image-edges"><a href="#3-5-2-Occlusion-of-the-image-edges" class="headerlink" title="3.5.2 Occlusion of the image edges"></a>3.5.2 Occlusion of the image edges</h3><p>与完整约束模型类似，作者根据bbox 边与图像边界的距离来判断该边是否有效。有效边利用式18获取正切平面，进一步构建正切约束：</p>
<p><img src="/2024/03/05/liao2020/f19.png" alt="f19" title="formula 19"></p>
<p><img src="/2024/03/05/liao2020/f20.png" alt="f20" title="formula 20"></p>
<p>若所有的bbox 边均有效，则单帧图片可构建4自由度的约束，式19的约束方程与式15相比，<strong>缺少了角度约束</strong>。但由此完整约束模型和部分约束模型形成了统一的形式。</p>
<h2 id="3-6-Object-Level-SLAM"><a href="#3-6-Object-Level-SLAM" class="headerlink" title="3.6 Object-Level SLAM"></a>3.6 Object-Level SLAM</h2><p>作者将未知的语义数据关联和物体语义标签作为<strong>离散随机变量</strong>添加进系统中，因此，系统变成了包含连续变量（相机位姿）和离散变量（数据关联、语义标签）的<strong>混合系统</strong>，使得位姿图优化传统的非线性优化方法<strong>失效</strong>；为此，作者提出了一种<strong>非参数化位姿图</strong>对该问题进行建模，并得到最优解。</p>
<h3 id="3-6-1-Semantic-Data-Association"><a href="#3-6-1-Semantic-Data-Association" class="headerlink" title="3.6.1 Semantic Data Association"></a>3.6.1 Semantic Data Association</h3><p>作者提到，不同于传统SLAM 中拥有众多的特征点，物体级SLAM 中只包含少量的物体，若发生DA 错误，则会对解算结果产生巨大影响；此外，同一类别的物体还会包含多个不同的个体，且常出现不同物体相隔较近甚至出现物体相接的情况。所以，在前端直接进行物体DA 比较困难。基于前人研究基础，作者进一步将<strong>二次曲面模型</strong>引进非参数化位姿图中来解决DA 问题。</p>
<h2 id="3-7-Nonparametric-Pose-Graph"><a href="#3-7-Nonparametric-Pose-Graph" class="headerlink" title="3.7 Nonparametric Pose Graph"></a>3.7 Nonparametric Pose Graph</h2><h3 id="3-7-1-The-Definition-of-a-Nonparametric-Pose-Graph"><a href="#3-7-1-The-Definition-of-a-Nonparametric-Pose-Graph" class="headerlink" title="3.7.1 The Definition of a Nonparametric Pose Graph"></a>3.7.1 The Definition of a Nonparametric Pose Graph</h3><p>在传统位姿图的基础上进行扩展，得到了如下式所示的联合优化问题：</p>
<p><img src="/2024/03/05/liao2020/f28.png" alt="f28" title="formula 28"></p>
<p>其中，$o,z$ 分别是里程计观测和地标观测；$\mathbf{X}, \mathbf{L}$ 分别是相机位姿与地标参数，$\mathbf{L}_i = \{\mathbf{Q}_i, l_i\}$ 包含物体的椭球体参数和语义标签；$u, y$ 分别是语义标签的观测和数据关联DA；物体个数M 是未知的，需要进行优化求解。</p>
<p><img src="/2024/03/05/liao2020/fig8.png" alt="fig8" title="figure 8"></p>
<h3 id="3-7-2-The-Solution-of-a-Nonparametric-Pose-Graph"><a href="#3-7-2-The-Solution-of-a-Nonparametric-Pose-Graph" class="headerlink" title="3.7.2 The Solution of a Nonparametric Pose Graph"></a>3.7.2 The Solution of a Nonparametric Pose Graph</h3><p>前人使用 <strong>Dirichlet Process（DP）方法</strong>来计算非参数化位姿图，如Fig. 9所示，主要分为两步：</p>
<ol>
<li>固定数据关联 y，利用最大似然法解算 $\mathbf{X}, \mathbf{L}$ ；</li>
<li>固定 $\mathbf{X}, \mathbf{L}$ ，利用最大似然法解算数据关联 y。</li>
</ol>
<p><img src="/2024/03/05/liao2020/fig9.png" alt="fig9" title="figure 9"></p>
<p>具体来讲，可分为以下步骤：</p>
<ol>
<li><p><strong>初始化：</strong>根据里程计数据初始化位姿 $\mathbf{X}$ ，将所有观测物体视为新物体来初始化物体地标 $\mathbf{L}$ ，初始化物体语义标签 $\beta_0$ ，然后重复执行2-4步直到收敛；</p>
</li>
<li><p><strong>优化DA：</strong>固定 $\mathbf{X}, \mathbf{L},\beta$ ，使用下式<strong>更新数据关联</strong> $y_t^k$ ：</p>
<p><img src="/2024/03/05/liao2020/f29.png" alt="f29" title="formula 29"></p>
<p>上式为<strong>DP 先验</strong>与<strong>观测似然</strong>的乘积；$p(u_t^k;l_i)$ 通过语义标签的数学模型来给定；$p(z_t^k;\mathbf{X}_t,\mathbf{L}_i)$ 通过二次曲面观测模型给定。然后使用最大似然确定数据关联 $y_t^k$ ：</p>
<p><img src="/2024/03/05/liao2020/f30.png" alt="f30" title="formula 30"></p>
</li>
<li><p><strong>更新物体标签：</strong>固定数据关联 $y_t^k$ ，更新物体标签Dirichlet 分布的<strong>后验参数</strong>，直觉上讲，有最多观测次数的类别就是该物体的语义类别；</p>
</li>
<li><p><strong>优化位姿与物体参数：</strong>固定DA $y_t^k$ ，解算 $\mathbf{X}, \mathbf{L}$ ，此处相当于<strong>传统的位姿图优化</strong>；</p>
</li>
<li><p><strong>滤除false positive：</strong>对于观测次数少于设定阈值的物体，将其视为FP。</p>
</li>
</ol>
<h1 id="4-Experiment"><a href="#4-Experiment" class="headerlink" title="4 Experiment"></a>4 Experiment</h1><p><img src="/2024/03/05/liao2020/t6.png" alt="t6" title="table 6"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Pose Optimization</tag>
        <tag>Object SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 View-Invariant Loop Closure with Oriented Semantic Landmarks</title>
    <url>/2024/02/26/li2020/</url>
    <content><![CDATA[<p>Li, Jimmy, Karim Koreitem, David Meger, and Gregory Dudek. “View-Invariant Loop Closure with Oriented Semantic Landmarks.” In <em>2020 IEEE International Conference on Robotics and Automation (ICRA)</em>, 7943–49. Paris, France: IEEE, 2020. <a href="https://doi.org/10.1109/ICRA40945.2020.9196886">https://doi.org/10.1109/ICRA40945.2020.9196886</a>.</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>作者使用物体标识与物体间几何关系来实现具有视角不变性的回环检测与偏移矫正。</p>
<blockquote>
<p>use <strong>object identity and inter-object geometry</strong> for view-invariant loop detection and drift correction</p>
</blockquote>
<p>此外，作者还提出了一个对物体方向进行估计的方法，来克服由于物体对称性造成的模糊度；最终，作者构建了可绘制带有几何细节语义地图（包含物体方向、距离与尺寸信息）的SLAM系统。</p>
<span id="more"></span>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者在前作(Li 等, 2019)（使用物体地标进行视角不变性的重定位）的基础上，使用物体作为高等级的语义地标进行回环检测。</p>
<p><img src="/2024/02/26/li2020/fig1.png" alt="fig1" title="figure 1"></p>
<p>本文利用移动相机捕捉的多视角图像来估计物体的方向，进而解决物体对称性模糊度，这使得本系统可以匹配一个更大的环境来确定位姿。</p>
<h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h1><p>作者使用bounding cuboids这种可以很好容纳目标物体的表示方式来代表物体；且没有选择RGB-D 相机，而是使用更为常见的RGB 相机，有利于本系统的广泛应用。</p>
<p>作者利用SLAM 进行多视角物体方向推断；相较于已有的实例级别方法，作者是在种类级别category level进行方向估计的，允许系统在陌生环境中正常运行。</p>
<h1 id="3-SLAM-System"><a href="#3-SLAM-System" class="headerlink" title="3 SLAM System"></a>3 SLAM System</h1><h2 id="3-1-Overview"><a href="#3-1-Overview" class="headerlink" title="3.1 Overview"></a>3.1 Overview</h2><p>作者认为，基于基于外观的局部特征对于精确跟踪相机的局部位姿是有意义的，但对于大视角变化等明显改变场景外观的情况下不具有鲁棒性；而环境中的物体信息（如常见的家居用品）对于视角变化等情况具有较强的鲁棒性，但在缺少其他信息（如基于外观的局部特征）时难以得到利用，因为在图片平面中计算物体的精确位置是很困难的。因此，作者采取如下的混合策略：</p>
<ol>
<li>使用基于外观的局部特征来跟踪相机局部位姿；</li>
<li>使用已知的相机位姿来简化物体在3D 空间中位姿的推断；</li>
<li>当相机在跨越长基线情况下需要进行回环检测或者重定位时，使用物体地标进行视角不变性的匹配。</li>
</ol>
<p>作者在前作(Li 等, 2019)进行了重定位，本文针对回环检测进行研究。回环识别是通过对由于相机位姿漂移造成的重复物体进行匹配（包括物体标识与几何布局）而实现的。</p>
<p><img src="/2024/02/26/li2020/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-2-Semantic-Mapping"><a href="#3-2-Semantic-Mapping" class="headerlink" title="3.2 Semantic Mapping"></a>3.2 Semantic Mapping</h2><p>制图系统输入为RGB 图像流，输出为估计的相机轨迹，以及以9自由度bounding cuboids表示的物体地标，文章(Lin 等, 2021)借鉴了本文的9自由度bounding cuboids表示方法，包含位置、旋转以及尺寸信息。</p>
<p>本系统是以ORB-SLAM 为基础进行构建的，bounding box探测是由Faster-RCNN 完成的，且作者引入了一种<strong>实例级物体方向回归算法</strong>来提升制图系统的性能。</p>
<p>如Fig. 2所示，作者使用 expectation maximization (EM) 算法来更新物体地标：</p>
<ol>
<li>物体地标投影到每个关键帧中，并与object detections 进行匹配；</li>
<li>将匹配到的detections 与关键帧相机位姿进行结合，进行三角测量并更新物体地标位姿；</li>
<li>没有与已知物体地标匹配成功的detections 初始化为新的地标。</li>
</ol>
<h2 id="3-3-Loop-Detection"><a href="#3-3-Loop-Detection" class="headerlink" title="3.3 Loop Detection"></a>3.3 Loop Detection</h2><p>在长轨迹上进行语义地图绘制时，由于相机位姿漂移会造成重复物体地标被添加进语义地图中。本文系统是建立在ORB-SLAM基础上的，因此保留了其基于外观特征的回环检测，为了应对大视角变化场景下的回环检测，增加了额外的物体级回环检测机制：<strong>在制图环节的每一次EM迭代后，将最近添加的物体地标视为一组近期地标，并与早期的地标进行匹配。</strong>假设地标 <em>l</em> 与 <em>m</em> 分别属于关键帧组合 $K_l, K_m$ （因为同一地标可能会出现在多个关键帧中），定义<em>keyframe separation</em> ：</p>
<p><img src="/2024/02/26/li2020/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$u - w$ 表示两个关键帧索引值（表示两个关键帧被加入地图中的次序）相减，定义一个阈值，若式 (1) 小于某个阈值则判定地标 <em>l</em> 与 <em>m</em> 相近。定义一个集合 <em>L</em>，包含最新的地标 <em>l</em> 以及根据上式得到的与其距离相近的地标集合，目标是找到轨迹中之前遇到过的一组地标与 <em>L</em> 的子集拥有相似的空间分布，这样就可能找到了回环。由于物体地标的稀疏性，可以穷尽迭代所有可能的匹配。</p>
<h2 id="3-4-Geometric-Loop-Verification"><a href="#3-4-Geometric-Loop-Verification" class="headerlink" title="3.4 Geometric Loop Verification"></a>3.4 Geometric Loop Verification</h2><p>假设两组物体地标 $(l_1, l_2, l_3), (m_1, m_2, m_3)$ 实现了上节的初步匹配，进行如下的匹配测试：</p>
<h3 id="3）-Object-layout"><a href="#3）-Object-layout" class="headerlink" title="3） Object layout"></a>3） Object layout</h3><p>若要比较匹配物体的几何布局，需要使用一个共同的参考框架来描述物体，本文使用点来表示各个物体的坐标，基于两组物体地标 $(l_1, l_2, l_3), (m_1, m_2, m_3)$ 分别构建局部坐标系统 A 与 B ，然后计算包括旋转 R、平移 t 以及尺寸 s 的相似度转换，将A中的任意点 $p_l$  映射到 B 中的相应位置 $p_m$ ：</p>
<p><img src="/2024/02/26/li2020/f2.png" alt="f2" title="formula 2"></p>
<p>将 $(l_1, l_2, l_3)$ 从 A 投影到 B 中，然后进行如下测试：</p>
<ul>
<li>Scale consistency：尺度变化不能过大</li>
<li>Translational consistency：位置尺寸不能过大，由于单目RGB相机无法提供绝对距离，作者选取一个尺度归一化的距离进行判断；</li>
<li>Rotational consistency：物体方向不能差异过大。</li>
</ul>
<h2 id="3-5-Loop-Correction"><a href="#3-5-Loop-Correction" class="headerlink" title="3.5 Loop Correction"></a>3.5 Loop Correction</h2><p>对于给定的回环检测，利用相似转换来将物体匹配与观测到这些物体地标的关键帧相机位姿进行对齐，并使用ORB-SLAM 的 essential graph optimizer 将矫正参数通过非线性最小二乘优化散发到所有关键帧，在此基础上更新语义地图。</p>
<p>对于每一个物体地标，选取一个观测到该地标的关键帧，计算该关键帧更新前后的相对位姿变换，并将该变换参数更新到地标位姿上去，使用EM算法细调所有的地标位姿。</p>
<h1 id="4-Orientation-Regression"><a href="#4-Orientation-Regression" class="headerlink" title="4 Orientation Regression"></a>4 Orientation Regression</h1><h2 id="4-1-Overview"><a href="#4-1-Overview" class="headerlink" title="4.1 Overview"></a>4.1 Overview</h2><p>Fig. 3展现了物体的对称性造成的物体方向回归算法不收敛的难题，作者在本文主要针对的是镜面对称问题，并简单讨论了圆柱对称问题。</p>
<p><img src="/2024/02/26/li2020/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="4-2-Multi-view-Orientation-Inference"><a href="#4-2-Multi-view-Orientation-Inference" class="headerlink" title="4.2 Multi-view Orientation Inference"></a>4.2 Multi-view Orientation Inference</h2><p>作者将视角球形分为几部分，其中任意部分的不同视角展现不同的外观，如Fig. 4所示，作者将视角分为4个象限，并对每一部分训练一个方向回归函数。</p>
<p><img src="/2024/02/26/li2020/fig4.png" alt="fig4" title="figure 4"></p>
<h1 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5 Experiments"></a>5 Experiments</h1><h2 id="5-1-Loop-Closure"><a href="#5-1-Loop-Closure" class="headerlink" title="5.1 Loop Closure"></a>5.1 Loop Closure</h2><p>现有的SLAM数据集在回环检测方面基本上使用视角变化不大的同一场景照片，本方法的优势无法展现，故作者采用自采的数据集进行实验，回环检测图片使用的是具有较大视角变化的图片，结果如Fig. 6所示，实验使用5种立方物体（屏幕、键盘、手机、遥控器以及微波炉）、4种圆柱物体（水杯、瓶子、碗以及花盆绿植）作为地标物体并进行回环检测。</p>
<p><img src="/2024/02/26/li2020/fig6.png" alt="fig6" title="figure 6"></p>
<p>实验中漂移减少百分比定义如下：</p>
<p><img src="/2024/02/26/li2020/f4.png" alt="f4" title="formula 4"></p>
<p>S是共同的起始位置坐标，E是本文方法估计的结束位置坐标，e是ORB-SLAM 估计的位置坐标。Table 1 展示了所有序列的偏移减少量。</p>
<p><img src="/2024/02/26/li2020/t1.png" alt="t1" title="table 1"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Topology</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Topology</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Topology Aware Object-Level Semantic Mapping Towards More Robust Loop Closure</title>
    <url>/2024/03/06/lin2021/</url>
    <content><![CDATA[<p>Lin, Shiqi, Jikai Wang, Meng Xu, Hao Zhao, and Zonghai Chen. “Topology Aware Object-Level Semantic Mapping Towards More Robust Loop Closure.” <em>IEEE Robotics and Automation Letters</em> 6, no. 4 (October 2021): 7041–48. <a href="https://doi.org/10.1109/LRA.2021.3097242">https://doi.org/10.1109/LRA.2021.3097242</a>.</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>SLAM 回环检测可有效消除累计误差，但是在大视角变化与环境外观变化时难以实现有效的检测，本文提出了基于物体建模object modeling与语义图匹配semantic graph matching的回环检测技术。作者使用体素与立方体cuboids对环境中的物体进行物体级别特征建模，将环境进一步表示为带有拓扑信息的语义图，在此基础上提出了一种<strong>基于edit distance的高效图匹配方法</strong>，最终通过语义地图的对齐来实现回环检测。</p>
<span id="more"></span>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>人类通过环境中物体的外观与轮廓来识别场景，并在短时间内判断是否来过该该处，通过借鉴人类的回环检测方式作者提出了一种基于<strong>对象级拓扑感知语义匹配topology aware object-level semantic mapping</strong>的回环检测方法：</p>
<blockquote>
<p><em>将环境表示为包含物体的语义图，通过图匹配来实现回环探测</em>。</p>
</blockquote>
<p><img src="/2024/03/06/lin2021/fig1.png" alt="fig1" title="figure 1"></p>
<p><img src="/2024/03/06/lin2021/fig2.png" alt="fig2" title="figure 2"></p>
<p>本文做出的贡献总结如下：</p>
<ol>
<li>提出一个构建<strong>多粒度语义地图</strong>的框架，包含地图点、体素与立方体；</li>
<li>提取物体的<strong>多维特征（包括语义、尺寸、颜色分布以及拓朴结构）</strong>作为描述子来实现物体间的精确关联；</li>
<li>提出了一个基于<strong>语义图匹配</strong>的回环检测方法，对于视角与外观变化具有较强的鲁棒性；</li>
<li>提出一种<strong>基于物体对齐的回环矫正</strong>方法，可精确计算漂移误差。</li>
</ol>
<h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h1><h2 id="2-1-Object-SLAM"><a href="#2-1-Object-SLAM" class="headerlink" title="2.1 Object SLAM"></a>2.1 Object SLAM</h2><p>相对于低级的几何信息，物体特征object features更稳定，且有着丰富的语义与先验信息。根据物体建模策略的不同，object SLAM 可分为specific type 与 general type：</p>
<ul>
<li><strong>specific type</strong>：需要提前建立精准的3D模型数据库，然后进行实时检索匹配，从而实现物体的精确估计，但由于先验数据库的要求导致该方法的通用性有限；</li>
<li><strong>general type</strong>：可在跟踪相机位姿的过程中实现物体在线建模，有着更优的应用能力。一些方法使用通用的三维几何结构，如立方体、椭球体来建立粗粒度的物体模型。</li>
</ul>
<h2 id="2-2-Loop-Closure"><a href="#2-2-Loop-Closure" class="headerlink" title="2.2 Loop Closure"></a>2.2 Loop Closure</h2><p>基于观测信息建立场景的鲁棒描述子是回环检测的核心问题，常见的描述子有以下几种：</p>
<ul>
<li>提取图像的<strong>局部特征</strong>作为图像的抽象描述子，如SIFT、ORB 等；</li>
<li>直接提取整幅图像的<strong>全局特征</strong>实现场景描述，如GIST 等；</li>
<li>结合上述两种方法的优势，从图片的多个局部区域提取全局特征，组建场景的鲁棒表示。</li>
</ul>
<p>以上基于外观的描述子构建方法受限于视角与场景外观的变化，而语义信息却具有更强的鲁棒性。据此，有作者提出<strong>利用环境中的物体来实现回环检测的视角不变性</strong>，然而，场景遮挡、布局改变以及多个重复物体的情况也很常见，这些因素对于基于穷尽搜索的物体匹配方法提出了挑战。</p>
<blockquote>
<p>因此，本文作者提出使用物体的<strong>多维信息（包括语义、颜色与拓扑关系）作为描述子</strong>来实现物体的精准匹配，进而实现语义图匹配的回环检测。</p>
</blockquote>
<h1 id="3-Object-SLAM"><a href="#3-Object-SLAM" class="headerlink" title="3 Object SLAM"></a>3 Object SLAM</h1><p>本文提出的架构是基于<strong>ORB-SLAM2</strong> 而设计的，使用<strong>RGB-D 数据</strong>来实现初步的位姿估计与<em>稀疏地图</em>构建；在此基础上，通过融合RGB-D 数据与它的实例分割结果来构建<em>稠密点云</em>，进而产生<em>体素模型</em>。基于体素模型使用<strong>离散采样与粒子群优化算法</strong>产生物体的<em>压缩立方体模型</em>，过程如Fig. 3所示。</p>
<p><img src="/2024/03/06/lin2021/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="3-1-Voxel-Model-Construction"><a href="#3-1-Voxel-Model-Construction" class="headerlink" title="3.1 Voxel Model Construction"></a>3.1 Voxel Model Construction</h2><p>从单张图片的RGB-D 与实例分割获取物体的<strong>稠密点云</strong>，<strong>体素</strong>被用来融合、量化多帧图片的稠密点云，根据连续观测信息更新<strong>体素被占据的概率。</strong>给定传感器测量数据$z_{1:t}$ ，体素被占据的概率可由下式计算获取：</p>
<p><img src="/2024/03/06/lin2021/f1.png" alt="f1" title="formula 1"></p>
<p>$P(n)$ 表示体素被占据的先验概率，本文设置为0.5；在得到传感器测量值$z_t$ 后可通过下式获取后验概率：</p>
<p><img src="/2024/03/06/lin2021/f2.png" alt="f2" title="formula 2"></p>
<p>在实验中，$l_{occ}, l_{free}$ 分别被设定为0.7，0.4。</p>
<p>由于传感器测量误差与实例分割误差，在物体初步建模中存在很多离散的外点，作者首先使用<strong>DBSCAN 算法</strong>进行density clustering 来去除孤立的小团点簇，然后将剩余的3D 体素投影到地面上，使用<strong>seed filling</strong> 来探测平面的连接区域，小团点簇被进一步去除。</p>
<h2 id="3-2-Cuboid-Generation"><a href="#3-2-Cuboid-Generation" class="headerlink" title="3.2 Cuboid Generation"></a>3.2 Cuboid Generation</h2><p>一个通用的立方体模型包含9个自由度：3个自由度的位置信息 $t = (t_x, t_y, t_z)$，3个自由度的旋转信息 $r = (roll, pitch, yaw)$ ，以及3个自由度的尺寸信息 $d = (l, w, h)$。因此，地图中的物体地标被表示为 $O = \{T_0, d\}$ ，其中 $T_0 \in SE(3)$ 表示6自由度的位姿参数。作者假设所有物体地标均位于水平平面，所以<em>roll, pitch</em> 参数设置为0。</p>
<p>如Fig. 4所示，首先将3D 体素投影到平面上，然后对 <em>yaw</em> 参数在<strong>二维投影平面上</strong>进行从0°到180°的离散采样，对于每个采样，可以获取一个完全覆盖2D 投影区域的最小bounding box，然后将这个box 设定为目标立方体的top view，由此可以获得立方体的5个参数：$\{t_x, t_y, yaw, l, w\}$ 。</p>
<p><img src="/2024/03/06/lin2021/fig4.png" alt="fig4" title="figure 4"></p>
<p>然后，令该立方体包含垂直方向上的所有3D 体素，即可获取剩余两个参数：$\{t_z, h\}$ ，由此可以得到一个初步的立方体模型。在此基础上，使用<strong>粒子群优化算法</strong>进一步优化该立方体模型，从而得到最优的模型：</p>
<p><img src="/2024/03/06/lin2021/f3.png" alt="f3" title="formula 3"></p>
<p>式中，$N_{occ}$ 表示立方体中被占据体素的体积，$N_{free}$ 表示未被占据的体积，权重参数 $\lambda$ 根据传感器噪声进行调整。</p>
<h2 id="3-3-Data-Association"><a href="#3-3-Data-Association" class="headerlink" title="3.3 Data Association"></a>3.3 Data Association</h2><p>当物体建模完成以后，我们可以获取到物体的语义与尺寸信息。当两个物体的尺寸差异小于它们平均值的1/3时，我们认为两个物体尺寸匹配成功；作者将图片转化为HSV 色域空间，计算两幅图片的颜色直方图并产生归一化特征向量，当两个特征向量的相关系数超过0.5时认为颜色匹配成功；使用<strong>random walk descriptor</strong>来提取物体周围的拓扑结构信息，本实验的 random walk descriptor 阈值设定为探索次数的0.5倍。</p>
<h1 id="4-Loop-Closure-With-Semantic-Graph"><a href="#4-Loop-Closure-With-Semantic-Graph" class="headerlink" title="4 Loop Closure With Semantic Graph"></a>4 Loop Closure With Semantic Graph</h1><h2 id="4-1-Graph-Representation"><a href="#4-1-Graph-Representation" class="headerlink" title="4.1 Graph Representation"></a>4.1 Graph Representation</h2><p>作者将环境中的物体作为顶点，将物体中心在世界坐标系中的坐标设定为该顶点坐标，将覆盖物体模型的<strong>最小球形区域定义为该节点的邻接空间</strong>，邻接空间有相交部分的节点使用<strong>无向边undirected edges</strong> 进行连接；此外，对于图中的每一个节点，遍历其余所有节点找到与其距离最近的一个节点，使用无向边进行连接，如Fig. 5所示。</p>
<p><img src="/2024/03/06/lin2021/fig5.png" alt="fig5" title="figure 5"></p>
<p>作者使用<strong>图嵌入方法graph embedding method</strong> 来将基于图的结构信息嵌入到<strong>线性向量空间</strong>，如Fig. 6所示，首先选取一个边缘节点作为根节点，并测量根节点距离其他所有节点的距离，根据距离进行降序排列得到一个支持向量 $V = \{v_1, v_2, …, v_n\}$ 来表示图的拓扑结构，支持向量中的元素 $v_i$ 表示物体的描述子（语义、尺寸、颜色以及random walk descriptor）；通过替换根节点并重复上述操作，即可从图中获取多个支持向量。</p>
<p><img src="/2024/03/06/lin2021/fig6.png" alt="fig6" title="figure 6"></p>
<h2 id="4-2-Loop-Detection"><a href="#4-2-Loop-Detection" class="headerlink" title="4.2 Loop Detection"></a>4.2 Loop Detection</h2><p>对于<strong>两个子图的支持向量</strong> $V_a = \{v^1_a, v^2_a,…, v^m_a\}$ , $V_b = \{v^1_b, v^2_b,…, v^n_b\}$ ，编辑 $V_a$ 中的节点来转化为 $V_b$ ，将最小的编辑次数定义为两者之间的edit distance。节点编辑操作包括四种：插入、删除、单节点替换与两个邻接节点的交换。相邻节点的交换操作可以阻止支持向量元素的排序混乱问题，只允许与根节点有相似距离的两个节点进行交换操作，定义两个支持向量的编辑距离：</p>
<p><img src="/2024/03/06/lin2021/f7.png" alt="f7" title="formula 7"></p>
<p>$sub_{a, b}(i, j)$ 表示交换 $v^i_a, v^{i-1}_a$ 之后的最小编辑距离，相应的，$nousub_{a, b}(i, j)$ 表示 $v^i_a, v^{i-1}_a$ 交换操作被禁止的最小编辑距离：</p>
<p><img src="/2024/03/06/lin2021/f9.png" alt="f9" title="formula 9"></p>
<p>作为拓扑结构的补充，两个地图中匹配物体之间的<strong>空间距离一致性</strong>也被考虑，若两个向量包含共同的节点，但是节点间的距离（在语义图中用边连接的<strong>任意两个节点</strong>）超出阈值，那么就视为不匹配。最终，当每一对支持向量中的编辑距离满足以下约束则视为构成回环：</p>
<p><img src="/2024/03/06/lin2021/f10.png" alt="f10" title="formula 10"></p>
<p>式中 $\lambda$ 被设为0.3。</p>
<h2 id="4-3-Loop-Correction"><a href="#4-3-Loop-Correction" class="headerlink" title="4.3 Loop Correction"></a>4.3 Loop Correction</h2><p>当检测到回环时，使用物体对齐来矫正偏移误差，偏移误差可通过对齐匹配物体来获取：</p>
<p><img src="/2024/03/06/lin2021/f11.png" alt="f11" title="formula 11"></p>
<p>然后使用偏移误差来矫正相机位姿 $T_c$ ：</p>
<p><img src="/2024/03/06/lin2021/f12.png" alt="f12" title="formula 12"></p>
<p>由于立方体的对称性，相机位姿有两个可行解，因此需要两对匹配立方体来消除模糊度。在获取了当前关键帧的初始位姿后，使用所有匹配的地标利用非线性最小二乘进行优化：</p>
<p><img src="/2024/03/06/lin2021/f13.png" alt="f13" title="formula 13"></p>
<p>在优化了当前关键帧的位姿后，使用图优化方法来矫正整个轨迹。</p>
<h1 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5 Experiments"></a>5 Experiments</h1><h2 id="5-1-Datasets"><a href="#5-1-Datasets" class="headerlink" title="5.1 Datasets"></a>5.1 Datasets</h2><p>使用TUM RGB-D benchmark，以及作者自采数据进行实验。</p>
<h2 id="5-2-Semantic-Mapping-Performance"><a href="#5-2-Semantic-Mapping-Performance" class="headerlink" title="5.2 Semantic Mapping Performance"></a>5.2 Semantic Mapping Performance</h2><p>Fig. 7展示了基于本文提出的方法构建的语义地图。</p>
<p><img src="/2024/03/06/lin2021/fig7.png" alt="fig7" title="figure 7"></p>
<h2 id="5-3-Loop-Detection-Performance"><a href="#5-3-Loop-Detection-Performance" class="headerlink" title="5.3 Loop Detection Performance"></a>5.3 Loop Detection Performance</h2><p>作者在同一场景中以不同的视角、距离采集图像，理论上来讲所有的图片都可以被视为回环，作者使用SLAM 算法计算每张图片代表的相机位姿，固定一些图片作为模板，剩余图片进行匹配预测测试；将本文提出的方法与基于ORB 特征的回环检测算法进行对比，回环检测评价标准是匹配成功的数量，实验结果如Fig. 8、Fig. 9所示。</p>
<p><img src="/2024/03/06/lin2021/fig8.png" alt="fig8" title="figure 8"></p>
<p><img src="/2024/03/06/lin2021/fig9.png" alt="fig9" title="figure 9"></p>
<h2 id="5-4-Localization-Accuracy"><a href="#5-4-Localization-Accuracy" class="headerlink" title="5.4 Localization Accuracy"></a>5.4 Localization Accuracy</h2><p>与BAD-SLAM、ORB-SLAM2 进行对比实验，计算绝对偏移误差，实验结果如Fig. 10、Table 1（RMSE）所示，</p>
<p><img src="/2024/03/06/lin2021/fig10.png" alt="fig10" title="figure 10"></p>
<p><img src="/2024/03/06/lin2021/t1.png" alt="t1" title="table 1"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Topology</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Topology</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 3D LiDAR Aided GNSS NLOS Mitigation for Reliable GNSS-RTK Positioning in Urban Canyons</title>
    <url>/2024/03/25/liu2022a/</url>
    <content><![CDATA[<p>Liu, Xikun, Weisong Wen, Feng Huang, Han Gao, Yongliang Wang, and Li-Ta Hsu. “3D LiDAR Aided GNSS NLOS Mitigation for Reliable GNSS-RTK Positioning in Urban Canyons.” arXiv, December 11, 2022. <a href="https://doi.org/10.48550/arXiv.2212.05477">https://doi.org/10.48550/arXiv.2212.05477</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ol>
<li>提出一个基于<strong>无漂移的3D 滑动窗口PCM (Point Cloud Map) 辅助的GNSS NLOS 信号消除</strong>方法，以此提高GNSS 原始观测量的质量。3D 滑动窗口PCM 的漂移是通过提高后的GNSS-RTK 进行矫正的；</li>
<li>提出一个<strong>基于LiDAR 地标的VS (Virtual Satellite) 约束模型</strong>，来提高城市峡谷GNSS <strong>卫星的几何布局</strong>。通过GNSS 原始观测数据、IMU 以及仔细挑选过的VS 观测量进行<strong>紧耦合</strong>来提高浮点解精度。此外，作者还推导了VS 引入对几何布局的提高程度；</li>
<li>在香港的城市峡谷中进行大量的实验验证。</li>
</ol>
<span id="more"></span>
<h1 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3 Methodology"></a>3 Methodology</h1><h2 id="3-1-System-Overview"><a href="#3-1-System-Overview" class="headerlink" title="3.1 System Overview"></a>3.1 System Overview</h2><p>本系统的流程图如Fig. 2所示，本系统主要包含两部分：<strong>LiDAR 辅助NLOS 信号剔除，利用VS 提高卫星几何布局进行定位</strong>。</p>
<p><img src="/2024/03/25/liu2022a/fig2.png" alt="fig2" title="figure 2"></p>
<p>系统大致流程如下所示：</p>
<ol>
<li>IMU 使用<strong>预积分技术</strong>进行处理，来获取滑动窗口中每个关键帧的<strong>初始位姿</strong>；</li>
<li>利用关键帧的初始位姿以及相应的局部PCM 进行<strong>NLOS 和周跳探测</strong>。同时，从提取到的特征和PCM 中获取<strong>VS 约束</strong>；</li>
<li>在前两步的预处理之后，利用因子图优化 FGO (Factor Graph Optimization) （包含VS 、GNSS 以及 IMU 约束）实现对<strong>滑动窗口状态的联合优化</strong>，从而获得提高过的<strong>浮点解及协方差矩阵</strong>；</li>
<li>提高过的浮点解及协方差矩阵被用于AR (Ambiguity Resolution) 进行<strong>固定解</strong>求解；</li>
<li>最终，基于固定解和浮点解进行<strong>全局位姿图优化</strong>，来获取最终的位姿结果。更新过的全局位姿以及相应的PCM 用于下一步的NLOS 探测。</li>
</ol>
<h2 id="3-2-3D-LiDAR-Aided-GNSS-NLOS-Mitigation"><a href="#3-2-3D-LiDAR-Aided-GNSS-NLOS-Mitigation" class="headerlink" title="3.2 3D LiDAR-Aided GNSS NLOS Mitigation"></a>3.2 3D LiDAR-Aided GNSS NLOS Mitigation</h2><p>利用3D LiDAR PCM 进行NLOS 信号探测参考文章(Wen 和 Hsu, 2022)，预期不同之处在于，本文<strong>利用GNSS-RTK 对3D PCM 的漂移进行矫正</strong>。</p>
<p>利用3D PCM 进行NLOS 探测严重依赖于<strong>朝向估计的准确性</strong>，而其容易受到漂移的影响，如Fig. 3（b）所示，因此，本文提出了使用提高的GNSS-RTK 实现对PCM 的优化。</p>
<p><img src="/2024/03/25/liu2022a/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="3-3-Virtual-Satellite-aided-GNSS-RTK-IMU-Factor-Graph-Optimization-and-Ambiguity-Resolution"><a href="#3-3-Virtual-Satellite-aided-GNSS-RTK-IMU-Factor-Graph-Optimization-and-Ambiguity-Resolution" class="headerlink" title="3.3 Virtual Satellite aided GNSS-RTK/IMU Factor Graph Optimization and Ambiguity Resolution"></a>3.3 Virtual Satellite aided GNSS-RTK/IMU Factor Graph Optimization and Ambiguity Resolution</h2><p>利用<strong>最大后验概率估计</strong>得到本模型的目标方程：</p>
<p><img src="/2024/03/25/liu2022a/f2.png" alt="f2" title="formula 2"></p>
<p>括号内各项分别是：<strong>边缘项（窗口外的约束）、VS 约束项、IMU 约束项、双差伪距、双差载波相位、双差整周模糊度、多普勒频移</strong>。</p>
<p>相应的因子图结构如Fig. 4所示，由于不同传感器观测频率的不同，GNSS 因子（双差伪距、双差载波相位、多普勒频移）通过插值状态 $x_t$  来对系统状态 $x_k$  和 $x_{k+1}$  进行约束。</p>
<p><img src="/2024/03/25/liu2022a/fig4.png" alt="fig4" title="figure 4"></p>
<p>对于整周模糊度因子，作者使用文章(Huang 等, 2022)提到的方法，利用LiDAR 辅助进行周跳探测，该方法是利用<strong>三差TD 一致性检测</strong>来实现对周跳的探测。</p>
<p><img src="/2024/03/25/liu2022a/f16.png" alt="f16" title="formula 16"></p>
<p>根据上式可以发现，TD 利用测距来估计双差DD 的整周模糊度，这对于初始位姿估计的精度要求较高，作者利用LiDAR 和 IMU 实现一个<strong>高质量的初始位姿估计</strong>，从而实现对较小周跳的成功探测。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>GNSS</category>
        <category>NLOS</category>
      </categories>
      <tags>
        <tag>GNSS</tag>
        <tag>NLOS</tag>
        <tag>LiDAR</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Global Localization with Object-Level Semantics and Topology</title>
    <url>/2024/02/20/liu2019/</url>
    <content><![CDATA[<p>Liu, Yu, Yvan Petillot, David Lane, and Sen Wang. “Global Localization with Object-Level Semantics and Topology.” In <em>2019 International Conference on Robotics and Automation (ICRA)</em>, 4909–15. Montreal, QC, Canada: IEEE, 2019. <a href="https://doi.org/10.1109/ICRA.2019.8794475">https://doi.org/10.1109/ICRA.2019.8794475</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文作者研究了使用3D 物体级语义信息来实现基于视觉的全局定位技术，主要贡献如下：</p>
<ol>
<li>综合利用现有的<strong>稠密语义</strong>、<strong>3D 拓扑</strong>、<strong>图匹配</strong>以及<strong>3D 对齐技术</strong>，实现一个新颖的物体级全局定位算法；</li>
<li>展现了物体级语义信息对于鲁棒地点识别与全局定位的作用，对于光强变化、场景改变等具有较强的鲁棒性；</li>
<li>证明了物体级对齐技术可以处理具有挑战性的3D 点对齐，并在没有完整观察数据的情况下实现精确定位。</li>
</ol>
<span id="more"></span>
<h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h1><h2 id="2-3-Graph-Matching"><a href="#2-3-Graph-Matching" class="headerlink" title="2.3 Graph Matching"></a>2.3 Graph Matching</h2><p>图匹配可以被视为一种<strong>匹配问题</strong>，解决两个图之间节点与边的确切对应关系。不幸的是，用这种方式解决该问题是典型的NP-hard 问题。选择另一种非确切图匹配方式来解决对应问题，如:</p>
<p>一些作者将一个图利用其邻接矩阵来表示重要的拓扑属性，在此基础上使用矩阵相似度来简化图匹配问题。</p>
<p>一些作者尝试使用 graph kernels based on walks 来解决图匹配：</p>
<blockquote>
<p>the authors compute <strong>pair-wise similarity between walks’ composing nodes and edges</strong>, and calculate a final matching score for the scene modeling problem.</p>
</blockquote>
<p>一些作者比较每个节点的random walk descriptors，其中每个描述子对相应节点的局部连接进行编码each descriptor encodes the local connectivity of the corresponding node.</p>
<h1 id="3-Global-Localization-With-Object-Level-Semantic-and-Topology"><a href="#3-Global-Localization-With-Object-Level-Semantic-and-Topology" class="headerlink" title="3 Global Localization With Object-Level Semantic and Topology"></a>3 Global Localization With Object-Level Semantic and Topology</h1><p><img src="/2024/02/20/liu2019/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-Semantic-Segmentation-and-Fusion"><a href="#3-1-Semantic-Segmentation-and-Fusion" class="headerlink" title="3.1 Semantic Segmentation and Fusion"></a>3.1 Semantic Segmentation and Fusion</h2><p>本文中，全局地图是在制图阶段利用稠密SLAM 算法构建的，后续增加语义特征，使用简单的投票方案voting scheme进行融合操作，地图中的点若从多个角度观测均被划分为某类则表明有较高的置信度。在最终的语义地图中，只保留那些一直具有高度一致语义标签的点云。query的局部语义地图以同样的方式来获取。</p>
<h2 id="3-2-Graph-Extraction"><a href="#3-2-Graph-Extraction" class="headerlink" title="3.2 Graph Extraction"></a>3.2 Graph Extraction</h2><p>从语义地图到语义图的转换过程：</p>
<ol>
<li>首先，使用Euclidean clustering来提取具有相同语义标签的近点，忽略墙壁、地板、天花板等不能提供有用拓扑关系的语义点云集合；</li>
<li>在图中，使用bounding sphere来表示每一个物体，因为球体具有旋转不变性，球形的尺寸表示物体的尺寸，由最远点到中心点的距离决定；</li>
<li>节点与边表示地图中的3D语义拓扑，当两个物体节点处于邻接距离内时使用无向边进行连接，此外，若两个物体的球体有相交，那么也用无向边进行连接，表明模型包含了<strong>空间与尺寸</strong>关系。</li>
</ol>
<p><img src="/2024/02/20/liu2019/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="3-3-Random-Walk-Descriptor"><a href="#3-3-Random-Walk-Descriptor" class="headerlink" title="3.3 Random Walk Descriptor"></a>3.3 Random Walk Descriptor</h2><p>本文使用随机游走描述子来对图中的节点进行描述：从根节点开始，探索邻接节点并记录访问过的标签序列，每次探索的深度被定义为该游走的长度，探索的数量被定义为该节点的特定描述子尺寸，过程如Fig. 4所示。在描述子中，除了访问过的标签，还需要跟踪每个标签在图中所对应的节点，来确保在association步骤中的空间一致性。</p>
<p><img src="/2024/02/20/liu2019/fig4.png" alt="fig4" title="figure 4"></p>
<h2 id="3-4-Object-Association"><a href="#3-4-Object-Association" class="headerlink" title="3.4 Object Association"></a>3.4 Object Association</h2><p>一旦全局与query图的随机游走描述子被建立之后，就开始基于它们<strong>共有的相同描述子数量</strong>来建立节点间的联系。由于环境中可能存在同一类的多个物体，两个匹配的描述子只表明是<strong>一个潜在的候选对</strong>，对该候选对节点的游走路径进行追溯，对空间一致性进行确认，如果两个路径拥有相同的标签，但是任意两个节点之间的距离超出一个阈值就判定为匹配失败。对于每个质询节点都与全局图中的所有节点进行描述子匹配，最终选取k个具有最多匹配描述子的节点作为匹配节点，由此可以得到质询图与全局图中物体的对应关系。</p>
<p>注意，本文允许非精确匹配inexact association，因为某些场景中的物体可能会出现过度分割的现象，导致一个物体被分割为多块，需要解决多对一的对应问题。</p>
<h2 id="3-5-Localization-based-on-Object-Level-Alignment"><a href="#3-5-Localization-based-on-Object-Level-Alignment" class="headerlink" title="3.5 Localization based on Object-Level Alignment"></a>3.5 Localization based on Object-Level Alignment</h2><p>实现质询图与全局图中物体的关联之后，使用物体的几何信息进行位姿估计：对两图中关联物体的点云进行稠密对齐。</p>
<p>使用Fast Point Feature Histograms (FPFH), Sample Consensus initial alignment method (SAC-IA) 算法来对关联点云进行对齐，由此得到的位姿转换关系提供了质询物体在地图中的初始定位信息，然后使用 Iterative Closest Point (ICP) 算法进行优化，最终得到6自由度的相机位姿。</p>
<h1 id="4-Experimental-Results"><a href="#4-Experimental-Results" class="headerlink" title="4 Experimental Results"></a>4 Experimental Results</h1><h2 id="4-1-Dataset"><a href="#4-1-Dataset" class="headerlink" title="4.1 Dataset"></a>4.1 Dataset</h2><p>作者选取了两个数据库：公开数据集SceneNN ，以及作者在living room，kitchen以及dining room自采的数据集——LKD，在LKD 数据集中作者采集了不同光线变化下的场景数据。</p>
<p><img src="/2024/02/20/liu2019/fig5.png" alt="fig5" title="figure 5"></p>
<h2 id="4-4-Localization-Performance"><a href="#4-4-Localization-Performance" class="headerlink" title="4.4 Localization Performance"></a>4.4 Localization Performance</h2><p>Fig. 7与Table 1是在两个数据集上的实验结果，可以发现在数据集SceneNN 上效果更好，作者给出的原因是：</p>
<blockquote>
<p>most errors are skewed toward the lower range.</p>
</blockquote>
<p><img src="/2024/02/20/liu2019/fig7.png" alt="fig7" title="figure 7"></p>
<p><img src="/2024/02/20/liu2019/t1.png" alt="t1" title="table 1"></p>
<p>Fig. 6展示了在SceneNN 数据集中质询范例成功在全局地图中进行匹配的例子。</p>
<p><img src="/2024/02/20/liu2019/fig6.png" alt="fig6" title="figure 6"></p>
<p>进一步地，作者使用FAB-MAP，NetVLAD 算法进行对比验证，其中FAB-MAP 是基于BoW词袋算法的，NetVLAD 利用CNN 网络在不同视角与光线条件下的同一场景数据进行训练来获取深度特征。实验结果如Fig. 8所示。</p>
<p><img src="/2024/02/20/liu2019/fig8.png" alt="fig8" title="figure 8"></p>
<h2 id="4-5-Discussion-on-Challenging-Scenarios"><a href="#4-5-Discussion-on-Challenging-Scenarios" class="headerlink" title="4.5 Discussion on Challenging Scenarios"></a>4.5 Discussion on Challenging Scenarios</h2><p>作者讨论了当观测数据不完整（可能原因包括遮挡、过度分割导致物体碎片等）、存在动态物体的场景（移除物体或新增物体）情况下本文算法的鲁棒性。</p>
<p><img src="/2024/02/20/liu2019/fig9.png" alt="fig9" title="figure 9"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Topology</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Topology</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Towards View-invariant and Accurate Loop Detection Based on Scene Graph</title>
    <url>/2024/03/06/liu2023/</url>
    <content><![CDATA[<p>Liu, Chuhao, and Shaojie Shen. “Towards View-Invariant and Accurate Loop Detection Based on Scene Graph.” arXiv, May 24, 2023. <a href="http://arxiv.org/abs/2305.14885">http://arxiv.org/abs/2305.14885</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>Lin 等人(Lin 等, 2021)提出了使用随机游走描述子建立物体实例的拓扑结构，进而实现语义图匹配，该方法可实现系数环境中大视角变化下的回环检测；但是，该方法难以处理<strong>距离较近的、具有相同语义标签的模糊实例</strong>，然而，室内场景中这种情况有很常见，如Fig. 1 所示，这些具有相同语义标签的临近物体实例（如图中的沙发）共享近邻neighbors，如果简单地使用随机游走描述子进行表示，它们<strong>会拥有相同的随机游走路线以及同样的拓扑结构描述子</strong>，进而会导致回环检测失败。</p>
<span id="more"></span>
<p><img src="/2024/03/06/liu2023/fig1.png" alt="fig1" title="figure 1"></p>
<p>除了模糊实例外，随机扫描的路线所生成的语义图会包含一些变化因素：如当从不同视角扫描一个稠密场景时，只有实例的一部分会被同时观测到，由此所重建的语义图会由于较小的重叠体积<strong>导致两图不相同</strong>；又如低动态物体，打开的窗帘或者移动的椅子，它们会在重建语义图中创形成一个<strong>micro-differences</strong> 。在使用语义地标的拓扑结构进行回环检测时需要考虑这些变化因素，来保证系统的鲁棒性。</p>
<p>此外，语义信息辅助的回环检测性能严重依赖于<strong>前端是否可以构建持续一致的语义地标</strong>，之前的方法(Lin 等, 2021)使用object map 作为前端，在重建之前对采集到的图片进行实例分割；但是，<strong>实例分割对于视角变化较为敏感</strong>，当图片中物体的形状、外观发生变化，或者被遮挡时可能会产生不同的语义分割结果，这就导致图片间的数据关联异常困难，因此，之前的语义信息辅助回环检测大部分是基于<strong>稀疏物体场景的应用</strong>，或者<strong>忽略许多不能确定联系的物体</strong>。</p>
<p>基于以上分析，本文提出了使用RGB-D 相机作为采集设备，通过<strong>匹配inactive 场景图与active 场景图</strong>来实现回环检测，作者不使用object map 作为前端，而是基于<strong>SceneGraphFusion</strong> 创建语义地标，SceneGraphFusion 可将一个重建的3D 图<strong>实时分割为语义实例</strong>。由于语义实例是从<strong>增量式图accumulated map</strong>中提取得到的，因此不需要帧间的数据关联。在后端中，作者使用一个随机游走描述子来创建<strong>全局拓扑结构</strong>，使用一个邻域游走描述子neighbor walk descriptor 来表示<strong>局部拓扑结构</strong>，利用一个<strong>占据体积相似度occupancy similarity</strong> 来联合考虑两种拓扑结构，实现关联查找。</p>
<h2 id="2-Related-Works"><a href="#2-Related-Works" class="headerlink" title="2 Related Works"></a>2 Related Works</h2><h2 id="2-1-Semantic-Mapping-with-RGB-D-Camera"><a href="#2-1-Semantic-Mapping-with-RGB-D-Camera" class="headerlink" title="2.1 Semantic Mapping with RGB-D Camera"></a>2.1 Semantic Mapping with RGB-D Camera</h2><p>object mapping 和 scene segmentation 都可以建立一个语义图，关键区别在于分割是在场景重建前还是重建后。</p>
<h2 id="2-2-Semantic-aided-Loop-Detection"><a href="#2-2-Semantic-aided-Loop-Detection" class="headerlink" title="2.2 Semantic-aided Loop Detection"></a>2.2 Semantic-aided Loop Detection</h2><p>RGB-D 相机位姿服从<strong>4自由度</strong>：x, y, z, yaw angle。</p>
<p>根据node2vec 的阐述，随机游走描述子中有两种不同的游走策略：<strong>breadth-first sampling (BFS), depth-first sampling (DFS)</strong>，前者代表了图的micro-view，后者代表了macro-view。X-View (Gawel 等, 2018)提出了使用DFS 策略处理随机游走描述子，然而，DFS 策略是基于<strong>同质性homophily</strong> 建立顶点的拓扑结构，代表了图中顶点的共同出现，因此，当出现具有共享邻域的模糊语义实例时会导致错误匹配；而且，DFS 对于图变化更敏感，特别是当使用不准确的特征来描述顶点时。这就导致当出现视角或分割变化时随机游走描述子的退化。</p>
<p>因此，为了确保实例拓扑的可区分描述discriminative description 来提高剧烈视角变化时的鲁棒性，作者联合使用<strong>随机游走描述子</strong>与<strong>邻域游走描述子</strong>来表示实例拓扑结构。</p>
<h1 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3 Methodology"></a>3 Methodology</h1><h2 id="3-1-System-Overview"><a href="#3-1-System-Overview" class="headerlink" title="3.1 System Overview"></a>3.1 System Overview</h2><p><img src="/2024/03/06/liu2023/fig2.png" alt="fig2" title="figure 2"></p>
<p>系统工作流程如下：</p>
<ol>
<li>本系统使用RGB-D 图片作为输入，并使用ORB-SLAM3 来计算相机位姿；</li>
<li>前端使用SceneGraphFusion 对重建场景进行<strong>增量式分割</strong>，基于此创建一个动态场景图activate scene graph，并<strong>为每个实例创建拓扑描述子</strong>；</li>
<li>然后，将动态图与一个提前构建的非动态图进行匹配实现回环检测，如果成功检测到回环，对两图之间的位姿漂移进行估计，并融合相应的场景图。</li>
</ol>
<h2 id="3-2-Active-Scene-Graph-Construction"><a href="#3-2-Active-Scene-Graph-Construction" class="headerlink" title="3.2 Active Scene Graph Construction"></a>3.2 Active Scene Graph Construction</h2><p>基于SceneGraphFusion 构建一个场景图 $G = \{V, \xi\}$ ，其中 $V$ 是顶点组合，$\xi$ 是边组合。</p>
<p>将SceneGraphFusion 产生的节点nodes 作为输入来产生顶点，一些节点可能是被过度分割的物体实例的部分，作者将这些节点进行融合作为一个实例；忽略那些体积太小或者低置信度的物体实例。每一个被创建的实例表示为顶点 $v_i = \{\mathbf{p, b, n}, l\}, v_i \in V$ ，其中 $\mathbf{p} \in R^3$  表示实例中心坐标，轴对齐的bounding box 尺寸 $\mathbf{b} \in R^3$ ，法向量 $\mathbf{n} \in R^3$ ，以及语义标签 $l$ ，其中，法向量 $\mathbf{n}$ 只适用于平面类实例，如墙壁、地板等。由于相机位姿为4自由度，所以各顶点会将其roll、pitch朝向进行对齐。</p>
<p>边 $\xi$ 的构建过程考虑顶点的语义和几何属性来添加<strong>有向边</strong>：</p>
<ul>
<li>对于两个物体，如果它们的中心点距离小于一个阈值则添加一条边；</li>
<li>对于一个物体和一个垂直的墙体，如果它们的垂直距离小于一个阈值则添加一条边；</li>
<li>对于两个墙体，如果它们的中心距离近且其法线角度大于一个阈值，则添加一条边；</li>
<li>对于任意的顶点以及一个地板，如果它们水平重叠，则添加一条边。</li>
</ul>
<h2 id="3-3-Topology-Descriptors"><a href="#3-3-Topology-Descriptors" class="headerlink" title="3.3 Topology Descriptors"></a>3.3 Topology Descriptors</h2><p>在构建活动场景图active scene graph $G^a$ 时，每个顶点的拓扑描述子也被增量式生成。使用随机游走描述子 $\mathbf{d_i^r} \in C^{N_i \times k}$ 来描述顶点 $v_i$ 的macro-view 拓扑结构，其中  $N_i$ 表示随机游走数量，$k=4$ 表示随机游走深度。随机游走描述子生成过程如Fig. 3所示，但是它<strong>不能区分模糊实例</strong>。</p>
<p><img src="/2024/03/06/liu2023/fig3.png" alt="fig3" title="figure 3"></p>
<p>为了解决模糊实例、提高鲁棒性，作者提出了顶点 $v_i$ 的邻域游走描述子 $\mathbf{d^n_i} \in C^{M_i \times q}$ ，其中 $M_i$ 表示邻域游走数量，$q=4$ 表示游走深度。<strong>沿着z 轴逆时针遍历</strong>顶点 $v_i$ 的邻域，若当前邻接物体与上个邻接物体之间的角度大于 150°，则忽略该次游走。如Fig. 3所示，使用邻域游走描述子可以区分模糊实例（图中的两把椅子）。邻域游走描述子是收到node2vec 的BFS 策略启发产生的，强调了顶点的<strong>结构一致性</strong>，另一个优点是对于<strong>4自由度位姿漂移的不变性</strong>。</p>
<p>顶点 $v_i$ 的完整拓扑结构描述子表示为 $\{\mathbf{d_i^r, d_i^n}\}$ 。</p>
<h2 id="3-4-Graph-Match"><a href="#3-4-Graph-Match" class="headerlink" title="3.4 Graph Match"></a>3.4 Graph Match</h2><h3 id="3-4-1-Build-Score-Matrix"><a href="#3-4-1-Build-Score-Matrix" class="headerlink" title="3.4.1 Build Score Matrix"></a>3.4.1 Build Score Matrix</h3><p>计算所有可能节点对的相似度得分，如对于活动场景图与先验场景图中的一对节点 $\{v_i, v_j | v_i\in V^a, v_j \in V^i\}$ ，它们的相似度计算公式如下所示：</p>
<p><img src="/2024/03/06/liu2023/f1.png" alt="f1" title="formula 1"></p>
<p>其中，随机游走项与邻域游走项分别计算并归一化：</p>
<p><img src="/2024/03/06/liu2023/f1-1.png" alt="f1-1" title="formula 1-1"></p>
<p>两式的分子均为<strong>两组描述子中相同行的个数</strong>，值得注意的是，两个式子的归一化分母取值不同：</p>
<ul>
<li>对于随机游走描述子，归一化分母取值为两个描述子行数的<strong>最小值</strong>，这样可以实现当 $v_i$ 由于不完整扫描导致部分游走缺失，但 $\mathbf{d_i^r}$ 中的大部分游走都可以在 $\mathbf{d_j^r}$ 中找到相同部分时，仍可实现较高的相似度得分；</li>
<li>而邻域游走描述子的归一化分母取值为两个描述子行数中的<strong>最大值</strong>，当 $v_i$ 与 $v_j$ 拥有不同的邻域描述子，即便部分游走相同，但是计算得到的相似度得分较低，这样就相当于设定了一个较高的匹配阈值，可有效减小模糊实例的错误匹配。</li>
</ul>
<p>此外，相似度计算还考虑了物体的体积相似度，计算如下所示：</p>
<p><img src="/2024/03/06/liu2023/f2.png" alt="f2" title="formula 2"></p>
<p>其中， $\mathbf{b} \in R^3$ 是轴对齐的bounding box 尺寸，而 $l(\mathbf{b})$ 是对角线长度。</p>
<p>由此可得到两节点的相似度得分 $S_{ij}$ ，进而得到<strong>两图的完整得分矩阵</strong> $S \in R^{|V^s| \times |V^t|}$ 。</p>
<h3 id="3-4-2-Find-Correspondences"><a href="#3-4-2-Find-Correspondences" class="headerlink" title="3.4.2 Find Correspondences"></a>3.4.2 Find Correspondences</h3><p>由上文得到的得分矩阵 $S$ ，将每行、每列的最高分保留，其余元素置零，如果最大相似度大于阈值 $\tau = 0.5$ ，则将其标记为一对匹配实例。最终，如果达到最小匹配数量 $\epsilon = 4$ ，表示回环检测成功。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Topology</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Topology</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Object-aware data association for the semantically constrained visual SLAM</title>
    <url>/2024/02/26/liu2023a/</url>
    <content><![CDATA[<p>Liu, Yang, Chi Guo, and Yingli Wang. “Object-Aware Data Association for the Semantically Constrained Visual SLAM.” <em>Intelligent Service Robotics</em> 16, no. 2 (April 1, 2023): 155–76. <a href="https://doi.org/10.1007/s11370-023-00455-9">https://doi.org/10.1007/s11370-023-00455-9</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者认为，将物体级语义信息和传统vSLAM 融合需要考虑两个关键问题：</p>
<ol>
<li>第一个问题是构建连续帧之间的<strong>数据关联</strong>问题。作者认为物体级vSLAM 需要充分利用相机运动以及3D 几何信息，最终产生一个带有全局一致性物体信息的3D 地图。现有的物体级vSLAM 大多依赖于点云的低级几何特征匹配(Yang 和 Scherer, 2019)、聚类或者统计分析进行数据关联，而忽略了物体整个的<strong>外观信息</strong>；</li>
<li>第二个问题是怎么利用物体级信息进行<strong>位姿估计</strong>。CubeSLAM(Yang 和 Scherer, 2019)、QuadricSLAM (Nicholson 等, 2019) 都是将物体建模为可观测的几何参数在位姿估计中建立约束；但是作者认为，根据物体级约束来提高传统数据关联的<strong>准确性和鲁棒性</strong>也可以提高位姿估计精度。</li>
</ol>
<p>总结来看，本文的贡献如下：</p>
<ol>
<li>提出一个基于<strong>物体外观和特征点路标</strong>的物体级数据关联框架，产生一个带有物体信息的语义地图；</li>
<li>提出一个<strong>语义重投影误差项</strong>并将其整合至位姿优化中，该重投影误差项<strong>结合了语义与特征点观测</strong>作为相机位姿的联合约束；</li>
<li>在公开数据集上进行了测试，证明了本方法在物体级数据关联中实现了较高的准确性，并超越了位姿估计基线方法。</li>
</ol>
<span id="more"></span>
<h1 id="3-Methods"><a href="#3-Methods" class="headerlink" title="3 Methods"></a>3 Methods</h1><h2 id="3-1-System-overview"><a href="#3-1-System-overview" class="headerlink" title="3.1 System overview"></a>3.1 System overview</h2><p>系统框架如Fig. 1所示，本系统基于ORB-SLAM2 并使用<strong>双目/RGB-D 图片</strong>作为输入。</p>
<p><img src="/2024/02/26/liu2023a/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-2-Build-object-level-features-and-landmarks"><a href="#3-2-Build-object-level-features-and-landmarks" class="headerlink" title="3.2 Build object-level features and landmarks"></a>3.2 Build object-level features and landmarks</h2><h3 id="3-2-1-Build-2D-objects"><a href="#3-2-1-Build-2D-objects" class="headerlink" title="3.2.1 Build 2D-objects"></a>3.2.1 Build 2D-objects</h3><p>2D 物体语义信息用来构建新的3D 物体、更新已有的3D 物体并为位姿优化提供约束。作者使用YOLACT 实例分割网络来获取2D 物体语义信息，实例级语义信息包含物体掩码、类别标签以及置信度，2D 物体集合表示为 $S = \{s_j\}$ ，其中，每个<strong>2D 物体</strong> $s_j = \{l_j, q_j, m_j, b_j, f_j, Z_j\}$ ：</p>
<ul>
<li>$l_j$  表示物体类别标签</li>
<li>$q_j$  表示分类置信度</li>
<li>$m_j$  表示物体二值掩码图像</li>
<li>$b_j$  表示物体的2D bbox</li>
<li>$f_j$  表示物体掩码区域的HSV (Hue-Saturation-Value) 直方图向量，从RGB 转换为HSV，向量维度为94，来描述物体外观</li>
<li>$Z_j$  <strong>物体掩码区域内的ORB 特征点集合</strong></li>
</ul>
<h2 id="3-2-2-Build-and-update-3D-objects"><a href="#3-2-2-Build-and-update-3D-objects" class="headerlink" title="3.2.2 Build and update 3D-objects"></a>3.2.2 Build and update 3D-objects</h2><p>3D 物体作为<strong>物体级地标</strong>存储了物体的外观和几何信息，$o = \{l, P, B, F, C\}$ :</p>
<ul>
<li>$l$  表示类别标签</li>
<li>$P$  表示物体的稀疏路标点云</li>
<li>$B$  表示3D bbox</li>
<li>$F$  表示之前所有帧中该2D 物体的HSV 特征向量集合</li>
<li>$C$  表示存储当前与之前物体点云中心点的集合</li>
</ul>
<h3 id="3-2-3-Outliers-elimination"><a href="#3-2-3-Outliers-elimination" class="headerlink" title="3.2.3 Outliers elimination"></a>3.2.3 Outliers elimination</h3><p>外点指的是那些位于本物体的特征点云中，但是不属于本物体的特征点。造成外点的原因包括：</p>
<ol>
<li>双目相机估计的<strong>深度误差</strong>或者RGB-D 相机的<strong>深度测量误差</strong>；</li>
<li><strong>错误的实例分割结果</strong>导致包含背景区域或其他物体；</li>
<li>在<strong>物体边界处的特征点</strong>难以区分类别。</li>
</ol>
<p><img src="/2024/02/26/liu2023a/fig2.png" alt="fig2" title="figure 2"></p>
<p>作者综合利用三种手段进行外点剔除：<strong>统计检验</strong>、<strong>聚类</strong>以及<strong>概率分析</strong>。</p>
<ul>
<li><strong>统计检验</strong>：作者假设一个物体的点云分布较为紧凑且近似符合高斯分布，点云中心作为其均值，统计所有点云计算标准差；</li>
<li><strong>聚类</strong>：作者假设物体点云应该是相连的，一小部分外点会被主体点云所孤立。作者使用DBSCAN 对物体点云进行聚类，将点云分为几个独立的聚类点云，计算各个聚类中点云所占的比例。如果某个点所属的聚类占总体点云比例较小且该点距离点云中心的距离超过3倍标准差，则将该点视为外点。值得注意的是，DBSCAN 算法消耗很大的计算资源，若某个物体的点云数量超过一定阈值则不使用聚类检测手段，只使用剩余两个方法；</li>
<li><strong>概率分析</strong>：利用<strong>语义一致性</strong>进行检测，考虑到由于模糊分割造成的外点其分类一般具有随机性，很难长期保持一个稳定的类别标签，因此，作者统计每个特征点的分类概率，若该点属于对应物体的概率值较低，则可判定该点为外点。</li>
</ul>
<h2 id="3-3-Object-level-data-association"><a href="#3-3-Object-level-data-association" class="headerlink" title="3.3 Object-level data association"></a>3.3 Object-level data association</h2><p>作者考虑多种度量方式来检查物体的<strong>外观和几何一致性</strong>，以此进行<strong>短期与长期的</strong>物体级数据关联。</p>
<h3 id="3-3-1-Data-association-metrics"><a href="#3-3-1-Data-association-metrics" class="headerlink" title="3.3.1 Data association metrics"></a>3.3.1 Data association metrics</h3><h4 id="Appearance-metric"><a href="#Appearance-metric" class="headerlink" title="Appearance metric"></a>Appearance metric</h4><p>本文的物体级数据关联核心度量手段是物体的<strong>外观相似度检查</strong>。基于从物体掩码区域提取的<strong>HSV 特征向量</strong>进行外观检测，考虑了物体的整体外观特征，且不需要物体具有突出的几何特征。外观相似度的计算过程如下所示，表征两个归一化HSV 向量 $f_1, f_2$ 的cosine 相似度：</p>
<p><img src="/2024/02/26/liu2023a/f2.png" alt="f2" title="formula 2"></p>
<h4 id="2D-IoU-metric"><a href="#2D-IoU-metric" class="headerlink" title="2D IoU metric"></a>2D IoU metric</h4><p>计算两个2D 物体的bbox IoU，被广泛应用于<strong>短期物体跟踪</strong>。</p>
<p><img src="/2024/02/26/liu2023a/f3.png" alt="f3" title="formula 3"></p>
<p><img src="/2024/02/26/liu2023a/fig3.png" alt="fig3" title="figure 3"></p>
<h4 id="3D-overlap-ratio-metric"><a href="#3D-overlap-ratio-metric" class="headerlink" title="3D overlap ratio metric"></a>3D overlap ratio metric</h4><p>由于相机在单一视角下只能观测到物体的一部分，因此，作者不直接计算3D IoU，而是使用重叠部分占两个视角3D 区域的最大比例：</p>
<p><img src="/2024/02/26/liu2023a/f4.png" alt="f4" title="formula 4"></p>
<h4 id="Object-center-metric"><a href="#Object-center-metric" class="headerlink" title="Object center metric"></a>Object center metric</h4><p>3D 物体的点云是随着观测数据的加入而<strong>增量式更新</strong>的，相应的点云中心坐标也会随着新的观测数据加入而改变；针对新观测的物体点云，作者计算该点云中心坐标与物体之前所有点云中心坐标的最近距离作为一个度量参数：</p>
<p><img src="/2024/02/26/liu2023a/f5.png" alt="f5" title="formula 5"></p>
<h3 id="3-3-2-Data-association-scheme"><a href="#3-3-2-Data-association-scheme" class="headerlink" title="3.3.2 Data association scheme"></a>3.3.2 Data association scheme</h3><p>本数据关联框架包含两种物体数据关联以及冗余的物体检查。</p>
<h4 id="Short-term-data-association"><a href="#Short-term-data-association" class="headerlink" title="Short-term data association"></a>Short-term data association</h4><p>作者在连续两帧中利用<strong>2D IoU</strong> 和<strong>外观检测</strong>来进行物体级的短期数据关联。</p>
<p><strong>查看作者源代码发现</strong>，作者利用两帧的2D物体匹配更新当前帧的3D物体，判断2D 物体匹配成功的准则为：两者外观相似度大于0.8，且2D IoU不小于0.5。</p>
<h4 id="Long-term-data-association"><a href="#Long-term-data-association" class="headerlink" title="Long-term data association"></a>Long-term data association</h4><p>当物体无法利用该短期数据关联完成时，作者利用前端的<strong>特征点匹配</strong>得到相机的<strong>初始位姿估计</strong>，基于此将物体的特征点映射至空间中以产生临时的3D 点云，结合<strong>外观检测、3D 重叠</strong>以及<strong>物体中心度量</strong>来实现图片中2D 物体与地图中3D 物体的关联。</p>
<p><strong>查看作者源代码发现</strong>，作者没有使用3D 重叠来建立长期数据关联，而是符合以下两种情况之一即建立数据关联：</p>
<ol>
<li><strong>外观相似度</strong>大于0.8，且该物体中心与3D 物体历史观测中心的<strong>平均距离</strong>小于阈值 $\mu_c$ （室内场景为0.3，室外场景为5.0）；</li>
<li>该物体中心与3D 物体历史观测中心的<strong>最小距离</strong>小于0.1。</li>
</ol>
<p>完整的数据关联如下所示：</p>
<p><img src="/2024/02/26/liu2023a/a1.png" alt="a1" title="algorithm 1"></p>
<h2 id="3-4-Pose-optimization-constrained-by-semantic-information"><a href="#3-4-Pose-optimization-constrained-by-semantic-information" class="headerlink" title="3.4 Pose optimization constrained by semantic information"></a>3.4 Pose optimization constrained by semantic information</h2><p>本文的语义重投影误差采用与文章(Lianos 等, 2018) VSO 相似的方法，主要不同之处在于本文建立<strong>物体对应关系的语义约束</strong>，此外还建立了一个新的损失函数以及优化策略。</p>
<p>理论上来讲，只要一个物体出现在视角中，那么该物体点云的投影像素应该位于相应的掩码区域内，如Fig. 4所示，物体级语义信息对于<strong>视角变换和尺度变换</strong>具有鲁棒性。</p>
<p><img src="/2024/02/26/liu2023a/fig4.png" alt="fig4" title="figure 4"></p>
<p>对于一个新观测帧，获取到一组<strong>匹配点对</strong> $\mathcal{M}_p = \{(z_i^g,p_i)\}$ ，其中，$z_i^g$  为图片中的ORB 点，$p_i$ 为地图中的特征点地标；进行物体级数据关联之后，得到一组<strong>物体匹配对</strong> $\mathcal{M}_o = \{(s_j, o_j)\}$ ，其中，$s_j$ 表示图片中的2D 物体，$o_j$ 表示地图中的3D 物体。利用物体匹配 $\mathcal{M}_o$ 来检查匹配点对 $\mathcal{M}_p$ 的语义一致性：</p>
<p><img src="/2024/02/26/liu2023a/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$\emptyset$ 表示背景类，示例如Fig. 5所示。其中，$\mathcal{M}_p^{base}$ 中的点对表示低等级几何一致性和物体级语义一致性；特别地，式（6）中 $\mathcal{M}_p^{base}$ 中属于不同语义类别的两种情况：</p>
<ul>
<li>一个平面语义特征点与一个背景点地标匹配时（上式第二行），该背景点地标可视为该物体的备选元素；</li>
<li>一个背景特征点与一个语义点地标匹配时（上式第三行），该3D 物体没能与2D 物体匹配上，可能是由于数据关联失败或者实例分割误差的存在。</li>
</ul>
<p>$\mathcal{M}_p^{joint}$ 中的匹配点对不满足语义一致性。</p>
<p><img src="/2024/02/26/liu2023a/fig5.png" alt="fig5" title="figure 5"></p>
<p>整个优化过程分为两步：<strong>初始位姿优化</strong>和<strong>语义约束优化</strong>。</p>
<p>在<strong>初始位姿优化</strong>中，利用 $\mathcal{M}_p^{base}$ 中匹配点对计算传统的重投影误差：</p>
<p><img src="/2024/02/26/liu2023a/f7.png" alt="f7" title="formula 7"></p>
<p>而对于 $\mathcal{M}_p^{joint}$ 中的匹配点对不具有语义一致性，因此<strong>需要更多的约束</strong>来进行位姿优化，此处，作者添加一个<strong>语义重投影误差项</strong>来作为额外约束，定义重投影像素与所属2D 物体最近的距离作为语义残差项，如Fig. 5（a）所示。</p>
<p><img src="/2024/02/26/liu2023a/f8.png" alt="f8" title="formula 8"></p>
<p>式（9）中的 $h(z, m)$ 表示在 m 中搜寻距离 z 最近的255数值点（即2D 物体的掩码区域）；$\hat{T}$ 表示相机的初始位姿。</p>
<p><strong>疑问：</strong>式8中是否还需要第一项，是否只有第二项时会有更优的结果？？？</p>
<p>作者将上述两个误差项在初始优化中进行结合：</p>
<p><img src="/2024/02/26/liu2023a/f10.png" alt="f10" title="formula 10"></p>
<p>在初始位姿优化后，作者为优化<strong>增加额外的语义约束</strong>。基于初始位姿优化得到的位姿，作者将3D 物体中<strong>没有被匹配的地图点</strong>投影至图像中，并搜寻2D 图像中对应最近的像素作为语义观测项，剔除掉那些距离过远的点，如Fig. 5（b）所示，由此构建新的误差项：</p>
<p><img src="/2024/02/26/liu2023a/f11.png" alt="f11" title="formula 11"></p>
<p>从而构建整个位姿优化的误差函数：</p>
<p><img src="/2024/02/26/liu2023a/f14.png" alt="f14" title="formula 14"></p>
<p>上述优化策略可应用于对相机位姿和特征点地标的同时优化中，但是会大幅增加计算复杂度。在作者的实验中，仅利用其对相机位姿进行优化（match local map）来限制约束数量以及计算负载。作者还利用Kd-tree 来加速最邻近点的搜寻。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Pose Optimization</tag>
        <tag>Object SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>寒武纪MLU220 开发环境Docker搭建</title>
    <url>/2024/01/26/mlu220/</url>
    <content><![CDATA[<p>首先进入<a href="https://cair.cambricon.com/#/home/catalog">寒武纪开发主页</a>并登录寒武纪账号，进入cambricon_pytorch docker 页面。由于本人只使用MLU220 进行边缘端推理，所以不需要在主机上安装MLU 驱动，因此可跳过第一步直接按照提示安装docker。本人在尝试按照页面说明时遇到了一些问题，在此记录一下。</p>
<span id="more"></span>
<p>若是初次使用docker，则建议将Ubuntu个人账户添加进docker组里，这样就避免每次使用时都要输入sudo了，该部分参考<a href="https://www.cnblogs.com/jzcn/p/16591083.html">文章</a>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">## 将当前用户添加进docker组，并更新</span><br><span class="line">sudo gpasswd -a user docker</span><br><span class="line">newgrp docker</span><br><span class="line"></span><br><span class="line">## 然后重启电脑才可永久生效</span><br></pre></td></tr></table></figure>
<p>然后按照寒武纪提示，依次进行如下操作：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">## 登录harbor</span><br><span class="line">docker login cair.cambricon.com</span><br><span class="line"></span><br><span class="line">## 输入用户名、API密钥（网页用户名下拉框中有API密钥选项）</span><br><span class="line">Username: (username)</span><br><span class="line">Password: (API密钥)</span><br></pre></td></tr></table></figure>
<p>注意，这里网页提示使用docker pull命令下载相应的镜像文件，但经过本人尝试之后发现，在后续的docker run命令中会重复下载，因此<strong>跳过使用docker pull，直接使用docker run命令创建容器</strong>：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -it --name mlu220 -v /home/***/work:/work cair.cambricon.com/cambricon/cambricon_pytorch:ubuntu18.04_sdk_v1.7.0_pytorch_v0.15.0-2 /bin/bash</span><br></pre></td></tr></table></figure>
<p>上述命令会自动下载镜像文件，并改名为“mlu220”，且将主机的“/home/<em>*</em>/work”映射至docker端的“/work”。</p>
<p>此时，可以查看新建的容器：</p>
<p><img src="/2024/01/26/mlu220/docker1.png" alt="docker" title="容器查看"></p>
<p>启动mlu220容器并查看：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">❯ docker start mlu220</span><br><span class="line">mlu220</span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/mlu220/docker2.png" alt="docker" title="启动容器"></p>
<p>最后进入docker并激活开发环境：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it mlu220 /bin/bash</span><br><span class="line"><span class="built_in">source</span> torch/venv3/pytorch/bin/activate</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>Cambricon</tag>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title>MMSegmentation平台安装报错问题解决记录</title>
    <url>/2024/05/29/mmseg/</url>
    <content><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>在介绍MMSegmentation之前首先要介绍一下<a href="https://openmmlab.com/">OpenMMLab</a>，源自官方的介绍：</p>
<blockquote>
<p>OpenMMLab（浦视）是上海人工智能实验室的计算机视觉算法开源体系，是深度学习时代全球领域最全面、最具影响力的视觉算法开源项目，全球最大最全的开源计算机视觉算法库，为学术和产业界提供一个可跨方向、结构精良、易复现的统一算法工具库。</p>
<p>OpenMMLab 已经累计开源了超过 30 个算法库，涵盖分类、检测、分割、视频理解等众多研究领域，拥有超过 300 种算法、2,400  多个预训练模型。在 GitHub 上获得超过 72,000 个标星，同时吸引了超过 1,500 名社区开发者参与项目贡献，用户遍及超过 110  个国家和地区，覆盖全国全球顶尖高校、研究机构和企业。</p>
</blockquote>
<span id="more"></span>
<p>总之，OpenMMLab就是一个计算机视觉方向的开源算法库，是为方便开发者进行统一管理、使用而建立的，其包含了非常丰富的开源视觉算法，许多新发表的论文也会基于该平台开源自己的代码；<a href="https://github.com/open-mmlab/mmsegmentation/blob/main/README.md">MMSegmentation</a>是OpenMMLab 项目中的语义分割部分，同样类似地还有物体检测的<a href="https://github.com/open-mmlab/mmdetection">MMDetection</a>等。</p>
<p>本人在安装MMSegmentation过程中遇到了一个问题，在网上也找到了许多关于同样问题的内容，官方给出解决方案极具模糊性，无法帮助本人解决该问题。本人在经过数次的卸载重装后发现了一个简单的解决方案（有点取巧的嫌疑），使得安装完可以正常运行，但不知道后续会有什么其他幺蛾子出现，但最起码目前可以正常运行了，所以在此记录一下；同时，也希望官方可以出具更合理的解决方案。</p>
<h1 id="问题与解决过程"><a href="#问题与解决过程" class="headerlink" title="问题与解决过程"></a>问题与解决过程</h1><h2 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h2><p>按照官方提供的<a href="https://mmsegmentation.readthedocs.io/zh-cn/latest/get_started.html">使用手册</a>，创建conda环境，然后安装PyTorch，注意，这是第一个坑：使用手册推荐的conda安装命令会安装CPU版本的PyTorch，所以此处应该进入<a href="https://pytorch.org/get-started/locally/">PyTorch官网</a>、使用官方推荐的命令进行安装，如Fig.1所示：</p>
<p><img src="/2024/05/29/mmseg/image-20240529204944850.png" alt="image-20240529204944850" title="fig 1"></p>
<p>然后，手册建议安装<a href="https://github.com/open-mmlab/mim">openmim</a>，后续通过MIM命令（类似pip）来安装mmengine、mmcv等：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">pip install -U openmim</span><br><span class="line">mim install mmengine</span><br><span class="line">mim install &quot;mmcv&gt;=2.0.0&quot;</span><br><span class="line">pip/mim install &quot;mmsegmentation&gt;=1.0.0&quot;</span><br></pre></td></tr></table></figure>
<p>到这一部分安装都很顺利，然后验证安装是否成功，手册中也给出了一个推理demo来验证，验证过程就出现了报错：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Traceback (most recent call last):                                                   </span><br><span class="line">  File &quot;demo/image_demo.py&quot;, line 6, in &lt;module&gt;                                     </span><br><span class="line">    from mmseg.apis import inference_model, init_model, show_result_pyplot           </span><br><span class="line">  File &quot;/home/***/codes/python/mmsegmentation/mmseg/__init__.py&quot;, line 61, in &lt;modul</span><br><span class="line"><span class="meta prompt_">e&gt; </span><span class="language-bash"> </span>                                                                                 </span><br><span class="line">    assert (mmcv_min_version &lt;= mmcv_version &lt; mmcv_max_version), \                  </span><br><span class="line">AssertionError: MMCV==2.2.0 is used but incompatible. Please install mmcv&gt;=2.0.0rc4.</span><br></pre></td></tr></table></figure>
<h2 id="解决过程"><a href="#解决过程" class="headerlink" title="解决过程"></a>解决过程</h2><p>在网上搜索该报错后，发现很多网友遇到类似的问题，官方给的解释是mmcv、mmengine、mmsegmentation版本要兼容，具体版本可参考<a href="https://mmsegmentation.readthedocs.io/zh-cn/latest/notes/faq.html">该页面</a>，该页面的主要内容如Fig.2所示：</p>
<p><img src="/2024/05/29/mmseg/image-20240529210109070.png" alt="image-20240529210109070" title="fig 2"></p>
<p>根据官方的安装教程，安装完的软件版本如下所示：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">Package         Version    Source</span><br><span class="line">--------------  ---------  --------------------------------------------</span><br><span class="line">mmcv            2.2.0      https://github.com/open-mmlab/mmcv</span><br><span class="line">mmengine        0.10.4     https://github.com/open-mmlab/mmengine</span><br><span class="line">mmsegmentation  1.2.1      https://github.com/open-mmlab/mmsegmentation</span><br></pre></td></tr></table></figure>
<p>但是Fig.2中给定的版本关系都是&gt;=关系，根据查询的程序安装版本明明是满足&gt;=关系的，但就是会报错。包括官方Github页面Issues中也是一直强调程序版本问题，但就是不知道哪一版本才可以正确对应…</p>
<p>于是乎，开始了程序的卸载与重装。在安装完mmcv==2.1.0之后，该demo跑通了，但跑自己的程序时仍然报错：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">File &quot;inference_manul.py&quot;, line 1, in &lt;module&gt;</span><br><span class="line">    from mmseg.apis import init_model, inference_model, show_result_pyplot</span><br><span class="line">  File &quot;/home/***/codes/python/mmsegmentation/mmseg/__init__.py&quot;, line 61, in &lt;modul</span><br><span class="line"><span class="meta prompt_">e&gt;</span><span class="language-bash"></span></span><br><span class="line"><span class="language-bash">    assert (mmcv_min_version &lt;= mmcv_version &lt; mmcv_max_version), \</span></span><br><span class="line"><span class="language-bash">AssertionError: MMCV==2.1.0 is used but incompatible. Please install mmcv&gt;=2.0.0rc4.</span></span><br></pre></td></tr></table></figure>
<p>这就见了鬼了，明明从<code>mmseg.apis</code> 导入的函数都一样，为什么demo跑通了自己写的程序就是跑不通！后续又开始尝试卸载重装，甚至到了自己瞎猜mmcv版本的情况了：</p>
<p><img src="/2024/05/29/mmseg/image-20240529211155057.png" alt="image-20240529211155057" title="fig 3"></p>
<p>突然间，想到可以查看一下具体报错的代码，于是在ipython中导入<code>mmseg.apis</code> 的函数，查看具体报错内容：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">AssertionError                            Traceback (most recent call last)          </span><br><span class="line">Cell In[<span class="number">1</span>], line <span class="number">1</span>                                                                   </span><br><span class="line">----&gt; <span class="number">1</span> <span class="keyword">from</span> mmseg.apis <span class="keyword">import</span> init_model, inference_model, show_result_pyplot       </span><br><span class="line">                                                                                     </span><br><span class="line">File ~/codes/python/mmsegmentation/mmseg/__init__.py:<span class="number">61</span>                              </span><br><span class="line">     <span class="number">57</span> mmcv_max_version = digit_version(MMCV_MAX)                                   </span><br><span class="line">     <span class="number">58</span> mmcv_version = digit_version(mmcv.__version__)                               </span><br><span class="line">---&gt; <span class="number">61</span> <span class="keyword">assert</span> (mmcv_min_version &lt;= mmcv_version &lt; mmcv_max_version), \              </span><br><span class="line">     <span class="number">62</span>     <span class="string">f&#x27;MMCV==<span class="subst">&#123;mmcv.__version__&#125;</span> is used but incompatible. &#x27;</span> \                 </span><br><span class="line">     <span class="number">63</span>     <span class="string">f&#x27;Please install mmcv&gt;=2.0.0rc4.&#x27;</span>                                        </span><br><span class="line">     <span class="number">65</span> mmengine_min_version = digit_version(MMENGINE_MIN)                           </span><br><span class="line">     <span class="number">66</span> mmengine_max_version = digit_version(MMENGINE_MAX)                           </span><br><span class="line">                                                                                     </span><br><span class="line">AssertionError: MMCV==<span class="number">2.1</span><span class="number">.0</span> <span class="keyword">is</span> used but incompatible. Please install mmcv&gt;=<span class="number">2.0</span><span class="number">.0</span>rc4.</span><br></pre></td></tr></table></figure>
<p>发现原来在__init__.py文件中给定了程序版本限制参数：MMCV_MIN、MMCV_MAX等，那直接在该文档中修改该参数是否可行？于是打开文件，修改为以下内容：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">MMCV_MIN = <span class="string">&#x27;2.0.0rc4&#x27;</span></span><br><span class="line">MMCV_MAX = <span class="string">&#x27;2.2.0&#x27;</span></span><br><span class="line">MMENGINE_MIN = <span class="string">&#x27;0.5.0&#x27;</span></span><br><span class="line">MMENGINE_MAX = <span class="string">&#x27;1.0.0&#x27;</span></span><br></pre></td></tr></table></figure>
<p>然后，就成功跑通了…没想到问题就这么戏剧性地解决了。</p>
<h2 id="问题总结"><a href="#问题总结" class="headerlink" title="问题总结"></a>问题总结</h2><p>根据上述解决过程可以发现本人是通过一种取巧的方式解决了该问题，这同时也说明了官方对于版本管理的混乱，没有一个清晰的兼容版本对应关系，反而在一团乱麻之后修改程序中的数值来解决版本兼容问题，显得有点儿戏了；尽管目前工作良好，但也不知道后续会不会有其他问题出现。所以，还是希望官方后续出具一个清晰的程序版本对应关系吧。</p>
]]></content>
      <categories>
        <category>随记</category>
        <category>问题记录</category>
      </categories>
      <tags>
        <tag>Semantic Segmentation</tag>
        <tag>MMSegmentation</tag>
        <tag>OpenMMLab</tag>
      </tags>
  </entry>
  <entry>
    <title>Object SLAM部署过程</title>
    <url>/2024/01/26/object-slam/</url>
    <content><![CDATA[<p>系统代码包下载地址为<a href="https://github.com/yangliu9527/Object_SLAM.git">github地址</a>，论文为(Liu 等, 2023).</p>
<h2 id="1-ORB-SLAM2基础问题"><a href="#1-ORB-SLAM2基础问题" class="headerlink" title="1 ORB-SLAM2基础问题"></a>1 ORB-SLAM2基础问题</h2><p>该算法是在ORB-SLAM2 的基础上进行改进的，编译过程可参考<a href="https://echo-gh.github.io/2024/01/26/orbslam2-env/">ORB-SLAM2的部署教程</a>，此处不再赘述。除此之外，本人在部署过程中还遇到了其他问题，这里记录一下。</p>
<span id="more"></span>
<h2 id="2-其他问题"><a href="#2-其他问题" class="headerlink" title="2 其他问题"></a>2 其他问题</h2><ul>
<li>PCL 报错：</li>
</ul>
<blockquote>
<p>error: #error PCL requires C++14 or above</p>
</blockquote>
<p>参考这篇<a href="https://blog.csdn.net/handily_1/article/details/122421305">文章</a>解释，使用C++14编译器，更改主目录下的CMakeLists.txt：</p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line">### 修改前</span><br><span class="line"># Check C++<span class="number">11</span> <span class="keyword">or</span> C++<span class="number">0</span><span class="function">x support</span></span><br><span class="line"><span class="function"><span class="title">include</span><span class="params">(CheckCXXCompilerFlag)</span></span></span><br><span class="line"><span class="function"><span class="title">CHECK_CXX_COMPILER_FLAG</span><span class="params">(<span class="string">&quot;-std=c++11&quot;</span> COMPILER_SUPPORTS_CXX11)</span></span></span><br><span class="line"><span class="function"><span class="title">CHECK_CXX_COMPILER_FLAG</span><span class="params">(<span class="string">&quot;-std=c++0x&quot;</span> COMPILER_SUPPORTS_CXX0X)</span></span></span><br><span class="line"><span class="function"><span class="title">if</span><span class="params">(COMPILER_SUPPORTS_CXX11)</span></span></span><br><span class="line"><span class="function">   <span class="title">set</span><span class="params">(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++11&quot;</span>)</span></span></span><br><span class="line"><span class="function">   <span class="title">add_definitions</span><span class="params">(-DCOMPILEDWITHC11)</span></span></span><br><span class="line"><span class="function">   <span class="title">message</span><span class="params">(STATUS <span class="string">&quot;Using flag -std=c++11.&quot;</span>)</span></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function"></span></span><br><span class="line"><span class="function">### 修改后</span></span><br><span class="line"><span class="function"># Check C++11 <span class="keyword">or</span> C++0x support</span></span><br><span class="line"><span class="function"><span class="title">include</span><span class="params">(CheckCXXCompilerFlag)</span></span></span><br><span class="line"><span class="function"><span class="title">CHECK_CXX_COMPILER_FLAG</span><span class="params">(<span class="string">&quot;-std=c++11&quot;</span> COMPILER_SUPPORTS_CXX11)</span></span></span><br><span class="line"><span class="function"><span class="title">CHECK_CXX_COMPILER_FLAG</span><span class="params">(<span class="string">&quot;-std=c++0x&quot;</span> COMPILER_SUPPORTS_CXX0X)</span></span></span><br><span class="line"><span class="function"><span class="title">if</span><span class="params">(COMPILER_SUPPORTS_CXX11)</span></span></span><br><span class="line"><span class="function">   <span class="title">set</span><span class="params">(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; -std=c++14&quot;</span>)</span></span></span><br><span class="line"><span class="function">   <span class="title">add_definitions</span><span class="params">(-DCOMPILEDWITHC11)</span></span></span><br><span class="line"><span class="function">   <span class="title">message</span><span class="params">(STATUS <span class="string">&quot;Using flag -std=c++14.&quot;</span>)</span></span></span><br></pre></td></tr></table></figure>
<p>修改完之后不再报错，可正常编译。</p>
<ul>
<li>针对不同的输入图片，需要设置不同的通道变换方式，否则会报与数据通道相关的错误，如下图所示；修改方法为修改frame.cc 代码 392行附近的内容：</li>
</ul>
<p><img src="/2024/01/26/object-slam/err1.png" alt="err1" title="err1"></p>
<figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="comment">//* for gray pictures (e.g. KITTI-odometry dataset)</span></span><br><span class="line">cv::Mat imRGB_1;</span><br><span class="line">cv::<span class="built_in">cvtColor</span>(imRGB.<span class="built_in">clone</span>(), imRGB_1, CV_GRAY2BGR);</span><br><span class="line">cv::<span class="built_in">cvtColor</span>(imRGB_1.<span class="built_in">clone</span>(), Img_HSV, CV_BGR2HSV);</span><br><span class="line"></span><br><span class="line"><span class="comment">//* for RGB pictures (e.g. TUM dataset)</span></span><br><span class="line"><span class="comment">// cv::cvtColor(imRGB.clone(), Img_HSV, CV_BGR2HSV);</span></span><br></pre></td></tr></table></figure>
<ul>
<li>在VS Code 里对程序进行调试运行时，可能会报错：</li>
</ul>
<p><img src="/2024/01/26/object-slam/err2.png" alt="err2"></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 只有在VS Code 中调试才会出现该错误，解决办法是在VS Code 中unset GTK_PATH即可：</span></span><br><span class="line"><span class="built_in">unset</span> GTK_PATH</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>C++</tag>
        <tag>Pose Optimization</tag>
        <tag>Object SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 ORB-SLAM_A Versatile and Accurate Monocular SLAM System</title>
    <url>/2024/01/29/orb-slam/</url>
    <content><![CDATA[<p>Mur-Artal, Raul, J. M. M. Montiel, and Juan D. Tardos. “ORB-SLAM: A Versatile and Accurate Monocular SLAM System.” <em>IEEE Transactions on Robotics</em> 31, no. 5 (October 2015): 1147–63. <a href="https://doi.org/10.1109/TRO.2015.2463671">https://doi.org/10.1109/TRO.2015.2463671</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献如下：</p>
<ol>
<li>对所有任务使用相同的特性：tracking, mapping, relocalization and loop closing, 这使得我们的系统更加<strong>高效</strong>、<strong>简单</strong>且<strong>可靠</strong>；使用ORB 特征，可在CPU 上实现实时运行，且具有较好的<strong>视角、光照不变性</strong>；</li>
<li>利用<strong>共视图</strong>使得跟踪与制图都聚焦于一个局部共视区域，从而实现在大规模环境中地<strong>实时</strong>操作，可不受全局地图尺寸地影响；</li>
<li>基于<strong>位姿图优化</strong>的实时回环检测（作者称其为 Essential Graph），其构建于系统维护的spanning tree、回环连接以及共视图中的线；</li>
<li>基于良好的视角和光照不变性实现的<strong>实时相机重定位</strong>，可在跟踪失败时进行<strong>重初始化</strong>，并增强了<strong>地图的重用性</strong>；</li>
<li>提出一个基于模型选择的自动鲁棒的<strong>初始化程序</strong>，可为平面和非平面场景创建一个<strong>初始化地图</strong>；</li>
<li>对于地图点和关键帧采取“适者生存” <strong>survival of the fittest</strong> 策略，在生成时非常宽松，而在剔除时非常严格，该策略提高了跟踪的鲁棒性，且由于冗余的关键帧被舍弃了，从而增强了 lifelong operation。</li>
</ol>
<span id="more"></span>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3 System Overview"></a>3 System Overview</h1><h2 id="3-1-Feature-Choice"><a href="#3-1-Feature-Choice" class="headerlink" title="3.1 Feature Choice"></a>3.1 Feature Choice</h2><p>本系统的一个重要设计是：制图与跟踪所使用的<strong>同样特性</strong>会用于<strong>地点重识别</strong>，来进行帧级的重定位和回环检测。</p>
<h2 id="3-2-Three-Threads-Tracking-Local-Mapping-and-Loop-Closing"><a href="#3-2-Three-Threads-Tracking-Local-Mapping-and-Loop-Closing" class="headerlink" title="3.2 Three Threads: Tracking, Local Mapping, and Loop Closing"></a>3.2 Three Threads: Tracking, Local Mapping, and Loop Closing</h2><p>系统整体架构如Fig. 1所示：</p>
<p><img src="/2024/01/29/orb-slam/overview.png" alt="overview" title="overview"></p>
<h2 id="3-3-Map-Points-Keyframes-and-Their-Selection"><a href="#3-3-Map-Points-Keyframes-and-Their-Selection" class="headerlink" title="3.3 Map Points, Keyframes, and Their Selection"></a>3.3 Map Points, Keyframes, and Their Selection</h2><p>每个地图点 $p_i$ 存储以下信息：</p>
<ol>
<li>世界坐标系中的3D 位置信息 $\mathbf{X}_{w, i}$ ；</li>
<li>视角朝向 $\mathbf{n}_i$，是所有视角方向（观测到该点的关键帧的相机光心与该点的连线）的平均单位向量；</li>
<li>一个代表性的 ORB 描述子 $\mathbf{D}_i$ ，利用所有观测到该点的关键帧中的描述子计算一个最小汉明距离的ORB 描述子；</li>
<li>根据ORB 特征的尺度不变约束，计算该点可被观测到的最大距离和最小距离 $d_{max}, d_{min}$ 。</li>
</ol>
<p>每个关键帧 $K_i$ 存储以下信息：</p>
<ol>
<li>相机<strong>位姿</strong> $\mathbf{T}_{iw}$ ，是从世界坐标系到相机坐标系的刚体转换关系；</li>
<li>相机<strong>内参</strong>，包含焦距和光心；</li>
<li>该帧图片中提取的<strong>所有ORB 特征</strong>，是否与地图点关联。</li>
</ol>
<h2 id="3-4-Covisibility-Graph-and-Essential-Graph"><a href="#3-4-Covisibility-Graph-and-Essential-Graph" class="headerlink" title="3.4 Covisibility Graph and Essential Graph"></a>3.4 Covisibility Graph and Essential Graph</h2><p>关键帧之间的<strong>共视信息</strong>对于本系统的许多任务而言至关重要，该共视信息使用<strong>无向加权图</strong>来表示，图中每个节点代表一个关键帧，如果两个关键帧之间的共视地图点超过15个，则进行节点间的连线，并使用共视地图点的数量来表示权重参数 $\theta$ 。</p>
<p>作者使用<strong>位姿图优化</strong>对回环检测到的位姿进行整体优化，为了避免包含共视图中所有的边（过于稠密），作者提出 <strong>Essential Graph</strong>只保留<strong>所有的节点与部分边</strong>，仍然可以保留强壮的网络结构来产生精确的结果。</p>
<p>系统会从初始帧开始构建一个<strong>增量式spanning tree</strong>，提供一个具有最小边数量的共视图的<strong>连接子图</strong>，当一个新的关键帧被插入时，被包含在该树中，并和与其<strong>有最多共视点</strong>的关键帧连接；当某个关键帧被剔除后，会更新相应的受影响的连线。</p>
<p>而Essential Graph 包含<strong>spanning tree</strong>、<strong>共视图</strong>中共视点数超过100个的边，以及<strong>回环检测边</strong>，从而形成一个强壮的相机轨迹网络。</p>
<p><img src="/2024/01/29/orb-slam/reconstruction.png" alt="reconstruction" title="reconstruciton and graphs"></p>
<h2 id="3-5-Bags-of-Words-Place-Recognition"><a href="#3-5-Bags-of-Words-Place-Recognition" class="headerlink" title="3.5 Bags of Words Place Recognition"></a>3.5 Bags of Words Place Recognition</h2><p>系统集成了一个基于DBoW2 的词袋库地点重识别模块来进行回环检测和重定位，本系统创建一个增量式的数据集以进行查询和更新。</p>
<h1 id="4-Automatic-Map-Initialization"><a href="#4-Automatic-Map-Initialization" class="headerlink" title="4 Automatic Map Initialization"></a>4 Automatic Map Initialization</h1><p>地图初始化的目标是计算两帧图片之间的<strong>相对位姿</strong>来<strong>三角化</strong>一组地图点，作者提出并行计算两种几何模型：平面场景下的<strong>单应矩阵（homography）</strong>，及非平面场景下的<strong>基础矩阵</strong>。本系统的地图初始化步骤如下所示：</p>
<ol>
<li><strong>寻找初始关系：</strong>提取当前帧中的ORB 特征，并寻找与参考帧之间的匹配，如果没有找到足够的匹配，重置参考帧；</li>
<li><strong>并行计算两个模型：</strong>分别计算单应矩阵和基础矩阵，并在迭代中计算相应的得分，最后保留得分最高的矩阵；</li>
<li><strong>模型选择：</strong>若场景属于平面、近似平面或者存在较小的视差，可选用单应矩阵；如果是有足够视差的非平面场景，应当选用基础矩阵；</li>
<li><strong>从移动恢复（motion recovery）中得到动作和结构：</strong>对于单应矩阵，作者直接对8组解进行三角化，并检查是否存在一个解使得大部分点位于相机前部且有着较低的重投影误差，如果不存在一个具有明显优势的解，则返回第一步重新开始初始化，该方法被认为是本系统鲁棒性的关键所在；对于基础矩阵，利用内参计算出本质矩阵，利用奇异值分解恢复出四个运动假设，然后采用与单应矩阵相同的方法进行求解；</li>
<li><strong>BA：</strong>最后，利用<strong>full BA</strong> 来优化初始重建。</li>
</ol>
<p>一个初始化的例子如下图所示，PTAM 和 LSD-SLAM 初始化一个平面上的所有点，而本系统会等到有足够的视差后利用基础矩阵才进行正确的初始化。</p>
<p><img src="/2024/01/29/orb-slam/initialization.png" alt="initiallization" title="initialization"></p>
<h1 id="5-Tracking"><a href="#5-Tracking" class="headerlink" title="5 Tracking"></a>5 Tracking</h1><h2 id="5-2-Initial-Pose-Estimation-From-Previous-Frame"><a href="#5-2-Initial-Pose-Estimation-From-Previous-Frame" class="headerlink" title="5.2 Initial Pose Estimation From Previous Frame"></a>5.2 Initial Pose Estimation From Previous Frame</h2><p>如果上一帧跟踪成功，本系统使用一个<strong>固定速度运动模型</strong>来预测相机位姿，并对上一帧观测到的地图点进行一个<strong>引导式搜索</strong>，然后利用寻找到的关联对位姿进行优化。</p>
<h2 id="5-3-Initial-Pose-Estimation-via-Global-Relocalization"><a href="#5-3-Initial-Pose-Estimation-via-Global-Relocalization" class="headerlink" title="5.3 Initial Pose Estimation via Global Relocalization"></a>5.3 Initial Pose Estimation via Global Relocalization</h2><p>如果跟踪失败，则将该帧图片转换为<strong>词袋</strong>并进行词袋库搜索以实现<strong>全局重定位</strong>，对每个候选关键帧进行RANSAC 迭代并使用PnP 计算相机的位姿；若找到具有足够内点的相机位姿，则根据匹配关键帧的地图点搜寻更多的匹配以进行位姿优化。</p>
<h2 id="5-4-Track-Local-Map"><a href="#5-4-Track-Local-Map" class="headerlink" title="5.4 Track Local Map"></a>5.4 Track Local Map</h2><p>获取相机位姿的估计和一组初始化特征匹配后，将<strong>局部地图</strong>投影至当前帧中来搜寻更多的匹配。该局部地图包含与当前帧有匹配特征点的关键帧 $\mathcal{K}_1$ ，以及在共视图中 $\mathcal{K}_1$ 的邻接关键帧 $\mathcal{K}_2$ ，关键帧 $\mathcal{K}_1$ 、$\mathcal{K}_2$ 中的所有点进行以下搜索策略：</p>
<ol>
<li>将点投影至当前帧中，舍弃掉超出图片界限的点；</li>
<li>比较当前帧中点和地图中点的视角朝向，舍弃小于60读夹角的点；</li>
<li>计算地图点到相机光心的距离，舍弃掉超范围的点 $d \notin [d_{min}, d_{max}]$ ；</li>
<li>计算尺度信息 $d/d_{min}$ ；</li>
<li>将地图点的代表性描述子与当前帧中未匹配的ORB 特征进行比较，寻找地图点的最佳匹配。</li>
</ol>
<p>最终利用所有匹配的点对相机位姿进行优化。</p>
<h2 id="5-5-New-Frame-Decision"><a href="#5-5-New-Frame-Decision" class="headerlink" title="5.5 New Frame Decision"></a>5.5 New Frame Decision</h2><p><strong>新的关键帧</strong>需满足以下所有的要求：</p>
<ol>
<li>距离上一次全局重定位不少于20帧（为了更好地重定位）；</li>
<li>局部制图线程空闲，或者距离上次插入关键帧已超过20帧；</li>
<li>当前帧至少跟踪了50个地图点（为了更好地跟踪）；</li>
<li>当前帧比参考帧少跟踪90%的点（限制最小的视角变化）。</li>
</ol>
<h1 id="6-Local-Mapping"><a href="#6-Local-Mapping" class="headerlink" title="6 Local Mapping"></a>6 Local Mapping</h1><h2 id="6-1-Keyframe-Insertion"><a href="#6-1-Keyframe-Insertion" class="headerlink" title="6.1 Keyframe Insertion"></a>6.1 Keyframe Insertion</h2><p>每当插入一个新的关键帧，进行一下操作：</p>
<ul>
<li>更新<strong>共视图</strong>，增加新的节点和边；</li>
<li>更新<strong>spanning tree</strong>；</li>
<li>计算该关键帧的<strong>词袋表示</strong>。</li>
</ul>
<h2 id="6-2-Recent-Map-Points-Culling"><a href="#6-2-Recent-Map-Points-Culling" class="headerlink" title="6.2 Recent Map Points Culling"></a>6.2 Recent Map Points Culling</h2><p>地图点要想保留在地图中，需要在创建后的前三个关键帧中经过严格的测试：</p>
<ol>
<li>跟踪线程需要在其被预测可见的关键帧中至少有25%的比例被观测到；</li>
<li>若该点创建后已经过了一个关键帧，那么至少需要被三个关键帧观测到。</li>
</ol>
<h2 id="6-3-New-Map-Point-Creation"><a href="#6-3-New-Map-Point-Creation" class="headerlink" title="6.3 New Map Point Creation"></a>6.3 New Map Point Creation</h2><p>新的地图点通过共视图中的相连关键帧对ORB 特征点进行三角化来创建。</p>
<h2 id="6-4-Local-Bundle-Adjustment"><a href="#6-4-Local-Bundle-Adjustment" class="headerlink" title="6.4 Local Bundle Adjustment"></a>6.4 Local Bundle Adjustment</h2><p>局部BA 的优化对象为：当前关键帧，共视图中与当前关键帧相连的所有关键帧，以及这些关键帧中的所有地图点。至于其余可观测到这些地图点但是未与当前关键帧相连的那些关键帧也会参与到优化过程中，但是其自身保持固定。</p>
<h2 id="6-5-Local-Keyframe-Culling"><a href="#6-5-Local-Keyframe-Culling" class="headerlink" title="6.5 Local Keyframe Culling"></a>6.5 Local Keyframe Culling</h2><p>之所以要控制关键帧的数量，是因为BA 的计算复杂度与该数量正相关，而且在lifelong operation 中，同一场景中的关键帧数量不能无限制增长。</p>
<p>本系统根据以下准则舍弃掉关键帧：其90%的地图点在其他至少三个关键帧中以相同或更好的尺度被观测到。这个尺度条件确保了地图点可以保留那些自身被最好精度观测到的关键帧。</p>
<h1 id="7-Loop-Closing"><a href="#7-Loop-Closing" class="headerlink" title="7 Loop Closing"></a>7 Loop Closing</h1><h2 id="7-1-Loop-Candidates-Detection"><a href="#7-1-Loop-Candidates-Detection" class="headerlink" title="7.1 Loop Candidates Detection"></a>7.1 Loop Candidates Detection</h2><p>首先，将当前帧和共视图中与其相连的所有帧（共视点数量不少于30个）计算BoW 向量的相似性，得到一个最低的阈值 $s_{min}$ ；然后再词袋库中进行匹配，舍弃掉低于该阈值的关键帧，此外，还需舍弃掉与当前帧直接相连的关键帧；只有当检测到连续三个一致的回环候选帧（在共视图中是相连的），才判定存在回环。</p>
<h2 id="7-2-Compute-the-Similarity-Transformation"><a href="#7-2-Compute-the-Similarity-Transformation" class="headerlink" title="7.2 Compute the Similarity Transformation"></a>7.2 Compute the Similarity Transformation</h2><p>在单目SLAM 中，地图有<strong>7自由度</strong>：旋转、平移，以及一个尺度因子。因此，为了闭合一个回环，需要计算一个从当前帧到回环帧之间的<strong>相似转换</strong>，以获取回环的累积误差；同时，该相似度计算也作为该回环的<strong>几何验证</strong>。</p>
<p>首先计算当前帧与回环候选帧之间的<strong>地图点关联</strong>，然后为每个候选回环帧进行RANSAC ，若获取足够多的内点，则进行优化并进一步获取更多的关联，并进一步进行优化，此时若有足够的内点数量支持该相似度，则确定回环。</p>
<h2 id="7-3-Loop-Fusion"><a href="#7-3-Loop-Fusion" class="headerlink" title="7.3 Loop Fusion"></a>7.3 Loop Fusion</h2><p>回环矫正的第一步是将<strong>重复的地图点</strong>进行融合，并在<strong>共视图</strong>中插入新的边以完成回环闭合。使用计算得到的<strong>相似转换</strong>来矫正当前关键帧的位姿，并将该矫正传递至与其邻接的关键帧，使得回环两侧进行<strong>对齐</strong>。</p>
<h2 id="7-4-Essential-Graph-Optimization"><a href="#7-4-Essential-Graph-Optimization" class="headerlink" title="7.4 Essential Graph Optimization"></a>7.4 Essential Graph Optimization</h2><p>利用essential graph 将回环闭合误差分布至整个图上以进行优化；利用<strong>相似转换</strong>进行优化来矫正尺度偏移。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>ORB-SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>C++</tag>
        <tag>ORB-SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 ORB-SLAM2_An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras</title>
    <url>/2024/01/29/orb-slam2/</url>
    <content><![CDATA[<p>Mur-Artal, Raul, and Juan D. Tardos. “ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras.” <em>IEEE Transactions on Robotics</em> 33, no. 5 (October 2017): 1255–62. <a href="https://doi.org/10.1109/TRO.2017.2705103">https://doi.org/10.1109/TRO.2017.2705103</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ol>
<li>第一个适用于<strong>单目</strong>、<strong>双目</strong>以及<strong>RGB-D 相机</strong>的开源SLAM 系统，该系统包含回环检测、重定位以及地图重用；</li>
<li>本系统运行RGB-D 的结果证明：使用<strong>BA</strong> 可以实现比基于ICP 或者光度深度误差最小化的SOTA 方法更高的精度；</li>
<li>通过使用<strong>近远立体点和单目观测</strong>，本系统运行双目的结果要比直接双目SLAM 的 SOTA 算法精度更高；</li>
<li>提出了一种关闭制图功能情况下，有效<strong>重使用地图</strong>的轻量级定位模式。</li>
</ol>
<span id="more"></span>
<p><img src="/2024/01/29/orb-slam2/fig1.png" alt="fig1" title="fig 1"></p>
<h1 id="3-ORB-SLAM2"><a href="#3-ORB-SLAM2" class="headerlink" title="3 ORB-SLAM2"></a>3 ORB-SLAM2</h1><p>ORB-SLAM2 的整体架构如下所示：</p>
<p><img src="/2024/01/29/orb-slam2/fig2.png" alt="fig2" title="fig 2"></p>
<p>系统包含三个主要的并行线程：</p>
<ol>
<li><strong>跟踪线程：</strong>寻找与<strong>局部地图</strong>相匹配的特征点，利用<strong>motion-only BA</strong> <strong>最小化重投影误差</strong>，来解算<strong>每帧图片</strong>对应的相机位姿；</li>
<li><strong>局部制图：</strong>管理<strong>局部地图</strong>，并使用<strong>局部BA</strong> 对其进行优化；</li>
<li><strong>回环检测：</strong>检测大回环，并使用<strong>位姿图优化</strong>来消除累积漂移；然后开启第四个线程，进行<strong>全局BA</strong> 优化解算地图与位姿的最优解。</li>
</ol>
<p>本系统还嵌入了一个基于<strong>DBoW2</strong> 的地点重识别模块进行<strong>重定位</strong>，在跟踪失败或利用现有地图进行重初始化时使用；本系统维护一个<strong>共视图</strong>，来关联任意两个具有共同观测特征点的关键帧，并使用一个<strong>最小化spanning tree</strong> 来连接所有的关键帧；这些图结构方便恢复关键帧的<strong>局部窗口</strong>以进行局部的跟踪与制图，并为回环检测中的位姿图优化提供结构。</p>
<h2 id="3-1-Monocular-Close-Stereo-and-Far-Stereo-Keypoints"><a href="#3-1-Monocular-Close-Stereo-and-Far-Stereo-Keypoints" class="headerlink" title="3.1 Monocular, Close Stereo, and Far Stereo Keypoints"></a>3.1 Monocular, Close Stereo, and Far Stereo Keypoints</h2><p>本系统经过如图Fig. 2（b）的<strong>图片预处理操作</strong>，提取出关键点的特征，系统后续的操作均是基于这些<strong>特征点（立体关键点和单目关键点）</strong>的，实现独立于所使用的传感器类型。后续的操作均基于立体关键点和单目关键点。</p>
<p><strong>立体关键点</strong>使用三个坐标进行定义：$\mathbf{x}_s = (u_L, v_L, u_R)$ ，其中 $(u_L, v_L)$ 是特征点在左边图片中的坐标，$u_R$ 是特征点在右边图片的水平座标。对于RGB-D 相机，作者将深度值 d 转化为一个<strong>虚拟的右图坐标</strong>：</p>
<p><img src="/2024/01/29/orb-slam2/f1.png" alt="f1" title="formula 1"></p>
<p>作者定义，如果一个关键点的<strong>深度小于基线长度的40倍</strong>，则被视为近点，否则视为远点。对于近点，可以使用一帧图片进行安全的三角化，因为其深度信息得到了准确估计，可以提供相应的<strong>尺度、平移和旋转信息</strong>；而对于远点，可以提供<strong>准确的旋转信息</strong>，但是尺度与平移信息较不可靠，只对多视角观测的远点进行三角化。</p>
<p><strong>单目关键点</strong>使用左图的两个坐标进行定义 $\mathbf{x}_m = (u_L, v_L)$ ，是针对那些立体匹配失败或RGB-D 深度参数不可靠的点；这些点只通过多视角观测进行三角化，且不提供尺度信息，但会参与旋转与平移估计的解算。</p>
<h2 id="3-2-System-Bootstrapping"><a href="#3-2-System-Bootstrapping" class="headerlink" title="3.2 System Bootstrapping"></a>3.2 System Bootstrapping</h2><p>使用立体相机或RGB-D 相机的一个主要优势在于：可仅使用一帧图片获取<strong>深度信息</strong>，而不需要单目相机的动作初始化操作。系统启动后，使用第一帧作为关键帧，将其位姿定为<strong>原点</strong>，并利用所有的<strong>立体关键点</strong>创建一个初始地图。</p>
<h2 id="3-3-Bundle-Adjustment-with-Monocular-and-Stereo-Constraints"><a href="#3-3-Bundle-Adjustment-with-Monocular-and-Stereo-Constraints" class="headerlink" title="3.3 Bundle Adjustment with Monocular and Stereo Constraints"></a>3.3 Bundle Adjustment with Monocular and Stereo Constraints</h2><p>BA 在本系统中的应用：</p>
<ul>
<li>在跟踪线程中优化相机位姿（motion-only BA）</li>
<li>在局部制图线程中优化局部窗口内的关键帧和特征点（local BA）</li>
<li>回环检测之后优化所有的关键帧与特征点（full BA）</li>
</ul>
<h3 id="Motion-only-BA"><a href="#Motion-only-BA" class="headerlink" title="Motion-only BA"></a>Motion-only BA</h3><p>对关键点 $\mathbf{x}_{(.)}^i$ （包括单目点 $\mathbf{x}_m^i \in \mathbb{R}^2$ 和立体点 $\mathbf{x}_s^i \in \mathbb{R}^3$ ）进行最小化重投影误差，其中 $i\in \mathcal{X}$ 为所有匹配点集合：</p>
<p><img src="/2024/01/29/orb-slam2/f2.png" alt="f2" title="formula 2"></p>
<p>其中，$\rho$ 是<strong>鲁棒Huber 损失函数</strong>，$\sum$ 是关键点尺度参数对应的协方差矩阵。单目点和立体点的投影矩阵如下所示：</p>
<p><img src="/2024/01/29/orb-slam2/f3.png" alt="f3" title="formula 3"></p>
<h3 id="Local-BA"><a href="#Local-BA" class="headerlink" title="Local BA"></a>Local BA</h3><p><img src="/2024/01/29/orb-slam2/f4.png" alt="f4" title="formula 4"></p>
<p>其中，$\mathcal{K}_L$ 为一组共视关键帧；$\mathcal{P}_L$ 为这些共视关键帧中的所有点；至于其他观测到 $\mathcal{P}_L$ 中的点且不属于 $\mathcal{K}_L$ 的关键帧 $\mathcal{K}_F$ ，会参与损失函数的构建，但是在优化中<strong>保持固定</strong>；$\mathcal{X}_k$ 表示 $\mathcal{P}_L$ 中与关键帧 k 匹配的点列表。</p>
<h3 id="Full-BA"><a href="#Full-BA" class="headerlink" title="Full BA"></a>Full BA</h3><p>是一种局部BA 的特殊情况，除了<strong>初始关键帧</strong>是固定的，地图中其余的所有关键帧和点都参与优化过程。</p>
<h2 id="3-4-Loop-Closing-and-Full-BA"><a href="#3-4-Loop-Closing-and-Full-BA" class="headerlink" title="3.4 Loop Closing and Full BA"></a>3.4 Loop Closing and Full BA</h2><p>回环检测包含两步：</p>
<ol>
<li>回环的检测与验证；</li>
<li>通过位姿图优化来矫正回环。</li>
</ol>
<h2 id="3-5-Keyframe-Insertion"><a href="#3-5-Keyframe-Insertion" class="headerlink" title="3.5 Keyframe Insertion"></a>3.5 Keyframe Insertion</h2><p>本系统遵循ORB-SLAM 的关键帧插入策略，此外，基于立体远近点创建了一个新的关键帧挑选策略：如果跟踪的近点数量低于 $\tau_t = 100$，且可新增近点数量大于 $\tau_c = 70$ 时，将其作为新的关键帧进行插入。</p>
<p><img src="/2024/01/29/orb-slam2/fig3.png" alt="fig3" title="fig 3"></p>
<h2 id="3-6-Localization-Mode"><a href="#3-6-Localization-Mode" class="headerlink" title="3.6 Localization Mode"></a>3.6 Localization Mode</h2><p>本系统引入了一个<strong>定位模式</strong>，可在已经制图的区域进行<strong>轻量级长期定位</strong>。该过程利用<strong>视觉里程计匹配点</strong>和<strong>地图匹配点</strong>进行定位，其中视觉里程计匹配是基于当前帧的ORB 和历史帧创建的3D 点之间进行的，这些匹配可在无地图区域进行定位，但会存在累积漂移；而地图点匹配会得到与地图无偏的定位结果。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>ORB-SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>C++</tag>
        <tag>ORB-SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 ORB-SLAM3_An Accurate Open-Source Library for Visual, Visual–Inertial, and Multimap SLAM</title>
    <url>/2024/04/08/orb-slam3/</url>
    <content><![CDATA[<p>Campos, Carlos, Richard Elvira, Juan J. Gómez Rodríguez, José M. M. Montiel, and Juan D. Tardós. “ORB-SLAM3: An Accurate Open-Source Library for Visual, Visual–Inertial, and Multimap SLAM.” <em>IEEE Transactions on Robotics</em> 37, no. 6 (December 2021): 1874–90. <a href="https://doi.org/10.1109/TRO.2021.3075644">https://doi.org/10.1109/TRO.2021.3075644</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>ORB-SLAM3 的主要创新点包括：</p>
<ol>
<li><strong>一个单目、双目视觉-惯性SLAM 系统</strong>。该系统完全依赖MAP 最大后验估计，即使是在IMU 初始化阶段。</li>
<li><strong>增强型地点识别技术</strong>。现有的许多SLAM、VO 系统使用DBoW2 进行地点识别，但DBoW2 在检查几何一致性之前需要进行<strong>时间一致性检测</strong>，即匹配连续三个关键帧，这增加了地点识别的代价，导致回环检测或者地图重用的<strong>速度很慢</strong>。针对于此，作者提出了一个新颖的地点识别算法：对候选关键帧首先进行<strong>几何一致性检测</strong>，然后与三个共视关键帧进行<strong>局部一致性检测 *local consistency*</strong>。</li>
<li><strong>ORB-SLAM Atlas</strong>。第一个可以处理单目和双目视觉&amp;视觉-惯性系统的完整多地图SLAM 系统。该地图集可表示一组不连续的地图，并顺滑地使用所有的地图操作，包括：地点识别、相机重定位、回环检测以及精确地图融合等。</li>
<li><strong>抽象的相机参数化表示</strong>使得SLAM 系统可以兼容不同映射模型的相机，如针孔相机、鱼眼相机等。</li>
</ol>
<span id="more"></span>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3 System Overview"></a>3 System Overview</h1><p>ORB-SLAM3 整体架构如Fig.1所示，较ORB-SLAM2 有以下特性：</p>
<ol>
<li><strong>Atlas</strong>包含一组不连续的地图，包含活动地图和非活动地图；该系统建立了一个独特的关键帧DBoW2 数据库用于<strong>重定位、回环检测</strong>和<strong>地图融合</strong>。</li>
<li><strong>跟踪线程</strong>与实时地图相结合，利用最小化重投影误差来计算相机的位姿；在VI （visual-inertial）模式下，通过将<strong>惯导残差</strong>包含进优化过程实现对载体速度、IMU 偏差的计算。当跟踪失败时，跟踪线程会在地图集中所有的地图上进行<strong>重定位</strong>。</li>
<li><strong>局部制图线程</strong>在VI 模式下会对IMU 参数进行<strong>初始化及优化</strong>。</li>
<li><strong>回环与地图融合线程</strong>以关键帧的速率检测活动地图与整个地图集之间的共同区域，以进行<strong>回环检测</strong>和<strong>地图融合</strong>。</li>
</ol>
<p><img src="/2024/04/08/orb-slam3/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="4-Camera-Model"><a href="#4-Camera-Model" class="headerlink" title="4 Camera Model"></a>4 Camera Model</h1><p>作者的目标是通过提取所有与相机模型相关的属性和函数（投影方程、Jacobian 等）来实现对<strong>相机模型的抽象化</strong>，这使得该系统可以兼容<strong>任意类型</strong>的相机。</p>
<p>经典的SLAM 算法默认使用<strong>针孔相机模型</strong>，会对<strong>整幅图片</strong>或<strong>特征坐标</strong>进行矫正，使得该系统运行在一个理想的平面视网膜上。然而，该方法不适用于具有超大FOV 的鱼眼相机，鱼眼相机获取的图片中，边缘处的物体会被放大，中间部分的物体不清晰，无法使用<strong>图片矫正</strong>进行处理；而对<strong>特征坐标进行矫正</strong>需要FOV 小于180°，而且与经典SLAM 算法的特征重投影误差在整个平面上服从<strong>均匀分布的假设</strong>不符。而对鱼眼图片进行裁剪又会失去其大视角的优势。</p>
<h2 id="4-1-Relocalization"><a href="#4-1-Relocalization" class="headerlink" title="4.1 Relocalization"></a>4.1 Relocalization</h2><p>ORB-SLAM 是通过ePnP 算法来解决重定位问题的，这个算法是基于校准过的针孔相机模型，因此，需要对该算法进行修改以适应不同的相机模型，这里，作者使采用了<strong>最大似然PnP 算法</strong>来解决该问题。</p>
<h2 id="4-2-Nonrectified-Stereo-SLAM"><a href="#4-2-Nonrectified-Stereo-SLAM" class="headerlink" title="4.2 Nonrectified Stereo SLAM"></a>4.2 Nonrectified Stereo SLAM</h2><p>双目相机默认使用针孔相机投影模型进行矫正，并认为两幅图片对齐至<strong>水平极线</strong>，从而对两幅图中的特征在<strong>同一行中</strong>进行匹配，但这些假设过于严格，且不适用于鱼眼相机。因此，作者在本系统中不依赖于图片矫正，而是将双目相机是做两个独立的单目相机，并满足以下特性：</p>
<ol>
<li>固定的位姿转换关系 SE(3)；</li>
<li>两幅图中有公共的观测区域。</li>
</ol>
<h1 id="5-Visual-Inertial-SLAM"><a href="#5-Visual-Inertial-SLAM" class="headerlink" title="5 Visual-Inertial SLAM"></a>5 Visual-Inertial SLAM</h1><h2 id="5-1-Fundamentals"><a href="#5-1-Fundamentals" class="headerlink" title="5.1 Fundamentals"></a>5.1 Fundamentals</h2><p>在VI 中，代求的参数较纯视觉SLAM 更多，除了位姿 $\mathbf{T}_i = [\mathbf{R}_i, \mathbf{p}_i]$ 外，还包括速度 $\mathbf{v}_i$ ，陀螺仪和加速度计偏差 $\mathbf{b}_i^g, \mathbf{b}_i^a$ ，即状态向量为：</p>
<p><img src="/2024/04/08/orb-slam3/f1.png" alt="f1" title="formula 1"></p>
<p>对于VI SLAM，对连续帧之间的IMU 观测进行<strong>预积分</strong>，获取预积分得到的旋转、速度和位置观测，表示为 $\Delta \mathbf{R}_{i, i+1}， \Delta \mathbf{v}_{i, i+1}，\Delta \mathbf{p}_{i, i+1}$ ，以及相应的协方差矩阵 $\sum_{\mathcal{I}_{i, i+1}}$ 。给定这些预计分项和状态 $\mathcal{S}_i, \mathcal{S}_{i+1}$ ，定义如下所示的<strong>惯导残差</strong> $\mathbf{r}_{\mathcal{I}_{i, i+1}}$ ：</p>
<p><img src="/2024/04/08/orb-slam3/f2.png" alt="f2" title="formula 2"></p>
<p>其中， $Log: SO(3) -&gt; \mathbb{R}^3$ 将李群映射至向量空间。</p>
<p>相机观测特征点的重投影误差项：</p>
<p><img src="/2024/04/08/orb-slam3/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$\Pi: \mathbb{R}^3 -&gt; \mathbb{R}^n$ 表示不同相机模型对应的投影方程；$\mathbf{T}_{CB} \in SE(3)$ 表示从body-IMU 到相机的位姿转换矩阵（根据标定已知）；$\mathbf{x}_j$ 表示3D 物体地标。</p>
<p>结合惯导和视觉的观测残差，<strong>VI SLAM 的优化问题</strong>可表示为：</p>
<p><img src="/2024/04/08/orb-slam3/f4.png" alt="f4" title="formula 4"></p>
<p>其中，$\overline{\mathcal{S}}_k, \chi$ 分别表示 k+1 个<strong>关键帧状态向量</strong>和 l 个3D <strong>观测地标点</strong>；$\mathcal{K}^j$ 表示观测到3D 地标点 j 的关键帧集合。</p>
<p>该优化问题可通过Fig.2（a）所示的因子图进行表示，该优化需要进行效率上的优化，更关键的是，需要一个<strong>良好的初始化</strong>来收敛到精确解。</p>
<p><img src="/2024/04/08/orb-slam3/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="5-2-IMU-Initialization"><a href="#5-2-IMU-Initialization" class="headerlink" title="5.2 IMU Initialization"></a>5.2 IMU Initialization</h2><p>这一步的目标在于获取较好的IMU 参数初始值：速度、重力方向以及IMU 偏差。作者基于以下几点见解提出了一种快速且准确的初始化方法：</p>
<ol>
<li>单目SLAM 可以提供<strong>非常精确</strong>的初始化地图，但主要问题在于<strong>尺度未知</strong>；解决这个问题可以增强IMU 初始化能力。</li>
<li>有研究表明，相较于将尺度视为BA 的一个隐含表示，直接将其视为一个<strong>待优化参数</strong>可以极大加速其收敛过程。</li>
<li>在IMU 初始化中忽略<strong>传感器不确定度</strong>会造成较大的误差。</li>
</ol>
<p>为了较好地考虑传感器的不确定度，作者将IMU 初始化看作一个MAP 估计问题，分为以下三步进行：</p>
<ol>
<li><p><strong>纯视觉的MAP 估计</strong>：以纯视觉单目SLAM 运行2s，关键帧速率为4Hz，进行仅包含视觉的BA 优化（Fig.2（b）），得到<strong>相机位姿</strong>和<strong>特征点</strong>，以及相应的<strong>比例尺地图</strong>；将位姿转换至<strong>载体坐标系</strong>，得到相应的轨迹 $\overline{\mathbf{T}}_{0:k} = [\mathbf{R}, \overline{\mathbf{p}}]_{0:k}$ ，其中，上划线表示单目情况下的尺度不确定性。</p>
</li>
<li><p><strong>纯惯导的MAP 估计</strong>：该步骤目的在于仅利用 $\overline{\mathbf{T}}_{0:k}$ 和惯导观测的情况下，获取惯导参数的MAP 估计，状态向量表示为：</p>
<p><img src="/2024/04/08/orb-slam3/f5.png" alt="f5" title="formula 5"></p>
<p>其中，s 表示尺度参数；$\mathbf{R}_{wg} \in SO(3)$ 表示计算重力向量 $\mathbf{g}$ 在世界坐标系下的旋转矩阵，表示为 $\mathbf{g} = \mathbf{R}_{wg} \mathbf{g}_I$ ，其中，$\mathbf{g}_I = (0,0,G)^T$ ；$\mathbf{b} = (\mathbf{b}^a, \mathbf{b}^g) \in \mathbb{R}^6$ 表示加速度计和陀螺仪的偏差，<strong>在初始化过程中假定为固定常数</strong>；$\overline{\mathbf{v}}_{0:k}$ 表示尺度不定的载体速度，利用 $\overline{\mathbf{T}}_{0:k}$ 进行<strong>初始估计</strong>。此阶段只考虑惯导观测，则后验分布表示为：</p>
<p><img src="/2024/04/08/orb-slam3/f6.png" alt="f6" title="formula 6"></p>
<p>考虑到观测之间的独立性，上式可进一步转换为：</p>
<p><img src="/2024/04/08/orb-slam3/f7.png" alt="f7" title="formula 7"></p>
<p>转换为-log 形式，并假设IMU 预积分误差服从高斯分布，则最终优化问题表示为：</p>
<p><img src="/2024/04/08/orb-slam3/f8.png" alt="f8" title="formula 8"></p>
<p>该优化过程如Fig.2（c）所示，其中，矩阵 $\sum_b^{-1}$ 表示IMU 偏差取值范围的<strong>先验知识</strong>。</p>
<p>由于重力方向的旋转不会改变重力，则 $\mathbf{R}_{wg}$ 的更新过程为：</p>
<p><img src="/2024/04/08/orb-slam3/f9.png" alt="f9" title="formula 9"></p>
<p>为了保证尺度因子保持为正数，定义其更新为：</p>
<p><img src="/2024/04/08/orb-slam3/f10.png" alt="f10" title="formula 10"></p>
</li>
<li><p><strong>VI MAP 估计</strong>：在前获取了视觉和惯导的良好初始化估计的基础上，可以进行VI 联合优化来对参数进行细调优化，该过程可表示为 Fig.2（a）。</p>
</li>
</ol>
<p>作者提出的这个初始化方法较现有的方法具有较大的优势，且在公开数据集EuRoC 上进行了测试，可在2s 初始化后实现5%的尺度误差，为了进一步提高初始化估计精度，在初始化后5s、15s进行VI BA，可实现1%尺度误差的效果。同时，作者也提到，在<strong>慢动态场景</strong>中无法为惯导参数提供较好的观测，初始化在15s内无法达到良好的收敛，为了增强此种场景下的鲁棒性，作者提出了一种<strong>基于修改的纯惯导优化的尺度优化技术scale refinement technique</strong>，考虑所有的关键帧，只对尺度和重力方向进行估计，如Fig.2（d）所示，值得注意的是，此种情况下，常数偏差的假设不再成立，而是在优化过程中使用从地图中获取的估计值并将其固定。该优化过程<strong>非常高效</strong>，在<strong>局部制图线程</strong>中每10s进行一次，直到地图包含超过100个关键帧或者距离初始化超过75s。</p>
<h2 id="5-3-Tracking-and-Mapping"><a href="#5-3-Tracking-and-Mapping" class="headerlink" title="5.3 Tracking and Mapping"></a>5.3 Tracking and Mapping</h2><p>跟踪线程解决<strong>简单的VI 优化问题</strong>：只优化最后两帧的状态，且保持地图点固定。</p>
<p>地图线程为了避免过大地图造成的问题，只对<strong>滑动窗口内</strong>的关键帧和地图点进行优化，当然也包括共视关键帧中的观测，但不对这些共视关键帧的位姿进行优化。</p>
<h2 id="5-4-Robustness-to-Tracking-Loss"><a href="#5-4-Robustness-to-Tracking-Loss" class="headerlink" title="5.4 Robustness to Tracking Loss"></a>5.4 Robustness to Tracking Loss</h2><p>在纯视觉SLAM 或 VO 中，遮挡物与快速运动会导致跟踪失败，ORB-SLAM 使用基于词袋库的重定位方法来解决该问题，但在部分EuRoC 数据集场景中无法完全解决该问题。本文提出的VI 系统会在跟踪少于15个地图点的情况下进入<em>视觉跟踪失败状态</em>，并通过以下步骤继续实现鲁棒定位：</p>
<ol>
<li><strong>短期跟踪失败</strong>：使用IMU 观测估计位姿信息，在一个大图像窗口内搜寻匹配地图点，然后进行VI 优化；若超过5s未成功，进入下一步骤。</li>
<li><strong>长期跟踪失败</strong>：初始化一个新的VI 地图，并将该地图设为活动地图。</li>
</ol>
<h1 id="6-Map-Merging-and-Loop-Closing"><a href="#6-Map-Merging-and-Loop-Closing" class="headerlink" title="6 Map Merging and Loop Closing"></a>6 Map Merging and Loop Closing</h1><p>为了避免false positive回环检测，DBoW2 进行<strong>时间和几何一致性检查</strong>，使得回环检测的准确率可达100%，但召回率只有30%-40%。而且，时间一致性检查太耗时间，作者发现在Atlas 中使用这种回环检测方法会导致频繁的延迟与低召回率。</p>
<p>为解决该问题，作者提出了一种新的<strong>地点识别算法</strong>，该算法有两个创新点：</p>
<ol>
<li>当制图线程插入一个新的关键帧时，与Atlas 中现有的<strong>所有关键帧</strong>进行匹配，如果匹配关键帧属于活动地图，则进行<strong>回环检测</strong>；否则，将匹配关键帧所属的地图与活动地图进行<strong>地图融合</strong>。</li>
<li>此外，得到匹配关键帧后，将匹配关键帧对及其共视图中的邻居构建为一个<strong>局部窗口</strong>，在窗口内进行<strong>中期数据关联</strong>，以提高回环检测和地图融合的精度。</li>
</ol>
<h2 id="6-1-Place-Recognition"><a href="#6-1-Place-Recognition" class="headerlink" title="6.1 Place Recognition"></a>6.1 Place Recognition</h2><p>地点重识别的步骤：</p>
<ol>
<li><strong>DBoW2 候选关键帧</strong>：在Atlas DBoW2 数据库中搜寻当前关键帧 $K_a$ 的三个最相似的候选关键帧 $K_m$ ，注意，这里不包括该关键帧的共视帧；</li>
<li><strong>局部窗口</strong>：对于每个 $K_m$ ，构建一个局部窗口包含 $K_m$ 、共视关键帧以及观测到的地图点；然后根据DBoW2 建立 $K_a$ 与局部窗口内的假定地图点匹配。</li>
<li><strong>3D 对齐转换</strong>：利用RANSAC 算法，根据匹配点解算位姿转换矩阵 $\mathbf{T}_{am}$ ，对于纯单目或单目-惯导未构建成熟地图的情况，计算相似转换 Sim(3)，否则计算 SE(3)。</li>
<li><strong>匹配细调</strong>：根据解算的 $\mathbf{T}_{am}$ 搜寻当前关键帧 $K_a$ 与局部窗口内所有地图点的匹配，在此基础上进行<strong>双向重投影误差</strong>非线性优化；如果内点数量超过一定阈值，则在更小图片窗口内进行第二次细调。</li>
<li><strong>三个共视关键帧的验证</strong>：DBoW2 回环检测的<strong>时间一致性检验</strong>需检验连续三个关键帧，会造成延迟或遗漏地点识别；作者认为，大部分情况下验证所需的信息已经存在于地图中，可在活动地图中搜寻与 $K_a$ 相匹配的两个关键帧（超过一定匹配地图点数），然后使用这两个关键帧来验证  $\mathbf{T}_{am}$ 。</li>
<li><strong>VI 重力方向验证</strong>：在VI 中，若地图已经构建成熟，可获得 $\mathbf{T}_{am}\in SE(3)$ ，然后进一步检验pitch、roll角度是否在设定阈值下来确定地点识别假设是否成立。</li>
</ol>
<h2 id="6-2-Visual-Map-Merging"><a href="#6-2-Visual-Map-Merging" class="headerlink" title="6.2 Visual Map Merging"></a>6.2 Visual Map Merging</h2><p>融合过程如下所示：</p>
<ol>
<li><strong>Welding window assembly</strong>：该融合窗口包含 $K_a$、$K_m$ 、它们的共视关键帧以及所有的观测地图点，属于 $M_a$ 的所有关键帧和地图点通过 $\mathbf{T}_{ma}$ 与 $M_m$ 进行对齐。</li>
<li><strong>Merging maps</strong>：$M_a$、$M_m$融合起来变成新的活动地图，然后对重复地图点进行清除，对共视图、essential 图进行更新。</li>
<li><strong>Welding bundle adjustment</strong>：在窗口内对所有的关键帧进行局部BA 优化，如Fig.3（a）所示。</li>
<li><strong>Essential-graph optimization</strong>：地图融合之后，使用essential 图进行基于因子图的优化，在此过程中welding 窗口内的关键帧位姿保持固定；经过优化，welding窗口内的位姿矫正信息会进一步传递到剩余地图中。</li>
</ol>
<p><img src="/2024/04/08/orb-slam3/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="6-3-Visual-Inertial-Map-Merging"><a href="#6-3-Visual-Inertial-Map-Merging" class="headerlink" title="6.3 Visual-Inertial Map Merging"></a>6.3 Visual-Inertial Map Merging</h2><p>VI 地图融合较纯视觉有部分改变，在步骤1和3中进行了以下调整：</p>
<ol>
<li><strong>VI welding window assembly</strong>：$\mathbf{T}_{ma}$ 会根据地图是否成熟分别选用SE(3), Sim(3)。</li>
<li><strong>VI welding bundle adjustment</strong>：对位姿、速度、 $K_a$ 和 $K_m$ 的IMU 偏差，以及之前的5个关键帧进行优化，如Fig.3（b）所示，除了关键帧位姿，所有的观测地图点也进行优化。</li>
</ol>
<h1 id="7-Experimental-Results"><a href="#7-Experimental-Results" class="headerlink" title="7 Experimental Results"></a>7 Experimental Results</h1><p>作者在EuRoC 数据集上进行测试，取<strong>十次实验结果中的中位数</strong>与SOTA 算法进行比较，结果如表2所示：</p>
<p><img src="/2024/04/08/orb-slam3/t2.png" alt="t2" title="table 2"></p>
<p>上表结果可证明本系统的优越性，特别是VI 系统的优势更为明显；但是，作者也指出双目-惯导较单目-惯导的<strong>优势很小</strong>，特别是在困难场景V203中，作者总结为加入惯导不仅<strong>提高了精度</strong>，减少了ATE 误差，而且也<strong>极大增强了系统的鲁棒性</strong>。</p>
<p>TUM-VI 数据集上的测试结果如表3所示，可以清楚看出本系统在单目-惯导、双目-惯导上的优势。</p>
<p><img src="/2024/04/08/orb-slam3/t3.png" alt="t3" title="table 3"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>ORB-SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>C++</tag>
        <tag>ORB-SLAM2</tag>
      </tags>
  </entry>
  <entry>
    <title>ORB-SLAM2环境搭建与运行</title>
    <url>/2024/01/26/orbslam2-env/</url>
    <content><![CDATA[<p>本文主要参考<a href="https://blog.csdn.net/meng_152634/article/details/127570220">该文章</a>。</p>
<h2 id="1-Eigen3安装与卸载"><a href="#1-Eigen3安装与卸载" class="headerlink" title="1 Eigen3安装与卸载"></a>1 Eigen3安装与卸载</h2><h3 id="1-1-安装"><a href="#1-1-安装" class="headerlink" title="1.1 安装"></a>1.1 安装</h3><p>可通过apt命令安装，由于使用源码安装的方式在后续编译ORB-SLAM2 过程中可能会遇到一些问题，因此，本人<strong>建议使用apt命令进行安装</strong>：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install libeigen3-dev</span><br></pre></td></tr></table></figure>
<p>也可在<a href="https://eigen.tuxfamily.org/index.php?title=Main_Page">官网</a>下载eigen3 源码，然后编译安装（本人不推荐该方法，后续会出现程序找不到eigen3的问题）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> eigen-xxx</span><br><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>版本查看命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">❯ pkg-config --modversion eigen3</span><br><span class="line">3.4.0</span><br></pre></td></tr></table></figure>
<h3 id="1-2-卸载"><a href="#1-2-卸载" class="headerlink" title="1.2 卸载"></a>1.2 卸载</h3><p>通过apt 方式安装的采用以下方式进行卸载：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 通过remove 卸载</span></span><br><span class="line">sudo apt remove libeigen3-dev</span><br><span class="line"></span><br><span class="line"><span class="comment">## 可通过locate 进一步定位残余文件进行手动删除，并手动删除/usr/local/和/usr/include/目录下的eigen目录</span></span><br><span class="line">sudo updatedb</span><br><span class="line">locate eigen</span><br></pre></td></tr></table></figure>
<p>通过源码安装的，需要手动删除/usr/local/include/等目录下的eigen3目录：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo updatedb</span><br><span class="line">locate eigen3  <span class="comment"># 查看eigen3的位置</span></span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/include/eigen3</span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/lib/cmake/eigen3</span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/local//include/eigen3</span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/share/doc/libeigen3-dev</span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/local/share/pkgconfig/eigen3.pc /usr/share/pkgconfig/eigen3.pc /var/lib/dpkg/info/libeigen3-dev.list /var/lib/dpkg/info/libeigen3-dev.md5sums</span><br></pre></td></tr></table></figure>
<h2 id="2-Pangolin-安装与卸载"><a href="#2-Pangolin-安装与卸载" class="headerlink" title="2 Pangolin 安装与卸载"></a>2 Pangolin 安装与卸载</h2><h3 id="2-1-安装"><a href="#2-1-安装" class="headerlink" title="2.1 安装"></a>2.1 安装</h3><p>首先安装依赖项：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install libglew-dev</span><br><span class="line">sudo apt install libboost-dev libboost-thread-dev libboost-filesystem-dev</span><br><span class="line">sudo apt install libboost-all-dev</span><br><span class="line">sudo apt install libwayland-dev wayland-protocols</span><br><span class="line">sudo apt install libxkbcommon-dev</span><br><span class="line">sudo apt install libegl1-mesa-dev</span><br><span class="line">sudo apt install ninja-build</span><br></pre></td></tr></table></figure>
<p>然后下载源码，编译安装</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/stevenlovegrove/Pangolin.git</span><br><span class="line"></span><br><span class="line"><span class="comment">## 编译安装</span></span><br><span class="line"><span class="built_in">cd</span> Pangolin</span><br><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>然后可在/usr/local/include/和/usr/local/lib/目录下找到pangolin相关的目录及库文件：</p>
<p><img src="/2024/01/26/orbslam2-env/pangolin.png" alt="pangolin" title="Pangolin目录及库文件"></p>
<p>安装完成后进行测试：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> Pangolin/build/examples/HelloPangolin/</span><br><span class="line">./HelloPangolin</span><br></pre></td></tr></table></figure>
<p>出现以下界面说明安装成功：</p>
<p><img src="/2024/01/26/orbslam2-env/pangolin-success.png" alt="pangolin success" title="Pangolin安装成功"></p>
<h3 id="2-2-卸载"><a href="#2-2-卸载" class="headerlink" title="2.2 卸载"></a>2.2 卸载</h3><p>Pangolin 的卸载需要手动删除相关文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo updatedb</span><br><span class="line">locate pangolin</span><br><span class="line">sudo <span class="built_in">rm</span> -rf /usr/local/include/pangolin</span><br><span class="line">sudo <span class="built_in">rm</span> /usr/local/lib/libpango_*.so</span><br></pre></td></tr></table></figure>
<h2 id="3-opencv-安装与卸载"><a href="#3-opencv-安装与卸载" class="headerlink" title="3 opencv 安装与卸载"></a>3 opencv 安装与卸载</h2><p>分别到<a href="https://opencv.org/releases/">opencv</a>、<a href="https://github.com/opencv/opencv_contrib/tags">contrib</a> 开发库下载对应版本，注意，两个文件的版本需要保持一致。</p>
<h3 id="3-0-pkg-config"><a href="#3-0-pkg-config" class="headerlink" title="3.0 pkg-config"></a>3.0 pkg-config</h3><h4 id="3-0-1-介绍"><a href="#3-0-1-介绍" class="headerlink" title="3.0.1 介绍"></a>3.0.1 介绍</h4><p>opencv 多版本管理主要参考<a href="https://www.cntofu.com/book/46/opencv/ubuntuxia_duo_ban_ben_opencv_qie_huan.md">文章1</a>和<a href="https://ivanzz1001.github.io/records/post/linux/2017/09/08/linux-pkg-config#1-pkg-config简单介绍">文章2</a>，该文章主要利用pkg-config 包管理工具来管理多版本的opencv。</p>
<p>一般用第三方库的时候，就少不了要使用到<strong>第三方的头文件</strong>和<strong>库文件</strong>。我们在编译、链接的时候必须要指定这些头文件和库文件的位置。对于一个比较大的第三方库，其头文件和库文件的数量是比较多的，如果我们一个个手动地写，那将是相当的麻烦的。因此，pkg-config就应运而生了。pkg-config能够把这些头文件和库文件的位置指出来，给编译器使用。pkg-config主要提供了下面几个功能：</p>
<ul>
<li>检查库的版本号。 如果所需要的库的版本不满足要求，它会打印出错误信息，避免链接错误版本的库文件</li>
<li>获得编译预处理参数，如宏定义、头文件的位置</li>
<li>获得链接参数，如库及依赖的其他库的位置，文件名及其他一些链接参数</li>
<li>自动加入所依赖的其他库的设置</li>
</ul>
<p>如以下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">gcc -o <span class="built_in">test</span> test.c `pkg-config --libs --cflags glib-2.0`</span><br></pre></td></tr></table></figure>
<p>其中，—libs 用于指定<strong>库文件</strong>，—cflags 用于指定<strong>头文件</strong>。</p>
<p>pkg-config 命令基本用法如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">pkg-config &lt;options&gt; &lt;library-name&gt;</span><br></pre></td></tr></table></figure>
<h4 id="3-0-2-配置环境变量"><a href="#3-0-2-配置环境变量" class="headerlink" title="3.0.2 配置环境变量"></a>3.0.2 配置环境变量</h4><p>事实上，pkg-config只是一个工具，所以不是你安装了一个第三方库，pkg-config就能知道第三方库的头文件和库文件的位置的。为了让pkg-config可以得到一个库的信息，就要求库的提供者提供一个<strong>.pc 文件</strong>。例如，本人安装的opencv-4.6.0 中包含了对应的opencv4.pc 文件：</p>
<p><img src="/2024/01/26/orbslam2-env/opencv4.png" alt="opencv4" title="opencv4.pc"></p>
<p>首先，将.pc 文件拷贝至pkgconfig 路径下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo <span class="built_in">cp</span> ~/Softwares/opencv/opencv-4.6.0/lib/pkgconfig/opencv4.pc /usr/lib/pkgconfig/opencv4.pc</span><br></pre></td></tr></table></figure>
<p>然后，添加链接库路径：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 新建文件并编辑</span></span><br><span class="line">sudo vi /etc/ld.so.conf.d/opencv4.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">## 添加链接库路径</span></span><br><span class="line">/home/echo/Softwares/opencv/opencv-4.6.0/lib/</span><br><span class="line"></span><br><span class="line"><span class="comment">## 刷新</span></span><br><span class="line">sudo ldconfig</span><br></pre></td></tr></table></figure>
<p>刷新之后，即可检验相应版本是否添加成功：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&gt; pkg-config --modversion opencv4</span><br><span class="line">4.6.0</span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/orbslam2-env/opencv4-1.png" alt="opencv4" title="opencv4库文件与头文件"></p>
<p>相应地，编译工程时即可使用如下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">g++ -o cv_test cv_test.cpp `pkg-config --libs --cflags opencv4`</span><br></pre></td></tr></table></figure>
<h3 id="3-1-opencv-4-6-0"><a href="#3-1-opencv-4-6-0" class="headerlink" title="3.1 opencv-4.6.0"></a>3.1 opencv-4.6.0</h3><h4 id="3-1-1-编译与安装"><a href="#3-1-1-编译与安装" class="headerlink" title="3.1.1 编译与安装"></a>3.1.1 编译与安装</h4><p>编译命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cmake -D CMAKE_BUILD_TYPE=RELEASE \</span><br><span class="line">	-D CMAKE_INSTALL_PREFIX=/home/***/Softwares/opencv/opencv-4.6.0 \ <span class="comment">## 更换为相应位置</span></span><br><span class="line">	-D INSTALL_C_EXAMPLES=OFF \</span><br><span class="line">	-D OPENCV_ENABLE_NONFREE=ON \</span><br><span class="line">	-D WITH_CUDA=ON \</span><br><span class="line">	-D WITH_CUDNN=ON \</span><br><span class="line">	-D OPENCV_DNN_CUDA=ON \</span><br><span class="line">	-D ENABLE_FAST_MATH=1 \</span><br><span class="line">	-D CUDA_FAST_MATH=1 \</span><br><span class="line">	-D CUDA_ARCH_BIN=8.6 \  <span class="comment">## 一定更改为显卡对应的算力版本</span></span><br><span class="line">	-D WITH_CUBLAS=1 \</span><br><span class="line">	-D OPENCV_EXTRA_MODULES_PATH=/home/***/Softwares/opencv/opencv-4.6.0/opencv_contrib/modules \</span><br><span class="line">	-D BUILD_EXAMPLES=ON ..</span><br></pre></td></tr></table></figure>
<p>因为要使用pkg-config 对不同版本的opencv 进行管理，但是<strong>opencv4默认将opencv.pc 的产生选项关闭了</strong>，查看CMakelist.txt 相关语句如下所示：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">OCV_OPTION(OPENCV_GENERATE_PKGCONFIG <span class="string">&quot;Generate .pc file for pkg-config build tool (deprecated)&quot;</span> OFF)</span><br></pre></td></tr></table></figure>
<p>所以需将CMakelist.txt 中对应语句的<strong>参数改为ON</strong>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 将对应命令更改如下：</span></span><br><span class="line">OCV_OPTION(OPENCV_GENERATE_PKGCONFIG <span class="string">&quot;Generate .pc file for pkg-config build tool (deprecated)&quot;</span> ON)</span><br></pre></td></tr></table></figure>
<p>编译成功之后，make、安装：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">make -j13 -w</span><br><span class="line">sudo make install</span><br></pre></td></tr></table></figure>
<p>值得注意的是，由于是安装到了自定义目录下，在利用CMakeLists.txt 进行编译前，需要进行以下操作：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 将build文件夹中的OpenCVConfig.cmake、OpenCVModules.cmake移至share/opencv4中</span></span><br><span class="line">sudo <span class="built_in">cp</span> OpenCVConfig.cmake OpenCVModules.cmake ../../share/opencv4/</span><br><span class="line"></span><br><span class="line"><span class="comment">## 修改CMakeLists.txt 文件，设置搜寻路径</span></span><br><span class="line"><span class="built_in">set</span>(OpenCV_DIR /home/echo/Softwares/opencv/opencv-4.6.0/share/opencv4)</span><br><span class="line">find_package(OpenCV)</span><br></pre></td></tr></table></figure>
<h4 id="3-1-2-多版本控制"><a href="#3-1-2-多版本控制" class="headerlink" title="3.1.2 多版本控制"></a>3.1.2 多版本控制</h4><p>按照pkgconfig  版本控制，使用opencv-4.6.0 对自带的示例进行测试，修改CMakeLists.txt 文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 设置搜寻路径</span></span><br><span class="line"><span class="built_in">set</span>(OpenCV_DIR /home/echo/Softwares/opencv/opencv-4.6.0/share/opencv4)</span><br><span class="line">find_package(OpenCV)</span><br></pre></td></tr></table></figure>
<p>进行cmake编译并执行：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># cmake 过程中会出现相应的版本号、库文件等信息</span></span><br><span class="line">cmake .</span><br><span class="line">make</span><br><span class="line">./opencv_example</span><br></pre></td></tr></table></figure>
<p>得到如下输出结果，证明安装成功：</p>
<p><img src="/2024/01/26/orbslam2-env/opencv4-success.png" alt="opencv-success" title="opencv安装成功"></p>
<p>同样地，使用g++ 命令进行编译：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">g++ -o opencv_example example.cpp `pkg-config --libs --cflags opencv4`</span><br></pre></td></tr></table></figure>
<p>执行生成文件，得到与上图相同的结果证明安装成功。</p>
<h3 id="3-2-opencv-3-4-11"><a href="#3-2-opencv-3-4-11" class="headerlink" title="3.2 opencv-3.4.11"></a>3.2 opencv-3.4.11</h3><p>编译命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cmake -D CMAKE_BUILD_TYPE=RELEASE \</span><br><span class="line">	-D CMAKE_INSTALL_PREFIX=/home/***/Softwares/opencv/opencv-3.4.11 \</span><br><span class="line">	-D INSTALL_C_EXAMPLES=OFF \</span><br><span class="line">	-D OPENCV_ENABLE_NONFREE=ON \</span><br><span class="line">	-D WITH_CUDA=ON \</span><br><span class="line">	-D WITH_CUDNN=ON \</span><br><span class="line">	-D OPENCV_DNN_CUDA=ON \</span><br><span class="line">	-D ENABLE_FAST_MATH=1 \</span><br><span class="line">	-D CUDA_FAST_MATH=1 \</span><br><span class="line">	-D CUDA_ARCH_BIN=8.6 \</span><br><span class="line">	-D WITH_CUBLAS=1 \</span><br><span class="line">	-D OPENCV_EXTRA_MODULES_PATH=/home/***/Softwares/opencv/opencv-3.4.11/opencv_contrib/modules \</span><br><span class="line">	-D BUILD_EXAMPLES=ON ..</span><br></pre></td></tr></table></figure>
<p>该版本的opencv 会自动在share/OpenCV 文件夹下生成OpenCVConfig.cmake、OpenCVModules.cmake 等文件，所以无需从build 文件夹中进行复制。</p>
<p>其他使用pkg-config 进行多版本控制方法与opencv-4.6.0 一致，在此不做赘述。</p>
<h3 id="3-3-opencv-卸载"><a href="#3-3-opencv-卸载" class="headerlink" title="3.3 opencv 卸载"></a>3.3 opencv 卸载</h3><p>首先，到编译目录build 下执行卸载命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo make uninstall</span><br></pre></td></tr></table></figure>
<p>然后，到opencv 的安装目录下，将bin、lib、share、include 等文件删除即可。若不再安装本版本，则将pkg-config 的相关配置清除掉。</p>
<h2 id="4-ORB-SLAM2-编译与运行"><a href="#4-ORB-SLAM2-编译与运行" class="headerlink" title="4 ORB-SLAM2 编译与运行"></a>4 ORB-SLAM2 编译与运行</h2><h3 id="4-1-前期准备"><a href="#4-1-前期准备" class="headerlink" title="4.1 前期准备"></a>4.1 前期准备</h3><p>在此注明一下本人各个软件包的<strong>最终版本</strong>如下所示：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">opencv: 3.4.11</span><br><span class="line">eigen3: 3.4.0</span><br><span class="line">pabgolin: 0.6</span><br></pre></td></tr></table></figure>
<p>明确完软件包版本后，首先到<a href="https://github.com/raulmur/ORB_SLAM2">ORB-SLAM2仓库</a>下载，进入下载的文件夹后对以下文件进行更改：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 对主目录及DBoW2下的CMakeLists.txt 文件中的opencv版本及搜寻路径进行相应修改</span></span><br><span class="line"><span class="built_in">set</span>(OpenCV_DIR /home/echo/Softwares/opencv/opencv-3.4.11/share/OpenCV)</span><br><span class="line">find_package(OpenCV 3.4.11 QUIET)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 对主目录下的CMakeLists.txt 文件中的eigen3 版本进行修改</span></span><br><span class="line">find_package(Eigen3 3.4.0 REQUIRED)</span><br></pre></td></tr></table></figure>
<h3 id="4-2-编译"><a href="#4-2-编译" class="headerlink" title="4.2 编译"></a>4.2 编译</h3><p>按照官方介绍进行编译：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">chmod</span> +x build.sh</span><br><span class="line">./build.sh</span><br></pre></td></tr></table></figure>
<p>正常编译成功的话会在<em>lib</em> 文件夹内生成<strong>libORB_SLAM2.so</strong> 函数库，在<em>Examples</em> 文件夹内生成对应的可执行文件：<strong>mono_tum, mono_kitti, rgbd_tum, stereo_kitti, mono_euroc</strong> 和 <strong>stereo_euroc</strong>。但一般都会有各种报错，这里记录一下本人的报错及相应处理。</p>
<ul>
<li>首先是可能因为软件包版本问题报错：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 报错内容：</span></span><br><span class="line">Pangolin could not be found because dependency Eigen3 could not be found</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 解决方案：</span></span><br><span class="line">根据网上找的资料，可能是Pangolin和Eigen3的版本问题；按照网上教程，卸载了Pangolin和Eigen3，由于要手动选取版本，所以这里Eigen3选择了源文件安装，但后续又出现了其他问题，故又卸载新安装的Eigen3，重新使用apt安装了Eigen3，所以最终情况是：Pangolin降级为v0.6，Eigen3仍是原来的3.4.0。至此，该问题得到解决</span><br></pre></td></tr></table></figure>
<ul>
<li>再次编译可能会出现<strong>无法找到Eigen3 函数库</strong>的报错，此时根据报错内容，将Eigen3 的安装位置：/usr/include/目录下的eigen目录软链接到相应位置即可。</li>
<li>过程中没有其他报错内容，DBoW2、g2o等均编译成功，但最终编译ORB-SLAM2 各个运行程序时会提示：</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 输出提示</span></span><br><span class="line">make: *** No targets specified and no makefile found.  Stop.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 在ORB-SLAM2的issues中找到解决方案：</span></span><br><span class="line"><span class="comment">## 删掉build文件，直接执行build.sh中的最后一步</span></span><br><span class="line"><span class="built_in">mkdir</span> build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br></pre></td></tr></table></figure>
<ul>
<li>出现usleep() 未声明的报错：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">## 报错内容：</span><br><span class="line">error: ‘usleep’ was <span class="keyword">not</span> declared in <span class="keyword">this</span> scope</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 解决方案：在system.h文件中加入</span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;unistd.h&gt;</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li>LoopClosing.h中的报错：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">## 报错内容：</span><br><span class="line">/usr/include/c++/<span class="number">9</span>/bits/stl_map.h: In instantiation of ‘<span class="keyword">class</span> <span class="title class_">std</span>::map&lt;ORB_SLAM2::KeyFrame*, g2o::Sim3, std::less&lt;ORB_SLAM2::KeyFrame*&gt;, Eigen::aligned_allocator&lt;std::pair&lt;<span class="type">const</span> ORB_SLAM2::KeyFrame*, g2o::Sim3&gt; &gt; &gt;’:</span><br><span class="line">ORB_SLAM2/src/LoopClosing.cc:<span class="number">438</span>:<span class="number">21</span>: required from here</span><br><span class="line">/usr/include/c++/<span class="number">9</span>/bits/stl_map.h:<span class="number">122</span>:<span class="number">71</span>: error: <span class="type">static</span> assertion failed: std::map must have the same value_type as its allocator</span><br><span class="line"><span class="number">122</span> | <span class="built_in">static_assert</span>(is_same&lt;<span class="keyword">typename</span> _Alloc::value_type, value_type&gt;::value,</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">## 解决方案：</span><br><span class="line">修改LoopClosing.h文件中的<span class="number">49</span>和<span class="number">50</span>行：</span><br><span class="line">修改前：</span><br><span class="line"><span class="keyword">typedef</span> map&lt;KeyFrame*,g2o::Sim3,std::less&lt;KeyFrame*&gt;,</span><br><span class="line">        Eigen::aligned_allocator&lt;std::pair&lt;<span class="type">const</span> KeyFrame*, g2o::Sim3&gt; &gt; &gt; KeyFrameAndPose;</span><br><span class="line">修改后：</span><br><span class="line"><span class="keyword">typedef</span> map&lt;KeyFrame*,g2o::Sim3,std::less&lt;KeyFrame*&gt;,</span><br><span class="line">       Eigen::aligned_allocator&lt;std::pair&lt;KeyFrame *<span class="type">const</span>, g2o::Sim3&gt; &gt; &gt; KeyFrameAndPose;</span><br></pre></td></tr></table></figure>
<p>至此，安装成功，生成了相应的可执行文件。</p>
<h3 id="4-3-运行测试"><a href="#4-3-运行测试" class="headerlink" title="4.3 运行测试"></a>4.3 运行测试</h3><p>本处使用了TUM RGB-D 数据集进行测试，测试命令如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./Examples/RGB-D/rgbd_tum Vocabulary/ORBvoc.txt Examples/RGB-D/TUM1.yaml ~/datasets/TUM/rgbd_dataset_freiburg1_desk ~/datasets/TUM/rgbd_dataset_freiburg1_desk/associate.txt </span><br></pre></td></tr></table></figure>
<p>运行界面如下图所示：</p>
<p><img src="/2024/01/26/orbslam2-env/orbslam2-success.png" alt="orbslam2" title="ORB-SLAM2成功运行"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>ORB-SLAM</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Ubuntu</tag>
        <tag>C++</tag>
        <tag>ORB-SLAM2</tag>
        <tag>OpenCV</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 A Light-Weight Semantic Map for Visual Localization towards Autonomous Driving</title>
    <url>/2024/03/02/qin2021/</url>
    <content><![CDATA[<p>Qin, Tong, Yuxin Zheng, Tongqing Chen, Yilun Chen, and Qing Su. “A Light-Weight Semantic Map for Visual Localization towards Autonomous Driving.” In <em>2021 IEEE International Conference on Robotics and Automation (ICRA)</em>, 11248–54. Xi’an, China: IEEE, 2021. <a href="https://doi.org/10.1109/ICRA48506.2021.9561663">https://doi.org/10.1109/ICRA48506.2021.9561663</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ol>
<li>提出一个新颖的面向无人驾驶的<strong>轻量级定位架构</strong>，该架构包括<strong>车载制图</strong>、<strong>云端地图维护</strong>，以及<strong>用户终端定位</strong>；</li>
<li>提出一个新颖的想法：让具有丰富传感器的车辆来辅助低成本量产汽车，具体途径为让具有丰富传感器的汽车每天收集数据并自动更新地图；</li>
<li>利用真实场景实验验证了该系统的可行性。</li>
</ol>
<span id="more"></span>
<p><img src="/2024/03/02/qin2021/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3 System Overview"></a>3 System Overview</h1><p>系统架构如Fig. 2所示，该系统包含三个部分：</p>
<ol>
<li><strong>车载制图</strong>：汽车装备有前向摄像头、RTK-GPS，以及一些基础导航传感器（IMU、轮速记等）；利用分割网络从前向摄像头获取的图像中提取语义信息，然后基于<strong>优化后的汽车位姿</strong>将语义信息投影至<strong>世界坐标系</strong>，由此车辆本地可构建一个<strong>局部的语义地图</strong>，然后被上传至云端服务器。</li>
<li><strong>云端制图</strong>：将众多局部地图融合为一个<strong>全局地图</strong>，然后该全局地图经边界提取进行<strong>压缩</strong>，最终，压缩后的全局地图被分发至终端用户。</li>
<li><strong>终端定位</strong>：终端用户为量产汽车，装备有低成本传感器，如相机、GPS、IMU 及轮速记等；与<strong>车载制图步骤</strong>一样，终端用户从本地的车载摄像头获取的图像中提取语义信息；终端用户将下载的语义地图进行解码，然后根据<strong>语义特征匹配</strong>进行定位。</li>
</ol>
<p><img src="/2024/03/02/qin2021/fig2.png" alt="fig2" title="figure 2"></p>
<h1 id="4-On-Vehicle-Mapping"><a href="#4-On-Vehicle-Mapping" class="headerlink" title="4 On-Vehicle Mapping"></a>4 On-Vehicle Mapping</h1><h2 id="4-1-Image-Segmentation"><a href="#4-1-Image-Segmentation" class="headerlink" title="4.1 Image Segmentation"></a>4.1 Image Segmentation</h2><p>利用CNN 网络进行语义分割，分类结果中使用<strong>地面、车道线、停止线、道路标志信息</strong>进行语义制图，语义语义分割结果如Fig. 3所示：</p>
<p><img src="/2024/03/02/qin2021/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="4-2-Inverse-Perspective-Transformation"><a href="#4-2-Inverse-Perspective-Transformation" class="headerlink" title="4.2 Inverse Perspective Transformation"></a>4.2 Inverse Perspective Transformation</h2><p>获取到语义信息之后，在汽车坐标系下，将语义像素点从图像平面<strong>逆投影</strong>至<strong>地平面</strong>，该过程也被称为 Inverse Perspective Mapping（<strong>IPM</strong>）。相机内参和相机到车辆中心的外部转换矩阵已经<strong>提前标定</strong>好了。由于观测噪声，越远的地方误差越大，所以作者只选取Region Of Interest（<strong>ROI</strong>）区域的像素进行IPM，如Fig. 3（a）所示。</p>
<p>假设地面是个平面，语义像素点投影至地平面（z 等于0）的<strong>IPM 过程</strong>如下所示：</p>
<p><img src="/2024/03/02/qin2021/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$\pi_c()$ 是相机的畸变+投影模型，$\pi_c()^{-1}$ 是逆投影过程；$[\mathbf{R}_c, \mathbf{t}_c]$ 是每个相机相对于汽车中心的<strong>外参矩阵</strong>；$[u, v]$ 表示图像坐标系中的像素坐标；$[x^v, y^v]$ 表示特征在汽车中心坐标系下的位置；$\lambda$ 为尺度参数；$()_{col:i}$ 表示选取矩阵的第 i 列。IPM 的过程如Fig. 3（c）所示。</p>
<h2 id="4-3-Pose-Graph-Optimization"><a href="#4-3-Pose-Graph-Optimization" class="headerlink" title="4.3 Pose Graph Optimization"></a>4.3 Pose Graph Optimization</h2><p>为了构建地图，需要获取车辆精确的位姿信息。为了减小在GNSS 拒止环境下<strong>里程计的漂移</strong>，作者使用<strong>位姿图</strong>进行优化，如Fig. 5所示：</p>
<p><img src="/2024/03/02/qin2021/fig5.png" alt="fig5" title="figure 5"></p>
<p>位姿图优化过程可建模为：</p>
<p><img src="/2024/03/02/qin2021/f2.png" alt="f2" title="formula 2"></p>
<p>其中，$\mathbf{s}$ 为汽车位姿信息（位置和朝向信息）；$\mathbf{r}_o$ 表示里程计因子的残差；$\hat{\mathbf{m}}_{i-1, i}^o$ 表示里程计观测信息，其中包含相邻两个状态间的位置变化 $\delta \hat{\mathbf{p}}_{i-1, i}$ 和朝向变化 $\delta \hat{\mathbf{q}}_{i-1, i}$ ；$\mathbf{r}_g$ 表示里程计因子的残差；$\mathcal{G}$ 为GNSS 良好观测区域的状态组；$\hat{\mathbf{m}}_{i}^g$ 表示GNSS 观测信息，即全局坐标系下的位置信息 $\hat{\mathbf{p}}_i$ 。</p>
<p>残差因子的定义如下所示：</p>
<p><img src="/2024/03/02/qin2021/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$\mathbf{q}$ 为四元数表示的朝向信息；$\mathbf{R}(\mathbf{q})$ 表示将四元数转换为旋转矩阵；$[]_{xyz}$ 表示取四元数的前三个元素，近似等于<strong>流形上的误差扰动</strong>。</p>
<h2 id="4-4-Local-Mapping"><a href="#4-4-Local-Mapping" class="headerlink" title="4.4 Local Mapping"></a>4.4 Local Mapping</h2><p>基于位姿图优化获取的位姿，将语义特征从汽车坐标系转换至全局坐标系：</p>
<p><img src="/2024/03/02/qin2021/f4.png" alt="f4" title="formula 4"></p>
<p>对于语义特征，每一个像素点代表了真实世界中的一小块区域，随着汽车移动，该区域会被观测到多次；然而，由于分割噪声的存在，该区域可能会被识别为不同的种类，为了减少噪声的影响，作者<strong>使用统计数据来滤除噪声</strong>：作者将地图分为 $0.1 \times 0.1 \times 0.1 m$ 的网格，每个网格包含<strong>位置</strong>、<strong>语义标签</strong>以及每种标签的<strong>计数信息</strong>，随着观测的增多，每个网格内不同标签的计数逐渐变化，最终选取计数最多的标签作为该网格的语义类别，作者通过这种统计策略来增强鲁棒性。全局制图的结果如Fig. 6（a）所示：</p>
<p><img src="/2024/03/02/qin2021/fig6.png" alt="fig6" title="figure 6"></p>
<h1 id="5-On-Cloud-Mapping"><a href="#5-On-Cloud-Mapping" class="headerlink" title="5 On-Cloud Mapping"></a>5 On-Cloud Mapping</h1><h2 id="5-1-Map-Merging-Updating"><a href="#5-1-Map-Merging-Updating" class="headerlink" title="5.1 Map Merging / Updating"></a>5.1 Map Merging / Updating</h2><p>为了节省带宽，只将局部地图的占据网格信息上传至云端，网格的分辨率同样是 $0.1 \times 0.1 \times 0.1 m$ ，值得注意的是，局部地图的网格被加到全局地图上时，会将标签的计数信息同时加上去，并根据<strong>计数信息来更新</strong>当前网格的语义标签。更新过程如Fig. 9所示：</p>
<p><img src="/2024/03/02/qin2021/fig9.png" alt="fig9" title="figure 9"></p>
<h2 id="5-2-Map-Compression"><a href="#5-2-Map-Compression" class="headerlink" title="5.2 Map Compression"></a>5.2 Map Compression</h2><p>为了节省带宽，需要进一步压缩语义地图。作者使用<strong>边界提取</strong>方法来进一步压缩语义地图：</p>
<ol>
<li>首先，产生语义地图的顶视图，每个像素代表一个网格；</li>
<li>然后，提取每个语义群的边界；</li>
<li>最终，保存边界点然后分发至各个终端车辆。</li>
</ol>
<p>该过程如Fig. 6所示。</p>
<h1 id="6-User-End-Localization"><a href="#6-User-End-Localization" class="headerlink" title="6 User-End Localization"></a>6 User-End Localization</h1><p>终端代表的是量产汽车，装备了低成本传感器。</p>
<h2 id="6-1-Map-Decompression"><a href="#6-1-Map-Decompression" class="headerlink" title="6.1 Map Decompression"></a>6.1 Map Decompression</h2><p>终端汽车将边界点地图<strong>解压缩</strong>为语义地图：在边界内填充相同语义标签的像素。</p>
<h2 id="6-2-ICP-Localization"><a href="#6-2-ICP-Localization" class="headerlink" title="6.2 ICP Localization"></a>6.2 ICP Localization</h2><p>与制图过程类似，终端汽车通过CNN 网络提取前视摄像头捕捉图片中的语义信息，并投影至汽车坐标系中，然后当前汽车的位姿通过<strong>语义特征匹配</strong>进行获取。这里，作者使用<strong>ICP 方法</strong>来进行位姿估计：</p>
<p><img src="/2024/03/02/qin2021/f5.png" alt="f5" title="formula 5"></p>
<p>其中，$\mathbf{q, p}$ 分别是当前帧的四元数和位置参数；$\mathcal{S}$ 表示当前帧中的语义特征点；$[x_k^v, y_k^v, 0]$ 表示语义特征在汽车坐标系下的坐标；$[x_k^w, y_k^w, 0]$ 表示世界坐标系下地图中最近的点坐标。</p>
<p>最终，作者采用<strong>EKF 架构</strong>将里程计和视觉定位结果进行<strong>融合</strong>，得到鲁棒、顺滑的轨迹。</p>
<h1 id="7-Experimental-Results"><a href="#7-Experimental-Results" class="headerlink" title="7 Experimental Results"></a>7 Experimental Results</h1><p>在定位精度测试中，作者使用RTK GPS 作为真值，对于自动驾驶任务，只解算 $[x, y, yaw]$ 信息，作者将本文方法与基于Lidar 的方法进行比较，结果如下所示：</p>
<p><img src="/2024/03/02/qin2021/t1.png" alt="t1" title="table 1"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 RDS-SLAM_Real-Time Dynamic SLAM Using Semantic Segmentation Methods</title>
    <url>/2024/02/23/rds-slam/</url>
    <content><![CDATA[<p>Liu, Yubao, and Jun Miura. “RDS-SLAM: Real-Time Dynamic SLAM Using Semantic Segmentation Methods.” <em>IEEE Access</em> 9 (2021): 23772–85. <a href="https://doi.org/10.1109/ACCESS.2021.3050617">https://doi.org/10.1109/ACCESS.2021.3050617</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献如下：</p>
<ol>
<li>提出一个基于语义的<strong>实时动态vSLAM 算法</strong>——RDS-SLAM，该算法的跟踪线程不需要等待语义结果，可在保持实时的情况下高效利用语义分割结果进行<strong>动态物体检测和外点剔除</strong>；</li>
<li>作者提出了一种<strong>关键帧选取策略</strong>，使得在利用任意语义分割方法的情况下，尽可能使用<strong>最新的语义信息</strong>进行外点剔除；</li>
<li>在TUM 数据集上证明了本方法的实时性能。</li>
</ol>
<span id="more"></span>
<p>Fig .2 是利用语义信息进行动态特征点检测方法常用的模式，该模式会为了获取语义信息而阻挡跟踪、制图等线程，作者称这种模式为<strong>blocked model</strong>。</p>
<p><img src="/2024/02/23/rds-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3 System Overview"></a>3 System Overview</h1><p>本系统的架构如Fig. 3所示，语义线程与其他线程<strong>并行运行</strong>，因此不会阻挡跟踪线程的运行，并将语义信息保存至atlas 中。语义标签用于产生先验动态物体的掩码，并利用语义信息更新与关键帧特征点相匹配的<strong>地图点的移动概率</strong>，最终，使用atlas 中的语义信息优化相机位姿。</p>
<p>atlas 管理两种地图：<strong>动态地图</strong> active map 与<strong>非动态地图</strong> non-active map，当相机跟踪失败且数帧重定位失败时，动态地图转换为非动态地图，并初始化一个新的地图。在atlas 中，利用共视图和spanning tree 管理关键帧和地图点。</p>
<p><img src="/2024/02/23/rds-slam/fig3.png" alt="fig3" title="figure 3"></p>
<h1 id="4-Semantic-Thread"><a href="#4-Semantic-Thread" class="headerlink" title="4 Semantic Thread"></a>4 Semantic Thread</h1><p>语义线程的过程如Fig. 4所示，作者将选取关键帧序列KF 中的关键帧去请求语义标签的过程称为<strong>semantic keyframe selection 语义关键帧选取过程</strong>。</p>
<p><img src="/2024/02/23/rds-slam/fig4.png" alt="fig4" title="figure 4"></p>
<p>语义线程的实施细节如算法1所示：</p>
<p><img src="/2024/02/23/rds-slam/a1.png" alt="a1" title="algorithm 1"></p>
<h2 id="4-1-Semantic-Keyframe-Selection-Algorithm"><a href="#4-1-Semantic-Keyframe-Selection-Algorithm" class="headerlink" title="4.1 Semantic Keyframe Selection Algorithm"></a>4.1 Semantic Keyframe Selection Algorithm</h2><p>作者定义<strong>semantic delay 语义延迟</strong>来表示具有最新语义信息的帧id 与当前帧id 的距离：</p>
<p><img src="/2024/02/23/rds-slam/f1.png" alt="f1" title="formula 1"></p>
<p>Fig. 7展示了本系统的关键帧选取策略，作者选取关键帧序列KF 中的<strong>头尾关键帧</strong>进行语义信息获取：选取KF 尾部的关键帧进行语义获取可以使得系统可以获取<strong>相对新的语义信息</strong>；选取KF 头部关键帧的原因在于，与blocked model 不同的是，本系统会缺失前面几帧图片的语义信息，由于跟踪线程处理速度远快于语义分割线程速度，因此前期会因为动态物体的存在而积累大量的误差，因此需要利用前期关键帧中的语义信息来<strong>矫正相机位姿</strong>。</p>
<p><img src="/2024/02/23/rds-slam/fig7.png" alt="fig7" title="figure 7"></p>
<h2 id="4-3-Semantic-Mask-Generation"><a href="#4-3-Semantic-Mask-Generation" class="headerlink" title="4.3 Semantic Mask Generation"></a>4.3 Semantic Mask Generation</h2><p>本系统将实例分割的二维掩码图片融合进一张掩码图片中，如Fig. 8所示，然后利用该掩码图片计算地图点的<strong>先验移动概率</strong>；为了减小因分割精度不足导致部分边缘动态点检测失败的情况，作者使用<strong>掩码膨胀策略</strong>将属于物体边缘的特征点也囊括进去，如Fig. 9所示。</p>
<p><img src="/2024/02/23/rds-slam/fig8.png" alt="fig8" title="figure 8"></p>
<p><img src="/2024/02/23/rds-slam/fig9.png" alt="fig9" title="figure 9"></p>
<h2 id="4-4-Moving-Probability-Update"><a href="#4-4-Moving-Probability-Update" class="headerlink" title="4.4 Moving Probability Update"></a>4.4 Moving Probability Update</h2><p>本系统利用<strong>移动概率</strong>来将语义信息从语义线程传递至跟踪线程，该移动概率被用于跟踪线程的<strong>外点检测和剔除</strong>。</p>
<h3 id="4-4-1-Definition-of-Moving-Probability"><a href="#4-4-1-Definition-of-Moving-Probability" class="headerlink" title="4.4.1 Definition of Moving Probability"></a>4.4.1 Definition of Moving Probability</h3><p>由于CNN 存在准确性、鲁棒性的问题，所以不能简单使用单帧图片的结果进行判定；针对该问题，作者考虑使用多帧图片的<strong>空间-时间一致性</strong>来实现对动态特征点的精确检测，因此，使用移动概率来利用<strong>连续关键帧</strong>的语义信息。</p>
<p><img src="/2024/02/23/rds-slam/fig11.png" alt="fig11" title="figure 11"></p>
<h3 id="4-4-2-Definition-of-Observed-Moving-Probability"><a href="#4-4-2-Definition-of-Observed-Moving-Probability" class="headerlink" title="4.4.2 Definition of Observed Moving Probability"></a>4.4.2 Definition of Observed Moving Probability</h3><p>由于分割结果不是100%准确的，因此，作者定义<strong>观测移动概率</strong>：</p>
<p><img src="/2024/02/23/rds-slam/f1-1.png" alt="f1-1" title="formula 1-1"></p>
<p>其中，$\alpha, \beta$ 的值是与语义分割网络的精度相关的。本系统中，将两者的值设定为0.9，即认为语义分割的结果是<strong>相对可靠</strong>的。</p>
<h3 id="4-4-3-Moving-Probability-Update"><a href="#4-4-3-Moving-Probability-Update" class="headerlink" title="4.4.3 Moving Probability Update"></a>4.4.3 Moving Probability Update</h3><p>当前时间的移动概率 $bel(m_t)$ 是基于语义观测 $z_{1:t}$ 和初始状态 $m_0$ 进行预测的，利用<strong>贝叶斯滤波器</strong>进行更新：</p>
<p><img src="/2024/02/23/rds-slam/f2.png" alt="f2" title="formula 2"></p>
<h1 id="5-Tracking-Thread"><a href="#5-Tracking-Thread" class="headerlink" title="5 Tracking Thread"></a>5 Tracking Thread</h1><p>算法2展示了在跟踪上帧模型中的数据关联方法，系统会设定一个<strong>阈值</strong> $\tau$ ，<strong>优先使用</strong>静态特征点进行位姿解算；如果静态特征点的数量小于该阈值，会进一步使用<strong>未知状态的特征点</strong>进行位姿解算。</p>
<p><img src="/2024/02/23/rds-slam/a2.png" alt="a2" title="algorithm 2"></p>
<h1 id="6-Optimization"><a href="#6-Optimization" class="headerlink" title="6 Optimization"></a>6 Optimization</h1><p>本系统使用语义关键帧选取策略给定的关键帧来优化相机位姿：根据地图点的运动概率该修改ORB-SLAM3 的误差项，优化过程中<strong>只使用静态特征点</strong>。</p>
<p><img src="/2024/02/23/rds-slam/f6.png" alt="f6" title="formula 6"></p>
<p>其中，X 表示地图点的3D 位姿；T 表示待优化关键帧位姿；x 表示关键帧中的匹配特征点。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Monocular Localization in HD Maps by Combining Semantic Segmentation and Distance Transform</title>
    <url>/2024/03/01/pauls2020/</url>
    <content><![CDATA[<p>Pauls, Jan-Hendrik, Kursat Petek, Fabian Poggenhans, and Christoph Stiller. “Monocular Localization in HD Maps by Combining Semantic Segmentation and Distance Transform.” In <em>2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 4595–4601. Las Vegas, NV, USA: IEEE, 2020. <a href="https://doi.org/10.1109/IROS45743.2020.9341003">https://doi.org/10.1109/IROS45743.2020.9341003</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ol>
<li>提出使用语义分割网络来克服之前常用的物体探测网络生成<strong>bboxing 的空间限制</strong>，这使得我们可以检测到稀疏、可共享、独立于传感器的HD 地图中的所有地图元素；</li>
<li>使用distance transform 来解决稠密语义信息的<strong>数据关联</strong>问题，且该关联过程本质上是动态的；</li>
<li>将<strong>语义信息和标准的汽车里程计</strong>组成为一个鲁棒的位姿图优化，克服了语义定位只使用图片的缺点。</li>
</ol>
<span id="more"></span>
<p><img src="/2024/03/01/pauls2020/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-Semantic-Segmentation-and-Post-processing"><a href="#3-Semantic-Segmentation-and-Post-processing" class="headerlink" title="3 Semantic Segmentation and Post-processing"></a>3 Semantic Segmentation and Post-processing</h1><p>作者使用包含多个检测头的ResNet-38 网络进行语义分割：</p>
<ul>
<li>其中一个检测头按照<strong>增强型Cityscapes 数据集种类</strong>进行分割，种类的增强在于增加了所有的道路标志lane marking 类别，该结果被命名为Cityscapes+<em>LM</em>。</li>
<li>第二个检测头预测<strong>车道</strong>，特别是本车所在的车道区域。</li>
</ul>
<p>只关注与地图元素对应的语义类别，curbs (C), lane borders (LB), lane markings (LM), traffic lights (TL), traffic signs (TS)。由于本网络无法检测车道边界LB，作者采用<strong>形态学方法</strong>根据<strong>车道区域</strong>提取出相应的LB。</p>
<h1 id="4-Distance-Transform"><a href="#4-Distance-Transform" class="headerlink" title="4 Distance Transform"></a>4 Distance Transform</h1><p>将某个类别的<strong>二值图片</strong>记为 $B_c$ ，distance transform 用于将 $B_c$ 转化为一个<strong>距离图片</strong> $D_c$ ，具有相同的维度，但是是连续的像素数值。</p>
<p><img src="/2024/03/01/pauls2020/f1.png" alt="f1" title="formula 1"></p>
<h1 id="5-Semantic-Localization"><a href="#5-Semantic-Localization" class="headerlink" title="5 Semantic Localization"></a>5 Semantic Localization</h1><p>作者将不同地标采样成<strong>3D 点</strong>来表示，这样可以适用于任何形状的地标。利用<strong>初始位姿估计</strong>将附近的地表点映射至距离图片 $D_c$ ，构建如下所示的<strong>损失函数</strong>：</p>
<p><img src="/2024/03/01/pauls2020/f2.png" alt="f2" title="formula 2"></p>
<p>原理在于最优位姿p 会使得所有投影后的地标位于相应的<strong>语义标签图片块</strong>中，对于非最优的情况，每个地标需要移动至下一个合适的图片块，该信息可通过对距离图片进行<strong>插值</strong>以生成一个<strong>顺滑的梯度</strong>。因此，插值后的距离图片可以视为一个<strong>快速查找表</strong>，只需计算一次然后就可用于每一次优化步骤。另一个优势在于这个查找可<strong>动态</strong>构建地标与图片块之间的联系，也就是说，不需要额外的处理就可以在每一次优化中改变。</p>
<p>对于外点剔除，作者使用带有<strong>变量宽度</strong>的Tukey’s biweight 损失作为<strong>鲁棒损失函数</strong> $\rho$ ，距离较远的地图元素，如交通牌、交通灯等可以相应调整变量宽度。</p>
<h1 id="6-Pose-Graph-Optimization"><a href="#6-Pose-Graph-Optimization" class="headerlink" title="6 Pose Graph Optimization"></a>6 Pose Graph Optimization</h1><p>作者构建了一个包含图片定位和汽车VO 的<strong>滑动窗口位姿图优化</strong>，VO 主要用来获取径向longitudinal 速度与 yaw 转向速率，作者假定在连续帧间VO 满足常数速度和转向速率 $v,w$ ，因此，部分2D 位姿在汽车坐标系中的非线性更新 $\widetilde{p} = (x, y, \theta)$ ：</p>
<p><img src="/2024/03/01/pauls2020/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$\Delta T$  表示连续两帧之间的时间间隔。然后该部分2D 位姿被转换至相机坐标系，并使用在高度、pitch、roll 方向的弱正则化进行补充，作为一个简化的6自由度运动模型。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 QuadricSLAM_Dual Quadrics From Object Detections as Landmarks in Object-Oriented SLAM</title>
    <url>/2024/02/21/quadricslam/</url>
    <content><![CDATA[<p>Nicholson, Lachlan, Michael Milford, and Niko Sunderhauf. “QuadricSLAM: Dual Quadrics From Object Detections as Landmarks in Object-Oriented SLAM.” <em>IEEE Robotics and Automation Letters</em> 4, no. 1 (January 2019): 1–8. <a href="https://doi.org/10.1109/LRA.2018.2866205">https://doi.org/10.1109/LRA.2018.2866205</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者认为语义地图应该是<strong>面向对象的 object-oriented</strong>，即将对象视为地图的中心实体，而<strong>二次曲面</strong>（如椭球体 ellipsoids）有着很多具有吸引力的特性来作为面向对象语义地图的地标表示方法，如Fig. 1 所示，二次曲面可以紧凑表示，且可在投影集合框架中实现高效操纵；二次曲面可以表示物体的<strong>尺寸、位置及朝向信息</strong>，而且如果必要的话，可以作为更详细3D 重建的anchors。此外，二次曲面表示形式在整合角度来看也具有吸引力，作者会在后文中证明，二次曲面可以<strong>直接利用物体检测bbox</strong> 进行构建，而且很方便整合进<strong>基于因子图的SLAM 框架</strong>中。</p>
<span id="more"></span>
<p><img src="/2024/02/21/quadricslam/image-20240221095315076.png" alt="image-20240221095315076" title="figure 1"></p>
<p>本文做出的贡献：</p>
<ol>
<li>将SLAM 中的物体地标使用<strong>受限二次曲面</strong> constrained dual quadrics 进行<strong>参数化表示</strong>；</li>
<li>作者将物体检测网络（如YOLOv3）视为SLAM系统中的传感器，将其观测（物体 bbox）通过<strong>几何误差方程</strong>来直接约束二次曲面参数；</li>
<li>为了将二次曲面参数表示整合进SLAM 系统中，作者构建了一个基于因子图的SLAM 方程，可联合估计二次曲面和载体位姿参数（在假设解决了物体关联前提下 assuming solved data association）。</li>
</ol>
<h1 id="3-Dual-Quadrics——Fundamental-Concepts"><a href="#3-Dual-Quadrics——Fundamental-Concepts" class="headerlink" title="3 Dual Quadrics——Fundamental Concepts"></a>3 Dual Quadrics——Fundamental Concepts</h1><h2 id="3-1-Dual-Quadrics"><a href="#3-1-Dual-Quadrics" class="headerlink" title="3.1 Dual Quadrics"></a>3.1 Dual Quadrics</h2><p>二次曲面是3D 空间中的曲面，可使用<strong>4x4 对称矩阵</strong> $Q$ 来表示；in dual form，一个二次曲面可以使用一组正切平面来定义，这些平面组成一个二次曲面的包络envelope，因此，对偶二次曲面dual quadric $Q^\ast$ 定义为满足 $\pi^TQ^\ast\pi = 0$ 的所有平面 $\pi$ 。二次曲面包含的立方体包括球体、椭球体、双曲面、圆锥体和圆柱体。</p>
<p>二次曲面包含<strong>9自由度</strong>，对应于4x4 对称矩阵 $Q$ 中10个独立的元素，多余的一个元素表示尺寸信息。因此，可以使用10维向量来表示一个<strong>对偶二次曲面</strong>：$\hat{\mathbf{q}} = (\hat{q}_1, …, \hat{q}_{10})$ ，其中的每个元素对应于 $Q^\ast$ 的10个独立元素。</p>
<p>对偶二次曲面投影至图像平面上为一个对偶二次曲线：$\mathbf{C}^\ast = \mathbf{P}\mathbf{Q}^\ast\mathbf{P}^T$ ，其中 $\mathbf{P} = \mathbf{K}[\mathbf{R} | \mathbf{t}]$ 为相机投影矩阵，包含内参和外参。二次曲面包含的曲线包括圆、椭圆、抛物线和双曲线。</p>
<h2 id="3-2-Constrained-Dual-Quadric-Parameterization"><a href="#3-2-Constrained-Dual-Quadric-Parameterization" class="headerlink" title="3.2 Constrained Dual Quadric Parameterization"></a>3.2 Constrained Dual Quadric Parameterization</h2><p>由于二次曲面可以是闭合曲面也可以是非闭合曲面（如抛物面和双曲面），只有<strong>闭合曲面</strong>才可以表示有意义的物体地标，所以作者使用<strong>受限对偶二次曲面</strong>表示形式来确保曲面是闭合的椭球体或球体。作者将对偶二次曲面表示为：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221104708225.png" alt="image-20240221104708225" title="formula 1"></p>
<p>其中，$\overset{\frown}{\mathbf{Q}}^\ast$ 为一个中心位于原点的椭球体，$\mathbf{Z}$ 表示齐次变换矩阵，包含旋转和平移：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221105215815.png" alt="image-20240221105215815" title="formula 2"></p>
<p>其中，$\mathbf{t} = (t_1, t_2, t_3)$ 为平移向量；$\mathbf{R}(\mathbf{\theta})$ 为由角度 $\mathbf{\theta} = (\theta_1, \theta_2, \theta_3)$ 定义的旋转矩阵；$\mathbf{s} = (s_1, s_2, s_3)$ 为椭球体三个轴的尺度参数。在后续中，作者使用9维向量表示一个<strong>受限对偶二次曲面</strong>：$\mathbf{q} = (\theta_1, \theta_2, \theta_3, t_1, t_2, t_3, s_1, s_2, s_3)$ ，并根据式1来重建完整的对偶二次曲面 $\mathbf{Q}^\ast$ 。</p>
<h1 id="4-A-Sensor-Model-for-Image-Based-Object-Detections"><a href="#4-A-Sensor-Model-for-Image-Based-Object-Detections" class="headerlink" title="4 A Sensor Model for Image-Based Object Detections"></a>4 A Sensor Model for Image-Based Object Detections</h1><h2 id="4-1-Motivation"><a href="#4-1-Motivation" class="headerlink" title="4.1 Motivation"></a>4.1 Motivation</h2><p>作者的目标是将SOTA 物体检测器<strong>作为一个传感器</strong>整合进SLAM中，因此，需要构建一个<strong>传感器模型</strong>，在给定预估相机位姿 $\mathbf{x}_i$ 和预估地图结构（如二次曲面参数 $\mathbf{q}_j$ ）时可以预测物体检测器的观测参数。物体检测器的观测包含物体检测bbox 和对应的物体标签，本文主要关注bbox 参数，作者使用 $\mathbf{b} = (x_{min}, y_{min}, x_{max}, y_{max})$ 来表示bbox ，因此，该传感器模型可表示为：$\beta(\mathbf{x}_i, \mathbf{q}_j) = \hat{\mathbf{b}}_{ij}$ ，即根据相机位姿 $\mathbf{x}_i$ 和二次曲面参数 $\mathbf{q}_j$ 来预测bbox 观测 $\hat{\mathbf{b}}_{ij}$ 。</p>
<p>在构建了该传感器模型后，作者可以<strong>在预测和观测之间形成几何误差项</strong>，这对于本文提出的SLAM 系统至关重要。</p>
<h2 id="4-2-Deriving-the-Object-Detection-Sensor-Model-beta"><a href="#4-2-Deriving-the-Object-Detection-Sensor-Model-beta" class="headerlink" title="4.2 Deriving the Object Detection Sensor Model $\beta$"></a>4.2 Deriving the Object Detection Sensor Model $\beta$</h2><p>将二次曲面 $\mathbf{q}_j$ 根据相机位姿 $\mathbf{x}_i$ 投影至图像中：$\mathbf{C}^\ast_{ij} = \mathbf{P}_i\mathbf{Q}^\ast_{(q_j)}\mathbf{P}^T_i$ ，其中 $\mathbf{P} = \mathbf{K}[\mathbf{R} | \mathbf{t}]$ 为相机投影矩阵，包含相机内参 $\mathbf{K}$ 和相机位姿；由此获得对偶双曲线 $\mathbf{C}^\ast$ ，并进一步获取其原始形式 $\mathbf{C}$ 。简单的模型会直接计算二次曲线 $\mathbf{C}$ 的闭合bbox，并将该bbox 截断来适应图片边界，如Fig. 2（a）所示，当该二次曲线大部分位于图像边界外时，二次曲线 $\mathbf{C}$ 在图片内对应的bbox（蓝色虚线）与真值bbox （绿实线）之间存在较大的差异，即该方法会引入极大的误差。</p>
<p><img src="/2024/02/21/quadricslam/image-20240221112358306.png" alt="image-20240221112358306" title="figure 2"></p>
<p>一个准确的传感器模型需要获取二次曲线和图片边界的交点，对于物体检测bbox 的正确预测应该是最小轴对齐的矩形，该矩形在将图片尺寸内的二次曲线完全包括进去，该矩形bbox 表示为：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221113652000.png" alt="image-20240221113652000" title="formula 3"></p>
<h2 id="4-3-Calculating-the-On-Image-Conic-Bounding-Box"><a href="#4-3-Calculating-the-On-Image-Conic-Bounding-Box" class="headerlink" title="4.3 Calculating the On-Image Conic Bounding Box"></a>4.3 Calculating the On-Image Conic Bounding Box</h2><p>通过以下算法来获取图片中双曲线的正确bbox：</p>
<ol>
<li>寻找该二次曲线上包含极值坐标的四个点 $\{\mathbf{p}_1, …, \mathbf{p}_4\}$ ；</li>
<li>寻找该二次曲线与图片边界的交点（最多8个） $\{\mathbf{p}_5, …, \mathbf{p}_{12}\}$ ；</li>
<li>移除 $\mathcal{P} = \{\mathbf{p}_1, …, \mathbf{p}_4\}$  中所有的non-real 点和位于图片边界外的点；</li>
<li>从 $\mathcal{P}$ 中的剩余点获取极值坐标点。</li>
</ol>
<p>经过以上步骤，可以得到向量：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221114446827.png" alt="image-20240221114446827" title="formula 4"></p>
<p>上式表示的bbox 即可完整包含二次曲线 $\mathbf{C}$ 在图片可见区域的部分。</p>
<h1 id="5-SLAM-With-Dual-Quadric-Landmark-Representations"><a href="#5-SLAM-With-Dual-Quadric-Landmark-Representations" class="headerlink" title="5 SLAM With Dual Quadric Landmark Representations"></a>5 SLAM With Dual Quadric Landmark Representations</h1><h2 id="5-1-General-Problem-Setup"><a href="#5-1-General-Problem-Setup" class="headerlink" title="5.1 General Problem Setup"></a>5.1 General Problem Setup</h2><p>作者构建如下的SLAM 问题方程：$\mathbf{x}_{i+1} = f(\mathbf{x}_i, \mathbf{u}_i) + \mathbf{w}_i$ ，其中，$\mathbf{u}_i$ 表示连续两个位姿 $\mathbf{x}_i, \mathbf{x}_{i+1}$ 之间的里程计观测；$f$ 通常为非线性方程；$\mathbf{w}_i$ 为零均值高斯分布，协方差为 $\sum_i$ 。此处不具体讨论里程计观测 $\mathbf{u}_i$ 的具体获取形式。</p>
<p>此外，还有物体路标观测 $B = \{\mathbf{b}_{ij}\}$ ，表示在位姿 $\mathbf{x}_i$ 处对物体 $j$ 观测到的bbox 。</p>
<blockquote>
<p> 值得注意的是，作者在此假设数据关联问题已经得到解决。</p>
</blockquote>
<h2 id="5-2-Building-and-Solving-a-Factor-Graph-Representation"><a href="#5-2-Building-and-Solving-a-Factor-Graph-Representation" class="headerlink" title="5.2 Building and Solving a Factor Graph Representation"></a>5.2 Building and Solving a Factor Graph Representation</h2><p>条件概率分布可表示为因子式：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221150906286.png" alt="image-20240221150906286" title="formula 5"></p>
<p>包含：载体位姿 $X = \{\mathbf{x}_i\}$ ，地标  $Q= \{\mathbf{q}_i\}$ ，里程计观测 $U= \{\mathbf{u}_i\}$，物体观测 $B= \{\mathbf{b}_{ij}\}$。</p>
<p>里程计因子 $P(\mathbf{x}_{i+1} | \mathbf{x}_i, \mathbf{u}_i)$ 假设服从高斯分布，即 $\mathbf{x}_{i+1} \sim \mathcal{N}(f(\mathbf{x}_i, \mathbf{u}_i), \sum_i)$ 。为了将地标因子集成至高斯因子图中，应用贝叶斯法则：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221152014709.png" alt="image-20240221152014709" title="formula 6"></p>
<p>因为目标是求取MAP，所以忽略分母项（只发挥归一化作用）。进一步地，作者假设 $P(\mathbf{q}_{j} | \mathbf{x}_i)$ 服从均匀分布，则式6可转化为求取最大似然 $P(\mathbf{b}_{ij} | \mathbf{q}_j, \mathbf{x}_i)$ 。该似然项可建模为高斯分布 $\mathcal{N}(\beta_{(\mathbf{x}_i, \mathbf{q}_j)}, \Lambda_{ij})$ ，其中，$\beta$ 为上节构建的传感器观测模型，$\Lambda$ 为协方差矩阵，表示观测到的检测物体在图片空间中的不确定性。</p>
<p>式5的解可视为求解<strong>非线性最小二乘问题</strong>：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221153522001.png" alt="image-20240221153522001" title="formula 7"></p>
<p>其中，第一项的差值是SE(3) 空间的差值。</p>
<h2 id="5-3-The-Geometric-Error-Term"><a href="#5-3-The-Geometric-Error-Term" class="headerlink" title="5.3 The Geometric Error Term"></a>5.3 The Geometric Error Term</h2><p>式7的第二项 $||\mathbf{b}_{ij} - \beta_{(\mathbf{x}_{i}, \mathbf{q}_{j})}||^2_{\Lambda_{ij}}$ 是一个几何误差项，因为 $\mathbf{b}, \beta$ 都是包含像素坐标的向量，作者发现该几何误差项要比之前工作中的代数误差项更具优势。且，可以通过协方差矩阵项 $\Lambda_{ij}$ 更方便地将物体检测器的<strong>空间不确定性</strong>传递至SLAM 系统中。</p>
<h2 id="5-4-Variable-Initialization"><a href="#5-4-Variable-Initialization" class="headerlink" title="5.4 Variable Initialization"></a>5.4 Variable Initialization</h2><p>载体位姿 $\mathbf{x}_i$ 可通过里程计观测值 $\mathbf{u}_i$ 进行初始化，接下来讨论对偶二次曲面地标 $\mathbf{q}_j$ 的初始化。可通过最小二乘满足以下定义式来进行初始化：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221155058842.png" alt="image-20240221155058842" title="formula 8"></p>
<p>其中，$\hat{\mathbf{q}_j}$ 为3.1节的通用对偶二次曲面：$\hat{\mathbf{q}} = (\hat{q}_1, …, \hat{q}_{10})$ 。</p>
<p>可使用地标bbox 观测 $\mathbf{b}_{ij}$ 和对应的线 $\mathbf{l}_{ijk}$ （bbox的4条线）来定义平面 $\mathbf{\pi}_{ijk}$ : $\mathbf{\pi}_{ijk} = \mathbf{P}^T_i \mathbf{l}_{ijk}$，其中，相机位姿 $\mathbf{P}_i$ 可通过初始位姿估计获取，最终，式8可转换为：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221160103483.png" alt="image-20240221160103483" title="formula 9"></p>
<p>使用不同视角 $i$ 和平面 $k$ ，基于式9构建线性方程组，通过SVD 分解获取最小二乘解 $\hat{\mathbf{q}}_j$ 。该解是一个通用的二次曲面，并没有被约束为一个椭球体，这里会进一步使用3.2节的方式将其进一步参数化表示，参数求取过程如下所述：</p>
<p><img src="/2024/02/21/quadricslam/image-20240221162842265.png" alt="image-20240221162842265" title="formula 10"></p>
<p>其中，$\mathbf{Q}_{33}$ 为二次曲面矩阵 $\mathbf{Q}$ 的左上侧3x3 子矩阵；$\lambda_i$ 为 $\mathbf{Q}$ 的特征值；旋转矩阵 $\mathbf{R}(\theta)$ 为 $\mathbf{Q}_{33}$ 的特征矩阵；平移向量表示为 $\mathbf{t} = (\hat{q}_4, \hat{q}_7, \hat{q}_9)/\hat{q}_{10}$ 。</p>
<h1 id="6-Experiments-and-Evaluation"><a href="#6-Experiments-and-Evaluation" class="headerlink" title="6 Experiments and Evaluation"></a>6 Experiments and Evaluation</h1><p>作者在TUM 数据集上进行测试，与两种里程计算法进行比较，分别是Fovis 和 ORB-SLAM2 算法，物体检测器使用YOLOv3网络；里程计噪声模型中的平移和旋转标准差均设置为0.001，物体bbox 的标准差是计算该物体所有观测bbox 长与宽的标准差之和。定位误差ATE 如Table 1所示，</p>
<p><img src="/2024/02/21/quadricslam/image-20240221171102454.png" alt="image-20240221171102454" title="table 1"></p>
<p>可以发现，相较于fovis 视觉里程计，加入二次曲面物体地标后定位精度提升明显，但较ORB-SLAM2 有所下降，作者认为原因可能有两个：</p>
<ol>
<li>bbox 观测噪声不太符合高斯分布；</li>
<li>物体遮挡导致过小的bbox观测，从而影响定位精度。</li>
</ol>
<p>作者通过进一步实验：对于fr1_desk2, fr3_office 数据序列，作者舍弃宽高标准差大于一定值的物体bbox ，从而实现两个数据序列的定位精度分别达到0.0239和0.0087。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Pose Optimization</tag>
        <tag>Object SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 RDMO-SLAM_Real-Time Visual SLAM for Dynamic Environments Using Semantic Label Prediction With Optical Flow</title>
    <url>/2024/02/23/rdmo-slam/</url>
    <content><![CDATA[<p>Liu, Yubao, and Jun Miura. “RDMO-SLAM: Real-Time Visual SLAM for Dynamic Environments Using Semantic Label Prediction With Optical Flow.” <em>IEEE Access</em> 9 (2021): 106981–97. <a href="https://doi.org/10.1109/ACCESS.2021.3100426">https://doi.org/10.1109/ACCESS.2021.3100426</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献如下：</p>
<ol>
<li>提出一个面向动态环境的<strong>实时语义vSLAM 算法</strong>——RDMO-SLAM，该算法使用Mask R-CNN（语义分割）和 PWC-Net（光流估计），可同时实现良好的<strong>跟踪表现以及实时特性</strong>；</li>
<li>利用<strong>光流法</strong>来预测Mask R-CNN 的语义结果，使得跟踪线程可利用<strong>尽可能多的语义信息</strong>；</li>
<li>实验验证了本算法的实时性能（<strong>30 Hz</strong>）。</li>
</ol>
<span id="more"></span>
<h1 id="3-Rigid-Scene-Assumption-Problem"><a href="#3-Rigid-Scene-Assumption-Problem" class="headerlink" title="3 Rigid Scene Assumption Problem"></a>3 Rigid Scene Assumption Problem</h1><p>如Fig. 1所示，展示了动态物体对SLAM 算法的影响。</p>
<p><img src="/2024/02/23/rdmo-slam/fig1.png" alt="fig1" title="figure 1"></p>
<p>对于场景中的动态物体，正确的误差项应为：</p>
<p><img src="/2024/02/23/rdmo-slam/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$H_{t-1}^t$ 表示地标的<strong>运动参数</strong>。据作者所言，有两种方法来解算该问题：方法一是使用<strong>多物体跟踪</strong>对物体的运动参数 $H$ 和相机位姿 $T(\epsilon)$ 进行<strong>联合优化</strong>（如VDO-SLAM、DynaSLAMⅡ），但该方法是基于<strong>刚性物体的假设</strong>，即假设物体及其表面的特征点拥有相同的运动模式，该假设不适用于人类等非刚性物体；而且，由于这些方法使用<strong>blocked model</strong>（如Fig. 3所示）导致无法做到实时。</p>
<p><img src="/2024/02/23/rdmo-slam/fig3.png" alt="fig3" title="figure 3"></p>
<p>第二种方法是<strong>将动态特征点进行剔除</strong>，构建的损失函数如下所示：</p>
<p><img src="/2024/02/23/rdmo-slam/f5.png" alt="f5" title="formula 5"></p>
<p>大部分算法都是根据特征点的动态属性将<strong>权重参数</strong> $W_j$ 设置为0和1，而作者在其前作RDS-SALM 中将 $W_j$ 设置为<strong>静态概率</strong>（1-移动概率），作者不选择直接将动态地图点进行剔除，因为可以避免静态匹配点过少的场景；作者会随着时间<strong>更新每个地图点的移动概率</strong>来减少动态物体带来的影响。</p>
<h1 id="4-System-Overview"><a href="#4-System-Overview" class="headerlink" title="4 System Overview"></a>4 System Overview</h1><p>RDMO-SLAM 算法框架如Fig. 4所示，本系统是基于ORB-SLAM3 和 RDS-SLAM 开发的，本算法继续使用RDS-SLAM 中的语义线程，根据<strong>移动概率</strong>将地标分为三组：未知、静态以及动态，然后在跟踪线程中使用尽可能多的静态特征点。此外，作者新增了一个Label Prediction 模块，在等待语义分割结果的时候<strong>使用光流来预测语义标签</strong>；速度估计线程是利用光流来计<strong>算并更新地图点的速度</strong>，利用地图点的速度信息可作为<strong>额外的约束</strong>来滤除跟踪线程中的错误关联。最终，包含<strong>运动概率和速度的语义信息</strong>被用于滤除外点。</p>
<p><img src="/2024/02/23/rdmo-slam/fig4.png" alt="fig4" title="figure 4"></p>
<h1 id="5-Optical-Flow-Thread"><a href="#5-Optical-Flow-Thread" class="headerlink" title="5 Optical Flow Thread"></a>5 Optical Flow Thread</h1><p>作者使用PWC-NET 进行<strong>光流估计</strong>，对1024*436分辨率的图片实现35FPS 的处理速度。每个像素的光流结果存储两个浮点数，分别表示像素在相邻两帧之间的位移，结果如Fig. 5所示，光流只检测身体的运动部位，如手臂与腿部。</p>
<p><img src="/2024/02/23/rdmo-slam/fig5.png" alt="fig5" title="figure 5"></p>
<p>光流的结果有两个用途：</p>
<ol>
<li>在<strong>语义线程</strong>中协助预测关键帧的语义标签，来提高语义信息的生成速度；</li>
<li>在<strong>速度估计线程</strong>中计算地图点的速度。</li>
</ol>
<h1 id="6-Semantic-Thread"><a href="#6-Semantic-Thread" class="headerlink" title="6 Semantic Thread"></a>6 Semantic Thread</h1><p>Mask R-CNN 运行速度太慢，接近200ms，为了获取更多的语义信息，作者提出一个算法：在等待 Mask R-CNN 结果的同时，利用<strong>之前获取的语义标签</strong>和参考关键帧的<strong>光流模式</strong>来预测当前关键帧的语义标签。</p>
<h2 id="6-1-Semantic-Keyframe-Selection"><a href="#6-1-Semantic-Keyframe-Selection" class="headerlink" title="6.1 Semantic Keyframe Selection"></a>6.1 Semantic Keyframe Selection</h2><p>由于Mask R-CNN 的处理速度，如果所有的关键帧都经过语义分割处理，那么语义线程与跟踪线程之间的<strong>语义延迟</strong>会逐渐增大，导致跟踪线程无法实时获取丰富的最新语义信息。为了减少语义延迟，RDS-SLAM 的关键帧选取策略会导致许多关键帧<strong>无法获取语义结果</strong>，不充分的语义信息导致RDS-SLAM 处理<strong>速度较慢</strong>（15Hz）。</p>
<p>为解决该问题，作者提出了新的关键帧选取策略，如Fig. 6所示，每次使用Mask R-CNN 对<strong>最新的关键帧</strong>（如 $KF_0, KF_3, KF_7$ ）进行处理，而之间未处理的关键帧（如 $KF_1, KF_2$ ）使用<strong>预测方法</strong>处理。</p>
<p><img src="/2024/02/23/rdmo-slam/fig6.png" alt="fig6" title="figure 6"></p>
<h2 id="6-3-Semantic-Label-Prediction"><a href="#6-3-Semantic-Label-Prediction" class="headerlink" title="6.3 Semantic Label Prediction"></a>6.3 Semantic Label Prediction</h2><p>使用已分割关键帧（如 $KF_0$ ）作为参考关键帧，利用参考关键帧的<strong>语义标签</strong>和目标关键帧的<strong>光流结果</strong>来对目标帧（如 $KF_1, KF_2$ ）的语义标签进行预测，如下式所示：</p>
<p><img src="/2024/02/23/rdmo-slam/f7.png" alt="f7" title="formula 7"></p>
<p>其中，$I_r(x_r, y_r)$ 表示参考关键帧的语义分割标签；$(f_x, f_y)$ 表示光流向量；$I_p(x_p, y_p)$ 表示待预测的标签。</p>
<h2 id="6-5-Moving-Probability-Update"><a href="#6-5-Moving-Probability-Update" class="headerlink" title="6.5 Moving Probability Update"></a>6.5 Moving Probability Update</h2><p>作者定义地图点的<strong>移动概率</strong> $p(m_t^j), m_t^j \in M=\{static(s), dynamic(d)\}$ 。本系统在语义线程中利用<strong>贝叶斯滤波器</strong>来更新移动概率：</p>
<p><img src="/2024/02/23/rdmo-slam/f8.png" alt="f8" title="formula 8"></p>
<p>其中，$p(m_0) = 0.5$ 是初始概率；$p(z_t | m_t)$ 是观测似然，根据<strong>语义标签</strong>进行设定。</p>
<h2 id="6-6-Algorithm-Implementation"><a href="#6-6-Algorithm-Implementation" class="headerlink" title="6.6 Algorithm Implementation"></a>6.6 Algorithm Implementation</h2><p>语义线程的具体算法如下所示：</p>
<p><img src="/2024/02/23/rdmo-slam/a1.png" alt="a1" title="algorithm 1"></p>
<p>其中，各个指针代表的含义如Fig. 12所示，workId 指的是当前工作的指针，会遍历每一个关键帧；refId 指的是参考关键帧；reqId 是最新语义请求的关键帧；latestId 是当前最新的关键帧。</p>
<p><img src="/2024/02/23/rdmo-slam/fig12.png" alt="fig12" title="figure 12"></p>
<p>利用语义分割或者预测获取到语义标签后，更新语义信息、获取<strong>动态物体的掩码</strong>并更新地图点的<strong>移动概率</strong>。</p>
<p><img src="/2024/02/23/rdmo-slam/a2.png" alt="a2" title="algorithm 2"></p>
<h1 id="7-Velocity-Estimation-Thread"><a href="#7-Velocity-Estimation-Thread" class="headerlink" title="7 Velocity Estimation Thread"></a>7 Velocity Estimation Thread</h1><p>语义分割只能识别出预定义的动态物体，且分割结果不一定都是准确的，因此，作者引入<strong>速度约束</strong>来进一步减少外点的影响。为了获取更为准确的速度估计，作者使用<strong>卡尔曼滤波器</strong>来更新速度：</p>
<p><img src="/2024/02/23/rdmo-slam/f17.png" alt="f17" title="formula 17"></p>
<p><img src="/2024/02/23/rdmo-slam/fig13.png" alt="fig13" title="figure 13"></p>
<h1 id="8-Tracking"><a href="#8-Tracking" class="headerlink" title="8 Tracking"></a>8 Tracking</h1><p>为了让vSLAM 实时运行，作者将语义线程、速度估计线程与跟踪线程分离开，利用地图中存储的地标<strong>移动概率</strong>和<strong>速度</strong>来滤除外点。实验中，作者设定 $\theta_d = 0.6, \theta_s = 0.4$ 。跟踪线程是用来进行初始位姿估计的，作者优先使用静态特征点；若不足，则进一步使用未知特征点；若还不足，进一步使用动态特征点。</p>
<p><img src="/2024/02/23/rdmo-slam/fig10.png" alt="fig10" title="figure 10"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 RLD-SLAM_A Robust Lightweight VI-SLAM for Dynamic Environments Leveraging Semantics and Motion Information</title>
    <url>/2024/03/15/rld-slam/</url>
    <content><![CDATA[<p>Zheng, Zengrui, Shifeng Lin, and Chenguang Yang. “RLD-SLAM: A Robust Lightweight VI-SLAM for Dynamic Environments Leveraging Semantics and Motion Information.” <em>IEEE Transactions on Industrial Electronics</em>, 2024, 1–11. <a href="https://doi.org/10.1109/TIE.2024.3363744">https://doi.org/10.1109/TIE.2024.3363744</a>.</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>现存的主流动态SLAM 可分为两个方向：基于<strong>语义分割</strong>和基于<strong>物体检测</strong>，两者均存在明显的优劣势，为此，作者提出了一种robust lightweight dynamic SLAM (RLD-SLAM)，利用语义、运动信息在动态环境中实现<strong>鲁棒、轻量</strong>的视觉-惯性SLAM系统。</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本算法结合<strong>物体检测</strong>和<strong>贝叶斯滤波器</strong>来准确识别出环境中的动态特征点，不依赖于深度信息，因此可适用于各种输入设备。此外，RLD-SLAM 可使用<strong>IMU观测预积分</strong>来辅助高动态场景中动态物体的跟踪，如Fig.1所示。</p>
<span id="more"></span>
<p><img src="/2024/03/15/rld-slam/fig1.png" alt="fig1" title="figure 1"></p>
<p>本文的贡献：</p>
<ol>
<li>结合物体检测和贝叶斯滤波器来快速获取动态环境中的静态特征点；</li>
<li>使用IMU 信息来跟踪动态物体，以增强SLAM 在高动态场景中的鲁棒性；</li>
<li>设计了RLD-SLAM，可适应各种输入传感器和动态场景，实验证明了算法的有效性。</li>
</ol>
<h1 id="3-System-Introduction"><a href="#3-System-Introduction" class="headerlink" title="3 System Introduction"></a>3 System Introduction</h1><h2 id="3-1-Overall-Framework"><a href="#3-1-Overall-Framework" class="headerlink" title="3.1 Overall Framework"></a>3.1 Overall Framework</h2><p>RLD-SLAM 包含以下四个线程：</p>
<ol>
<li>物体检测</li>
<li>特征跟踪</li>
<li>局部制图</li>
<li>回环检测</li>
</ol>
<p><img src="/2024/03/15/rld-slam/fig2.png" alt="fig2" title="figure 2"></p>
<p>本系统支持单目/双目/RGB-D 相机作为输入。通过IMU 预积分来获取位姿转换矩阵 $T_I$ ，可作为一个先验位姿，其作用在于：</p>
<ol>
<li>用于矫正图片以提高物体检测性能；</li>
<li>实现更好的<strong>3D 物体跟踪</strong>，特别是在超过半数特征均是<strong>语义动态</strong>的场景下。</li>
</ol>
<p>为了增强系统的<strong>容错性能</strong>，RLD-SLAM 会在IMU 失效时继续工作：图片矫正矩阵通过后端优化得到的参考关键帧来获取，且只实现2D 物体跟踪。</p>
<h2 id="3-2-Image-Rectification-and-Object-Detection"><a href="#3-2-Image-Rectification-and-Object-Detection" class="headerlink" title="3.2 Image Rectification and Object Detection"></a>3.2 Image Rectification and Object Detection</h2><p>本系统使用YOLOv5 进行物体检测，物体检测算法在<strong>相机旋转</strong>情况下会面临<strong>精度下降</strong>的问题，为解决该问题，作者利用IMU 数据来获取一个<strong>矫正矩阵</strong>：</p>
<ol>
<li><p>首先，计算重力加速度于图片y 轴的夹角：</p>
<p><img src="/2024/03/15/rld-slam/f1.png" alt="f1" title="formula 1"></p>
</li>
<li><p>然后，将该图片旋转一定的角度 $\theta$ ：</p>
<p><img src="/2024/03/15/rld-slam/f2.png" alt="f2" title="formula 2"></p>
</li>
</ol>
<p>当IMU 不可用时，利用后端优化的<strong>参考关键帧位姿</strong>来提供矫正矩阵。</p>
<p>图片矫正过程如Fig.3所示，虽然步骤简单，但是在相机大角度旋转情况下可大幅提高动静态特征点的识别精度。</p>
<p><img src="/2024/03/15/rld-slam/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="3-3-Multiview-Probability-Estimation-and-Weighted-Optimization"><a href="#3-3-Multiview-Probability-Estimation-and-Weighted-Optimization" class="headerlink" title="3.3 Multiview Probability Estimation and Weighted Optimization"></a>3.3 Multiview Probability Estimation and Weighted Optimization</h2><p>物体检测bbox 内会包含大量的静态背景，针对这个情况，作者设计一种<strong>基于贝叶斯滤波器的多视图概率估计算法</strong>来区分bbox 内的动静态特征点，该过程如Fig.4所示，本算法利用与当前帧具有最高共视率的N 个关键帧来计算特定<strong>特征点的动态概率</strong>。</p>
<p><img src="/2024/03/15/rld-slam/fig4.png" alt="fig4" title="figure 4"></p>
<p>语义概率的更新可表示为下述的贝叶斯滤波问题：</p>
<p><img src="/2024/03/15/rld-slam/f4-1.png" alt="f4-1" title="formula 4-1"></p>
<p><img src="/2024/03/15/rld-slam/f4.png" alt="f4" title="formula 4"></p>
<p>其中，$\eta$ 为归一化常数。当前观测 $z_i$ 仅依赖于当前状态 $x_i$ ，因此概率分布 $p(z_i | x_i)$ 可从<strong>物体检测的置信度</strong>获取。</p>
<p>假设服从一阶Markov 过程，则有：</p>
<p><img src="/2024/03/15/rld-slam/f5.png" alt="f5" title="formula 5"></p>
<p>根据关键帧的选取策略可知，关键帧之间的时间间隔是不确定的；且物体在动态环境中的运动状态会随着时间改变，基于此，可以自然作出假设：同一物体在不同关键帧之间的<strong>运动状态相关性</strong>会随着时间增长而下降。因此，特征点的动态概率估计中可增加一个<strong>衰减因子</strong>：</p>
<p><img src="/2024/03/15/rld-slam/f6.png" alt="f6" title="formula 6"></p>
<p>作者提出的该方法与其他使用帧间动态概率传递方法的不同之处在于：有效利用了相关的<strong>多个历史关键帧</strong>。贝叶斯滤波器中引入的<strong>衰减因子</strong>使得该算法可以更侧重临近观测，这使得系统可以<strong>快速识别</strong>出从动态转为静态的特征点，然后作为优化过程中的地标特征点。包含了<strong>静态概率权重</strong>的RLD-SLAM 后端优化方程表示如下：</p>
<p><img src="/2024/03/15/rld-slam/f7.png" alt="f7" title="formula 7"></p>
<p>其中，$\mathcal{Z}^j$ 为所有观测到的 3D 地标；$\mathbf{r}_{ij}$ 表示重投影误差；$\Omega$ 表示将地表点投影至像素坐标系；$\Lambda_{ij}$ 表示信息矩阵；$P_{staticij}$ 表示第j 个特征点在第i 次观测后的静态概率。</p>
<h2 id="3-4-Dynamic-Object-Tracking-With-IMU-Measurement"><a href="#3-4-Dynamic-Object-Tracking-With-IMU-Measurement" class="headerlink" title="3.4 Dynamic Object Tracking With IMU Measurement"></a>3.4 Dynamic Object Tracking With IMU Measurement</h2><p>仅利用语义信息无法判断物体的真实运动状态，部分算法使用物体跟踪来解决该问题，但同时会带来大量的算力负担。本算法融合IMU 观测和视觉里程计来进行<strong>动态物体的3D 跟踪</strong>，并<strong>实时更新速度与运动状态</strong>；根据物体的运动状态，系统可自动决定是否将这些特征点包含进优化过程。该过程如Fig.5所示：</p>
<p><img src="/2024/03/15/rld-slam/fig5.png" alt="fig5" title="figure 5"></p>
<p>当一个动态物体进入相机视野中时，系统利用<strong>卡尔曼滤波器</strong>来对观测进行处理：</p>
<ol>
<li><p>首先，假设观测到的动态物体（汽车）服从<strong>匀加速模型</strong>，<strong>初始状态</strong>表示为：</p>
<p><img src="/2024/03/15/rld-slam/f10.png" alt="f10" title="formula 10"></p>
</li>
<li><p>然后，<strong>预测</strong>该动态物体的下一状态：</p>
<p><img src="/2024/03/15/rld-slam/f11-1.png" alt="f11-1" title="formula 11-1"></p>
<p><img src="/2024/03/15/rld-slam/f11.png" alt="f11" title="formula 11"></p>
<p>当获取了观测信息后，<strong>更新</strong>状态：</p>
<p><img src="/2024/03/15/rld-slam/f12.png" alt="f12" title="formula 12"></p>
</li>
</ol>
<p>该算法的<strong>关键在于获取动态环境中的观测信息</strong>。当算法可获取充足的静态特征点时，VO 效果较好，此时可计算出动态物体的世界坐标：</p>
<p><img src="/2024/03/15/rld-slam/f13.png" alt="f13" title="formula 13"></p>
<p>当物体检测算法判断当前视野中大部分特征点属于先验动态物体时，难以获取相机自身位姿 $T_{WC}^i$ ，也就无法实现对动态物体速度等状态信息的计算。在此情况下，本算法利用IMU 来增强系统的<strong>鲁棒性</strong>：对关键帧之间的IMU 观测进行短期预积分，假设(i-1) 和i 关键帧分别对应 j 和 n IMU 观测数据，则有</p>
<p><img src="/2024/03/15/rld-slam/f14.png" alt="f14" title="formula 14"></p>
<p><img src="/2024/03/15/rld-slam/f15.png" alt="f15" title="formula 15"></p>
<p>陀螺仪和加速度计的偏差可从后端的近期优化中获取，噪声影响可通过<strong>信息矩阵</strong>来表示，考虑到关键帧之间的时间<strong>间隔较短</strong>（一般不超过1s），可认为IMU 预积分<strong>误差可忽略</strong>，由此可计算汽车位姿：</p>
<p><img src="/2024/03/15/rld-slam/f17.png" alt="f17" title="formula 17"></p>
<p>利用上述观测信息来实现<strong>滤波器更新</strong>，以此计算物体的运动状态。对于静态物体，其特征点可为VO 提供更多的地标，由此可增强SLAM 的精度与鲁棒性；在此基础上，可将VO 与IMU 进行<strong>紧耦合优化</strong>来消除IMU 漂移：</p>
<p><img src="/2024/03/15/rld-slam/f20.png" alt="f20" title="formula 20"></p>
<p>总的来讲，本算法利用<strong>短期 IMU 预积分</strong>来恢复<strong>高动态场景</strong>下的 VO，然后进一步对VO 和 IMU 进行<strong>联合优化</strong>，解决IMU 的长期漂移问题。</p>
<h1 id="4-Experimental-Results"><a href="#4-Experimental-Results" class="headerlink" title="4 Experimental Results"></a>4 Experimental Results</h1><p><img src="/2024/03/15/rld-slam/t1.png" alt="t1" title="table 1"></p>
<p>作者在KITTI 数据集进行测试：在01-04 序列的郊区环境中，相机载体与动态物体速度较慢，本算法的提升效果较小；序列13和19 是城市场景，有各种动态物体，如自行车、行人、汽车等，作者认为对于非常规形状或较小形状的物体而言，没必要从中提取地标；序列08和20是高速路场景，移动物体数量较多、速度较快，该场景中RLD-SLAM 算法优势明显，这是使用<strong>历史视觉信息</strong>和<strong>IMU 运动先验</strong>带来的优势；此外，使用卡尔曼滤波器对动态物体进行松耦合状态估计也会大幅减小运算负担。</p>
<p><img src="/2024/03/15/rld-slam/t3.png" alt="t3" title="table 3"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Semantic Visual Localization</title>
    <url>/2024/02/24/schonberger2018/</url>
    <content><![CDATA[<p>Schonberger, Johannes L., Marc Pollefeys, Andreas Geiger, and Torsten Sattler. “Semantic Visual Localization.” In <em>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 6896–6906. Salt Lake City, UT: IEEE, 2018. <a href="https://doi.org/10.1109/CVPR.2018.00721">https://doi.org/10.1109/CVPR.2018.00721</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文遵循基于结构的视觉定位方法，并使用一个三维语义图作为数据库场景database scene 的表示；给定一个带有语义分割和深度图的查询图片，作者<strong>建立一个三维的语义查询图，并从中提取到的局部描述子</strong>，然后在查询描述子和数据库描述子之间使用3D-3D 匹配，来对齐图并获取所要查询的位姿估计。</p>
<p>所有的视觉定位方法（包括本文提出的方法）都直接或间接地测量一个查询图片与数据库场景表示之间的相似度（视觉或结构），因此，这些方法都默认一个<strong>先验假设：查询图与数据库图是在充分相似的条件（视角、光照以及场景外观等）下描述一个场景的</strong>。如Fig. 1所示，这种先验假设在实际应用场景中会被轻易地打破。</p>
<span id="more"></span>
<p><img src="/2024/02/24/schonberger2018/fig1.png" alt="fig1" title="figure 1"></p>
<p>主要挑战在于如何建立查询图与数据库图之间的data association。现有的基于图片或基于结构的方法都使用具有区别性的局部特征：同样物理点的描述子是相似的，不相关点的描述子差距很大。然而，视觉观测条件的巨变（外观或几何）却对同一物理点在不同外观下要求具有不变的embedding 表示，这与上述方法的区别性描述子相矛盾。</p>
<p>为解决该问题，作者提出了一种<strong>基于生成式的描述子学习方法</strong>。该方法的核心在于学习一个欧氏空间中的embedding，该embedding 包含有恢复不同视角条件下场景外观所需的所有信息。作者提出的该embedding 包含高等级的3D 几何信息和语义信息，使得我们可以处理强烈的视角改变以及轻微的场景几何改变（如由季节变化引起的）。此外，作者还提出一个基于3D 语义场景补全辅助任务的声称是描述子模型，该描述子可通过自监督学习进行获取，且泛化能力较好。</p>
<p>本文贡献如下：</p>
<ol>
<li>提出一个基于3D 几何信息和语义信息的视觉定位方法；</li>
<li>利用面向3D 语义场景补全的生成式模型进行描述子学习；</li>
<li>证明了本方法在两个挑战性问题中的有效性：在强烈视角变化以及光照/季节改变情况下相机位姿的精确估计。</li>
</ol>
<p>即使不用语义信息，本方法也远远超过现有的SOTA 方法，这证明了生成式描述子在定位应用中的强大，而包含语义信息后会取得更大的性能提升。</p>
<h1 id="3-Semantic-Visual-Localization"><a href="#3-Semantic-Visual-Localization" class="headerlink" title="3 Semantic Visual Localization"></a>3 Semantic Visual Localization</h1><p>本文提出的语义视觉定位方法包含以下三个步骤：</p>
<ol>
<li>离线操作：利用语义场景补全辅助任务来学习鲁棒的局部描述子；</li>
<li>在线操作：使用该局部描述子建立查询图与数据库图之间的3D-3D匹配；</li>
<li>在线操作：利用匹配来估计两个图之间的对齐关系，进而获取查询图对应的位姿。</li>
</ol>
<h2 id="3-1-Semantic-Segmentation-and-Fusion"><a href="#3-1-Semantic-Segmentation-and-Fusion" class="headerlink" title="3.1 Semantic Segmentation and Fusion"></a>3.1 Semantic Segmentation and Fusion</h2><p>将语义分割后的图片融合为语义3D体素地图，该地图在体素分辨率上具有强烈照明改变以及几何改变的不变性。尽管语义信息包含高层次的场景信息，但是为实现对场景的明确、实例级的理解就需要较大的空间上下文信息，这就会导致由于遮挡造成的观测缺失，比如不同视角（甚至是相反视角）下查询图与数据库图之间的结构重叠会非常小。因此，本文方法的一个主要挑战就在于<strong>在共同观测缺失的情况下如何实现查询图与数据库图之间的鲁棒匹配。</strong></p>
<h2 id="3-2-Generative-Descriptor-Learning"><a href="#3-2-Generative-Descriptor-Learning" class="headerlink" title="3.2 Generative Descriptor Learning"></a>3.2 Generative Descriptor Learning</h2><p>因为查询图和数据库图在尺寸和覆盖范围上的不同，作者选择建立两图中具有相同尺寸的子块 $ v_D \in M_D, v_Q \in M_Q$ 之间的联系，利用一个function来识别相似的子块，其中，语义场景理解是该function的关键。</p>
<p>针对该function的两种传统解决方法是：匹配function $f(v_D, v_Q)$ 和embedding $f(v)$ 。embedding $f(v)$ 是将相同的子块映射到欧氏空间内的相似点处；而匹配function $f(v_D, v_Q)$ 理论上具有更强大区分能力，但是其使用穷举配对比较方法具有太大的计算复杂度，在大规模数据上难以高效运行。因此，本文选用embedding $f(v)$ 方法，每个子块只需要评估一次即可。 $f(v)$ 将一个子块映射为一个低维空间描述子，同时编码场景语义和几何信息。</p>
<p>为了从不同视角甚至是相反视角下识别出同一物体，需要<strong>对子块的未观测部分进行补全</strong>，作者引入了语义场景补全辅助任务：使用function $h(v)$ 来补全输入未观测部分的语义和几何信息。作者使用一个3D 编码器-解码器结构 $h(v) = g(f(v))$ , 其中 $f$ 是对不完整子块进行编码的神经网络，$g$ 是对子块进行补全的神经网络。$h$ 的架构和示分别如Fig.2 和 Fig. 3所示。值得注意的是，网络训练完成后，本文的<strong>语义定位pipeline只使用 $h$ 的编码器部分 $f$ 来产生描述子</strong> $\mu$ 。</p>
<p><img src="/2024/02/24/schonberger2018/fig2.png" alt="fig2" title="figure 2"></p>
<p><img src="/2024/02/24/schonberger2018/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="3-3-Bag-of-Semantic-Words"><a href="#3-3-Bag-of-Semantic-Words" class="headerlink" title="3.3 Bag of Semantic Words"></a>3.3 Bag of Semantic Words</h2><p>利用编码器部分 $f$ 产生描述子 $\mu$ 来创建一个 Bag of Semantic Words。</p>
<h2 id="3-4-Semantic-Vocabulary-for-Indexing-and-Search"><a href="#3-4-Semantic-Vocabulary-for-Indexing-and-Search" class="headerlink" title="3.4 Semantic Vocabulary for Indexing and Search"></a>3.4 Semantic Vocabulary for Indexing and Search</h2><p>作者在离线步骤中使用训练数据集的bag of semantic words 建立一个语义词库 semantic vocabulary，使用层级k-means 和 一个 Hamming embedding 来量化描述子空间，将数据库图的所有语义词 semantic words 建立词库树索引，作为实现高效匹配的数据结构。在匹配过程中遍历词库树索引并找到汉明距离最近的匹配结果。</p>
<h2 id="3-5-Semantic-Alignment-and-Verification"><a href="#3-5-Semantic-Alignment-and-Verification" class="headerlink" title="3.5 Semantic Alignment and Verification"></a>3.5 Semantic Alignment and Verification</h2><p>根据3.4节得到的匹配结果，寻找将query与数据库地图实现最佳对齐的转换关系 $P \in SE(3)$ ，一个好的对齐定义为同时满足几何及语义对齐。本描述子<strong>不具有</strong>旋转不变性，因此，转换关系P 包含了旋转与平移参数。</p>
<p>作者穷尽匹配对所有的转换假设，对于每一个转换假设P，将query经P转换后与数据库地图进行对齐，计算正确对齐的体素个数，体素的正确对齐包含几何（占用）与语义的一致性。作者使用ICP (Iterative Closest Point) 算法进一步细调对齐关系。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 SaD-SLAM_A Visual SLAM Based on Semantic and Depth Information</title>
    <url>/2024/02/22/sad-slam/</url>
    <content><![CDATA[<p>Yuan, Xun, and Song Chen. “SaD-SLAM: A Visual SLAM Based on Semantic and Depth Information.” In <em>2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 4930–35. Las Vegas, NV, USA: IEEE, 2020. <a href="https://doi.org/10.1109/IROS45743.2020.9341180">https://doi.org/10.1109/IROS45743.2020.9341180</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献如下：</p>
<ol>
<li>在 ORB-SLAM2 的基础上提出了一种基于特征的RGB-D SLAM 算法——SaD-SLAM，该算法结合<strong>语义信息</strong>、<strong>几何信息</strong>和<strong>深度信息</strong>，可在<strong>动态环境</strong>中运行良好；</li>
<li>在当前帧和历史帧的特征点之间进行<strong>对极几何约束</strong>，从动态物体及静态但可移动的物体（如椅子等）上提取<strong>静态特征点</strong>，来提高相机位姿估计的准确度与鲁棒性。</li>
</ol>
<span id="more"></span>
<h1 id="3-System-Description"><a href="#3-System-Description" class="headerlink" title="3 System Description"></a>3 System Description</h1><p>本系统利用语义掩码和深度信息实行三步走算法：<strong>位姿初始化</strong>、<strong>移动一致性测试</strong>，以及<strong>位姿细调</strong>。</p>
<p>如Fig. 1所示，作者将室内场景中的特征点分为三类：<strong>动态点</strong>（人）、<strong>静态但可移动点</strong>（椅子），以及<strong>静态点</strong>。</p>
<p><img src="/2024/02/22/sad-slam/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-1-Framework-of-SaD-SLAM"><a href="#3-1-Framework-of-SaD-SLAM" class="headerlink" title="3.1 Framework of SaD-SLAM"></a>3.1 Framework of SaD-SLAM</h2><p>SaD-SLAM 是在ORB-SLAM2 基础上进一步开发的，其系统架构如Fig. 2所示：</p>
<p><img src="/2024/02/22/sad-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-2-Semantic-Segmentation"><a href="#3-2-Semantic-Segmentation" class="headerlink" title="3.2 Semantic Segmentation"></a>3.2 Semantic Segmentation</h2><p>本文使用Mask R-CNN 网络获取输入图片的像素级语义信息，并挑选出<strong>潜在动态物体</strong>及<strong>相对较大的但是可移动的物体</strong>的掩码，对于TUM 数据集而言，这里指的是人与椅子，位于人身上的特征点被标记为<strong>动态特征点</strong>，位于椅子上的特征点被标记为<strong>静态但可移动特征点</strong>，其他物体上的特征点被标记为<strong>静态特征点</strong>。</p>
<h2 id="3-3-Pose-Initialization"><a href="#3-3-Pose-Initialization" class="headerlink" title="3.3 Pose Initialization"></a>3.3 Pose Initialization</h2><p>一些<strong>小物件</strong>（如笔、书等）对于CNN 而言较难识别出来，或者精度较差。作者采取以下策略进行<strong>辅助检测</strong>：对于不属于人或椅子的特征点，检测以自身为中心、半径分别为5像素和10像素的区域内是否有像素属于人和椅子，若存在，且该像素的<strong>深度信息</strong>与本特征点相似，则将本特征点分别标记为动态点和静态但可移动点，其中，动态点（人）的<strong>优先级更高</strong>；以此可在不需要CNN 的情况下方便识别出人类手上的物体等。此外，作者还提到CNN 在<strong>物体边缘</strong>附近的识别精度较差，该方法也可以<strong>弥补</strong>这些错误识别。最终，可让CNN 识别更少的物体种类，实现CNN 的<strong>快速与小型化</strong>。</p>
<p>在实现对物体类别信息获取的基础上，系统利用<strong>静态点</strong>和<strong>静态但可移动点</strong>构建<strong>重投影误差</strong>，实现对位姿的<strong>初始估计</strong>。值得注意的是，这里也使用了静态但可移动特征点进行位姿求解，这是因为这些点更有可能是保持静止的，或者移动速度较慢。</p>
<h2 id="3-4-Moving-Consistency-Testing"><a href="#3-4-Moving-Consistency-Testing" class="headerlink" title="3.4 Moving Consistency Testing"></a>3.4 Moving Consistency Testing</h2><p>作者在<strong>连续5帧</strong>中跟踪<strong>可能移动的特征点</strong>及<strong>静态特征点</strong>，如果当前帧与之前第4帧（即连续5帧中的最前面一帧）中的对应特征点满足<strong>对极几何约束</strong>，则判定连续5帧中的这些点<strong>均为静态点</strong>。这里的对极几何约束与其他文章中的方法类似，判定<strong>匹配点</strong>与<strong>极线</strong>之间的距离是否超过阈值。</p>
<h2 id="3-5-Pose-Fine-tuning"><a href="#3-5-Pose-Fine-tuning" class="headerlink" title="3.5 Pose Fine-tuning"></a>3.5 Pose Fine-tuning</h2><p>作者将特征点的<strong>动态属性</strong>分为以下四种：1. 静态点，2. 动态点， 3. 静态但可移动点， 4. 从动态点或静态但可移动点转换过来的静态点。</p>
<p>当前阶段，已有的输入数据包括：相机初始位姿估计，静态特征点，动态特征点，静态但可移动物体上的特征点，以及利用移动一致性测试检测到的从移动特征点转换的静态特征点。位姿细调过程包含两步：</p>
<ol>
<li>第一步：利用静态特征点、静态但可移动物体上的特征点、利用移动一致性测试检测到的从移动特征点转换的静态特征点进行<strong>最小化重投影误差</strong>来优化相机位姿；</li>
<li>第二步：利用上步得到的相机位姿，将<strong>局部地图</strong>上的更多特征点重投影至当前帧，并保留地图点的动态属性，只选取属于<strong>静态的地图点</strong>与上步中使用的特征点，结合起来进行再一次的相机位姿优化。</li>
</ol>
<h1 id="4-Experimental-Results"><a href="#4-Experimental-Results" class="headerlink" title="4 Experimental Results"></a>4 Experimental Results</h1><p>为了证明移动一致性检验的作用，作者将该模块去掉，只使用先验静态物体上的特征点进行位姿求解，将该方法记为SaD-SLAM—，结果对比如表2所示：</p>
<p><img src="/2024/02/22/sad-slam/t2.png" alt="t2" title="table 2"></p>
<p>同时，作者与DynaSLAM、DS-SLAM 进行对比，结果如下所示：</p>
<p><img src="/2024/02/22/sad-slam/t1.png" alt="t1" title="table 1"></p>
<p><img src="/2024/02/22/sad-slam/t3.png" alt="t3" title="table 3"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 SO-SLAM_Semantic Object SLAM With Scale Proportional and Symmetrical Texture Constraints</title>
    <url>/2024/02/19/so-slam/</url>
    <content><![CDATA[<p>Liao, Ziwei, Yutong Hu, Jiadong Zhang, Xianyu Qi, Xiaoyu Zhang, and Wei Wang. “SO-SLAM: Semantic Object SLAM With Scale Proportional and Symmetrical Texture Constraints.” <em>IEEE Robotics and Automation Letters</em> 7, no. 2 (April 2022): 4008–15. <a href="https://doi.org/10.1109/LRA.2022.3148465">https://doi.org/10.1109/LRA.2022.3148465</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文主要解决单目 Object SLAM 的<strong>两个挑战</strong>：</p>
<ol>
<li>单目相机包含<strong>较少的物体约束信息</strong>，特别是在部分观测、遮挡等情况下更为严重，使得单目 Object SLAM 较为<strong>脆弱</strong>；</li>
<li>当前的 Object SLAM 主要用于约束<strong>物体的占用空间</strong>，没有充分利用物体的<strong>朝向信息</strong>。</li>
</ol>
<p>针对以上两个挑战，作者提出了<strong>单目 Semantic Object SLAM (SO-SLAM) 系统</strong>，如Fig. 1 所示，除了物体的语义信息，作者还引入了三种代表性的<strong>物体空间约束</strong>：<strong>尺寸比例约束、对称纹理约束</strong>以及<strong>平面支撑约束</strong>，作者推导约束模型并同时应用于前端初始化与后端优化中。</p>
<p>本文的贡献如下：</p>
<ol>
<li>提出一个面向室内环境的完全耦合三种空间约束的<strong>单目 Object SLAM</strong>；</li>
<li>基于空间约束提出两个新方法：<strong>单帧物体初始化</strong>方法和<strong>物体朝向优化</strong>方法；</li>
<li>在两个公开数据集与自采数据集上验证了本算法的有效性。</li>
</ol>
<span id="more"></span>
<p><img src="/2024/02/19/so-slam/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-Monocular-Object-SLAM-Framework"><a href="#3-Monocular-Object-SLAM-Framework" class="headerlink" title="3 Monocular Object SLAM Framework"></a>3 Monocular Object SLAM Framework</h1><p>表示物体的椭球体包含<strong>9自由度</strong>，可利用SVD 方法来获取，该方法至少3帧具有足够视差的观测才可以获取(Nicholson 等, 2019)。作者利用位姿图对相机和物体位姿进行优化：</p>
<p><img src="/2024/02/19/so-slam/f1.png" alt="f1" title="formula 1"></p>
<p>其中，X 表示相机位姿，Q 表示物体位姿；$F_Z$ 表示相机-物体观测约束，$F_O$ 表示里程计约束，这两个约束项在文章(Nicholson 等, 2019)中有详细介绍；本文着重介绍新增加的空间约束 $F_S$ ，包含平面支撑约束 $f_{sup}$ ，尺度比例约束 $f_{ssc}$ ，以及对称纹理约束 $f_{sym}$ ；H() 是鲁棒核用以提高系统对外点的鲁棒性，本文使用 Huber Kernel。</p>
<h1 id="4-Single-Frame-Initialization-with-Semantic-Priors"><a href="#4-Single-Frame-Initialization-with-Semantic-Priors" class="headerlink" title="4 Single-Frame Initialization with Semantic Priors"></a>4 Single-Frame Initialization with Semantic Priors</h1><p>作者根据人类认知习惯来设定物体坐标系：人造物体的上表面一般与物体的支撑面相反，前向一般是对称的方向，由此构建物体的 Z 轴和 X 轴，由此构建了物体坐标系。然后，就可以应用更多的约束，如Fig. 2所示，实现单帧图片对9自由度物体进行初始化的方法，克服了传统SVD 方法难以满足的要求。</p>
<p><img src="/2024/02/19/so-slam/fig2.png" alt="figure2" title="figure 2"></p>
<h2 id="4-1-Object-Detection-Constraints"><a href="#4-1-Object-Detection-Constraints" class="headerlink" title="4.1 Object Detection Constraints"></a>4.1 Object Detection Constraints</h2><p>物体检测框的四条边线经相机投影矩阵进行逆投影，可得到<strong>四个切面约束</strong>，如Fig. 2中的对 $l_i$ 逆投影得到切平面 $\pi_i$ ，由此形成对椭球体 $Q^*$ 的四自由度约束，如下所示：</p>
<p><img src="/2024/02/19/so-slam/f4.png" alt="f4" title="formula 4"></p>
<p>其中，$\sum_{det}$ 表示物体检测协方差矩阵，本文中令其为10。</p>
<h2 id="4-2-Plane-Supporting-Constraints"><a href="#4-2-Plane-Supporting-Constraints" class="headerlink" title="4.2 Plane Supporting Constraints"></a>4.2 Plane Supporting Constraints</h2><p>根据物体与其支撑平面的关系，可以构建<strong>三个约束</strong>，如Fig. 2所示，物体的X、Y轴与平面 $\pi_s$ 的法向量正交，椭球体与平面 $\pi_s$ 相切。由此构建如下函数模型：</p>
<p><img src="/2024/02/19/so-slam/f8.png" alt="f8" title="formula 8"></p>
<p>其中，$Rot_x(Q^\ast), Rot_y(Q^\ast)$ 分别为椭球体的 X、Y 轴；支撑平面为 $\pi_s=(n_s, d)$，  $n_s$ 为支撑平面的法向量。</p>
<h2 id="4-3-Semantic-Scale-Proportional-Constraint"><a href="#4-3-Semantic-Scale-Proportional-Constraint" class="headerlink" title="4.3 Semantic Scale Proportional Constraint"></a>4.3 Semantic Scale Proportional Constraint</h2><p>作者提出了一种灵活的物体尺度先验约束——Scale Proportional Constraint (SPC)，用来约束物体的<strong>尺寸比例</strong>而不是其精确的尺寸。假设物体的尺寸为 $s = [a, b, c]^T$ ，分别为X、Y、Z轴的尺寸，作者由此定义尺寸比例 $r = [\sigma, \beta]^T$ ：</p>
<p><img src="/2024/02/19/so-slam/f9.png" alt="f9" title="formula 9"></p>
<p>根据常见的物体尺寸可制作一个<strong>尺寸比例表</strong>。</p>
<p>给定一个物体 $Q_O^\ast$ ，可根据定义计算其尺寸比例 $r_O = r(Q^\ast_O)$ ，根据其语义类别标签 $l_O$ 经查表可得其对应的尺寸比例 $r_s = SemTable(l_O)$ 。由此构建物体的尺寸比例约束：</p>
<p><img src="/2024/02/19/so-slam/f11.png" alt="f11" title="formula 11"></p>
<h2 id="4-4-Solving-the-Single-Frame-Initialization"><a href="#4-4-Solving-the-Single-Frame-Initialization" class="headerlink" title="4.4 Solving the Single Frame Initialization"></a>4.4 Solving the Single Frame Initialization</h2><p>利用Levenberg-Marquardt 算法对目标方程进行迭代求解：</p>
<p><img src="/2024/02/19/so-slam/f12.png" alt="f12" title="formula 12"></p>
<h1 id="5-Orientation-Optimization-with-Texture-Symmetry"><a href="#5-Orientation-Optimization-with-Texture-Symmetry" class="headerlink" title="5 Orientation Optimization with Texture Symmetry"></a>5 Orientation Optimization with Texture Symmetry</h1><h2 id="5-1-Mathematical-Description-of-Object-Symmetry"><a href="#5-1-Mathematical-Description-of-Object-Symmetry" class="headerlink" title="5.1 Mathematical Description of Object Symmetry"></a>5.1 Mathematical Description of Object Symmetry</h2><p>人造物体一般是对称的，作者将其<strong>对称平面的方向</strong>定义为前向，也就是物体坐标系的 X 轴，如Fig. 3所示。物体对称性可<strong>数学表示</strong>为：物体上的任意一点 $v_0 \in V$ 都可经由其<strong>对称面</strong> $\pi_{xz}$ 找到物体上与其对称的点 $v_0^S \in V$ 。</p>
<p><img src="/2024/02/19/so-slam/fig3.png" alt="fig3" title="figure 3"></p>
<p>物体在图像中的一点 u 经逆投影得到其在3D 空间中的点 v，满足以下方程：</p>
<p><img src="/2024/02/19/so-slam/f13.png" alt="f13" title="formula 13"></p>
<p>将式（a）代入式（b），在（c）的约束下最多只有一个解，由此可以得到 u 的对称像素点：</p>
<p><img src="/2024/02/19/so-slam/f14.png" alt="f14" title="formula 14"></p>
<p>其中，$\mathcal{P}$ 表示相机的投影矩阵；$v_0^S = \mathcal{S}(v_0, Q)$ 表示3D 点 $v_0$  的对称点。该过程如Fig. 3（b）所示，根据找到的<strong>对称点对</strong>，作者构建一个<strong>描述子映射关系</strong> $\beta(<em>)$ ，作者称其为<em>*对称投影不变</em></em> symmetric projection invariant，其满足以下性质：</p>
<p><img src="/2024/02/19/so-slam/f15.png" alt="f15" title="formula 15"></p>
<p>由此，当观测噪声较严重时，可以构建如下的代价方程来优化椭球体 Q：</p>
<p><img src="/2024/02/19/so-slam/f16.png" alt="f16" title="formula 16"></p>
<h2 id="5-2-The-Construction-of-Symmetry-Descriptor"><a href="#5-2-The-Construction-of-Symmetry-Descriptor" class="headerlink" title="5.2 The Construction of Symmetry Descriptor"></a>5.2 The Construction of Symmetry Descriptor</h2><p>对于 $\beta(*)$ 的确定，作者先后考虑了灰度值（广泛用于直接法）、BRIEF 描述子等，之后，作者考虑像素的 Distance Transform：</p>
<p><img src="/2024/02/19/so-slam/f17.png" alt="f17" title="formula 17"></p>
<p>上式的含义为：<strong>从一个像素点到图像中任意边上像素的最近距离</strong>，部分反应了物体的<strong>纹理</strong>。但是这个描述子不一定可以满足对称投影不变的性质，如Fig. 4（a）所示。进而，作者发现了the nearest edge distance of point $v_0$ ，记为 $B_{3DT}(v_0)$ 可满足要求（但<strong>计算量太大</strong>）：</p>
<p><img src="/2024/02/19/so-slam/f19.png" alt="f19" title="formula 19"></p>
<p><img src="/2024/02/19/so-slam/fig4.png" alt="fig4" title="figure 4"></p>
<p>作者结合以上两种方法的优势，提出了 Improved-DT descriptor：</p>
<p><img src="/2024/02/19/so-slam/f20.png" alt="f20" title="formula 20"></p>
<p><img src="/2024/02/19/so-slam/f21.png" alt="f21" title="formula 21"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Pose Optimization</tag>
        <tag>Object SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Fusing Semantics and Motion State Detection for Robust Visual SLAM</title>
    <url>/2024/04/03/singh2020/</url>
    <content><![CDATA[<p>Singh, Gaurav, Meiqing Wu, and Siew-Kei Lam. “Fusing Semantics and Motion State Detection for Robust Visual SLAM.” In <em>2020 IEEE Winter Conference on Applications of Computer Vision (WACV)</em>, 2753–62. Snowmass Village, CO, USA: IEEE, 2020. <a href="https://doi.org/10.1109/WACV45572.2020.9093359">https://doi.org/10.1109/WACV45572.2020.9093359</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者将<strong>语义信息</strong>和<strong>物体的运动状态</strong>融合到一个<strong>概率框架</strong>中，进行准确鲁棒的动态区域检测，为位姿估计和制图保留可靠的特征点。本算法包括以下步骤：</p>
<ol>
<li>首先，利用一个轻量级的<strong>场景流估计</strong>和<strong>聚类方法</strong>来提取场景中的<strong>动态区域</strong>；</li>
<li>然后，利用<strong>语义分割</strong>提取场景的语义信息；</li>
<li>最后，为了解决前两步获取的动态区域和语义信息的<strong>不确定性</strong>，本算法使用一个<strong>概率框架</strong>来融合两个信息，来实现对移动区域的鲁棒检测。</li>
</ol>
<p>这里，作者不使用RANSAC 算法，因为该算法无法应对动态物体占据主要区域的情况；作者使用一个<strong>基于图的方法</strong>来检测几何移动区域。</p>
<span id="more"></span>
<h2 id="3-Proposed-Method"><a href="#3-Proposed-Method" class="headerlink" title="3 Proposed Method"></a>3 Proposed Method</h2><p>本文提出算法的架构如Fig.1所示：</p>
<p><img src="/2024/04/03/singh2020/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-1-Sparse-Scene-Flow-based-Segmentation"><a href="#3-1-Sparse-Scene-Flow-based-Segmentation" class="headerlink" title="3.1 Sparse Scene Flow based Segmentation"></a>3.1 Sparse Scene Flow based Segmentation</h2><h3 id="3-1-1-Sparse-Scene-Flow-Estimation"><a href="#3-1-1-Sparse-Scene-Flow-Estimation" class="headerlink" title="3.1.1 Sparse Scene Flow Estimation"></a>3.1.1 Sparse Scene Flow Estimation</h3><p>作者使用viso2 特征来计算相邻帧之间的2D 光流，以及双目相机之间的视差。</p>
<h2 id="3-2-Semantic-Segmentation"><a href="#3-2-Semantic-Segmentation" class="headerlink" title="3.2 Semantic Segmentation"></a>3.2 Semantic Segmentation</h2><p>作者使用语义信息作为额外的线索来对物体的状态进行最终确认，为了进行横向对比，作者使用<strong>SegNet</strong> 进行语义分割，结合语义信息，上节获取的移动聚类区域只有位于<strong>可移动物体区域内</strong>的才可被视为是移动的。使用AND 或 OR 策略来融合几何信息和语义信息会造成比较大的误差：OR 策略会导致大量静态特征点被错误判定为静态点，而AND 策略会保留部分动态特征点。为了避免这种简单的融合策略，作者提出使用<strong>概率模型</strong>来对两种信息进行鲁棒融合。</p>
<h2 id="3-3-Fusion-of-Geometric-and-Semantic-methods"><a href="#3-3-Fusion-of-Geometric-and-Semantic-methods" class="headerlink" title="3.3 Fusion of Geometric and Semantic methods"></a>3.3 Fusion of Geometric and Semantic methods</h2><p>基于几何聚类获取的场景流可以检测出图片中的移动区域，如Fig.2（a）所示，并将其转化为掩码图片 G，如（b）所示；同样地，作者将语义分割中的潜在动态物体转化为掩码图片 D，如（c）所示。</p>
<p><img src="/2024/04/03/singh2020/fig2.png" alt="fig2" title="figure 2"></p>
<p>作者认为不能直接根据G 和 S 来对动态区域进行判断，而是引入了概率模型，作者使用(Lianos 等, 2018)中提到的<strong>distance transform 方法</strong>来将G、S转换至一个<strong>距离地图</strong>中，并使用<strong>高斯模型</strong>来估计移动概率；该概率模型考虑了<strong>不确定度</strong>，使得G、S可以进行鲁棒融合。</p>
<p>对于几何观测掩码图 G，使用欧氏距离作为距离尺度来构建距离地图 $DT_g$ ：</p>
<p><img src="/2024/04/03/singh2020/f6.png" alt="f6" title="formula 6"></p>
<p>其中，B 表示掩码图。基于距离图 $DT_g$ 定义像素点 $x_i$ 属于动态区域的概率：</p>
<p><img src="/2024/04/03/singh2020/f8.png" alt="f8" title="formula 8"></p>
<p>其中，$\sigma_g$ 表示基于几何分割的<strong>场景光流的不确定性</strong>。</p>
<p>同样地，可以确定语义分割掩码图中像素点 $x_i$ 属于动态区域的概率：</p>
<p><img src="/2024/04/03/singh2020/f9.png" alt="f9" title="formula 9"></p>
<p>其中，$\sigma_s$ 表示<strong>语义分割的不确定度</strong>。</p>
<p>因为语义分割S 和基于场景流的几何分割G 是独立估计的，所以可以将两种估计进行融合来减少不确定度，以此获取<strong>移动区域似然</strong>：</p>
<p><img src="/2024/04/03/singh2020/f10.png" alt="f10" title="formula 10"></p>
<p>两者融合后的概率图如Fig.2（f）所示，其中 $\sigma_s = 40 pixels, \sigma_g = 80 pixels$ 。根据设定的移动概率阈值，得到最终的动态区域，如（g）所示。</p>
<p>概率融合来确定动态区域的优势在于可以将两种检测方式（几何和语义）进行<strong>优势互补、效果增强</strong>，如（b）中几何方法无法实现对完整动态物体的检测，（c）中语义信息无法确定先验动态物体的真正运动状态，（j）中可消除错误分类的影响。</p>
<h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h1><p>在TUM 数据集上的测试结果：</p>
<p><img src="/2024/04/03/singh2020/t1.png" alt="t1" title="table 1"></p>
<p>在KITTI 数据集上的测试结果：</p>
<p><img src="/2024/04/03/singh2020/t2.png" alt="t2" title="table 2"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 SOF-SLAM_A Semantic Visual SLAM for Dynamic Environments</title>
    <url>/2024/02/18/sof-slam/</url>
    <content><![CDATA[<p>Cui, Linyan, and Chaowei Ma. “SOF-SLAM: A Semantic Visual SLAM for Dynamic Environments.” <em>IEEE Access</em> 7 (2019): 166528–39. <a href="https://doi.org/10.1109/ACCESS.2019.2952161">https://doi.org/10.1109/ACCESS.2019.2952161</a>.</p>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>无论是DynaSLAM还是DS-SLAM，它们都是简单地将先验语义信息和几何一致性信息进行<strong>“松组合”</strong>，即利用语义信息和几何信息分别确定出各自认定的动态特征点，然后采用<strong>投票机制</strong>对最终的动态特征点进行确认；其中，DynaSLAM 认为只要有一种方法判定一个点属于动态点就认为该点是动态点（<strong>OR操作</strong>），而DS-SLAM 认为只有两种方法同时认定该点属于动态点才将该点视为动态点（<strong>AND操作</strong>）。由于动态语义先验信息的不确定性，如人或汽车存在静止的情况，而DynaSLAM所代表的方法会剔除掉大量静态特征点，导致位姿估计精度下降，甚至在部分场景中由于特征点的缺少导致解算失败；同时，语义分割存在准确度不够的情况，如物体边缘部分的分割效果较差，DS-SLAM 代表的方法可能会保留实际的动态特征点，从而导致位姿估计精度下降。</p>
<p>针对以上存在的问题，Cui 等人提出了 SOF-SLAM——Semantic Optical Flow SLAM，一种将语义信息和几何信息进行<strong>“紧组合”</strong>来实现动态目标检测和剔除的鲁棒SLAM方法；所谓的紧组合指的是<strong>利用语义信息来辅助几何信息进行动态物体的探测</strong>：作者首先利用语义分割结果去除掉场景中动态物体和潜在动态物体上的特征点，仅利用<strong>高可能性静态物体</strong>上的特征点进行基础矩阵的求解，进而利用该可靠性更高的基础矩阵应用<strong>对极几何理论</strong>实现对动态特征点的剔除，从而实现更为鲁棒可靠的动态特征点剔除方法；最终，作者在公开数据集和真实场景中进行了测试，证明了本方法较ORB-SLAM2以及其他语义SLAM算法的优势。</p>
<span id="more"></span>
<p><img src="/2024/02/18/sof-slam/image-20240218103807414.png" alt="image-20240218103807414" title="figure 2"></p>
<p><img src="/2024/02/18/sof-slam/image-20240218104037049.png" alt="image-20240218104037049" title="figure 3"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>SSH免密登录设置</title>
    <url>/2024/01/26/ssh-setup/</url>
    <content><![CDATA[<h1 id="1-ssh服务"><a href="#1-ssh服务" class="headerlink" title="1 ssh服务"></a>1 ssh服务</h1><p>Ubuntu开启ssh服务需要下载openssh-server，命令为：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br></pre></td></tr></table></figure>
<h1 id="2-ssh-key生成"><a href="#2-ssh-key生成" class="headerlink" title="2 ssh key生成"></a>2 ssh key生成</h1><p>生成ssh key的命令是：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure>
<p>后续一路默认设置即可，该命令会在主目录默认生成.ssh文件，内包含以下文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">id_rsa: 私钥</span><br><span class="line">id_rsa.pub: 公钥</span><br></pre></td></tr></table></figure>
<h1 id="3-无密码远程登陆服务器"><a href="#3-无密码远程登陆服务器" class="headerlink" title="3 无密码远程登陆服务器"></a>3 无密码远程登陆服务器</h1><p>将本地产生的公钥上传至服务器：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-copy-id -p <span class="comment">#port master@ubuntu</span></span><br></pre></td></tr></table></figure>
<h1 id="4-Windows免密登录Ubuntu"><a href="#4-Windows免密登录Ubuntu" class="headerlink" title="4 Windows免密登录Ubuntu"></a>4 Windows免密登录Ubuntu</h1><p>Windows在终端中执行以下命令生成公私密钥：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa</span><br></pre></td></tr></table></figure>
<p>会在C:\Users\xxx.ssh 文件夹中生成以下三个文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">id_rsa 本地私钥</span><br><span class="line">id_rsa.pub 公钥</span><br><span class="line">known_hosts 已知的ip</span><br></pre></td></tr></table></figure>
<p>将公钥上传至Ubuntu，可使用scp命令，然后执行以下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将公钥拷贝至Ubuntu的authorized_keys文件中</span></span><br><span class="line"><span class="built_in">cat</span> ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改sshd_config文件</span></span><br><span class="line">sudo vim /etc/ssh/sshd_config</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将一下三行命令添加至sshd_config文件</span></span><br><span class="line">RSAAuthentication <span class="built_in">yes</span></span><br><span class="line">PubkeyAuthentication <span class="built_in">yes</span></span><br><span class="line">PasswordAuthentication no</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重启ssh</span></span><br><span class="line">sudo service sshd restart</span><br></pre></td></tr></table></figure>
<p>然后即可实现Windows免密登录Ubuntu。</p>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 The STDyn-SLAM_A Stereo Vision and Semantic Segmentation Approach for VSLAM in Dynamic Outdoor Environments</title>
    <url>/2024/02/28/stdyn-slam/</url>
    <content><![CDATA[<p>Esparza, Daniela, and Gerardo Flores. “The STDyn-SLAM: A Stereo Vision and Semantic Segmentation Approach for VSLAM in Dynamic Outdoor Environments.” <em>IEEE Access</em> 10 (2022): 18201–9. <a href="https://doi.org/10.1109/ACCESS.2022.3149885">https://doi.org/10.1109/ACCESS.2022.3149885</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献如下：</p>
<ul>
<li>提出一个针对动态环境的立体SLAM 算法——STDyn-SLAM，结合<strong>语义分割</strong>神经网络和<strong>几何约束</strong>对动态物体进行剔除；</li>
<li>立体相机的深度图用于构建3D 八叉树地图重建，深度图对于本SLAM 不是必要的；</li>
<li>利用公开数据集进行测试，并于SOTA 方法进行对比；</li>
<li>开源代码：<a href="https://github.com/DanielaEsparza/STDyn-SLAM">https://github.com/DanielaEsparza/STDyn-SLAM</a></li>
</ul>
<span id="more"></span>
<h1 id="3-Method"><a href="#3-Method" class="headerlink" title="3 Method"></a>3 Method</h1><p>本文提出的STDyn-SLAM 算法框架如Fig. 2所示，本系统基于ORB-SLAM2 算法：</p>
<p><img src="/2024/02/28/stdyn-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-Stereo-Process"><a href="#3-1-Stereo-Process" class="headerlink" title="3.1 Stereo Process"></a>3.1 Stereo Process</h2><p>对立体图片的处理主要包含三个部分：</p>
<ul>
<li>ORB 特征提取</li>
<li>光流跟踪</li>
<li>对极几何</li>
</ul>
<p>首先对当前左右图像和前一帧左边图像进行ORB 特征提取；为了避免神经网络检测动态物体失败的情况，本系统计算当前与前一帧左图之间的<strong>光流</strong>，值得注意的是，这些光流点（Harris 点）需要与ORB 特征点<strong>保持不同</strong>，舍弃那些在/靠近边缘角点上的光流点；根据<strong>基础矩阵</strong>、<strong>ORB 特征点</strong>以及<strong>光流点</strong>来计算<strong>极线</strong>，然后将当前帧匹配的特征点映射到前一帧，根据极线与投影特征点之间的距离来判断是否属于外点。</p>
<h2 id="3-2-Artificial-Neural-Network‘s-Architecture"><a href="#3-2-Artificial-Neural-Network‘s-Architecture" class="headerlink" title="3.2 Artificial Neural Network‘s Architecture"></a>3.2 Artificial Neural Network‘s Architecture</h2><p>本算法使用SegNet 作为语义分割网络。</p>
<h3 id="3-2-1-Outliers-Removal"><a href="#3-2-1-Outliers-Removal" class="headerlink" title="3.2.1 Outliers Removal"></a>3.2.1 Outliers Removal</h3><p>Fig. 3展示了判断投影点的三种情况，设定一个阈值进行外点的筛选。</p>
<p><img src="/2024/02/28/stdyn-slam/fig3.png" alt="fig3" title="figure 3"></p>
<p>利用语义分割结果来剔除属于<strong>潜在动态物体</strong>上的ORB 特征点，如汽车。</p>
<h3 id="3-2-3-3D-Reconstruction"><a href="#3-2-3-3D-Reconstruction" class="headerlink" title="3.2.3 3D Reconstruction"></a>3.2.3 3D Reconstruction</h3><p>利用左图、语义分割图以及利用视觉里程计得到的深度图进行3D 重建，值得注意的是，本算法<strong>只对左图</strong>进行语义分割、光流跟踪以及几何约束，以减少时间消耗。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Long-Term Visual Localization Using Semantically Segmented Images</title>
    <url>/2024/02/24/stenborg2018/</url>
    <content><![CDATA[<p>Stenborg, Erik, Carl Toft, and Lars Hammarstrand. “Long-Term Visual Localization Using Semantically Segmented Images.” In <em>2018 IEEE International Conference on Robotics and Automation (ICRA)</em>, 6484–90. Brisbane, QLD: IEEE, 2018. <a href="https://doi.org/10.1109/ICRA.2018.8463150">https://doi.org/10.1109/ICRA.2018.8463150</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文利用近期在语义分割中的发展，设计了一个基于<strong>语义分割图片</strong>和<strong>语义点特征地图</strong>的定位算法，不再使用传统的描述子来描述特征，而是使用<strong>3D 位置坐标</strong>和<strong>语义标签</strong>来描述。</p>
<span id="more"></span>
<h1 id="2-Problem-Statement"><a href="#2-Problem-Statement" class="headerlink" title="2 Problem Statement"></a>2 Problem Statement</h1><h2 id="2-1-Observations"><a href="#2-1-Observations" class="headerlink" title="2.1 Observations"></a>2.1 Observations</h2><p>坐标系与传感器安装位置如Fig 2所示：</p>
<p><img src="/2024/02/24/stenborg2018/fig2.png" alt="fig2" title="figure 2"></p>
<p>相机采集到的图片经过预处理，得到一组包含 $n_t$ 个特征点（坐标）与描述子对（即<strong>观测量</strong>），$\mathbf{f}_t=\{(u_t^i,d_t^i)\}_{i=1}^{n_t}$ ，在本文中，依据我们使用的方法不同，$\mathbf{f}_t$ 会有不同的形式：如果使用<strong>语义点云地图方法</strong>，$\mathbf{f}_t$ 是稠密的，包含图片中每一个像素，且描述子为像素对应的语义标签；若使用<strong>SIFT 算法</strong>，则特征点是稀疏的，且描述子为128x1 的向量。</p>
<h2 id="2-2-Maps"><a href="#2-2-Maps" class="headerlink" title="2.2 Maps"></a>2.2 Maps</h2><p>假设预构建的特征地图包含M 个特征点，则地图可表示为 $\mathcal{M} = \{&lt;\mathbf{U}_i, \mathbf{D}_i, \mathcal{V}_i&gt;\}_{i=1}^M$ ，每个特征点包含<strong>位置信息</strong> $\mathbf{U}_i$ ，<strong>特征描述子</strong> $\mathbf{D}_i$ ，以及<strong>可视参数</strong> $\mathcal{V}_i = [\rho_i, \gamma_i^a,\gamma_i^b,r_i]^T$  （参考Fig. 2），其中 $\rho_i$ 表示特征被探测到的概率。</p>
<h2 id="2-3-Problem-Definition"><a href="#2-3-Problem-Definition" class="headerlink" title="2.3 Problem Definition"></a>2.3 Problem Definition</h2><p>问题被描述为：给定观测和地图 $\mathcal{M}$ ，通过迭代计算出汽车相对于地图的<strong>位姿后验概率估计</strong>。</p>
<p>假设 $t$ 时刻的汽车位姿为 $\mathbf{x}_t$ ，则我们的目标为计算 $p(\mathcal{x}_t|\mathcal{f}_{1:t}, \mathcal{M})$ 。</p>
<h1 id="3-Models"><a href="#3-Models" class="headerlink" title="3 Models"></a>3 Models</h1><h2 id="3-1-Process-model"><a href="#3-1-Process-model" class="headerlink" title="3.1 Process model"></a>3.1 Process model</h2><p><strong>状态方程</strong></p>
<p>作者使用一个简单的<strong>点质量模型 point mass model</strong> 来对汽车进行建模。本状态方程由两部分组成，<strong>部分一</strong>使用速度测量作为输入来<strong>对运动进行建模</strong>：</p>
<p><img src="/2024/02/24/stenborg2018/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$M(.) \in SE(3)$ ，是表示位姿的4x4 矩阵。</p>
<p><strong>部分二</strong>是一项来确保总有一小部分density 被留在路上，这保证了即便一个lost filter 丢失了，也可以重新获取其横向位置。</p>
<p>综合以上两部分，状态模型表示为：</p>
<p><img src="/2024/02/24/stenborg2018/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$p_m(.)$ 是由部分一给定的，$p_r(.)$ 是 $p_m(.)$ 到路上的投影。</p>
<h2 id="3-2-Measurement-model"><a href="#3-2-Measurement-model" class="headerlink" title="3.2 Measurement model"></a>3.2 Measurement model</h2><p>假定数据关联步骤已经解决，得到一个DA 向量 $\mathbf{\lambda}_t = [\lambda_t^1, …, \lambda_t^{n_t}]^T$ ，其中，$\lambda_t^i = j$ 表示，如果 $j&gt;0$ ，则图片特征 i 与地图特征 j 相关联；否则该特征在地图中不存在。</p>
<p>利用已有的DA，并假定特征位置坐标和描述子 $u_t^i,d_t^i$ 满足<strong>条件独立</strong>，则有：</p>
<p><img src="/2024/02/24/stenborg2018/f4.png" alt="f4" title="formula 4"></p>
<p>其中，$\mathcal{M}_{\lambda_t^i}$ 表示地图中带有描述子和可见参数的<strong>3D 点与图片中的特征点 i</strong> 根据DA $\mathbf{\lambda}_t$ 关联起来。</p>
<h3 id="3-2-1-SIFT-map"><a href="#3-2-1-SIFT-map" class="headerlink" title="3.2.1 SIFT map"></a>3.2.1 SIFT map</h3><p>当给定数据关联时，特征中的<strong>描述子对似然函数没有帮助</strong>，可以忽略掉。同时，假定图片中SIFT 特征点的探测误差服从零均值高斯分布，则有：</p>
<p><img src="/2024/02/24/stenborg2018/f5.png" alt="f5" title="formula 5"></p>
<p>其中，$\pi(.)$ 表示带有镜头畸变的标准针孔相机投影模型；$\sigma_{\pi}^2$ 表示探测器误差的方差。</p>
<h3 id="3-2-2-Semantic-map"><a href="#3-2-2-Semantic-map" class="headerlink" title="3.2.2 Semantic map"></a>3.2.2 Semantic map</h3><p>在该方法中，地图中的特征点描述子 $D_j$ ，以及图片特征的描述子 $d_t^i$ 都是来自CityScapes 数据集中的<strong>语义标签</strong>。</p>
<p>即便图片中相邻像素的语义分割结果是相关的，但是作者让做出了<strong>简化假设</strong>：假设像素坐标与语义类别是独立的，则有：</p>
<p><img src="/2024/02/24/stenborg2018/f6.png" alt="f6" title="formula 6"></p>
<p>这个因式分解会在观测中导致<strong>overconfidence</strong>，而且观测越多影响越严重，这使得作者在第四章中加入了尺度参数 s，以及测量截断值 $N_c$ 。</p>
<p>上式的第一个因子 $p(u_t^i|x_t, U_{\lambda_t^i})$ 表示在像素 i 处检测到特征的概率，然而本方法是对整幅图片进行语义分割，即每一个像素都被视为一个特征，则该因子对于任意的 i 是一个<strong>常数值</strong>。因此，上式可进一步化简为：</p>
<p><img src="/2024/02/24/stenborg2018/f7.png" alt="f7" title="formula 7"></p>
<p>对于上式的右半部分，分两种情况讨论：</p>
<p><strong>情况一：</strong>没有地图点投影到该像素，即 $\lambda_t^i = 0$ ：此时，我们无法从地图中获取该像素类别的信息，作者为所有这种像素（没有地图特征点投影的像素）假设一种分布，即在所有种类上的<strong>边缘分布</strong>：</p>
<p><img src="/2024/02/24/stenborg2018/f8.png" alt="f8" title="formula 8"></p>
<p>根据后文描述，该边缘分布是通过汇总制图所用到的所有图片的语义分割结果，统计各个种类所占的比例即为该边缘分布。</p>
<p><strong>情况二：</strong>有地图点投影到该像素，即 $\lambda_t^i &gt; 0$ ：虽然地图上的一个特征点投影到了该像素上，但是<strong>仍然不能确定</strong>是探测到了这个地图特征点，还是该处被别的物体遮挡住了，如汽车或行人。为解决该<strong>不确定度</strong>，作者引入了一个<strong>探测变量</strong> $\delta$ ：当该值为1时表示该地图特征点确实被探测到了，否则为0。利用该探测变量，将像素与对应的地图特征点似然函数表示为：</p>
<p><img src="/2024/02/24/stenborg2018/f9.png" alt="f9" title="formula 9"></p>
<p>其中，$Pr\{d_t^i|\delta=0, D_{\lambda^i_k}\}, Pr\{d_t^i|\delta=1, D_{\lambda^i_k}\}$ 分别表示给定特定地图点特征<strong>被遮挡</strong>或者<strong>可视状态</strong>下像素类别的概率；根据后文描述， $Pr\{d_t^i|\delta=0, D_{\lambda^i_k}\}$ 是通过手动调整设定的，动态物体的概率设定高一点，如汽车、行人等，静态物体设置低一点； $Pr\{d_t^i|\delta=1, D_{\lambda^i_k}\}$ 的确定是通过汇总确定该地图特征点的所有图片中，该点周围7x7 邻域像素的语义分类，并归一化为一个直方图作为各个种类的PMF；剩下的部分表示给定的地图点是可视的概率：</p>
<p><img src="/2024/02/24/stenborg2018/f10.png" alt="f10" title="formula 10"></p>
<p>其中，$v(.)$ 是一个函数，当位姿 $\mathbf{x}_t$ 在地图特征点的可视区域内时该函数为1；$P_0$ 表示一个可视地图特征点被遮挡的概率。</p>
<p>综上所述，语义地图法的似然计算需要考虑地图特征点是否映射到像素上的两种情况：</p>
<p><img src="/2024/02/24/stenborg2018/f11.png" alt="f11" title="formula 11"></p>
<h1 id="4-Algorithmic-Details"><a href="#4-Algorithmic-Details" class="headerlink" title="4 Algorithmic Details"></a>4 Algorithmic Details</h1><h2 id="4-1-SIFT-filter"><a href="#4-1-SIFT-filter" class="headerlink" title="4.1 SIFT filter"></a>4.1 SIFT filter</h2><p>针对该方法，作者使用了无迹卡尔曼滤波器UKF 进行优化，伪代码如下所示：</p>
<p><img src="/2024/02/24/stenborg2018/a1.png" alt="a1" title="algorithm 1"></p>
<p>其中，数据关联DA 步骤如下所示：作者基于当前的位姿估计以及地图特征点的可视参数 $\mathcal{V}$ 来选取一个<strong>子地图</strong> $\mathcal{M}_t$ ，将图片中提取到的SIFT 特征点与子地图 $\mathcal{M}_t$ 中的特征点进行<strong>匹配</strong>，并使用<strong>RANSAC 算法</strong>进行外点剔除，最终得到DA 。</p>
<h2 id="4-2-Semantic-filter"><a href="#4-2-Semantic-filter" class="headerlink" title="4.2 Semantic filter"></a>4.2 Semantic filter</h2><p>针对该方法，作者使用了bootstrap particle filter 进行递归优化，该算法的伪代码如下所示：</p>
<p><img src="/2024/02/24/stenborg2018/a2.png" alt="a2" title="algorithm 2"></p>
<p>为了可以评估一个粒子的似然，需要确认地图中的哪些特征点是<strong>潜在可视</strong>的，这与SIFT 方法类似，但是只需要做到近似即可，因此可以<strong>同时计算</strong>附近的几个粒子，例如使用这些粒子的<strong>平均位置</strong>和地图特征点的<strong>可视参数</strong>进行计算。然后，将这些潜在可视的地图特征点映射到图片中，从而<strong>为每个粒子</strong>构建了<strong>独有的数据关联</strong> $\lambda_t$ 。</p>
<p><img src="/2024/02/24/stenborg2018/fig3.png" alt="fig3" title="figure 3"></p>
<p>将式（11）除以常数 $\prod_iPr\{d_t^i\}$ 可以简化权重更新过程，因为只需要考虑那些有地图特征点投影到的像素，</p>
<p><img src="/2024/02/24/stenborg2018/f12.png" alt="f12" title="formula 12"></p>
<p>在3.2.2节我们做了像素位置与语义分类<strong>条件独立的假设</strong>，但实际情况是该假设不满足，所以会导致对观测的<strong>overly confident</strong>，为减小该假设的影响，作者增加测量似然到一个小于1的正数，因此，对于粒子 j 的<strong>权重更新</strong>变为了：</p>
<p><img src="/2024/02/24/stenborg2018/f13.png" alt="f13" title="formula 13"></p>
<p>其中，$w_t^{(j)}$ 是在状态 $\mathbf{x}_t^{(j)}$ 下粒子 j 的<strong>权重</strong>；s 是<strong>尺度参数</strong>，本文设定为3；$n_{\lambda_t}$ 是映射到图像中的地图特征点个数；$N_c = 400$ 是<strong>截断值</strong>，作者认为更多的映射特征点不会提供更多的信息，因为更多的特征点意味着它们之间的间距越小，那么它们之间的相关性越强。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 TextSLAM_Visual SLAM with Semantic Planar Text Features</title>
    <url>/2024/03/19/text-slam/</url>
    <content><![CDATA[<p>Li, Boying, Danping Zou, Yuan Huang, Xinghan Niu, Ling Pei, and Wenxian Yu. “TextSLAM: Visual SLAM with Semantic Planar Text Features.” arXiv, July 3, 2023. <a href="http://arxiv.org/abs/2305.10029">http://arxiv.org/abs/2305.10029</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ol>
<li>提出一个新颖的SLAM 架构，将<strong>文本特征</strong>集成至前端位姿跟踪、后端优化、回环检测和制图中，本工作开创性地将场景中的文本信息<strong>紧耦合</strong>至视觉SLAM 中，作者同时提供了一个包含丰富文本信息环境的<strong>数据集</strong>来进行测试；</li>
<li>作者从<strong>几何</strong>和<strong>语义表示</strong>两种层面实现对文本特征的提取、数据关联，并将其集成进SLAM 中；</li>
<li>提出一种使用文本特征语义信息进行<strong>回环检测</strong>的方法，可实现在挑战性环境中（如剧烈光照变化、遮挡、视角变化等）的<strong>可靠回环检测</strong>。</li>
</ol>
<span id="more"></span>
<p><img src="/2024/03/19/text-slam/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-Semantic-Text-Features"><a href="#3-Semantic-Text-Features" class="headerlink" title="3 Semantic Text Features"></a>3 Semantic Text Features</h1><p>文本语义特征表示为一个<strong>平面</strong>以及带有其<strong>独特语义含义的图像块</strong>。</p>
<h2 id="3-1-Geometric-model"><a href="#3-1-Geometric-model" class="headerlink" title="3.1 Geometric model"></a>3.1 Geometric model</h2><h3 id="3-1-1-Parameterization"><a href="#3-1-1-Parameterization" class="headerlink" title="3.1.1 Parameterization"></a>3.1.1 Parameterization</h3><p>对于每个文本块，被观测到的第一帧称为 host frame，如Fig. 2所示，文本块平面在该帧中的位置表示为 $n^Tp+d=0$ ，其中 $n = (n_1,n_2,n_3)^T$ 表示文本块的法向量，d 与平面和相机原点之间的距离相关，p 为平面上的3D 点。</p>
<p><img src="/2024/03/19/text-slam/fig2.png" alt="fig2" title="figure 2"></p>
<p>作者不使用 $(n_1,n_2,n_3,d)^T$ 来表示该平面，而使用一个压缩参数来表示：</p>
<p><img src="/2024/03/19/text-slam/f1.png" alt="f1" title="formula 1"></p>
<p>经过推算，作者发现该参数与平面上3D 点的<strong>逆深度</strong>紧密相关：</p>
<p><img src="/2024/03/19/text-slam/f2.png" alt="f2" title="formula 2"></p>
<p>其中，$\tilde{m}$ 表示像素坐标系下的齐次坐标。上式表明，在给定像素点的坐标 $\tilde{m}$ 和<strong>文本参数</strong> $\theta$ 后，可通过点积获取该像素点的<strong>逆深度</strong>；同样地，获取多个像素点坐标及其逆深度，即可求出文本参数 $\theta$ ：</p>
<p><img src="/2024/03/19/text-slam/f3.png" alt="f3" title="formula 3"></p>
<p>由此，可通过文本的<strong>边界框</strong>（通过文本检测器获取）来对文本参数进行<strong>快速初始化</strong>。</p>
<h3 id="3-1-2-Observation"><a href="#3-1-2-Observation" class="headerlink" title="3.1.2 Observation"></a>3.1.2 Observation</h3><p>为了更新文本的参数，需要定义观测模型。这里，作者选择使用<strong>观测到的文本框</strong>和<strong>3D 文本在图像上的投影</strong>之间的差异作为观测模型，借鉴直接法，作者以两个文本块的<strong>光度误差</strong>作为误差项，会有更高的精度和鲁棒性，特别是对于模糊图像而言。为解决直接法的<strong>光照变化</strong>问题，作者采用ZNCC 方法（zero-mean normalized cross-correlation）。</p>
<h2 id="3-2-Semantic-information-management"><a href="#3-2-Semantic-information-management" class="headerlink" title="3.2 Semantic information management"></a>3.2 Semantic information management</h2><p>由于具有<strong>外观变化不变性</strong>，场景中文本的语义信息对于<strong>场景理解</strong>和<strong>数据关联</strong>至关重要。作者使用两部分来表示文本物体的语义信息：语义含义 s 和语义损失 $g^{sem}$ ：</p>
<p><img src="/2024/03/19/text-slam/f14.png" alt="f14" title="formula 14"></p>
<p>其中，语义含义是一个文本字符串，语义损失 $g^{sem}$ 描述文本语义估计的质量，如Fig.3所示：</p>
<p><img src="/2024/03/19/text-slam/fig3.png" alt="fig3" title="figure 3"></p>
<p>当前帧观测到的语义信息表示为 $\hat{\chi}=\{\hat{s},\hat{g}^{sem}\}$ ，$\hat{s}$ 是文本提取器获取的语义字符串，语义损失 $\hat{g}^{sem}$ 表示如下：</p>
<p><img src="/2024/03/19/text-slam/f15.png" alt="f15" title="formula 15"></p>
<p>其中，$g^{mean}$ 描述了语义含义损失，与文本提取器识别的置信度相关；$g^{geo}$ 描述了该文本是否位于良好观测状态，由两部分组成：文本物体中心与相机的距离，以及观测方向与文本法向量之间的差异。</p>
<p>文本块语义信息的更新规则是：选取所有观测中具有<strong>最小语义损失</strong>的一项作为该文本块的语义信息。</p>
<h1 id="4-TextSLAM-System"><a href="#4-TextSLAM-System" class="headerlink" title="4 TextSLAM System"></a>4 TextSLAM System</h1><p>本TextSLAM 系统是基于特征点SLAM 系统构建的，采用了基于关键帧的架构来实现与文本信息的紧组合。</p>
<p><img src="/2024/03/19/text-slam/fig4.png" alt="fig4" title="figure 4"></p>
<h2 id="4-1-Initialization-of-text-objects"><a href="#4-1-Initialization-of-text-objects" class="headerlink" title="4.1 Initialization of text objects"></a>4.1 Initialization of text objects</h2><p>作者使用AttentionOCR 作为本系统的文本提取器。为了初始化关键帧中新检测到的文本目标的参数 $\theta$ ，作者使用文本块内部的 FAST特征点进行跟踪，直到新关键帧到来。对于关键帧之间的匹配，作者使用多个匹配点对来解算 $\theta$ 。</p>
<h2 id="4-2-Camera-pose-estimation-with-text-objects"><a href="#4-2-Camera-pose-estimation-with-text-objects" class="headerlink" title="4.2 Camera pose estimation with text objects"></a>4.2 Camera pose estimation with text objects</h2><p>本系统<strong>同时使用特征点和文本物体</strong>进行位姿估计。作者使用前两个关键帧观测到的物体参与位姿估计，损失函数如下所示：</p>
<p><img src="/2024/03/19/text-slam/f18.png" alt="f18" title="formula 18"></p>
<p>其中，第一项表示<strong>特征点的重投影误差</strong>；第二项只包含<strong>文本物体的光度误差</strong>，这里并没有使用所有像素点来计算光度误差，而是使用文本块内部的FAST 特征点作为代表像素来计算光度误差。</p>
<p>光度误差项具有<strong>严重的非线性</strong>，优化过程中需要良好的初始位姿估计：作者首先使用一个<strong>匀速运动模型</strong>来预测相机位姿，然后使用一个<strong>由粗到细的策略</strong>进行高效优化。由粗到细的优化策略指的是：将图片进行<strong>金字塔化</strong>，在最高层的压缩图像中开始优化，由于经过下采样，待优化的参数会<strong>减少</strong>；然后用该层优化后的结果作为下一层的<strong>初始化估计</strong>进行该层的优化，最终完成所有的优化过程。</p>
<h2 id="4-5-Loop-closing-using-scene-texts"><a href="#4-5-Loop-closing-using-scene-texts" class="headerlink" title="4.5 Loop closing using scene texts"></a>4.5 Loop closing using scene texts</h2><h3 id="4-5-1-Detection-of-loop-candidates"><a href="#4-5-1-Detection-of-loop-candidates" class="headerlink" title="4.5.1 Detection of loop candidates"></a>4.5.1 Detection of loop candidates</h3><p>进行回环检测时，作者直接将最新关键帧中的文本与<strong>地图中的3D文本</strong>进行匹配，而不是与历史关键帧进行匹配，这样可以<strong>大幅减小</strong>比较次数；然后根据匹配的3D 文本选取候选关键帧。</p>
<h3 id="4-5-2-Compute-the-relative-transformation"><a href="#4-5-2-Compute-the-relative-transformation" class="headerlink" title="4.5.2 Compute the relative transformation"></a>4.5.2 Compute the relative transformation</h3><p>获取完回环候选帧之后，需建立当前帧与回环帧之间的特征点数据关联，然后才可计算两帧之间的<strong>相似转换</strong>，当光照或视角变化剧烈时，难以建立准确的数据关联，此处可以凸显出文本语义的优势：因为在进行回环候选帧筛选时是通过文本匹配获取的，可以将文本匹配作为一个先验知识，即<strong>只在文本匹配块内部搜寻特征点的数据关联</strong>。如Fig.5所示，作者发现文本物体内部对比的改变不像我们想象的那么巨大，因此，作者使用表示像素间相对差异的<strong>BRIEF 描述子</strong>来进行DA。如Fig.6所示，如果是整幅图片之间进行描述子匹配，则会出现大量的错误匹配；但只在匹配文本内进行DA，则会大幅提高准确度。</p>
<p><img src="/2024/03/19/text-slam/fig5.png" alt="fig5" title="figure 5"></p>
<p><img src="/2024/03/19/text-slam/fig6.png" alt="fig6" title="figure 6"></p>
<p>在获取了匹配文本内特征点的DA 之后，求解相似转换分为两步：</p>
<ol>
<li>首先，作者使用RANSCA 来<strong>计算并优化相似转换</strong>；</li>
<li>然后，利用获取的相似转换在<strong>匹配文本外部</strong>进行进一步的匹配搜寻，以获取更多的匹配特征点，并在此基础上对相似转换进行<strong>进一步的优化</strong>。</li>
</ol>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Pose Optimization</tag>
        <tag>Object SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>双目相机的标定与校正</title>
    <url>/2024/04/28/stereo-calib/</url>
    <content><![CDATA[<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1 介绍"></a>1 介绍</h1><p>最近使用两个Flir相机采了数据，打算在ORB-SLAM2系列代码上进行测试，采集完数据后需要对双目相机进行标定，在网上搜寻了相关资料后，发现使用Matlab工具库可以比较方便地进行双目相机的标定，在此记录一下。注：双目标定部分主要参考了该<a href="https://www.cnblogs.com/champrin/p/17034043.html">文章</a>。</p>
<span id="more"></span>
<p>简单记录一下相机的相关参数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>参数</th>
</tr>
</thead>
<tbody>
<tr>
<td>型号</td>
<td>FLIR BFS-U3-31S4</td>
</tr>
<tr>
<td>分辨率</td>
<td>2048*1536</td>
</tr>
<tr>
<td>最大帧率</td>
<td>55 FPS</td>
</tr>
<tr>
<td>快门类型</td>
<td>全局快门</td>
</tr>
<tr>
<td>传感器型号</td>
<td>Sony IMX265, CMOS</td>
</tr>
</tbody>
</table>
</div>
<p><img src="/2024/04/28/stereo-calib/image-20240509093219013.png" alt="image-20240509093219013" title="fig 1" style="zoom:50%;"></p>
<h1 id="2-标定图片采集"><a href="#2-标定图片采集" class="headerlink" title="2 标定图片采集"></a>2 标定图片采集</h1><h2 id="2-1-标定棋盘"><a href="#2-1-标定棋盘" class="headerlink" title="2.1 标定棋盘"></a>2.1 标定棋盘</h2><p>标定棋盘可以从网上下载，也可使用程序生成特定参数的图片，此处可参考<a href="https://docs.opencv.org/4.x/da/d0d/tutorial_camera_calibration_pattern.html">OpenCV网站</a>提供的<a href="https://github.com/opencv/opencv/blob/4.x/doc/pattern.png">成品图片</a>或<a href="https://github.com/opencv/opencv/blob/4.x/doc/pattern_tools/gen_pattern.py">生成程序</a>。将生成的图片保存下来，并进行打印，在打印时选择1:1比例即可。</p>
<p>注：本人尝试了使用上文所述的OpenCV提供的成品图片与生成程序制作的图片，在打印后使用直尺测量棋盘网格大小，发现均无法实现严格定义的尺寸，不知道是不是打印时出现了问题；后面在进行标定时，手动量了网格大小作为标定参数。</p>
<h2 id="2-2-图片采集"><a href="#2-2-图片采集" class="headerlink" title="2.2 图片采集"></a>2.2 图片采集</h2><p>为了保证标定效果，建议将标定棋盘放在不同的位置，抓拍十张以上的照片。该<a href="https://www.cnblogs.com/champrin/p/17034043.html">文章</a>介绍了棋盘图的拍摄规范，现记录如下：</p>
<ol>
<li><p>在一次标定的整个过程中，不能调节相机的光圈、焦距。也就是说，在标定过程中，要保证摄像头进光量与焦距的一致，每次改变参数均需重新进行标定。</p>
</li>
<li><p>把相机图像分成四个象限，应保证拍摄的标定板图像均匀分布在五个位置（四个象限以及正中心）中，且在每个位置进行不同方向的两次倾斜，参考fig 2；或者，也可以固定住标定板，移动相机在不同角度进行拍摄。</p>
<p><img src="/2024/04/28/stereo-calib/1.png" alt="img2" title="fig 2" style="zoom:50%;"></p>
<p>图片参考自<a href="https://community.element14.com/technologies/fpga-group/b/blog/posts/stereo-calibration-for-the-dual-camera-mezzanine">文章</a>。</p>
</li>
<li><p>标定板的成像应大致占摄像头视野的1/4左右。</p>
</li>
<li><p>标定板成像不能过曝，过曝会导致特征轮廓的提取的偏移。</p>
</li>
<li><p>拍摄过程中可以对标定板适当的进行补光，调节标定板到镜头的距离，以便于拍出清晰的图片。</p>
</li>
<li><p>标定图片数量通常在15~25张之间，数量太少，标定的参数会不准确。</p>
</li>
<li><p>标定时用的标定板最好选择x方向与y方向棋盘格不同的，便于标定程序识别标定板方向。</p>
</li>
</ol>
<p>按照以上采集规则采集标定图片即可。</p>
<h1 id="3-标定过程"><a href="#3-标定过程" class="headerlink" title="3 标定过程"></a>3 标定过程</h1><p>使用Matlab标定的过程如下所示：</p>
<p>打开Matlab，在命令行窗口输入<code>stereoCameraCalibrator</code>：</p>
<p><img src="/2024/04/28/stereo-calib/image-20240428215622524.png" alt="fig3" title="fig 3" style="zoom: 67%;"></p>
<p>然后会跳出fig 4所示的窗口，点击<code>Add Images</code>：</p>
<p><img src="/2024/04/28/stereo-calib/image-20240428215905715.png" alt="fig4" title="fig 4" style="zoom:50%;"></p>
<p>在弹出的窗口中分别输入左图、右图的文件夹路径，并设置网格的尺寸（注意，此处的网格尺寸本人是使用实际测量得到的尺寸），：</p>
<p><img src="/2024/04/28/stereo-calib/image-20240428220159080.png" alt="fig5" title="fig 5" style="zoom:67%;"></p>
<p>然后点击确定，Matlab开始对标定图片进行处理：</p>
<p><img src="/2024/04/28/stereo-calib/image-20240428220342804.png" alt="fig6" title="fig 6" style="zoom: 50%;"></p>
<p>接下来对标定参数进行设定：</p>
<ul>
<li>选择 <code>Compute Intrinsics</code>；</li>
<li>选择 <code>2 Coefficients</code>（注：<code>3 Coefficients</code>是针对鱼眼相机标定的）；</li>
<li>选择 <code>Tangential Distortion</code>（注：无需勾选<code>Skew</code>，否则相机内参标定结果中会出现参数<code>s</code>，即相机内参矩阵的第一行会变成<code>[fx, s, u0]</code>，与OpenCV的参数格式不同）；</li>
<li>点击<code>Calibrate</code>按钮进行标定计算。</li>
</ul>
<p><img src="/2024/04/28/stereo-calib/image-20240428220540621.png" alt="fig7" title="fig 7" style="zoom:50%;"></p>
<p>然后即可得到标定后的结果，fig 8左下区域表示重投影误差，对误差较大的图片进行剔除，来实现达到较低像素精度的表现（<code>Overall Mean Error</code>小于0.1 pixels）。</p>
<p><img src="/2024/04/28/stereo-calib/image-20240428221338847.png" alt="fig8" title="fig 8" style="zoom:50%;"></p>
<p>最后，点击<code>Export Camera Parameters</code>按钮，即可返回工作区查看标定结果：</p>
<ul>
<li><p><code>stereoParams.CameraParameters1</code></p>
<ul>
<li><code>K</code>：左相机内参矩阵；</li>
<li><code>RadialDistortion</code>：左相机径向畸变参数 <code>k1, k2, k3</code>，只有两个数值时表示 <code>k3 = 0</code>；</li>
<li><code>TangentialDistortion</code>：左相机切向畸变参数 <code>p1, p2</code>。</li>
</ul>
</li>
<li><p><code>stereoParams.CameraParameters2</code> 参数同上</p>
</li>
<li><p><code>stereoParams.PoseCamera2</code></p>
<ul>
<li><p><code>R</code>: 左相机到右相机的旋转矩阵；本次标定得到的参数为 </p>
<p>0.995139840336347    -0.00663565457289115    0.0982479835097892<br>-0.000660670904084147    0.997254587919016    0.0740462719414676<br>-0.0984695977919431    -0.0737513048214236    0.992403387412513</p>
<p>可以看到，由于两个相机固定位置较好，旋转矩阵基本为一个单位矩阵；</p>
</li>
<li><p><code>Translation</code>: 左相机到右相机的平移向量；本次标定得到的参数为</p>
<p>[-3.858979029808597e+02,-2.237062834310436,-22.218127814587270]</p>
</li>
</ul>
</li>
</ul>
<p><img src="/2024/04/28/stereo-calib/image-20240428221819515.png" alt="fig9" title="fig 9" style="zoom:50%;"></p>
<blockquote>
<p>记录一下OpenCV的相机参数格式：</p>
<ul>
<li><p>内参矩阵: </p>
<script type="math/tex; mode=display">
\begin{bmatrix}

f_x & 0 & c_x \\

0 & f_y & c_y \\

0 & 0 & 1

\end{bmatrix}</script></li>
<li><p>畸变向量: $(k_1, k_2, p_1, p_2, k_3)$</p>
</li>
</ul>
</blockquote>
<h1 id="4-双目相机校正"><a href="#4-双目相机校正" class="headerlink" title="4 双目相机校正"></a>4 双目相机校正</h1><h2 id="4-1-校正原理介绍"><a href="#4-1-校正原理介绍" class="headerlink" title="4.1 校正原理介绍"></a>4.1 校正原理介绍</h2><p>经过上述双目标定过程，可以获取双目相机的内参：$f_x, f_y, c_x, c_y$ ，畸变系数：$k_1, k_2, k_3, p_1, p_2$ ，以及两个相机之间的旋转矩阵和平移向量：$\mathbf{R}, \mathbf{T}$。双目相机的校正过程主要包括<strong>去畸变</strong>与<strong>立体校正</strong>两个步骤：</p>
<ul>
<li>去畸变：主要是对两个相机的径向畸变、切向畸变进行处理；</li>
<li>立体校正：双目相机主要作用是利用视差对双目匹配点进行测距，该过程是在双目系统处于理想<strong>共面行对准</strong>情况下进行的：即两个相机成像平面位于同一平面上，两成像平面之间只有平移变换。这样即可保证真实世界的3D点在双目系统中的投影像素位于同一水平线上，只需对双目图片进行单行搜索即可完成双目特征点匹配。立体校正过程如Fig 10所示。</li>
</ul>
<p><img src="/2024/04/28/stereo-calib/图片2.png" alt="图片2" title="figure 10" style="zoom: 33%;"></p>
<p>经过立体校正之后，即可根据双目匹配点进行深度获取，该过程如Fig 11所示：</p>
<p><img src="/2024/04/28/stereo-calib/image-20240509095832658.png" alt="image-20240509095832658" title="fig 11" style="zoom: 50%;"></p>
<p>根据比例关系，可以求得深度信息：$z = \frac{bf}{x_l - x_r} = \frac{bf}{d}$ ，其中，$d$ 即为视差。此处参考<a href="https://437436999.github.io/2020/02/25/%E5%8F%8C%E7%9B%AE%E5%8C%B9%E9%85%8D/">文章</a>。</p>
<h2 id="4-2-OpenCV双目校正过程"><a href="#4-2-OpenCV双目校正过程" class="headerlink" title="4.2 OpenCV双目校正过程"></a>4.2 OpenCV双目校正过程</h2><p>在OpenCV中，立体校正的流程如下所示：</p>
<ol>
<li>利用 <code>stereoRectify()</code>  函数实现Bouguet算法，得到立体校正所需的变换矩阵和投影矩阵；</li>
<li>将上步得到的变换矩阵和投影矩阵传入 <code>initUndistortRectifyMap()</code> 函数，生成校正图像到原始图像的映射关系；</li>
<li>根据映射关系，利用 <code>remap()</code> 函数将原图像处理为校正后的双目图像。</li>
</ol>
<p>校正过程的代码如下所示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">R1, R2, P1, P2, Q, roi1, roi2 = cv2.stereoRectify(kLeft, coeffsLeft, kRight, </span><br><span class="line">                                                  coeffsRight, (width, height), </span><br><span class="line">                                                  R, T, alpha=-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">map1x, map1y = cv2.initUndistortRectifyMap(kLeft, coeffsLeft, R1, P1,</span><br><span class="line">                                               (width, height), cv2.CV_16SC2)</span><br><span class="line">map2x, map2y = cv2.initUndistortRectifyMap(kRight, coeffsRight, R2, P2,</span><br><span class="line">                                               (width, height), cv2.CV_16SC2)</span><br><span class="line"></span><br><span class="line">rectifiedLeftImg = cv2.remap(leftImg, map1x, map1y, cv2.INTER_LINEAR)</span><br><span class="line">rectifiedRightImg = cv2.remap(rightImg, map2x, map2y, cv2.INTER_LINEAR)</span><br></pre></td></tr></table></figure>
<h2 id="4-3-校正前后图片对比"><a href="#4-3-校正前后图片对比" class="headerlink" title="4.3 校正前后图片对比"></a>4.3 校正前后图片对比</h2><p>利用上述过程对双目系统进行校正，校正前后的对比如下所示：</p>
<p><img src="/2024/04/28/stereo-calib/fig3.png" alt="fig3" title="fig 12"></p>
]]></content>
      <categories>
        <category>SLAM</category>
        <category>Experiments</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Stereo</tag>
        <tag>Calibration</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Self-Calibration of the Offset Between GPS and Semantic Map Frames for Robust Localization</title>
    <url>/2024/03/02/tseng2021/</url>
    <content><![CDATA[<p>Tseng, Wei-Kang, Angela P. Schoellig, and Timothy D. Barfoot. “Self-Calibration of the Offset Between GPS and Semantic Map Frames for Robust Localization.” In <em>2021 18th Conference on Robots and Vision (CRV)</em>, 173–80. Burnaby, BC, Canada: IEEE, 2021. <a href="https://doi.org/10.1109/CRV52889.2021.00031">https://doi.org/10.1109/CRV52889.2021.00031</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>当利用语义信息（如车道线、交通灯等）进行定位时，会出现部分路段无语义信息观测的情况，导致定位失败；此时，可将GPS 与其结合起来进行定位，但是在<strong>制图与定位之间会有时间流逝</strong>，导致实时GPS 坐标系会与语义地图坐标系之间形成偏差，这就需要<strong>对准标定操作</strong>。对准操作的常规做法是为实验增加一个手动设定的矫正先验值，但经常会出现错误对准的情况，不太可靠。</p>
<span id="more"></span>
<p><img src="/2024/03/02/tseng2021/fig1.png" alt="fig1" title="figure 1"></p>
<p>本系统的架构如下所示：</p>
<p><img src="/2024/03/02/tseng2021/fig2.png" alt="fig2" title="figure 2"></p>
<h1 id="3-Semantic-Cue-Preprocessing"><a href="#3-Semantic-Cue-Preprocessing" class="headerlink" title="3 Semantic Cue Preprocessing"></a>3 Semantic Cue Preprocessing</h1><p>本文方法使用轻量级HD 语义地图进行辅助定位，包含车道线与交通灯。</p>
<p><strong>数据关联</strong>如Fig. 3所示，作者利用<strong>估计位姿</strong>将语义地图上的路标投影至图像平面，利用ICP 寻找数据关联。</p>
<p><img src="/2024/03/02/tseng2021/fig3.png" alt="fig3" title="figure 3"></p>
<h1 id="4-Vehicle-Localization"><a href="#4-Vehicle-Localization" class="headerlink" title="4 Vehicle Localization"></a>4 Vehicle Localization</h1><h2 id="4-1-Problem-Setup"><a href="#4-1-Problem-Setup" class="headerlink" title="4.1 Problem Setup"></a>4.1 Problem Setup</h2><p>问题描述如Fig. 4所示，一共有三个坐标系：<strong>汽车</strong>、<strong>GPS</strong> 以及<strong>语义地图</strong>。利用 $\mathbf{T}_{VM,k}$ 将语义地图中位置已知的点 $\mathbf{p}_M^j$ 投影至图像中，利用<strong>重投影误差</strong>进行定位，并对 GPS 与语义地图间的偏差 $\mathbf{T}_{GM,k}$ 进行矫正。</p>
<p><img src="/2024/03/02/tseng2021/fig4.png" alt="fig4" title="figure 4"></p>
<h2 id="4-2-Process-and-Observation-Models"><a href="#4-2-Process-and-Observation-Models" class="headerlink" title="4.2 Process and Observation Models"></a>4.2 Process and Observation Models</h2><p>包括：</p>
<ul>
<li>Vehicle Dynamic Process Model</li>
<li>GPS Offset Process Model</li>
<li>GPS Observation Model</li>
<li>Traffic Light Observation Model</li>
<li>Lane Marking Observation Model</li>
<li>Wheel Encoder Observation Model</li>
<li>Pseudo-Measurement Observation Model</li>
</ul>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 VDO-SLAM_A Visual Dynamic Object-aware SLAM System</title>
    <url>/2024/02/21/vdo-slam/</url>
    <content><![CDATA[<p>Zhang, Jun, Mina Henein, Robert Mahony, and Viorela Ila. “VDO-SLAM: A Visual Dynamic Object-Aware SLAM System,” 2020.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者提出了VDO-SLAM (Visual Dynamic Object-aware SLAM)，一个基于Stereo/RGB-D 相机的<strong>动态SLAM 系统</strong>，利用图像语义信息同时实现<strong>机器人定位</strong>、<strong>静动态结构制图</strong>，并在场景中<strong>跟踪物体的运动</strong>。本文的贡献如下：</p>
<ul>
<li>将动态场景建模为一个<strong>统一的估计框架</strong>，包括机器人位姿、静动态3D 点以及物体运动；</li>
<li>对动态物体 SE(3) <strong>位姿变换</strong>的精确估计，并提取<strong>物体速度</strong>；</li>
<li>一个利用语义信息来<strong>跟踪移动物体</strong>的鲁棒方法，且能够处理由语义分割失败导致的<strong>间接遮挡</strong>。</li>
</ul>
<span id="more"></span>
<h1 id="3-Methodology"><a href="#3-Methodology" class="headerlink" title="3 Methodology"></a>3 Methodology</h1><h2 id="3-1-Background-and-Notation"><a href="#3-1-Background-and-Notation" class="headerlink" title="3.1 Background and Notation"></a>3.1 Background and Notation</h2><p>本文的符号表示如Fig. 2所示，本文利用<strong>光流</strong>来发掘连续帧之间的联系。</p>
<p><img src="/2024/02/21/vdo-slam/fig2.png" alt="fig2" title="figure 2"></p>
<p>作者根据<strong>物体特征点在物体上的位置不变</strong>的性质，得到<strong>特征点</strong>在全局参考坐标系下的运动模型：</p>
<p><img src="/2024/02/21/vdo-slam/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$^0_{k-1}H_k\in SE(3)$ 表示物体特征点在全局参考坐标系下的<strong>位姿转换（相当于物体的位姿变换）</strong>。上式是本文运动估计的<strong>核心</strong>所在，因为它从物体特征点的角度描述了物体的位姿变换，而不需要将物体的3D 位姿设定为一个随机变量。</p>
<h2 id="3-2-Camera-Pose-and-Object-Motion-Estimation"><a href="#3-2-Camera-Pose-and-Object-Motion-Estimation" class="headerlink" title="3.2 Camera Pose and Object Motion Estimation"></a>3.2 Camera Pose and Object Motion Estimation</h2><h3 id="3-2-1-Camera-Pose-Estimation"><a href="#3-2-1-Camera-Pose-Estimation" class="headerlink" title="3.2.1 Camera Pose Estimation"></a>3.2.1 Camera Pose Estimation</h3><p>通过<strong>最小重投影误差</strong>来估计相机位姿：</p>
<p><img src="/2024/02/21/vdo-slam/f7.png" alt="f7" title="formula 7"></p>
<p>其中，$^0\mathbf{m}^i_{k-1}$ 表示在时间k-1 观测到的静态3D 点；$^{I_k}\tilde{\mathbf{p}}^i_k$ 表示在图片 $I_k$ 中相应的2D 点。</p>
<h3 id="3-2-2-Object-Motion-Estimation"><a href="#3-2-2-Object-Motion-Estimation" class="headerlink" title="3.2.2 Object Motion Estimation"></a>3.2.2 Object Motion Estimation</h3><p>相似地，利用重投影误差来求解物体的运动估计 $^0_{k-1}H_k$ ：</p>
<p><img src="/2024/02/21/vdo-slam/f10.png" alt="f10" title="formula 10"></p>
<h3 id="3-2-3-Joint-Estimation-with-Optical-Flow"><a href="#3-2-3-Joint-Estimation-with-Optical-Flow" class="headerlink" title="3.2.3 Joint Estimation with Optical Flow"></a>3.2.3 Joint Estimation with Optical Flow</h3><p>跟踪移动物体上的特征点难度很大，比如当物体运动较大或相机距离物体较远的情况。因此，本文提出的技术目标在于同时对<strong>光流估计</strong>和<strong>运动估计</strong>进行优化。</p>
<h2 id="3-3-Graph-Optimization"><a href="#3-3-Graph-Optimization" class="headerlink" title="3.3 Graph Optimization"></a>3.3 Graph Optimization</h2><p>作者将动态SLAM 问题建模为一个<strong>因子图优化</strong>，如Fig. 3所示，该因子图包含<strong>四种类型的观测信息</strong>：</p>
<ol>
<li>3D 点测量（白色圆圈）</li>
<li>视觉里程计观测（黄色圆圈）</li>
<li>动态物体特征点运动（品红色圆圈）：同一个动态物体上的特征点的位姿转换相同</li>
<li>物体平滑运动观测（靛蓝色圆圈）：考虑相机帧率、物理规则会阻止相对大型物体（汽车）快速剧烈的运动，因此引入该平滑运动因子来最小化物体连续运动的改变</li>
</ol>
<p><img src="/2024/02/21/vdo-slam/fig3.png" alt="fig3" title="figure 3"></p>
<h1 id="4-System"><a href="#4-System" class="headerlink" title="4 System"></a>4 System</h1><p>系统整体框架如Fig. 4所示，系统主要包含三个部分：Pre-processing， Tracking 以及 Mapping。</p>
<p><img src="/2024/02/21/vdo-slam/fig4.png" alt="fig4" title="figure 4"></p>
<h2 id="4-1-Pre-processing"><a href="#4-1-Pre-processing" class="headerlink" title="4.1 Pre-processing"></a>4.1 Pre-processing</h2><p>该模块需要满足两个挑战：</p>
<ol>
<li>区分静态背景与物体；</li>
<li>确保对动态物体的长期跟踪。</li>
</ol>
<p>为了实现目标，作者使用<strong>实例分割</strong>与<strong>稠密光流估计</strong>。</p>
<p>其中，作者利用稠密光流估计来最大化<strong>动态物体的跟踪点数量</strong>，本方法利用稠密光流在语义掩码中的所有点进行采样来大幅增加物体特征点的数量；此外，稠密光流法通过对同一物体掩码内的所有点赋予一个独特的<strong>物体识别码</strong>，实现同时跟踪多个物体，且在实例分割失败的情况下可以<strong>恢复物体掩码</strong>。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 VSO_Visual Semantic Odometry</title>
    <url>/2024/01/30/vso/</url>
    <content><![CDATA[<p>Lianos, Konstantinos-Nektarios, Johannes L. Schönberger, Marc Pollefeys, and Torsten Sattler. “VSO: Visual Semantic Odometry.” In <em>Computer Vision – ECCV 2018</em>, edited by Vittorio Ferrari, Martial Hebert, Cristian Sminchisescu, and Yair Weiss, 11208:246–63. Lecture Notes in Computer Science. Cham: Springer International Publishing, 2018. <a href="https://doi.org/10.1007/978-3-030-01225-0_15">https://doi.org/10.1007/978-3-030-01225-0_15</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者认为，VO 的核心问题在于<strong>数据关联DA</strong>。</p>
<p>通常来讲，有两种方法来减少VO 中的漂移：</p>
<ol>
<li>方法一：利用连续图片之间的<strong>短期关联</strong>来进行偏移矫正；</li>
<li>方法二：利用回环检测实现<strong>长期关联</strong>。</li>
</ol>
<p>传统的几何特征（点、线、面）在光照、视角变化下鲁棒性较差，不能在长距离上保持持续跟踪，而语义特征作为更高级的特征，语义特征在光照、视角、尺寸发生较大变化时仍然可以保持不变，使得<strong>中期关联</strong>成为可能，如Fig. 1所示。</p>
<span id="more"></span>
<p><img src="/2024/01/30/vso/fig1.png" alt="fig1" title="figure 1"></p>
<p>本文的贡献如下：</p>
<ul>
<li>构建一个新的<strong>损失函数</strong>来最小化<strong>语义特征的重投影误差</strong>，并使用<strong>EM 算法</strong>进行最小化，且可以适用于任何语义分割算法；</li>
<li>作者证明了将语义损失考虑进VO 中可以大幅减小平移漂移，且本算法可以<strong>直接集成</strong>到现有的VO 算法中，无论是直接法还是间接法；</li>
<li>作者进行了实验验证，表明了该算法在特定条件下的效果，并讨论了当前的限制。</li>
</ul>
<h1 id="3-Visual-Semantic-Odometry"><a href="#3-Visual-Semantic-Odometry" class="headerlink" title="3 Visual Semantic Odometry"></a>3 Visual Semantic Odometry</h1><p>与文章(Bowman 等, 2017)采用物体的离散DA不同，本文考虑与物体边界的<strong>连续距离</strong>来定义损失函数。</p>
<h2 id="3-1-Visual-Semantic-Odometry-Framework"><a href="#3-1-Visual-Semantic-Odometry-Framework" class="headerlink" title="3.1 Visual Semantic Odometry Framework"></a>3.1 Visual Semantic Odometry Framework</h2><p>传统里程计的目标函数是：</p>
<p><img src="/2024/01/30/vso/f1.png" alt="f1" title="formula 1"></p>
<p>作者定义的语义损失函数是：</p>
<p><img src="/2024/01/30/vso/f2.png" alt="f2" title="formula 2"></p>
<p>将传统的损失函数和语义损失函数结合起来便形成了本文算法的目标函数：</p>
<p><img src="/2024/01/30/vso/f3.png" alt="f3" title="formula 3"></p>
<h2 id="3-2-Semantic-Cost-Function"><a href="#3-2-Semantic-Cost-Function" class="headerlink" title="3.2 Semantic Cost Function"></a>3.2 Semantic Cost Function</h2><p>联系语义观测 $S_k$ ，相机位姿 $T_k$ 和3D 点 $P_i$ （包含标签 $Z_i$ 和位置 $X_i$ ）来定义<strong>观测似然模型</strong>：$p(S_k|T_k, X_i, Z_i=c)$ ，该似然函数应该随着 3D 点 $P_i$ 在图片中的投影位置 $\pi(T_k, X_i)$ 与标注为类别 c 的区域的最近<strong>距离成反比</strong>；基于此，作者利用了distance transform，如Fig. 2所示，作者首先由语义分割图片（a）为每一个分类 c 生成相应的二分值灰度图片（b），然后基于该灰度图片定义一个distance transform：</p>
<p><img src="/2024/01/30/vso/f4.png" alt="DT" title="DT"></p>
<p><img src="/2024/01/30/vso/fig2.png" alt="fig2" title="figure 2"></p>
<p>利用 $DT^{(c)}_k(p)$ 定义观测似然函数：</p>
<p><img src="/2024/01/30/vso/f4-1.png" alt="f4" title="formula 4"></p>
<p>其中，$\pi$ 表示投影函数；$\sigma$ 表示语义图片分类的<strong>不确定性</strong>。为了简明起见，作者省略了归一化因子。根据上式，可通过调整相机位姿与点的位置，使得投影点移动到正确标签的区域内，以此最大化该似然函数。</p>
<p>根据式（4）作者定义了语义损失项：</p>
<p><img src="/2024/01/30/vso/f5.png" alt="f5" title="formula 5"></p>
<p>其中，$w_i^{(c)}$ 表示点 $P_i$ 属于种类 c 的概率。从上式可以看出，$e_{sem}(k,i)$ 表示<strong>2D 距离的加权平均</strong>，每个投影点 $\pi(T_k, X_i)$ 到最近属于种类 c 的距离 $DT^{(c)}_k(\pi(T_k, X_i))$ 被相应的类别概率 $w_i^{(c)}$ 加权。</p>
<p>值得注意的是，对于点 $P_i$ 的标签概率向量 $w_i$ 是通过所有对到该点的观测计算得到的：</p>
<p><img src="/2024/01/30/vso/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$\mathcal{T}_i$ 表示观测到点 $P_i$ 的相机位姿集合；参数 $\alpha$ 为归一化参数。利用该准则可以通过累积语义观测来实现对标签概率向量 $w_i$ 的<strong>增量式细调</strong>。</p>
<h2 id="3-3-Optimization"><a href="#3-3-Optimization" class="headerlink" title="3.3 Optimization"></a>3.3 Optimization</h2><p>式（5）、（6）表明优化参数涉及3D 点位置坐标，相机位姿以及种类关联参数，作者使用EM 算法进行求解，步骤如下：</p>
<ul>
<li>E step：基于式（6）计算每个点的权重向量 $w_i$ ，此步骤点坐标和相机位姿保持固定；</li>
<li>M step：优化点坐标和相机位姿，此步骤权重参数保持固定。</li>
</ul>
<p>该优化框架中，将点 $P_i$ 的标签 $Z_i$ 视为<strong>隐变量</strong>。</p>
<p>作者提到，使用本文提出的语义约束，可以实现不变观测，但是<strong>缺乏结构信息</strong>；仅使用语义项对地图点和相机位姿进行优化会导致<strong>欠约束</strong>，因为等式（4）表示的似然函数在物体边界内部服从<strong>均匀分布</strong>，为解决该问题，对于 $E_{sem}$ 的优化操作如下：</p>
<ol>
<li>如式（3）所示，与传统的VO 进行<strong>联合优化</strong>；</li>
<li>利用<strong>多重点和语义约束</strong>来优化相机位姿；</li>
<li>仅提供语义约束的点得到固定，然后只优化与它们相关的相机位姿来减小漂移，该方法不仅可以限制优化参数的数量，而且会在点之间引入<strong>结构相关</strong>，从而约束位姿求解（如Fig. 3所示）；</li>
<li><strong>频繁</strong>使用语义优化可以减少一个点错误关联的概率，因为基于distance transform 梯度的优化可以保证该点足够靠近正确标签的区域，并不断拉向该区域。</li>
</ol>
<p><img src="/2024/01/30/vso/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="3-4-Obtaining-Semantic-Constraints-amp-System-Integration"><a href="#3-4-Obtaining-Semantic-Constraints-amp-System-Integration" class="headerlink" title="3.4 Obtaining Semantic Constraints &amp; System Integration"></a>3.4 Obtaining Semantic Constraints &amp; System Integration</h2><p>类似于传统VO 使用的关键帧active window (AW)，作者也定义了一个关键帧active semantic window (ASW)，一个关键帧从AW 中剔除后会被添加进ASW，本算法在尽可能覆盖更多的轨迹的同时，也会限制ASW 中关键帧的数量。且，ASW 中关键帧的位姿<strong>不再进行优化</strong>，因为这些关键帧通常与目前的帧缺少光度/几何约束。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Pose Optimization</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 VINS-Mono_A Robust and Versatile Monocular Visual-Inertial State Estimator</title>
    <url>/2024/01/31/vins/</url>
    <content><![CDATA[<p>Qin, Tong, Peiliang Li, and Shaojie Shen. “VINS-Mono: A Robust and Versatile Monocular Visual-Inertial State Estimator.” <em>IEEE Transactions on Robotics</em> 34, no. 4 (August 2018): 1004–20. <a href="https://doi.org/10.1109/TRO.2018.2853729">https://doi.org/10.1109/TRO.2018.2853729</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>集成IMU 观测可通过减少由于光照变化、纹理稀少区域或运动模糊造成的视觉跟踪精度损失，来大幅提高运动跟踪的表现。但是，单目VINS (Visual-Inertial System) 在使用中也有一些问题需要解决：</p>
<ol>
<li>第一个问题是初始化困难：由于缺失直接的距离观测，很难直接将单目视觉结构与惯性测量进行融合；</li>
<li>其次是VINS 严重的非线性问题：这会在估计器初始化过程中带来巨大的挑战，再大部分场景中系统需要放置在一个位置已知的静态区域，然后缓慢小心地移动，这会极大限制系统的应用场景；</li>
<li>另一个问题是VIO 的长期漂移问题：为了消除累积漂移，会使用回环检测、重定位及全局优化技术；</li>
<li>此外还有对于地图保存与重使用的需求正在不断增长。</li>
</ol>
<span id="more"></span>
<p>为解决上述问题，作者提出了VINS-Mono 系统——一个鲁棒且多功能的单目视觉-惯性状态估计器，该系统包含以下特点：</p>
<ol>
<li><strong>鲁棒的初始化程序</strong>使得系统可以在未知状态进行启动；</li>
<li>紧耦合、基于优化的单目VIO 带有相机-IMU <strong>外参标定</strong>和IMU <strong>偏差校正</strong>；</li>
<li><strong>在线重定位</strong>以及4自由度的<strong>全局位姿图优化</strong>；</li>
<li>位姿图重用可<strong>保存、载入</strong>并<strong>融合</strong>多个局部位姿图。</li>
</ol>
<p>该系统已经成功应用于小规模的AR 场景、中规模的无人机导航，以及大规模的状态估计任务，如Fig. 1所示：</p>
<p><img src="/2024/01/31/vins/image-20240131090730704.png" alt="image-20240131090730704" title="figure 1"></p>
<h1 id="3-Overview"><a href="#3-Overview" class="headerlink" title="3 Overview"></a>3 Overview</h1><p>本文提出的单目视觉-惯性状态估计器的架构如Fig. 2所示，</p>
<p><img src="/2024/01/31/vins/image-20240131094458729.png" alt="image-20240131094458729" title="figure 2"></p>
<p>相较于适用于双目相机的SOTA 算法OKVIS，本算法是专为单目相机设计的，作者针对性设计了初始化程序、关键帧选取标准，并使用具有大视角的相机进行更好地跟踪；此外，本算法作为一个完整的系统还包含了回环检测以及位姿图重用模块。</p>
<p>本文的标注规则：</p>
<ul>
<li>$(.)^w$ 表示世界坐标系，重力方向与世界坐标系z 轴对齐；</li>
<li>$(.)^b$ 表示物体坐标系，与IMU 坐标系相同；</li>
<li>$(.)^c$ 表示相机坐标系；</li>
<li>使用旋转矩阵 $\mathbf{R}$ 和四元数 $\mathbf{q}$ 来表示旋转；</li>
<li>$\mathbf{q}^w_b$ 和 $\mathbf{p}^w_b$ 分别表示从物体坐标系到世界坐标系的旋转和平移；</li>
<li>$b_k$ 表示第<em>k</em> 张图片时刻的物体坐标系，$c_k$ 表示相机坐标系；</li>
<li>$\mathbf{g}^w = [0, 0, g]^T$ 是世界坐标系下的重力向量；</li>
<li>使用 $\hat{(.)}$ 表示某个参数的噪声观测或估计值。</li>
</ul>
<h1 id="4-Measurement-Preprocessing"><a href="#4-Measurement-Preprocessing" class="headerlink" title="4 Measurement Preprocessing"></a>4 Measurement Preprocessing</h1><p>观测预处理步骤：</p>
<ul>
<li>对于视觉观测：跟踪连续帧之间的特征，并在最新帧中检测新的特征；</li>
<li>对于IMU 观测：对连续帧之间的IMU 进行预积分。</li>
</ul>
<h2 id="4-1-Vision-Processing-Front-End"><a href="#4-1-Vision-Processing-Front-End" class="headerlink" title="4.1 Vision Processing Front End"></a>4.1 Vision Processing Front End</h2><p>对每张新图片，使用KLT 稀疏光流法来跟踪已有的特征；此外，检测新的角点特征来维持图片的最小特征数量（100-300）。探测器通过设定一个特征间最小像素间隔来执行特征均匀提取，2D 特征首先经过去畸变、外点剔除，然后投影至一个单位球上，外点剔除是通过RANSAC 方法实现的。</p>
<p>关键帧选取标准有两个：</p>
<ol>
<li><strong>较前一个关键帧的平均视差</strong>：若当前帧与上一个关键帧之间的平均视差超过一定阈值，则将当前帧设为关键帧；值得注意的是，平移与旋转均可以造成视差，但是纯旋转运动无法对特征进行三角化，为解决该问题，作者在计算视差时使用短期的<strong>陀螺仪观测来补偿旋转</strong>，需要说明的是，旋转补偿只用于关键帧选取，并不用于VINS 的旋转计算。</li>
<li><strong>跟踪质量</strong>：如果当前帧跟踪到的特征数量低于一个阈值，则将当前帧设为关键帧。</li>
</ol>
<h2 id="4-2-IMU-Preintegration"><a href="#4-2-IMU-Preintegration" class="headerlink" title="4.2 IMU Preintegration"></a>4.2 IMU Preintegration</h2><p>作者使用之前工作中用的连续时间基于四元数的IMU 预积分推导方法。</p>
<h3 id="4-2-1-IMU-Noise-and-Bias"><a href="#4-2-1-IMU-Noise-and-Bias" class="headerlink" title="4.2.1 IMU Noise and Bias"></a>4.2.1 IMU Noise and Bias</h3><p>IMU 的观测信息是在<strong>物体坐标系</strong>下的，包含重力和平台的运动信息，此外，观测还包含加速度偏差 $\mathbf{b}_a$ 、陀螺仪偏差 $\mathbf{b}_w$ 以及额外噪声，陀螺仪和加速度计的原始观测量 $\hat{\mathcal{w}}, \hat{\mathcal{a}}$ 如下所示：</p>
<p><img src="/2024/01/31/vins/image-20240131103735319.png" alt="image-20240131103735319" title="formula 1"></p>
<p>作者假设加速度计和陀螺仪的观测噪声服从高斯分布：$\mathbf{n}_a\sim\mathcal{N}(0, \sigma_a^2), \mathbf{n}_w\sim\mathcal{N}(0, \sigma_w^2)$ ；偏差 $\mathbf{b}_a$ 、 $\mathbf{b}_w$ 被建模为<strong>随机游走</strong>，其对应导数为高斯白噪声，$\mathbf{n}_{b_a}\sim\mathcal{N}(0, \sigma_{b_a}^2), \mathbf{n}_{b_w}\sim\mathcal{N}(0, \sigma_{b_w}^2)$ 。</p>
<p><img src="/2024/01/31/vins/image-20240131104508327.png" alt="image-20240131104508327" title="formula 2"></p>
<h3 id="4-2-2-Preintegration"><a href="#4-2-2-Preintegration" class="headerlink" title="4.2.2 Preintegration"></a>4.2.2 Preintegration</h3><p>连续两帧图片 $b_k, b_{k+1}$ 之间存在多组惯性观测数据，给定偏差估计，在局部坐标系 $b_k$ 中进行积分：</p>
<p><img src="/2024/01/31/vins/image-20240131104847991.png" alt="image-20240131104847991" title="formula 3"></p>
<p>其中：</p>
<p><img src="/2024/01/31/vins/image-20240131105001484.png" alt="image-20240131105001484" title="formula 4"></p>
<p>$\alpha, \beta, \gamma$ 的协方差 $\mathbf{P}_{b_{k+1}}^{b_k}$ 也进行相应传递；可以看出预积分项（式3）可通过给定 $b_k$ 作为偏差的参考坐标系，仅用IMU 观测即可获取。</p>
<h3 id="4-2-3-Bias-Correction"><a href="#4-2-3-Bias-Correction" class="headerlink" title="4.2.3 Bias Correction"></a>4.2.3 Bias Correction</h3><p>如果偏差估计改变量较小，利用一阶导数近似进行调整：</p>
<p><img src="/2024/01/31/vins/image-20240131105701602.png" alt="image-20240131105701602" title="formula 5"></p>
<p>如果偏差估计改变量较大，则在新的偏差估计下进行传递。该策略可为基于优化的算法节省大量计算资源，因为不需要重复进行IMU 观测传递。</p>
<h1 id="5-Estimator-Initialization"><a href="#5-Estimator-Initialization" class="headerlink" title="5 Estimator Initialization"></a>5 Estimator Initialization</h1><p>单目相机紧耦合的VIO 是一个高非线性系统，需要在开始时刻进行准确的初始化估计。作者通过将IMU 预积分和视觉观测进行松对齐来获取必要的初始估计值。</p>
<h2 id="5-1-Vision-Only-SfM-in-Sliding-Window"><a href="#5-1-Vision-Only-SfM-in-Sliding-Window" class="headerlink" title="5.1 Vision-Only SfM in Sliding Window"></a>5.1 Vision-Only SfM in Sliding Window</h2><p>作者控制滑动窗口内的图像帧数来限制计算复杂度，SfM 过程采取以下步骤：</p>
<ol>
<li>首先，确定当前帧与窗口内所有历史帧之间的特征关联，如果可以找到稳定的特征跟踪（超过30个跟踪特征）以及充分的视差（超过20像素），则可利用five-point 算法获取两帧之间的相对旋转和位移；</li>
<li>然后，随意设定尺度参数，对两帧中的所有跟踪特征进行三角化，基于三角化特征，使用PnP 算法来估计窗口内其他所有帧的位姿；</li>
<li>最终，利用全局BA 来最小化所有观测特征的重投影误差。</li>
</ol>
<p>由于尚未确定世界坐标系，所以将第一帧图片的位姿 $(.)^{c_0}$ 作为SfM 的参考坐标系，所有图片代表的相机位姿 $(\overline{\mathbf{p}}_{c_k}^{c_0}, \mathbf{q}_{c_k}^{c_0})$ 和特征位置都表示为 $(.)^{c_0}$ 的相对量，给定相机和IMU 之间的外参 $(\mathbf{p}_c^b, \mathbf{q}_c^b)$ ，可将相机位姿从相机坐标系转换至物体坐标系：</p>
<p><img src="/2024/01/31/vins/image-20240131111947126.png" alt="image-20240131111947126" title="formula 6"></p>
<p>其中，<em>s</em> 是未知的尺度参数。</p>
<h2 id="5-2-Visual-Inertial-Alignment"><a href="#5-2-Visual-Inertial-Alignment" class="headerlink" title="5.2 Visual-Inertial Alignment"></a>5.2 Visual-Inertial Alignment</h2><p>视觉-惯性的对齐过程如Fig. 3所示，基本想法是将视觉SfM 和IMU 预积分匹配起来。</p>
<p><img src="/2024/01/31/vins/image-20240131112327470.png" alt="image-20240131112327470" title="figure 3"></p>
<h3 id="5-2-1-Gyroscope-Bias-Calibration"><a href="#5-2-1-Gyroscope-Bias-Calibration" class="headerlink" title="5.2.1 Gyroscope Bias Calibration"></a>5.2.1 Gyroscope Bias Calibration</h3><p>假设从SfM 获取窗口内连续两帧 $b_k, b_{k+1}$ 的<strong>旋转参数</strong> $\mathbf{q}_{b_k}^{c_0}, \mathbf{q}_{b_k+1}^{c_0}$ ，以及从IMU 预积分中获取两者之间的<strong>相对约束</strong> $\hat{\gamma}^{b_k}_{b_{k+1}}$ ，作者将IMU 预积分项做关于陀螺仪偏差的<strong>线性化</strong>，并最小化下面的<strong>损失方程</strong>：</p>
<p><img src="/2024/01/31/vins/image-20240131151351640.png" alt="image-20240131151351640" title="formula 7"></p>
<p>其中，$\mathcal{B}$ 表示窗口内的所有图像帧。这样可以得到陀螺仪偏差 $\mathbf{b}_w$ 的初始标定值，然后使用新的陀螺仪偏差来传递所有的IMU 预积分项 $\hat{\alpha}^{b_k}_{b_{k+1}}, \hat{\beta}^{b_k}_{b_{k+1}}, \hat{\gamma}^{b_k}_{b_{k+1}}$ 。</p>
<h3 id="5-2-2-Velocity-Gravity-Vector-and-Metric-Scale-Initialization"><a href="#5-2-2-Velocity-Gravity-Vector-and-Metric-Scale-Initialization" class="headerlink" title="5.2.2 Velocity, Gravity Vector, and Metric Scale Initialization"></a>5.2.2 Velocity, Gravity Vector, and Metric Scale Initialization</h3><p>在对陀螺仪偏差进行初始化之后，继续对其他导航状态参数进行初始化，包括<strong>速度</strong>、<strong>重力向量</strong>以及<strong>尺度参数</strong>。</p>
<p><img src="/2024/01/31/vins/image-20240131152147563.png" alt="image-20240131152147563" title="formula 8"></p>
<p>其中，$\mathbf{v}_{b_k}^{b_k}$ 为第<em>k</em> 张图片时在物体坐标系中的速度；$\mathbf{g}^{c_0}$ 表示 $c_0$ 帧的重力向量；<em>s</em> 代表单目SfM 的尺度单位。</p>
<p>对于窗口内的连续两帧 $b_k, b_{k+1}$ ，有以下等式表示：</p>
<p><img src="/2024/01/31/vins/image-20240131152817355.png" alt="image-20240131152817355" title="formula 9"></p>
<p>结合式6和式9，得到以下的线性观测模型：</p>
<p><img src="/2024/01/31/vins/image-20240131152915603.png" alt="image-20240131152915603" title="formula 10-11"></p>
<p>其中，$\mathbf{R}_{b_k}^{c_0}, \mathbf{R}_{b_{k+1}}^{c_0}, \overline{\mathbf{p}}_{c_k}^{c_0}, \overline{\mathbf{p}}_{c_{k+1}}^{c_0}$ 可从单目视觉SfM 获取；$\Delta t_k$ 表示连续两帧的时间间隔。通过求解下面的最小二乘问题可得到每一帧图片代表的物体坐标系下的<strong>速度参数</strong>、视觉参考坐标系下的<strong>重力向量</strong>，以及<strong>尺度参数</strong>：</p>
<p><img src="/2024/01/31/vins/image-20240131153711975.png" alt="image-20240131153711975" title="formula 12"></p>
<h3 id="5-2-3-Gravity-Refinement"><a href="#5-2-3-Gravity-Refinement" class="headerlink" title="5.2.3 Gravity Refinement"></a>5.2.3 Gravity Refinement</h3><p>从上述线性初始化中获取的重力向量可通过<strong>约束数值大小</strong>进行细调，大部分情况下重力向量的大小是已知的，这就导致该重力向量是2自由度的，因此，作者使用正切空间中的两个向量对重力进行扰动：$g(\hat{\overline{\mathbf{g}}} + \delta \mathbf{g}, \delta \mathbf{g} = w_1\mathbf{b}_1 + w_2\mathbf{b}_2)$，其中，<em>g</em> 为重力的已知大小，$\hat{\overline{\mathbf{g}}}$ 表示重力方向的单位向量，$\mathbf{b}_1, \mathbf{b}_2$ 为正切平面上的两个正交偏差，如Fig. 4所示，$w_1, w_2$ 为两个方向上的扰动值。</p>
<p><img src="/2024/01/31/vins/image-20240131154525405.png" alt="image-20240131154525405" title="figure 4"></p>
<p>使用 $g(\hat{\overline{\mathbf{g}}} + \delta \mathbf{g})$ 替代式9中的 $\mathbf{g}$ ，与其他参数共同求解2自由度的 $\delta \mathbf{g}$ 。</p>
<h3 id="5-2-4-Completing-Initialization"><a href="#5-2-4-Completing-Initialization" class="headerlink" title="5.2.4 Completing Initialization"></a>5.2.4 Completing Initialization</h3><p>在细调重力向量之后，通过将重力向量对齐世界坐标系的<em>z</em> 轴，来获取世界坐标系和相机坐标系 $c_0$ 之间的旋转参数 $\mathbf{q}_{c_0}^w$ ，然后即可将获取到的所有参数从相机坐标系转换至世界坐标系。至此，系统初始化完成。</p>
<h1 id="6-Tightly-Coupled-Monocular-VIO"><a href="#6-Tightly-Coupled-Monocular-VIO" class="headerlink" title="6 Tightly Coupled Monocular VIO"></a>6 Tightly Coupled Monocular VIO</h1><p>基于滑动窗口的紧耦合单目VIO 进行高精度与鲁棒状态估计，过程如Fig. 5所示：</p>
<p><img src="/2024/01/31/vins/image-20240131160055792.png" alt="image-20240131160055792" title="figure 5"></p>
<h2 id="6-1-Formulation"><a href="#6-1-Formulation" class="headerlink" title="6.1 Formulation"></a>6.1 Formulation</h2><p>滑动窗口内的状态向量定义为：</p>
<p><img src="/2024/01/31/vins/image-20240131160353726.png" alt="image-20240131160353726" title="formula 13"></p>
<p>其中，$\mathbf{x}_k$ 表示第<em>k</em> 张图片代表的IMU 状态，包含世界坐标系下的位置、速度及朝向，以及IMU 物体坐标系下的加速度偏差和陀螺仪偏差；<em>n</em> 表示关键帧的总数；<em>m</em> 表示窗口内的特征总数；$\lambda_l$ 表示第 <em>l</em> 个特征在首次观测的逆距离。</p>
<p>作者使用视觉-惯性BA 方程，最小化先验和所有观测残差M 范数的总和来获取最大后验估计：</p>
<p><img src="/2024/01/31/vins/image-20240131161246881.png" alt="image-20240131161246881" title="formula 14"></p>
<p>其中，$\mathbf{r}_{\mathcal{B}}, \mathbf{r}_{\mathcal{C}}$ 分别表示IMU 和视觉观测的残差；$\mathcal{B}$ 表示所有的IMU 观测；$\mathcal{C}$ 表示当前滑动窗口内至少被观测2次的特征集合；$\{\mathbf{r}_p, \mathbf{H}_p\}$ 表示边缘化先验信息。本系统使用Ceres 求解非线性问题。</p>
<h2 id="6-2-IMU-Measurement-Residual"><a href="#6-2-IMU-Measurement-Residual" class="headerlink" title="6.2 IMU Measurement Residual"></a>6.2 IMU Measurement Residual</h2><p>考虑在窗口内连续两帧间 $b_k, b_{k+1}$ 的IMU 观测，其预积分残差可被定义为：</p>
<p><img src="/2024/01/31/vins/image-20240131162230978.png" alt="image-20240131162230978" title="formula 16"></p>
<p>其中，$[.]_{xyz}$ 表示提取误差状态表示四元数的向量部分；$\delta \theta ^{b_k}_{b_{k+1}}$ 表示四元数的3D 误差状态表示； $[\hat{\alpha}^{b_k}_{b_{k+1}}, \hat{\beta}^{b_k}_{b_{k+1}}, \hat{\gamma}^{b_k}_{b_{k+1}}]$ 表示IMU 预积分观测项。</p>
<h2 id="6-3-Visual-Measurement-Residual"><a href="#6-3-Visual-Measurement-Residual" class="headerlink" title="6.3 Visual Measurement Residual"></a>6.3 Visual Measurement Residual</h2><p>相较于传统针孔相机模型将重投影误差定义在一个图像平面上的做法，本文作者将<strong>相机观测残差定义在一个单位球上</strong>。几乎所有类型相机的光学特性（包括广角相机、鱼眼相机、全向相机等）都可以建模为连接单位球表面的单位射线。假设第 $l$ 个特征是在第 $i$ 帧图片被首次观测到，则该特征在第 $j$ 帧图片上的观测残差定义为：</p>
<p><img src="/2024/01/31/vins/image-20240131163915327.png" alt="image-20240131163915327" title="formula 17"></p>
<p>其中，$[\hat{u}^{c_i}_l, \hat{v}^{c_i}_l]$ 表示第 $l$ 个特征在第 $i$ 帧图片被首次观测到的像素坐标；$[\hat{u}^{c_j}_l, \hat{v}^{c_j}_l]$ 表示该特征在图片 $j$ 中的像素坐标；由于视觉残差是2自由度的，所以作者将残差向量投影至正切平面，$\mathbf{b}_1, \mathbf{b}_2$ 表示 $\hat{\overline{\mathcal{P}}}^{c_j}_l$ 正切平面上的两个正交向量，如Fig. 6所示；式14中的协方差 $\mathbf{P}_l^{c_j}$ 也从像素坐标系传递至单位球上。</p>
<p><img src="/2024/01/31/vins/image-20240131170737502.png" alt="image-20240131170737502" title="figure 6"></p>
<h2 id="6-4-Marginalization"><a href="#6-4-Marginalization" class="headerlink" title="6.4 Marginalization"></a>6.4 Marginalization</h2><p>为了限制计算复杂度，作者使用边缘化策略，有选择性地从滑动窗口中边缘化掉IMU 状态 $\mathbf{x}_k$ 和特征 $\lambda_l$ ，同时将对应观测的边缘状态量转换为先验信息。边缘化过程如Fig. 7所示，作者选择不边缘化掉非关键帧的所有观测是为了保持系统的稀疏性；本系统的边缘化策略是为了保持窗口内关键帧的分隔状态，以保证特征三角化所需的充分视差，以及大激励加速度计观测的概率。边缘化是通过Schur complement 实现的。</p>
<p><img src="/2024/01/31/vins/image-20240131172037799.png" alt="image-20240131172037799" title="figure 7"></p>
<h2 id="6-5-Motion-Only-Visual-Inertial-Optimization-for-Camera-Rate-State-Estimation"><a href="#6-5-Motion-Only-Visual-Inertial-Optimization-for-Camera-Rate-State-Estimation" class="headerlink" title="6.5 Motion-Only Visual-Inertial Optimization for Camera-Rate State Estimation"></a>6.5 Motion-Only Visual-Inertial Optimization for Camera-Rate State Estimation</h2><p>由于非线性优化对算力的要求，低算力设备（如手机等）上的紧耦合单目VIO 无法实现相机采集率级别的输出，因此，除了full optimization 外，作者还实现了一个轻量级motion-only visual-inertial 优化来提高状态估计的速率至30Hz。</p>
<p>该轻量级优化方法的损失函数与单目VIO 相同（式14），但不会优化窗口内的所有参数，而是只优化几个的最新IMU 状态的位姿与速度；在优化中会使用所有的视觉和惯性观测信息，这会实现较单帧 PnP 方法更为顺滑的状态估计解，该方法的介绍如Fig. 8所示。</p>
<p><img src="/2024/01/31/vins/image-20240131174250572.png" alt="image-20240131174250572" title="figure 8"></p>
<p>两种优化方法的速度对比：在最新的嵌入式电脑中，full optimization 的处理时间约为50ms，而该轻量级优化的处理时间约为5ms。</p>
<h2 id="6-6-IMU-Forward-Propagation-for-IMU-Rate-State-Estimation"><a href="#6-6-IMU-Forward-Propagation-for-IMU-Rate-State-Estimation" class="headerlink" title="6.6 IMU Forward Propagation for IMU-Rate State Estimation"></a>6.6 IMU Forward Propagation for IMU-Rate State Estimation</h2><p>本文提出的VIO 输出频率被限制到相机采集速率上，但是仍然可以使用附近的IMU 观测信息来直接传递最新的VIO 估计，以实现IMU 采集速率级别的输出表现。高速率状态估计可被用于回环的状态反馈，作者使用该方法测试了无人机飞行试验。</p>
<h1 id="7-Relocalization"><a href="#7-Relocalization" class="headerlink" title="7 Relocalization"></a>7 Relocalization</h1><p>本系统采用了滑动窗口和边缘化策略来限制计算复杂度，但同时也引入了<strong>累积漂移</strong>；为了消除累积漂移，作者提出了一种可与单目VIO 无缝集成的<strong>紧耦合重定位模块</strong>。重定位过程首先使用<strong>回环检测模块</strong>来判断某场景是否出现过，然后建立回环候选帧与当前帧之间的<strong>特征级关联</strong>，这些特征关联<strong>紧耦合至单目VIO 模块</strong>，在使用最小计算的情况下实现<strong>无漂移状态估计</strong>。对多个特征的多次观测可被直接用于重定位，以实现更高的定位精度与顺滑度。重定位过程如Fig. 9 (a)所示。</p>
<p><img src="/2024/01/31/vins/image-20240131193322044.png" alt="image-20240131193322044" title="figure 9"></p>
<h2 id="7-1-Loop-Detection"><a href="#7-1-Loop-Detection" class="headerlink" title="7.1 Loop Detection"></a>7.1 Loop Detection</h2><p>作者使用<strong>DBoW2</strong> 进行回环检测，除了用于单目VIO 的角点特征外，还使用额外500个BRIEF 描述子代表的角点特征，以实现更高的回环检测召回率。经过时间与几何维度的一致性检验后，DBoW2 输出若干个回环检测候选帧。本系统保留所有的BRIEF 描述子以进行特征恢复，但为了节约内存消耗会舍弃所有的原始图像数据。</p>
<h2 id="7-2-Feature-Retrieval"><a href="#7-2-Feature-Retrieval" class="headerlink" title="7.2 Feature Retrieval"></a>7.2 Feature Retrieval</h2><p>当检测到回环后，局部窗口与回环候选帧之间会通过恢复特征关联（BRIEF 描述子匹配）建立联系，作者使用两步的几何外点方法来剔除错误匹配，如Fig. 10所示：</p>
<p><img src="/2024/01/31/vins/image-20240131194802806.png" alt="image-20240131194802806" title="figure 10"></p>
<ul>
<li>2D-2D：利用RANSAC 进行基础矩阵测试；</li>
<li>3D-2D：利用RANSAC 进行PnP 测试，基于窗口内特征点的已知3D 位置及回环候选帧中的2D 观测进行PnP 测试。</li>
</ul>
<h2 id="7-3-Tightly-Coupled-Relocalization"><a href="#7-3-Tightly-Coupled-Relocalization" class="headerlink" title="7.3 Tightly Coupled Relocalization"></a>7.3 Tightly Coupled Relocalization</h2><p>在重定位中，本系统将所有回环帧的位姿设为常量（不进行优化），使用所有的IMU 观测、局部视觉观测，以及恢复的特征关联对窗口进行联合优化。其中，视觉观测模型与式17相同，除了回环帧的位姿 $(\hat{\mathbf{q}}_v^w, \hat{\mathbf{p}}_v^w)$ 被设为常量，该常量来自位姿图，或直接取自里程计输出（若是第一次重定位）。基于此，将式14更改为如下所示：</p>
<p><img src="/2024/01/31/vins/image-20240131200032028.png" alt="image-20240131200032028" title="formula 18"></p>
<p>其中，$\mathcal{L}$ 表示从回环帧中恢复的特征观测，$(l, v)$ 表示在回环帧 $v$ 中观测到的第 $l$ 个特征。</p>
<h1 id="8-Global-Pose-Graph-Optimization-and-Map-Reuse"><a href="#8-Global-Pose-Graph-Optimization-and-Map-Reuse" class="headerlink" title="8 Global Pose Graph Optimization and Map Reuse"></a>8 Global Pose Graph Optimization and Map Reuse</h1><p>在重定位之后，使用额外的位姿图优化来确保历史位姿保持全局一致。</p>
<h2 id="8-1-Four-Accumulated-Drift-Direction"><a href="#8-1-Four-Accumulated-Drift-Direction" class="headerlink" title="8.1 Four Accumulated Drift Direction"></a>8.1 Four Accumulated Drift Direction</h2><p>受益于对重力的惯性测量，roll 与 pitch 角度可在VINS 中得到完整观测，如Fig. 11所示，随着物体运动，物体的 3D 位置和旋转会在参考坐标系下发生相对变化，可通过重力向量判断水平面，从而实现对roll 和 pitch 角的确定。因此，累积漂移仅出现在 $x, y, z, yaw$ 上，为了充分利用已知信息来校正漂移，作者固定roll 和 pitch 角度，利用位姿图实现<strong>对4自由度的优化</strong>。</p>
<p><img src="/2024/01/31/vins/image-20240131200758379.png" alt="image-20240131200758379" title="figure 11"></p>
<h2 id="8-2-Adding-Keyframes-Into-the-Pose-Graph"><a href="#8-2-Adding-Keyframes-Into-the-Pose-Graph" class="headerlink" title="8.2 Adding Keyframes Into the Pose Graph"></a>8.2 Adding Keyframes Into the Pose Graph</h2><p>关键帧在VIO 处理之后被添加进位姿图中，每个关键帧在位姿图中都作为一个顶点，与其他顶点之间有两种类型的边，如Fig. 12所示。</p>
<ol>
<li>Sequential Edge：一个关键帧会与之前的关键帧之间建立序列边，序列边表示两帧之间的相对位姿转换，该转换直接取自于VIO，且该转换关系只包含平移 $\hat{\mathbf{p}}^i_{ij}$ 和yaw 角 $\hat{\psi}_{ij}$；</li>
</ol>
<p><img src="/2024/01/31/vins/image-20240131204337669.png" alt="image-20240131204337669" title="formula 19"></p>
<ol>
<li>Loop-Closure Edge：回环边的值来自重定位，同样也只包含平移和yaw 角的4自由度参数。</li>
</ol>
<p><img src="/2024/01/31/vins/image-20240131203724102.png" alt="image-20240131203724102" title="figure 12"></p>
<h2 id="8-3-4-DOF-Pose-Graph-Optimization"><a href="#8-3-4-DOF-Pose-Graph-Optimization" class="headerlink" title="8.3 4-DOF Pose Graph Optimization"></a>8.3 4-DOF Pose Graph Optimization</h2><p>定义两帧间边的残差为：</p>
<p><img src="/2024/01/31/vins/image-20240131204626147.png" alt="image-20240131204626147" title="formula 20"></p>
<p>其中，$\hat{\phi}_i, \hat{\theta}_i$ 分别是固定的roll pitch 角。</p>
<p>序列边和回环边的联合损失函数如下式所示：</p>
<p><img src="/2024/01/31/vins/image-20240131204832609.png" alt="image-20240131204832609" title="formula 21"></p>
<p>尽管紧耦合的重定位模块已经对回环检测进行了筛选，作者还是增加了Huber 核函数 $\rho(.)$ 来进一步减少错误回环带来的影响。相对应的，序列边部分不使用额外处理，这是因为VIO 已经包含了充足的外点剔除机制。</p>
<p>位姿图优化和重定位在两个独立线程进行异步运行，这使得重定位可以使用最新处理过的位姿图优化结果。</p>
<h2 id="8-4-Pose-Graph-Merging"><a href="#8-4-Pose-Graph-Merging" class="headerlink" title="8.4 Pose Graph Merging"></a>8.4 Pose Graph Merging</h2><p>位姿图不仅可以优化当前地图，而且还可以利用<strong>回环检测</strong>将当前地图与历史构建地图进行融合，如Fig. 13所示。</p>
<p><img src="/2024/01/31/vins/image-20240131205629702.png" alt="image-20240131205629702" title="figure 13"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Multi-source</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>C++</tag>
        <tag>IMU</tag>
        <tag>VINS</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 A Unified Framework for Mutual Improvement of SLAM and Semantic Segmentation</title>
    <url>/2024/02/21/wang2019/</url>
    <content><![CDATA[<p>Wang, Kai, Yimin Lin, Luowei Wang, Liming Han, Minjie Hua, Xiang Wang, Shiguo Lian, and Bill Huang. “A Unified Framework for Mutual Improvement of SLAM and Semantic Segmentation.” In <em>2019 International Conference on Robotics and Automation (ICRA)</em>, 5224–30. Montreal, QC, Canada: IEEE, 2019. <a href="https://doi.org/10.1109/ICRA.2019.8793499">https://doi.org/10.1109/ICRA.2019.8793499</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献 总结如下：</p>
<ul>
<li>提出了一个<strong>同时增强</strong>vSLAM 和语义分割的统一框架；</li>
<li>通过识别并处理<strong>移动物体</strong>和<strong>潜在动态物体</strong>来增强制图与定位的<strong>精度</strong>；</li>
<li>提出一个利用3D 位姿来有效<strong>优化语义分割</strong>的策略。</li>
</ul>
<span id="more"></span>
<h1 id="3-Framework"><a href="#3-Framework" class="headerlink" title="3 Framework"></a>3 Framework</h1><h2 id="3-1-Overall-Workflow"><a href="#3-1-Overall-Workflow" class="headerlink" title="3.1 Overall Workflow"></a>3.1 Overall Workflow</h2><p>系统框架如图一所示，系统以RGBD 图像作为输入，主要包含两个模块：<strong>vSLAM</strong> 以及<strong>语义分割模块</strong>。</p>
<p><img src="/2024/02/21/wang2019/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-2-Initial-Segmentation"><a href="#3-2-Initial-Segmentation" class="headerlink" title="3.2 Initial Segmentation"></a>3.2 Initial Segmentation</h2><p>本系统使用FCIS 进行<strong>初始化（粗糙coarse）分割</strong>，FCIS 可以计算每个物体的bbox，如果bbox 内的像素值the pixel value大于一定阈值，则被视为物体的一部分，否则被标记为背景。分割之后可以得到所有的<strong>先验动态物体</strong>。</p>
<h2 id="3-3-vSLAM-based-on-Segmentation-Result"><a href="#3-3-vSLAM-based-on-Segmentation-Result" class="headerlink" title="3.3 vSLAM based on Segmentation Result"></a>3.3 vSLAM based on Segmentation Result</h2><p>作者使用ORB-SLAM2来处理RGBD 图片。对于每一帧新的图片，提取ORB 特征并将其<strong>与深度图对齐</strong>以得到其3D 坐标，然后根据<strong>最小重投影误差</strong>计算<strong>初始（粗糙coarse）位姿估计</strong>。</p>
<p>在初始位姿估计基础上来判断物体的<strong>运动状态</strong>：作者根据分割结果将图片像素分为A组：代表背景区域，$\{B_i|i=1…n\}$ 表示不同的物体区域；根据初始位姿估计，计算相应匹配点在当前帧中的投影位置，根据两者之间的欧式距离判断该点是否是动态特征点；若组 $B_i$ 内判定为动态特征点的数量超过一个阈值，则判定该物体为<strong>动态物体</strong>。</p>
<p><img src="/2024/02/21/wang2019/fig2.png" alt="fig2" title="figure 2"></p>
<p>在识别出动态物体后，根据背景A及静态物体 $\{B_s\}$ 上的特征点重新计算重投影误差，得到<strong>精确的位姿估计</strong>；然后，将该精确位姿传递至分割模块，利用精确的位姿估计实现对<strong>语义分割的优化</strong>。</p>
<p>在vSLAM 模块中创建并维护两个地图：<strong>跟踪地图</strong>和<strong>长期地图</strong>。跟踪地图用于跟踪过程中计算相机的轨迹，利用精确的位姿估计将<strong>背景特征点</strong>和<strong>静态物体特征点</strong>添加进跟踪地图中。而长期地图是用于长期使用的，如回环检测等，因此需要将更加稳定的特征点添加进长期地图中，在本系统中<strong>只考虑背景中</strong>的特征点。</p>
<h2 id="3-4-Refinement-of-Segmentation-Result"><a href="#3-4-Refinement-of-Segmentation-Result" class="headerlink" title="3.4 Refinement of Segmentation Result"></a>3.4 Refinement of Segmentation Result</h2><p>利用前一帧图片的精确位姿估计 $(R_f, T_f)$ 和分割结果，结合当前帧的初始位姿估计 $(R_c, T_c)$ 可以实现对<strong>当前帧分割结果的优化</strong>。</p>
<p>将前一帧分割区域的每个点 $(p_u, p_v)$ 投影至当前帧 $(p_u’, p_v’)$ 处，转换过程如下所示：</p>
<p><img src="/2024/02/21/wang2019/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$R = R_c^{-1}R_f, T = T_f - T_x$ 代表两帧之间的相对位姿变换。然后采用如下策略实现对当前帧初始位姿估计的优化，值得注意的是，本策略的前提是基于连续两帧图片<strong>不会发生剧烈改变的假设</strong>。</p>
<p><img src="/2024/02/21/wang2019/a1.png" alt="a1" title="algorithm 1"></p>
<p><img src="/2024/02/21/wang2019/f5.png" alt="f5" title="formula 5"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Robust Loop Closure Detection Integrating Visual–Spatial–Semantic Information via Topological Graphs and CNN Features</title>
    <url>/2024/02/29/wang2020/</url>
    <content><![CDATA[<p>Wang, Yuwei, Yuanying Qiu, Peitao Cheng, and Xuechao Duan. “Robust Loop Closure Detection Integrating Visual–Spatial–Semantic Information via Topological Graphs and CNN Features.” <em>Remote Sensing</em> 12, no. 23 (November 27, 2020): 3890. <a href="https://doi.org/10.3390/rs12233890">https://doi.org/10.3390/rs12233890</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的主要贡献：</p>
<ul>
<li>提出一个包含<strong>视觉、空间以及语义信息</strong>的鲁棒<strong>回环检测</strong>方法，提高大视角变化及动态场景下的鲁棒性；</li>
<li>使用预训练的语义分割网络和AlexNet 特征提取网络，可不经再训练直接应用于其他场景。</li>
</ul>
<span id="more"></span>
<h1 id="2-Materials-and-Methods"><a href="#2-Materials-and-Methods" class="headerlink" title="2 Materials and Methods"></a>2 Materials and Methods</h1><p>算法架构如Fig. 1所示，包含以下关键模块：</p>
<ol>
<li>语义地标提取；</li>
<li>消除动态地标并选择区分度高的地标；</li>
<li>地标区域的CNN 特征计算，及特征维度压缩处理；</li>
<li>语义拓扑图及随机游走描述子的构建；</li>
<li>使用随机游走描述子计算几何相似度；</li>
<li>整体相似度计算以进行回环检测。</li>
</ol>
<p><img src="/2024/02/29/wang2020/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="2-1-Semantic-Topology-Graph"><a href="#2-1-Semantic-Topology-Graph" class="headerlink" title="2.1 Semantic Topology Graph"></a>2.1 Semantic Topology Graph</h2><p><strong>语义拓扑图</strong>的构建过程如下所示：</p>
<p><img src="/2024/02/29/wang2020/fig2.png" alt="fig2" title="figure 2"></p>
<h3 id="2-1-1-Landmark-Extraction"><a href="#2-1-1-Landmark-Extraction" class="headerlink" title="2.1.1 Landmark Extraction"></a>2.1.1 Landmark Extraction</h3><p>作者使用ADE20K 训练的DeepLabV3+ 网络进行<strong>语义分割</strong>来提取地标。</p>
<h3 id="2-1-2-Landmark-Selection"><a href="#2-1-2-Landmark-Selection" class="headerlink" title="2.1.2 Landmark Selection"></a>2.1.2 Landmark Selection</h3><p>对语义分割图片进行处理，来去除面积小于一定阈值（本文中设定为100）的区域，并滤掉动态物体，最终获取具有清晰边界的地标区域，过程如Fig. 3所示。</p>
<p><img src="/2024/02/29/wang2020/fig3.png" alt="fig3" title="figure 3"></p>
<p>为了克服<strong>动态场景</strong>的影响，作者利用语义信息消除<strong>行人动态地标</strong>，将行人与长期停放的车辆区域进行<strong>融合</strong>，以作为后续工作的汽车地标。</p>
<p>作者结合地标包含的<strong>像素个数</strong>以及<strong>语义信息</strong>来选择区分度高的地标；而动态物体地标是由场景内容和语义信息来决定的，也就是说，根据地标在每个实验场景中的移动状态，来移除数据集中的移动地标，从而阻止其参与后续的算法步骤。</p>
<h3 id="2-1-3-CNN-Features"><a href="#2-1-3-CNN-Features" class="headerlink" title="2.1.3 CNN Features"></a>2.1.3 CNN Features</h3><p>CNN 特征具有<strong>外观不变性 appearance invariance</strong>。</p>
<p>根据前人的研究，AlexNet 的<strong>第三卷积层</strong>输出的特征图在<strong>外观变化</strong>下具有优异的表现，作者发现全连接层输出的特征图有丰富的语义信息，对于视角变化具有很强的鲁棒性，但是在外观变化下较弱。因此，作者选取AlexNet Conv3 的输出作为地标区域的<strong>全局特征</strong>。</p>
<p>为了保持地标的<strong>原始尺寸信息</strong>，作者将地标轮廓的Hu moment 添加进CNN 特征中来描述地标。</p>
<h3 id="2-1-4-Graph-Representation"><a href="#2-1-4-Graph-Representation" class="headerlink" title="2.1.4 Graph Representation"></a>2.1.4 Graph Representation</h3><p>本文中，每个地标被描述为包含有<strong>类别与像素数量信息</strong>的节点，节点的位置位于地标区域的中心。描述子的构建过程如Fig. 5所示：</p>
<p><img src="/2024/02/29/wang2020/fig5.png" alt="fig5" title="figure 5"></p>
<h2 id="2-2-Loop-Closure-Detection"><a href="#2-2-Loop-Closure-Detection" class="headerlink" title="2.2 Loop Closure Detection"></a>2.2 Loop Closure Detection</h2><p>回环检测的流程如Fig. 6所示，同时检测外观相似度（CNN 与轮廓特征）与几何相似度（随机游走描述子），从而得到总体相似度。</p>
<p><img src="/2024/02/29/wang2020/fig6.png" alt="fig6" title="figure 6"></p>
<h3 id="2-2-1-Obtain-Candidate-Images"><a href="#2-2-1-Obtain-Candidate-Images" class="headerlink" title="2.2.1 Obtain Candidate Images"></a>2.2.1 Obtain Candidate Images</h3><p>通过控制query image 与历史图片的<strong>相同标签地标数量</strong>，来获取候选匹配图片；作者设定为1，即当query image 与历史图片有一个相同标签的地标时，就将该历史图片作为候选图片。</p>
<h3 id="2-2-2-Appearance-Similarity"><a href="#2-2-2-Appearance-Similarity" class="headerlink" title="2.2.2 Appearance Similarity"></a>2.2.2 Appearance Similarity</h3><p>作者使用基于cos 距离（式 1）的最近邻域搜索方法来对具有相同标签的地标进行匹配，</p>
<p><img src="/2024/02/29/wang2020/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$v_i^q$ 表示query 图片的第 i 个地标的特征向量，$v_j^c$ 表示候选图片的第 j 个地标的特征向量。</p>
<p>此外，利用地标的<strong>几何形状</strong>作为<strong>惩罚因子</strong>来消除误匹配，使用Hu moments 来描述地标不规则的轮廓特征，具有旋转、平移及尺寸的不变性。</p>
<h3 id="2-2-3-Geometric-Similarity"><a href="#2-2-3-Geometric-Similarity" class="headerlink" title="2.2.3 Geometric Similarity"></a>2.2.3 Geometric Similarity</h3><p>将随机游走描述子矩阵M 进行级联转化为向量 G，然后计算两个图描述子向量之间的相似度：</p>
<p><img src="/2024/02/29/wang2020/f6.png" alt="f6" title="formula 6"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Topology</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Topology</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Visual Semantic Localization based on HD Map for Autonomous Vehicles in Urban Scenarios</title>
    <url>/2024/03/04/wang2021a/</url>
    <content><![CDATA[<p>Wang, Huayou, Changliang Xue, Yanxing Zhou, Feng Wen, and Hongbo Zhang. “Visual Semantic Localization Based on HD Map for Autonomous Vehicles in Urban Scenarios.” In <em>2021 IEEE International Conference on Robotics and Automation (ICRA)</em>, 11255–61. Xi’an, China: IEEE, 2021. <a href="https://doi.org/10.1109/ICRA48506.2021.9561459">https://doi.org/10.1109/ICRA48506.2021.9561459</a>.</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>本文提出了一种考虑<strong>局部结构一致性、全局模式一致性以及时间一致性</strong>的DA 方法；同时，使用了一种滑动窗口因子图优化框架来融合里程计和数据关联信息。</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ul>
<li>提出一种利用轻量级HD地图（不需要知道地图特征的高精度绝对高度信息）和视觉语义特征的精确、鲁棒的定位算法；</li>
<li>提出了一种考虑<strong>局部结构一致性、全局模式一致性以及时间一致性</strong>的DA 方法；</li>
<li>提出了一种视觉语义测量和里程计测量紧耦合的因子图优化框架；</li>
<li>进行了仿真和真实场景的实验来验证DA的效果和定位的精度。</li>
</ul>
<span id="more"></span>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3 System Overview"></a>3 System Overview</h1><p>定位问题可表示为最大后验（MAP）估计问题：</p>
<p><img src="/2024/03/04/wang2021a/f1.png" alt="f1" title="formula 1"></p>
<p>上式的MAP可分为两步：DA以及基于DA的位姿估计问题：</p>
<p><img src="/2024/03/04/wang2021a/f2.png" alt="f2" title="formula 2"></p>
<p>因此，定位框架可分为四个组件：传感器和地图、探测、关联和优化，如Fig. 2所示：</p>
<p><img src="/2024/03/04/wang2021a/fig2.png" alt="fig2" title="figure 2"></p>
<ul>
<li>系统的传感器包括：<strong>单目相机，IMU，两个里程计以及一个GNSS接收机</strong>。相机用来探测语义特征，IMU 与里程计用来提供局部相对运动估计，GNSS 接收机可提供一个当前位姿的粗略估计，用于系统的初始化。</li>
<li>探测层实现对图片中道路标志，杆状物，交通灯和指示牌进行探测。</li>
<li>关联层将图片中提取到的语义特征与HD 地图进行关联，该步骤可细分为5步：<ul>
<li>1、在先验位姿附近产生候选项，将地图特征基于每个采样位置<strong>映射到图片中</strong>；</li>
<li>2、基于<strong>局部结构一致性</strong>进行粗数据关联，来寻找一个近似最优的采样位置；</li>
<li>3、利用考虑<strong>匹配数量、匹配相似度以及局部结构相似度</strong>的最优数据关联方法来实现最优的全局匹配；</li>
<li>4、进行连续帧间的<strong>特征跟踪</strong>；</li>
<li>5、使用<strong>temporal smoothing</strong> 来获取时间一致性的DA。</li>
</ul>
</li>
<li>在优化层，利用位姿图优化来融合DA 和里程计测量的位姿估计。</li>
</ul>
<h1 id="4-Methodology"><a href="#4-Methodology" class="headerlink" title="4 Methodology"></a>4 Methodology</h1><h2 id="4-1-Semantic-Features-and-Detection"><a href="#4-1-Semantic-Features-and-Detection" class="headerlink" title="4.1 Semantic Features and Detection"></a>4.1 Semantic Features and Detection</h2><p>考虑到容易获取、频繁出现、高效存储，以及天气、光照、视角和外观不变性，作者使用YOLOV3 来提取道路标志，杆状物，交通灯和指示牌作为语义提取特征：</p>
<ul>
<li>指示牌：包含类别、表示探测置信度的得分，以及一个bbox；</li>
<li>杆状物：包含类别，表示探测置信度的得分，以及两个顶点；</li>
<li>道路标志：采样点。</li>
</ul>
<h2 id="4-2-Semantic-Data-Association-with-HD-map"><a href="#4-2-Semantic-Data-Association-with-HD-map" class="headerlink" title="4.2 Semantic Data Association with HD map"></a>4.2 Semantic Data Association with HD map</h2><p>DA 是视觉定位系统中最具挑战性的问题之一。</p>
<p>根据前文所述，本文提出的DA 包含五个步骤，分别叙述如下：</p>
<h3 id="4-2-1-Step-1"><a href="#4-2-1-Step-1" class="headerlink" title="4.2.1 Step 1"></a>4.2.1 Step 1</h3><p>在里程计获取的先验相对位姿周围生成数个可能的位姿估计，根据每个位姿估计来将地图特征映射到图片平面：</p>
<p><img src="/2024/03/04/wang2021a/f3.png" alt="f3" title="formula 3"></p>
<p>其中 $P^m_i$ 表示第 i 个地图特征的位置。</p>
<h3 id="4-2-2-Step-2"><a href="#4-2-2-Step-2" class="headerlink" title="4.2.2 Step 2"></a>4.2.2 Step 2</h3><p>基于局部结构一致性进行粗数据关联，来寻找近似最优的可能位姿，以消除由较大先验位姿误差造成的误匹配。<strong>局部结构一致性</strong>指的是保持感知到的特征和相应的地图映射特征的横向位置分布保持一致。</p>
<p>首先，将感知到的和映射过来的特征根据横向位置以降序排列；然后计算每个感知特征 $s_t$ 和每个映射特征 $r_k$ 的相似度：</p>
<p><img src="/2024/03/04/wang2021a/f4.png" alt="f4" title="formula 4"></p>
<p>此处可参考文章(Bowman 等, 2017)，上式的前两项<strong>分类与得分相似度</strong>可通过感知结果的离线学习来获取；最后一项需考虑不同的语义种类，对于指示牌，该项包含位置与尺寸相似项：</p>
<p><img src="/2024/03/04/wang2021a/f5.png" alt="f5" title="formula 5"></p>
<p>杆状物的似然包含位置、朝向以及重叠的相似项。</p>
<p>如果一个感知特征的最大相似度得分大于一个阈值，且局部特征被保留了，那么这就认为是一对成功的数据关联；对于每个可能的位姿计算相应的损失函数C，包含匹配数量 $N_m$ 和匹配误差 $e_{ii’}$ ：</p>
<p><img src="/2024/03/04/wang2021a/f6.png" alt="f6" title="formula 6"></p>
<p>其中 $e_{ii’}$ 被定义为特征 $i, i’$ 之间的横向距离，如Fig. 3所示：</p>
<p><img src="/2024/03/04/wang2021a/fig3.png" alt="fig3" title="figure 3"></p>
<p>最终，C 值最大的位姿作为近似最优位姿。</p>
<h3 id="4-2-3-Step-3"><a href="#4-2-3-Step-3" class="headerlink" title="4.2.3 Step 3"></a>4.2.3 Step 3</h3><p>基于上步得到的近似最优位姿，考虑<strong>匹配数量、匹配相似性和局部结构相似性</strong>来实现全局匹配，这被建模为一个多阶图匹配问题：</p>
<p><img src="/2024/03/04/wang2021a/f7.png" alt="f7" title="formula 7"></p>
<p>其中，N 和 M 分别表示感知到的和映射的特征数量；$N_e$ 表示两组特征的边的数量；$x_{ii’}$ 表示感知特征i 是否与映射特征 $i’$ 匹配；$s_{ii’}$ 表示感知特征i 与映射特征 $i’$ 的相似度，由式（4）计算得到；$s_{ij, i’j’}$ 表示边 $e_{ij}, e_{i^{‘}j^{‘}}$ （如Fig. 3所示）之间的相似度：</p>
<p><img src="/2024/03/04/wang2021a/f8.png" alt="f8" title="formula 8"></p>
<h3 id="4-2-4-Step-4：Feature-tracking"><a href="#4-2-4-Step-4：Feature-tracking" class="headerlink" title="4.2.4 Step 4：Feature tracking"></a>4.2.4 Step 4：Feature tracking</h3><p>本步骤建立连续帧间的特征关联，因为感知到的特征是静态的且保持局部结构，作者将该步骤建模为类似等式（7）的多阶图匹配问题。</p>
<h3 id="4-2-5-Step-5：Temporal-smoothing"><a href="#4-2-5-Step-5：Temporal-smoothing" class="headerlink" title="4.2.5 Step 5：Temporal smoothing"></a>4.2.5 Step 5：Temporal smoothing</h3><p>本步骤构建<strong>连续帧中感知到的特征</strong>与<strong>地图特征</strong>之间的最优一致性匹配，当前帧的匹配正确性也许会与滑动窗口内的早先匹配结果不同，进一步地，如果当前帧发生误匹配，可以基于早先的匹配与跟踪进行矫正。</p>
<p>时间顺滑temporal smoothing 指的是：根据匹配权重 $D_{1:T}$ 和匹配置信度 $c_{t,i}$ 来获取与地图特征 $x^l$ 对应的感知特征 $s_i$ ：</p>
<p><img src="/2024/03/04/wang2021a/f9.png" alt="f9" title="formula 9"></p>
<p>其中，$I(s_i, D_t)$ 表示地图特征 $x^l$ 是否与感知特征 $s_i$ 相匹配；匹配置信度 $c_{t,i}$ 是根据特征和局部结构相似性来给定的：</p>
<p><img src="/2024/03/04/wang2021a/f10.png" alt="f10" title="formula 10"></p>
<p>若最优感知特征的累计置信度要远大于第二优，则该最优感知特征被视为地图特征 $x^l$ 的<strong>正确匹配</strong>，否则，地图特征 $x^l$ 被认为<strong>具有不确定匹配</strong>，且每个感知特征的匹配概率可以得到。本步骤可以区分确定匹配与不确定匹配，可以解决因奇异性造成的误匹配问题。</p>
<h2 id="4-3-Pose-Graph-Optimization"><a href="#4-3-Pose-Graph-Optimization" class="headerlink" title="4.3 Pose Graph Optimization"></a>4.3 Pose Graph Optimization</h2><p>等式（2）表示的位姿估计可以表示为<strong>先验概率</strong>与<strong>似然</strong>的乘积：</p>
<p><img src="/2024/03/04/wang2021a/f2.png" alt="f2" title="formula 2"></p>
<p><img src="/2024/03/04/wang2021a/f11.png" alt="f11" title="formula 11"></p>
<p>基于里程计测量 $z_{i,i+1}^o$ 和匹配特征对 $z_i^l$ ，作者构建了一个<strong>滑动窗口非线性最小二乘估计器</strong>来估计最近的 T 位姿，相较于常用的滤波器方法，优化方法可以解决非同步和延迟测量，并在相同计算资源下获得更高的准确度，优化目标表示为：</p>
<p><img src="/2024/03/04/wang2021a/f12.png" alt="f12" title="formula 12"></p>
<p>其中，每个<strong>误差项及其对应的信息矩阵（协方差矩阵的逆矩阵）</strong>被视为一个因子，每个状态变量被视为一个节点，因此，定位问题可被表示为Fig. 4所示的因子图：</p>
<p><img src="/2024/03/04/wang2021a/fig4.png" alt="fig4" title="figure 4"></p>
<p>误差项包含<strong>里程计误差</strong> $e^o$ ，<strong>语义测量误差</strong> $e^l$ ，以及<strong>地图误差</strong> $e_j^m$ 。</p>
<p><img src="/2024/03/04/wang2021a/f13.png" alt="f13" title="formula 13"></p>
<p><img src="/2024/03/04/wang2021a/f14.png" alt="f14" title="formula 14"></p>
<p>其中，$[.]_0$ 表示向量中的第一个元素，此处的测量误差只考虑横向误差，来消除高度误差的影响和对于地图特征准确绝对高度的需求，如Fig. 5所示。</p>
<p><img src="/2024/03/04/wang2021a/fig5.png" alt="fig5" title="figure 5"></p>
<p><img src="/2024/03/04/wang2021a/f15.png" alt="f15" title="formula 15"></p>
<p>其中，$m_j$ 是第j 个地图特征的位置。</p>
<p>非线性优化问题可通过<strong>迭代</strong>进行求解，本文使用<strong>滑动窗口法</strong>在保证解算精度的前提下提高了计算效率。窗口之外的旧状态直接被舍弃，本文不使用边际化方法，因为边际化会<strong>累积线性误差</strong>，使得系统矩阵稠密化并造成死锁。虽然边际化方法可以使用旧数据来约束位姿，但使用地图特征作为先验可以充分约束汽车位姿。</p>
<p><img src="/2024/03/04/wang2021a/t1.png" alt="t1" title="table 1"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Dynamic SLAM_A Visual SLAM in Outdoor Dynamic Scenes</title>
    <url>/2024/03/01/wen2023a/</url>
    <content><![CDATA[<p>Wen, Shuhuan, Xiongfei Li, Xin Liu, Jiaqi Li, Sheng Tao, Yidan Long, and Tony Qiu. “Dynamic SLAM: A Visual SLAM in Outdoor Dynamic Scenes.” <em>IEEE Transactions on Instrumentation and Measurement</em> 72 (2023): 1–11. <a href="https://doi.org/10.1109/TIM.2023.3317378">https://doi.org/10.1109/TIM.2023.3317378</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ol>
<li>提出一个新颖的视觉SLAM 系统，结合<strong>语义分割</strong>和<strong>多视角几何</strong>来有效识别<strong>同一种类不同物体</strong>的运动状态，从而减少动态特征点对系统的影响；</li>
<li>使用SegNet 网络获取语义信息，基于<strong>匀速运动模型</strong>判断特征点的当前运动状态、估计相机位姿、恢复特征点的空间位置；当计算特征点速度时，为了减少系统噪声的干扰，作者设计了<strong>均值滤波窗口 mean filter window</strong> 来滤除噪声；</li>
<li>基于<strong>长期观测地图点</strong>提出了一种新的<strong>动态空间点筛选方法</strong> screening method，来应对动态点错误识别或不完整识别的问题。</li>
</ol>
<span id="more"></span>
<h1 id="3-System-Description"><a href="#3-System-Description" class="headerlink" title="3 System Description"></a>3 System Description</h1><h2 id="3-1-Overview-of-the-Proposed-Approach"><a href="#3-1-Overview-of-the-Proposed-Approach" class="headerlink" title="3.1 Overview of the Proposed Approach"></a>3.1 Overview of the Proposed Approach</h2><p>本系统是基于ORB-SLAM2 系统搭建的，系统架构如Fig. 1所示，大致流程如下所示：</p>
<ol>
<li>首先，将双目相机中左侧图片输入SegNet 获取<strong>语义信息</strong>；</li>
<li>然后提取图片中的ORB 特征点，并<strong>“移除”</strong>属于动态类别区域的特征点，值得注意的是，此处只是<strong>暂时隐藏</strong>动态物体的特征点，而不是真正地移除它们；本步骤的目的是获取具有<strong>相对准确度的相机位姿</strong>；</li>
<li>最终，利用剩余的静态特征点进行匹配、估计相机位姿；</li>
<li>在此基础上，维持一个<strong>窗口</strong>来计算并记录特征点的速度，来减少噪声的影响。</li>
</ol>
<p><img src="/2024/03/01/wen2023a/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-3-Dynamic-Feature-Point-Selection"><a href="#3-3-Dynamic-Feature-Point-Selection" class="headerlink" title="3.3 Dynamic Feature Point Selection"></a>3.3 Dynamic Feature Point Selection</h2><p>简单地移除掉属于潜在动态物体上的特征点会导致部分场景定位效果下降甚至失败，如Fig. 3所示；因此，作者利用<strong>动态运动检测</strong>来判断属于潜在动态物体上的特征点是否处于运动状态。</p>
<p><img src="/2024/03/01/wen2023a/fig3.png" alt="fig3" title="figure 3"></p>
<h3 id="3-3-1-Scene-Flow-Calculation"><a href="#3-3-1-Scene-Flow-Calculation" class="headerlink" title="3.3.1 Scene Flow Calculation"></a>3.3.1 Scene Flow Calculation</h3><p>作者通过场景光流计算来获取特征点的空间运动信息，此处的特征点使用的是ORB 特征点，并使用图片金字塔方法来解决特征点提取过程的尺度不变。</p>
<p>过程如Fig. 4所示，在静态场景中，特征点在两帧之间的位移应该具有<strong>相同的整体趋势</strong>，观察Fig. 4可以发现动态特征点位移的<strong>整体方向</strong>和<strong>大小</strong>与静态点不同。当前帧的位姿是通过匀速运动模型和特征点匹配获取的；作者使用双目相机获取<strong>特征点的深度信息</strong>，并不计算整幅图的深度信息以减少运算量。</p>
<p><img src="/2024/03/01/wen2023a/fig4.png" alt="fig4" title="figure 4"></p>
<p>匹配点在相邻两帧计算的空间位置信息如下所示：</p>
<p><img src="/2024/03/01/wen2023a/f1.png" alt="f1" title="formula 1"></p>
<p><strong>场景光流</strong>的计算如下所示：</p>
<p><img src="/2024/03/01/wen2023a/f5.png" alt="f5" title="formula 5"></p>
<h3 id="3-3-2-Variable-Length-Window-Filter"><a href="#3-3-2-Variable-Length-Window-Filter" class="headerlink" title="3.3.2 Variable-Length Window Filter"></a>3.3.2 Variable-Length Window Filter</h3><p>为减少噪声对确定静态点与动态点阈值的影响，作者使用一个窗口进行<strong>均值滤波</strong>。该滤波器记录相同特征点在连续帧中的速度，从该特征点出现的第一帧开始记录，维持一个最大长度的速度纪录，然后计算序列中所有值的均值作为当前帧中该特征点的速度。实际操作中，<strong>位移信息</strong>被转化为特征点的速度参数来进行阈值计算。</p>
<p><img src="/2024/03/01/wen2023a/f6.png" alt="f6" title="formula 6"></p>
<p>利用计算的速度信息，地图点可<strong>根据阈值</strong>分为动态点和静态点。</p>
<h2 id="3-4-Feature-Point-Threshold-Calculation"><a href="#3-4-Feature-Point-Threshold-Calculation" class="headerlink" title="3.4 Feature Point Threshold Calculation"></a>3.4 Feature Point Threshold Calculation</h2><p>首先，选取属于<strong>静态物体类别</strong>的特征点进行阈值计算，来判断属于潜在动态物体上的特征点是否是运动的。</p>
<p><img src="/2024/03/01/wen2023a/f7.png" alt="f7" title="formula 7"></p>
<p>在ORB-SLAM2 中，特征点分为近点和远点，它们的估计精度<strong>存在差异</strong>，且近远点在图中的位移或速度也会不同，因此，作者引入一个参数 $\alpha$（<strong>深度的倒数</strong>）来减少噪声对阈值计算的影响：</p>
<p><img src="/2024/03/01/wen2023a/f8.png" alt="f8" title="formula 8"></p>
<p>值得注意的是，作者认为，为了提高精度，当特征点数量充足时，应<strong>优先选择静态物体类别区域的特征点</strong>进行计算。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 VOOM_Robust Visual Object Odometry and Mapping using Hierarchical Landmarks</title>
    <url>/2024/03/04/wang2024/</url>
    <content><![CDATA[<p>Wang, Yutong, Chaoyang Jiang, and Xieyuanli Chen. “VOOM: Robust Visual Object Odometry and Mapping Using Hierarchical Landmarks.” arXiv, February 26, 2024. <a href="https://doi.org/10.48550/arXiv.2402.13609">https://doi.org/10.48550/arXiv.2402.13609</a>.</p>
<h1 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h1><p>作者提到，物体SLAM 可以在保证计算效率的同时提供高等级的语义信息，部分研究者将建模的物体观测残差添加进BA 中进行位姿优化，但由于物体模型的精度问题，导致优化效果不如基于特征点的方法。基于此，作者提出了一种<strong>视觉物体里程计和制图</strong>架构——VOOM (Visual Object Odometry and Mapping)，该架构不直接将观测物体残差添加进BA 中，而是使用一种<strong>由粗到细</strong>的方式将高等级物体和低等级特征点作为<strong>层级地标观测</strong>。</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>作者认为，本方法是第一个利用<strong>对偶二次曲面和特征点</strong>实现比基于特征点的SOTA SLAM 算法更好定位精度的物体SLAM 算法。</p>
<p>本文的贡献如下：</p>
<ul>
<li>提出了一种新颖的<strong>视觉里程计和制图架构</strong>，该架构同时使用<strong>特征点和对偶二次曲面</strong>作为地标；</li>
<li>提出了一种有效的算法，使用<strong>层级地标</strong>进行物体优化、物体关联和基于物体的地图点关联，以进行地图构建；</li>
<li>大量实验证明了本算法较SOTA 算法的优越性。</li>
</ul>
<span id="more"></span>
<p><img src="/2024/03/04/wang2024/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-VOOM"><a href="#3-VOOM" class="headerlink" title="3 VOOM"></a>3 VOOM</h1><h2 id="3-1-System-Overview"><a href="#3-1-System-Overview" class="headerlink" title="3.1 System Overview"></a>3.1 System Overview</h2><p>VOOM 系统以RGB-D 图片为输入，输出包含地图点和3D 物体的3D 地图。整体架构如Fig. 2所示，包含两个主要部分：<strong>视觉物体里程计</strong>和<strong>视觉物体建图</strong>。</p>
<p><img src="/2024/03/04/wang2024/fig2.png" alt="fig2" title="figure 2"></p>
<p>系统的实例分割采用YOLOv8 网络进行处理，特征点部分是基于ORB-SLAM2 算法构建的。在物体级前端，基于实例分割获取的特征点，使用<strong>direct least squares 直接最小二乘法</strong>来估计椭圆形；物体级数据关联是基于3D 物体重投影与估计椭圆之间的<strong>Wasserstein distance</strong> 进行的。</p>
<h2 id="3-2-Observation-Model-of-Objects"><a href="#3-2-Observation-Model-of-Objects" class="headerlink" title="3.2 Observation Model of Objects"></a>3.2 Observation Model of Objects</h2><p>VOOM 对每张输入图片进行实例分割，QISO-SLAM (Wang 等, 2023)中提到，使用实例分割的轮廓线会比使用bbox 实现更高精度的物体建图与定位；基于此，作者对于第 f 张图片上的第 k 个物体，通过最小化<strong>所有分割轮廓点</strong>和<strong>椭圆边界</strong>的最小距离和来估计物体的椭圆观测 $E_{fk}^{obs}$ 。</p>
<p>作者使用受限对偶二次曲面 $Q_k^\ast \in \mathbb{R}^{4 \times 4}$  来表示物体，投影至图像平面后表示为对偶二次曲线：</p>
<p><img src="/2024/03/04/wang2024/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$\mathbf{P}_f = \mathbf{K}_f[\mathbf{R}_f | \mathbf{t}_f^T] \in \mathbb{R}^{3 \times 4}$ 为相机投影矩阵；$\mathbf{C}_{fk}^\ast$ 表示<strong>根据位姿估计</strong>得到的椭圆  $E_{fk}^{est}$ 。可根据 $E_{fk}^{obs}$ 和 $E_{fk}^{est}$ 构建相应的误差方程。</p>
<p>受到OA-SLAM (Zins 等, 2022)的启发，作者使用2D 高斯分布 $\mathcal{N}(\mu, \sum^{-1})$ 来表示椭圆：</p>
<p><img src="/2024/03/04/wang2024/f2.png" alt="f2" title="formula 2"></p>
<p>其中，$[p_x, p_y]^T, [\alpha, \beta]^T,  \theta$  分别表示椭圆的<strong>中心点</strong>、<strong>长短轴长度</strong>以及<strong>旋转角度</strong>。基于高斯分布，构建 $E_{fk}^{obs}$ 和 $E_{fk}^{est}$ 之间的二阶 Wasserstein distance 误差：</p>
<p><img src="/2024/03/04/wang2024/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$||.||_F$ 为F 范数。假设第k 个物体的观测帧集合为 $\mathcal{F}_L$ ，则该物体的<strong>椭球体参数</strong>可通过最小化下式来获取：</p>
<p><img src="/2024/03/04/wang2024/f4.png" alt="f4" title="formula 4"></p>
<h2 id="3-3-Object-level-Data-Association"><a href="#3-3-Object-level-Data-Association" class="headerlink" title="3.3 Object-level Data Association"></a>3.3 Object-level Data Association</h2><p>使用bbox 的IoU 进行物体级数据关联存在诸多问题，因此，作者使用<strong>归一化的 Wasserstein distance</strong> 进行数据关联，基于式3，可得到归一化的结果：</p>
<p><img src="/2024/03/04/wang2024/f5.png" alt="f5" title="formula 5"></p>
<p>其中，C 是一个常数。该度量方式包含了椭圆的<strong>中心点、轴长度以及方向信息</strong>，要比bbox IoU 方法更为准确。</p>
<h2 id="3-4-Coarse-to-fine-Odometry-and-Mapping"><a href="#3-4-Coarse-to-fine-Odometry-and-Mapping" class="headerlink" title="3.4 Coarse-to-fine Odometry and Mapping"></a>3.4 Coarse-to-fine Odometry and Mapping</h2><p>主要出发点在于使用3D 物体地标的鲁棒性来提高特征点法的效率与精度。</p>
<h3 id="3-4-1-Coarse-to-fine-Visual-Object-Odometry-Backend"><a href="#3-4-1-Coarse-to-fine-Visual-Object-Odometry-Backend" class="headerlink" title="3.4.1 Coarse-to-fine Visual Object Odometry Backend"></a>3.4.1 Coarse-to-fine Visual Object Odometry Backend</h3><p>利用<strong>特征级前端</strong>的运动模型得到<strong>粗略</strong>的<strong>位姿估计</strong>，结合<strong>物体级前端</strong>的<strong>关联物体</strong>，算法在属于<strong>每个物体的地图点</strong>和<strong>关联物体实例中的特征点</strong>之间中搜寻匹配关系（基于ORB 特征描述子），如Fig. 3所示。</p>
<p><img src="/2024/03/04/wang2024/fig3.png" alt="fig3" title="figure 3"></p>
<p>该方法通过<strong>减小搜索区域</strong>来实现更高效、更精准的地图点数据关联，并从<strong>更早的关联帧</strong>中恢复更多的地图点，而不是只能从最近的帧中进行关联。根据地图点和关键点之间的匹配，可基于重投影误差来<strong>优化相机位姿</strong>；然后，更新当前帧的local map，使用局部地图中更多的匹配关系对相机位姿进行进一步的优化。</p>
<h3 id="3-4-2-Coarse-to-fine-Visual-Object-Mapping"><a href="#3-4-2-Coarse-to-fine-Visual-Object-Mapping" class="headerlink" title="3.4.2 Coarse-to-fine Visual Object Mapping"></a>3.4.2 Coarse-to-fine Visual Object Mapping</h3><p>在为当前帧获取了可靠的位姿估计和地图点关联之后，基于<strong>物体共视图 object covisibility graph</strong> 来更新局部地图，物体共视图记录了不同帧中不同物体间的公式关系，如Fig. 4（a）所示，这可以帮助选择一组<strong>与当前帧最相关的关键帧</strong>，与传统的共视图（Fig. 4 b）相比，物体共视图<strong>体积更小</strong>、<strong>鲁棒性更强</strong>。在物体共视图的帮助下，可以为当前帧建立与更早观测帧之间的关联，以此实现对位姿更高精度的优化效果。作者这里提到，由于物体的鲁棒性，仅使用local map 就可以实现<strong>类似回环检测的功能</strong>。</p>
<p><img src="/2024/03/04/wang2024/fig4.png" alt="fig4" title="figure 4"></p>
<blockquote>
<p><strong>值得注意的是，</strong>由于物体建模的精确度问题，<strong>观测物体的残差不参与BA 优化</strong>。也就是说，物体在这里的作用仅在于增强帧之间特征点匹配的作用。</p>
</blockquote>
<h1 id="4-Experimental-Results"><a href="#4-Experimental-Results" class="headerlink" title="4 Experimental Results"></a>4 Experimental Results</h1><p>定位实验结果如表1所示：</p>
<p><img src="/2024/03/04/wang2024/t1.png" alt="t1" title="table 1"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Pose Optimization</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Pose Optimization</tag>
        <tag>Object SLAM</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 LTSR_Long-term Semantic Relocalization based on HD Map for Autonomous Vehicles</title>
    <url>/2024/03/05/wang2022/</url>
    <content><![CDATA[<p>Wang, Huayou, Changliang Xue, Yu Tang, Wanlong Li, Feng Wen, and Hongbo Zhang. “LTSR: Long-Term Semantic Relocalization Based on HD Map for Autonomous Vehicles.” In <em>2022 International Conference on Robotics and Automation (ICRA)</em>, 2171–78. Philadelphia, PA, USA: IEEE, 2022. <a href="https://doi.org/10.1109/ICRA46639.2022.9811855">https://doi.org/10.1109/ICRA46639.2022.9811855</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文做出以下贡献：</p>
<ul>
<li>提出一种基于语义特征和HD 地图的准确、鲁棒的<strong>长期重定位算法</strong>，该算法不依赖于GNSS；</li>
<li>提出一种基于<strong>局部语义描述子</strong>（编码了语义特征间的空间和法向normal 关系）的鲁棒语义特征匹配方法；</li>
<li>通过评估局部、全局几何一致性和时间一致性的准确高效简单的<strong>外点剔除方法</strong>；</li>
<li>大量的实验证明了方法的有效性。</li>
</ul>
<span id="more"></span>
<h1 id="3-System-Review"><a href="#3-System-Review" class="headerlink" title="3 System Review"></a>3 System Review</h1><p>参考作者去年的文章(Wang 等, 2021)，系统框架如Fig. 2所示：</p>
<p><img src="/2024/03/05/wang2022/fig2.png" alt="fig2" title="figure 2"></p>
<p>相机和Lidar 用来探测3D 语义特征。数据关联过程分为三个步骤：</p>
<ol>
<li>为每个感知到的语义特征生成描述子，该描述子编码了语义特征之间的空间和法向normal 关系；</li>
<li>基于特征描述子相似度的数据关联；</li>
<li>通过评估局部、全局几何一致性和时间一致性的外点剔除方法。</li>
</ol>
<h1 id="4-Methodology"><a href="#4-Methodology" class="headerlink" title="4 Methodology"></a>4 Methodology</h1><h2 id="4-1-Semantic-Features-and-Detection"><a href="#4-1-Semantic-Features-and-Detection" class="headerlink" title="4.1 Semantic Features and Detection"></a>4.1 Semantic Features and Detection</h2><p>本文使用的语义特征为：道路线、道路标志、杆状物、交通灯和指示牌。</p>
<p>与作者前作相同，使用YOLOV3 来进行语义探测，所不同的是，本文作者添加了Lidar 传感器，来为感知到的特征产生<strong>3D 位置信息和法向量</strong>。探测到的语义特征 $\mathcal{Z}_i$ 包含<strong>种类信息</strong>和<strong>用中心点与法向量表示的几何信息</strong>，如Fig. 3所示，其中，杆状物的中心点是其与地面的接触点，其法向量为杆状物所在道路线的方向。道路线表示为三次曲线线段。</p>
<p><img src="/2024/03/05/wang2022/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="4-2-Semantic-Feature-Description-and-Association"><a href="#4-2-Semantic-Feature-Description-and-Association" class="headerlink" title="4.2 Semantic Feature Description and Association"></a>4.2 Semantic Feature Description and Association</h2><p>编码了语义特征之间的空间和法向normal 关系的<strong>描述子</strong>。为了减少计算量，所有的点和法向量都被投影至XY 平面，对于每个语义特征，与它在特定距离内的邻接特征被用来构建描述子，该局部描述子编码了<strong>相对位置、相对角度以及相对法向量</strong>参数。</p>
<p>当前扫描中感知到的某个语义特征 $\mathcal{Z}_i$ 和<strong>HD</strong> <strong>地图中的每一个地标</strong> $\mathcal{L}j$ 对应的邻接特征集合分别表示为：</p>
<p><img src="/2024/03/05/wang2022/f3.png" alt="f3" title="formula 3"></p>
<p>根据邻接特征建立 $\mathcal{Z}_i$ 和 $\mathcal{L}j$ 的特征描述子（边组合）：</p>
<p><img src="/2024/03/05/wang2022/f4.png" alt="f4" title="formula 4"></p>
<p>其中，$\mathcal{E}_i^l$ 编码了相对位置 $d_i^l$ ，相对逆时针角度 $\theta_i^l$ 以及相对逆时针法向量 $n_i^l$ 。</p>
<p>如果 $\mathcal{Z}_i$ 和 $\mathcal{L}j$ 具有相同的语义标签，且它们匹配上的边个数超过一定比例，那么就可以认为两者为一个匹配对。</p>
<p><img src="/2024/03/05/wang2022/f5.png" alt="f5" title="formula 5"></p>
<p>这样，语义特征的匹配问题就转化为了语义边组合中的边匹配问题，边匹配需要满足以下4个条件：</p>
<ol>
<li>两条边的语义标签一致；</li>
<li>两条边的距离误差小于一个阈值；</li>
<li>两条边的角度误差小于一个阈值；</li>
<li>两条边的法向量误差小于一个阈值。</li>
</ol>
<p><img src="/2024/03/05/wang2022/f6.png" alt="f6" title="formula 6"></p>
<h2 id="4-3-Outlier-Removal"><a href="#4-3-Outlier-Removal" class="headerlink" title="4.3 Outlier Removal"></a>4.3 Outlier Removal</h2><p>外点剔除的目标为：寻找一个几何一致性最大的组，即该组拥有最大数量的几何一致特征对，问题表示为：</p>
<p><img src="/2024/03/05/wang2022/f7.png" alt="f7" title="formula 7"></p>
<p>其中，$\mathcal{I}$ 表示内点组，$\epsilon$  表示一个内点阈值，$(\mathcal{Z}_i, \mathcal{L}_{Z_i})$ 表示匹配点对，局部几何一致性可表示为：</p>
<p><img src="/2024/03/05/wang2022/f8.png" alt="f8" title="formula 8"></p>
<p>其中，$d_l, d_g$ 分别表示局部地图中两点之间的距离和全局HD 地图中对应特征点之间的距离，$n_l, n_j$ 表示法向量角度。</p>
<p>外点剔除算法如下所示：</p>
<p><img src="/2024/03/05/wang2022/a1.png" alt="a1" title="algorithm 1"></p>
<h2 id="4-4-Semantic-Tracking"><a href="#4-4-Semantic-Tracking" class="headerlink" title="4.4 Semantic Tracking"></a>4.4 Semantic Tracking</h2><p>作者采用前作(Wang 等, 2021)中的方法来构建连续帧间的特征关联，在此基础上，为了应对误匹配，作者使用CLEAR (Consistent Lifting, Embedding and Alignment Rectification) 算法来建立多视角下正确的语义特征关联。</p>
<h2 id="4-5-Matching-Consistency-Check"><a href="#4-5-Matching-Consistency-Check" class="headerlink" title="4.5 Matching Consistency Check"></a>4.5 Matching Consistency Check</h2><p>参考作者前作(Wang 等, 2021)。</p>
<h2 id="4-6-Robust-Long-term-Relocalization"><a href="#4-6-Robust-Long-term-Relocalization" class="headerlink" title="4.6 Robust Long-term Relocalization"></a>4.6 Robust Long-term Relocalization</h2><p>在匹配对的基础上，作者使用GNC（Graduated Non-Convexity）算法来解算重定位位姿，误差模型定位为中心位置和法向量的欧氏距离。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 X-View_Graph-Based Semantic Multi-View Localization</title>
    <url>/2024/01/30/x-view/</url>
    <content><![CDATA[<p>Gawel, Abel, Carlo Del Don, Roland Siegwart, Juan Nieto, and Cesar Cadena. “X-View: Graph-Based Semantic Multi-View Localization.” <em>IEEE Robotics and Automation Letters (RA-L)</em> 3, no. 3 (2018): 1687–94.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文做出的贡献：</p>
<ol>
<li>提出一个新颖的<strong>语义拓扑图表示</strong>方法；</li>
<li>引进了一个<strong>基于随机游走的图描述子</strong>，可以有效地使用既定的匹配方法进行高效匹配；</li>
<li>用于全局定位的语义分割完整pipeline；</li>
<li>开源<em>X-View</em> 算法；</li>
<li>公开数据集的测试结果。</li>
</ol>
<span id="more"></span>
<p><img src="/2024/01/30/x-view/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-X-View"><a href="#3-X-View" class="headerlink" title="3 X-View"></a>3 X-View</h1><p>X-View 利用从输入语义数据提取得到的图，并使用图描述子进行图匹配，本文提出的全局定位算法架构如Fig. 2所示，本系统被设计为可以用于任何给定的语义信息作为输入的里程估计系统，为了简化表示，Fig. 2中只列举了语义分割图片作为输入。</p>
<p><img src="/2024/01/30/x-view/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-System-input"><a href="#3-1-System-input" class="headerlink" title="3.1 System input"></a>3.1 System input</h2><p>将语义分割或实例分割图片作为系统输入，且假定外部里程系统的估计，以及一个参考语义图 $G_{db}$ 。</p>
<h2 id="3-2-Graph-extraction-and-assembly"><a href="#3-2-Graph-extraction-and-assembly" class="headerlink" title="3.2 Graph extraction and assembly"></a>3.2 Graph extraction and assembly</h2><p>将一系列语义分割图片 $I_q$ 转化为一个质询图 $G_q$ ，从连接区域中提取斑点 blobs，例如每幅图中具有相同语义标签 $l_j$ 的区域。为了应对语义分割的噪声（如标签中的洞、断连的边、边缘错误的标签等），作者使用 dilating 和 eroding 对每个班点的边界进行柔化，且设定最小像素数阈值为4以排除过小及次要物体的影响。斑点的中心位置 $p_j$ 被提取出来，与标签一起存储为顶点 $v_j = \{l_j, p_j\}$ 。</p>
<p>顶点之间的无向边可以是图像空间或3D 空间的，当考虑图像空间中的边时，认为图片在时间序列中根据几幅连续的输入图片流来生成图（利用连续图片获取3D距离信息），在3D 空间中不需要考虑这方面。</p>
<p>利用深度通道数据或者深度估计来组成3D空间结构，使用图片斑点的3D位置来计算欧氏距离，图提取与组合的过程如Fig. 3所示，当对多幅连续图片的图进行组合时，将距离较近且拥有相同语义标签的重复顶点融合为一个顶点，该顶点位置选择为初次观测到的位置。</p>
<p><img src="/2024/01/30/x-view/fig3.png" alt="fig3" title="figure 3"></p>
<h2 id="3-3-Descriptors"><a href="#3-3-Descriptors" class="headerlink" title="3.3 Descriptors"></a>3.3 Descriptors</h2><p>子图匹配是一个 NP-complete 难题，且为了实现机器人的实时定位问题，作者提出了为图中每个节点建立随机游走描述子并进行匹配，这样做的优势是根据给定的静态或者增长的参考图，匹配时间分别是常数或者线性变化的。每个顶点的描述子是一个 $n \times m$ 的矩阵，包含 <em>n</em> 个深度为 <em>m</em> 的随机游走，每个随机游走开始于源顶点 $v_j$ ，并存储访问过顶点的分类标签，随机游走描述子提取过程如Fig. 4所示。</p>
<p><img src="/2024/01/30/x-view/fig4.png" alt="fig4" title="figure 4"></p>
<h2 id="3-4-Descriptor-Matching"><a href="#3-4-Descriptor-Matching" class="headerlink" title="3.4 Descriptor Matching"></a>3.4 Descriptor Matching</h2><p>在质询图 $G_q$ 与 参考图 $G_{db}$ 完成之后，通过计算相应的图描述子之间的similarity score来建立两图顶点之间的联系：</p>
<ul>
<li>相似性是通过对质询图与参考图中顶点的语义描述子的每一行进行匹配计算获取的；</li>
<li>两个描述子拥有的相同随机游走数量决定了similarity score（分布在0与1之间）；</li>
<li>然后选取得分最高的前 k 个匹配顶点对来计算质询图在参考图中的定位信息。</li>
</ul>
<h2 id="3-5-Localization-Back-End"><a href="#3-5-Localization-Back-End" class="headerlink" title="3.5 Localization Back-End"></a>3.5 Localization Back-End</h2><p>一共有三种类型的约束：</p>
<ol>
<li>来自语义描述子匹配的约束；</li>
<li>联系定位图中连续位姿的机器人估计位姿约束；</li>
<li>每一个robot-vertex观测的转换信息编码得到的robot-vertex约束。</li>
</ol>
<p>作者根据以上三个约束参数计算最大后验估计 (MAP)来获取定位信息。</p>
<h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h1><p>在两个不同的合成户外数据集进行评估，包括<strong>前视——后视</strong>视角变化、<strong>前视——空视</strong>视角变化，以及一个真实世界户外数据集，包含<strong>前视——后视</strong>视角变化。</p>
<h2 id="4-1-Datasets"><a href="#4-1-Datasets" class="headerlink" title="4.1 Datasets"></a>4.1 Datasets</h2><p>SYNTHIA 数据集是在城市环境模拟生成的，采集汽车有8个摄像头，四个方向各有两个，相邻图片间的距离在0~1m。作者使用photo-realistic Arisim 生成了空对地观测的数据集，包含空中向地面观测与地面前向观测两种数据，两种数据之间只有在 z 轴存在偏差，其余均相同，相邻图片间的距离总是1m。真实户外场景的数据集是通过Google StreetView 获取的，类似于SYNTHIA 数据集，只使用前视与后视摄像头，相邻图片间的距离接近10m。</p>
<p><img src="/2024/01/30/x-view/fig5.png" alt="fig5" title="figure 5"></p>
<h2 id="4-3-Localization-performance"><a href="#4-3-Localization-performance" class="headerlink" title="4.3 Localization performance"></a>4.3 Localization performance</h2><p>设置两组实验：</p>
<ol>
<li>在SYNTHIA 数据集上测试不同的参数设置，如随机游走参数、质询图片数量、动态分类物体、图边缘构建技术等；</li>
<li>在SYNTHIA、Airsim 以及 StreetView 上进行对比分析实验。</li>
</ol>
<p>与基于外观匹配的算法进行对比，使用了BoW、NetVLAD 算法。</p>
<h2 id="4-4-Results"><a href="#4-4-Results" class="headerlink" title="4.4 Results"></a>4.4 Results</h2><p>Fig. 6是对SYNTHIA 数据集进行的参数对比试验结果。6(a) 中 n 代表随机游走的个数， m 代表随机游走的深度。</p>
<p><img src="/2024/01/30/x-view/fig6.png" alt="fig6" title="figure 6"></p>
<p>PR评估曲线以及不同定位误差下的成功率结果如Fig. 7所示。</p>
<p><img src="/2024/01/30/x-view/fig7.png" alt="fig7" title="figure 7"></p>
<p>系统组件的时间消耗如Table 1所示。</p>
<p><img src="/2024/01/30/x-view/t1.png" alt="t1" title="table 1"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Topology</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Topology</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows远程桌面控制ubuntu</title>
    <url>/2024/01/26/win-remote-ubuntu/</url>
    <content><![CDATA[<h2 id="1-Ubuntu设置与软件安装"><a href="#1-Ubuntu设置与软件安装" class="headerlink" title="1 Ubuntu设置与软件安装"></a>1 Ubuntu设置与软件安装</h2><p>首先打开Ubuntu设置，将Sharing-Remote Desktop打开，如下图所示：</p>
<span id="more"></span>
<p><img src="/2024/01/26/win-remote-ubuntu/ubuntu-setting.png" alt="ubuntu-setting" title="Ubuntu远程桌面设置"></p>
<p>然后安装xrdp：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install xrdp</span><br></pre></td></tr></table></figure>
<p>接下来的步骤网上有很多不同的做法，其中<a href="https://zhuanlan.zhihu.com/p/145614559">这篇文章</a>提到需要修改startwm.sh文件:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo vi /etc/xrdp/startwm.sh</span><br></pre></td></tr></table></figure>
<p><img src="/2024/01/26/win-remote-ubuntu/startwm.png" alt="startwm.sh" title="修改startwm.sh文件"></p>
<p>将最后两行注释掉，即：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 注释掉这两行</span></span><br><span class="line"><span class="comment"># test -x /etc/X11/Xsession &amp;&amp; exec /etc/X11/Xsession</span></span><br><span class="line"><span class="comment"># exec /bin/sh /etc/X11/Xsession</span></span><br></pre></td></tr></table></figure>
<p>然后即可通过win10自带的远程桌面（或在Microsoft Store安装的远程桌面）进行连接：</p>
<p><img src="/2024/01/26/win-remote-ubuntu/remote-access.png" alt="remote" title="win10远程桌面"></p>
<p><img src="/2024/01/26/win-remote-ubuntu/image-20240129103319493.png" alt="远程桌面" title="远程桌面"></p>
<p>到目前为止，本人可以在Windows中正常远程控制Ubuntu，且可实现Ubuntu显示和远程控制<strong>同时在线</strong>，即通过远程控制操作Ubuntu，同时另一个屏幕可正常显示Ubuntu界面，且实时显示远程控制的操作。</p>
<h2 id="2-黑屏问题"><a href="#2-黑屏问题" class="headerlink" title="2 黑屏问题"></a>2 黑屏问题</h2><p>但是，在这之后我重新安装了NVIDIA显卡驱动、CUDA、cuDNN，然后就发现远程控制无法进入Ubuntu系统了，点击“连接”后出现一段时间的黑屏后会自动退出。在网上查询了很多案例，有各种五花八门的解决方案，如换一个桌面程序、使用dconf-editor更改配置文件、远程控制与Ubuntu本地无法同时登陆（必须有一方log out）等等，以下是个人尝试的结果：</p>
<ul>
<li>换桌面程序：由于本人安装的是桌面版Ubuntu，故系统自带桌面程序gnome，之前配置远程控制时就受到网上各种五花八门意见的影响，使用了xfce4桌面程序，后来发现完全没必要，而且就在不久前还可以正常远程连接，说明应该不是桌面程序的原因，因此就没有尝试更换桌面程序；</li>
<li>使用dconf-editor更改配置文件：在<a href="https://zhuanlan.zhihu.com/p/345738274">文章</a>中提到，进入dconf-editor后，依次进入“org-&gt;gnome-&gt;desktop-&gt;remote-access”，将 requre-encryption 设为 False，这也是本人在上个系统（Ubuntu20.04）中的操作，但是在本系统（Ubuntu22.04）中没有发现有“remote-access”，故该方法也没有成功；</li>
<li>本地端与远程端无法共存：这个比较简单，本人试着将Ubuntu本地端账户log out，或者使用不同账户进行远程控制，发现仍然失败；但<strong>需要注意的是</strong>，在本人解决远程黑屏问题之后，发现该问题确实是存在的，若本地端或远程端没有log out，那么另一端就无法正常登陆显示界面，这边记录一下；</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 修改startwm.sh文件，添加下面几行，仍旧没有解决问题</span></span><br><span class="line"><span class="built_in">unset</span> DBUS_SESSION_BUS_ADDRESS</span><br><span class="line"><span class="built_in">unset</span> XDG_RUNTIME_DIR</span><br><span class="line">. <span class="variable">$HOME</span>/.profile</span><br></pre></td></tr></table></figure>
<p>最后，不知道怎么想起来，尝试把startwm.sh文件中注释掉的两行取消注释，峰回路转、柳暗花明，竟然成了！！！于是，本人最终的startwm.sh文件内容如下所示：</p>
<p><img src="/2024/01/26/win-remote-ubuntu/startwm-1.png" alt="startwm" title="startwm.sh文件最终版"></p>
<h2 id="3-后记"><a href="#3-后记" class="headerlink" title="3 后记"></a>3 后记</h2><p>虽然不知道原因是什么，只知道目前这样子是可行的，但同时也发现无法像安装显卡驱动前那样本地端和远程端同时登陆了，必须要有一端log out，另一端才可正常工作，现做以下记录：</p>
<ul>
<li>使用远程连接桌面后，不能简单地关掉远程桌面：若只是简单地关掉远程桌面后，在本地端可以正常进入账号选择、密码输入界面，输完密码后会显示黑屏；此时，只有重新进入远程桌面，选择log out，本地端方可正常登录。</li>
<li>对于本地端Todesk等远程控制软件：经过本人测试，Todesk 软件若想正常工作需要本地端正常登录，即要求远程端log out，然后本地端log in；那么也就意味着本地端电脑需要连接显示器。</li>
</ul>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>remote access</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 A semantic SLAM-based dense mapping approach for large-scale dynamic outdoor environment</title>
    <url>/2024/03/01/yang2022/</url>
    <content><![CDATA[<p>Yang, Linjie, and Luping Wang. “A Semantic SLAM-Based Dense Mapping Approach for Large-Scale Dynamic Outdoor Environment.” <em>Measurement</em> 204 (November 30, 2022): 112001. <a href="https://doi.org/10.1016/j.measurement.2022.112001">https://doi.org/10.1016/j.measurement.2022.112001</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文提出的方法是针对<strong>室外动态环境中的语义SLAM</strong>，对于动态特征点的筛选，不是简单地利用语义信息对先验动态物体直接剔除，而是采用一个精确、鲁棒的<strong>特征点滤波机制</strong>，实现对特征点动态性的精确识别。</p>
<p>本文的贡献如下：</p>
<ul>
<li>提出了一个应用于大范围<strong>室外动态环境</strong>的<strong>语义SLAM 系统</strong>，该系统是基于ORB-SLAM2 和 深度学习模型的；</li>
<li>设计了一个<strong>特征点滤除机制</strong>，该机制结合了<strong>语义特征</strong>、<strong>depth local contrast</strong> 以及<strong>多视角投影</strong>来进一步提高鲁棒性与准确性；</li>
<li>通过采用<strong>后验概率</strong>和<strong>增量更新</strong>，为高等级机器人导航创建了室外环境的<strong>稠密地图</strong>。</li>
</ul>
<span id="more"></span>
<h1 id="3-Method"><a href="#3-Method" class="headerlink" title="3 Method"></a>3 Method</h1><h2 id="3-1-The-framework-of-our-method"><a href="#3-1-The-framework-of-our-method" class="headerlink" title="3.1 The framework of our method"></a>3.1 The framework of our method</h2><p>本文方法框架如Fig. 1所示，本系统使用<strong>立体RGB 图像</strong>作为输入，基于ORB-SLAM2 算法，结合不同的深度学习模型分别用于<strong>语义特征</strong>和<strong>深度信息</strong>的获取；根据<strong>多模态特征约束</strong>实现对特征点的滤波，获取<strong>稳定的特征点</strong>进行位姿解算与建图。</p>
<p><img src="/2024/03/01/yang2022/fig1.png" alt="fig1" title="figure 1"></p>
<p><strong>深度学习模型：</strong>采用S2R-DepthNet, DeepLab-V3 分别获取深度图片和语义特征，由于缺少大规模标注的室外数据集，两个模型均使用transfer learning 进行训练。</p>
<p><strong>特征点滤波：</strong>在室外动态环境中如何获取稳定的特征点是本文的<strong>核心内容</strong>，该滤波机制将<strong>语义特征</strong>、<strong>depth local contrast</strong> 以及<strong>多视角投影</strong>进行结合来提高准确性与鲁棒性。</p>
<p><strong>系统定位：</strong>利用稳定的特征点，并结合立体视角来实现<strong>位姿的估计</strong>与<strong>尺度不确定的消除</strong>。</p>
<p><strong>建图：</strong>不仅构建用来位姿估计和定位的<strong>稀疏地图</strong>，也为高等级机器人导航以增量更新的方式构建<strong>室外稠密3D 地图</strong>。</p>
<h2 id="3-2-Deep-learning-model"><a href="#3-2-Deep-learning-model" class="headerlink" title="3.2 Deep learning model"></a>3.2 Deep learning model</h2><h3 id="3-2-1-Depth-estimation-module"><a href="#3-2-1-Depth-estimation-module" class="headerlink" title="3.2.1 Depth estimation module"></a>3.2.1 Depth estimation module</h3><p>S2R-DepthNet 网络包含三个模块：Structure Extraction, Depth-specific Attention, depth prediction。经该网络获取的深度图片不仅用于生成<strong>稠密地图</strong>，也利用local contrast 来<strong>滤除不稳定的特征点</strong>以进行位姿估计。</p>
<h3 id="3-2-2-Semantic-segmentation-module"><a href="#3-2-2-Semantic-segmentation-module" class="headerlink" title="3.2.2 Semantic segmentation module"></a>3.2.2 Semantic segmentation module</h3><p>考虑到<strong>室外环境的复杂性</strong>以及<strong>迁移学习的限制</strong>，作者使用性能强大的DeepLab-V3+ 作为语义分割模型。</p>
<p>本系统中，语义分割模块承担着两个作用：</p>
<ol>
<li>通过产生一个<strong>二值掩码图片</strong>（背景为1，先验动态实例为0），来协助多视角投影来区分<strong>真正的动态特征点</strong>；</li>
<li>用于移除所有的动态实例来<strong>增强3D 地图的重复使用性。</strong></li>
</ol>
<h2 id="3-3-Multiple-views-projection-constraint"><a href="#3-3-Multiple-views-projection-constraint" class="headerlink" title="3.3 Multiple views projection constraint"></a>3.3 Multiple views projection constraint</h2><p>多视角投影约束的构建过程如Fig. 3所示：</p>
<p><img src="/2024/03/01/yang2022/fig3.png" alt="fig3" title="figure 3"></p>
<p>具体步骤如下所示：</p>
<ul>
<li>首先，利用Dyna-SLAM 的<strong>轻量跟踪</strong>，只使用语义二值掩码图片中的静态特征点计算当前帧的<strong>初始位姿估计</strong>；</li>
<li>其次，利用初始位姿估计计算当前帧中每个特征点的<strong>3D 坐标</strong>；</li>
<li>然后，为当前帧选取几个共视区域足够大的<strong>几个参考帧</strong>，将上步计算的对应3D 地图点投影至参考帧中；</li>
<li>最后，比较特征点在不同参考帧中的<strong>类别标签</strong>，如果存在类别标签差异，则判定为动态特征点。</li>
</ul>
<h2 id="3-4-System-tracking"><a href="#3-4-System-tracking" class="headerlink" title="3.4 System tracking"></a>3.4 System tracking</h2><p>作者认为，如果一个特征点周围存在<strong>巨大的深度差异</strong>，那么该特征点被认为是不稳定的。</p>
<p>通常来讲，深度图片中的巨大差异也会反映在对应的RGB 图片中，太多、太近的<strong>伪特征点</strong>在跟踪过程中会由于太大的梯度值导致混乱；因此，作者使用深度图中的<strong>局部对比local contrast</strong> 来进一步提高特征点的筛选质量。作者使用简单的<strong>标准差</strong>来描述局部深度差异程度。</p>
<h2 id="3-5-Dense-mapping"><a href="#3-5-Dense-mapping" class="headerlink" title="3.5 Dense mapping"></a>3.5 Dense mapping</h2><p>本系统构建稠密地图的策略：</p>
<ol>
<li>利用获取的深度图片来创建并<strong>增量式更新</strong>稠密地图，而且使用<strong>基于体素的采样方法</strong>来控制稠密地图点的数量；</li>
<li>为了实现更高级的<strong>3D 场景感知</strong>，作者将关键帧中的语义标签利用ray-casting 策略投影至地图点中。</li>
</ol>
<p>此外，考虑到多观测数据融合，作者使用循环贝叶斯recursive Bayesian 进行数据关联，来更新每个地图点的概率分布：</p>
<p><img src="/2024/03/01/yang2022/f6.png" alt="f6" title="formula 6"></p>
<p>根据式6，可得到基于最大后验概率的稠密地图点标签。</p>
<h1 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h1><p>作者提到，在KITTI数据集的01、04中，出现的汽车与人绝大部分都是动态的，因此，移除掉所有动态物体的算法（dynamic-SLAM、SLAMANTIC）较ORB-SLAM系列算法取得了更好的效果。</p>
<p><img src="/2024/03/01/yang2022/t1.png" alt="t1" title="table 1"></p>
<p><img src="/2024/03/01/yang2022/t2.png" alt="t2" title="table 2"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Monocular Localization with Vector HD Map (MLVHM)_A Low-Cost Method for Commercial IVs</title>
    <url>/2024/03/01/xiao2020/</url>
    <content><![CDATA[<p>Xiao, Zhongyang, Diange Yang, Tuopu Wen, Kun Jiang, and Ruidong Yan. “Monocular Localization with Vector HD Map (MLVHM): A Low-Cost Method for Commercial IVs.” <em>Sensors</em> 20, no. 7 (March 27, 2020): 1870. <a href="https://doi.org/10.3390/s20071870">https://doi.org/10.3390/s20071870</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ol>
<li>提出一个基于<strong>语义向量提取</strong>和<strong>鲁棒地图匹配算法</strong>的低成本高精度定位算法；</li>
<li>提出一个<strong>基于滑动窗口的帧间运动融合（单目相机视觉里程计作为帧间约束）</strong>来有效提高定位的稳定性，特别是在<strong>稀疏定位特征的场景</strong>中也可以实现实时的稳定定位；</li>
<li>在真实世界中进行实验证明了精度与可靠性。</li>
</ol>
<span id="more"></span>
<p>不同种类的高精度地图：</p>
<p><img src="/2024/03/01/xiao2020/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3 System Overview"></a>3 System Overview</h1><p>MLVHM 系统概述如Fig. 2所示：</p>
<p><img src="/2024/03/01/xiao2020/fig2.png" alt="fig2" title="figure 2"></p>
<p>值得注意的是，ORB 特征只用来进行里程计计算作为<strong>帧间动作约束</strong>，并不参与地图匹配。</p>
<h1 id="4-Map-Based-Localization"><a href="#4-Map-Based-Localization" class="headerlink" title="4 Map-Based Localization"></a>4 Map-Based Localization</h1><h2 id="4-1-Line-and-Point-Based-Camera-Localization"><a href="#4-1-Line-and-Point-Based-Camera-Localization" class="headerlink" title="4.1 Line-and Point-Based Camera Localization"></a>4.1 Line-and Point-Based Camera Localization</h2><p>MLVHM 使用的是<strong>带有语义信息的几何特征</strong>，如Fig. 3所示，在Image processing 阶段，这些点与线特征被识别为带有语义信息，相应地，地图中的地标也是环境中关键元素的几何描述。</p>
<p><img src="/2024/03/01/xiao2020/fig3.png" alt="fig3" title="figure 3"></p>
<p>建立图片与地图间点、线特征的Mahalanobis 范数非线性优化：</p>
<p><img src="/2024/03/01/xiao2020/f13.png" alt="f13" title="formula 13"></p>
<p>其中，$r^{\mathcal{P}}(z_{i,t}^{(P)},x_t),r^{\mathcal{L}}(z_{m,t}^{(L)},x_t)$  分别是点与线的<strong>观测残差</strong>。如Fig. 4所示：</p>
<p><img src="/2024/03/01/xiao2020/fig4.png" alt="fig4" title="figure 4"></p>
<p><strong>点特征</strong>的观测残差表示为 $r^{\mathcal{P}}(z_{i,t}^{(P)},x_t) = h_j^{(P)} - \hat{p}_i^{(P)}$ 。<strong>线特征两个端点</strong>的观测残差分别为 $r^{\mathcal{L}}(z_{m,t}^{(L)},x_t) = [d_1, d_2]^T$ 。</p>
<h2 id="4-2-Data-Association-Method"><a href="#4-2-Data-Association-Method" class="headerlink" title="4.2 Data Association Method"></a>4.2 Data Association Method</h2><p>作者采用一个RANSAC 升级版进行数据关联。</p>
<h3 id="Basic-RANSAC-method"><a href="#Basic-RANSAC-method" class="headerlink" title="Basic RANSAC method"></a>Basic RANSAC method</h3><p>随机选取语义标签正确匹配的一组<strong>可能匹配点子集</strong>来评估该子集的质量，传统RANSAC 是通过测量内点的数量来评价子集的质量，即将地图投影到像素坐标系中，根据符合一定阈值内的<strong>匹配点数量</strong>来评判该子集的质量。</p>
<h3 id="Improved-RANSAC-method"><a href="#Improved-RANSAC-method" class="headerlink" title="Improved RANSAC method"></a>Improved RANSAC method</h3><p>伪代码如Algorithm 1所示，$\mathbf{c}_{1-3}^{(L)}$  表示选取三个（计算位姿的最小匹配数量）<strong>线匹配</strong>计算相机位姿 $\hat{\mathbf{x}}^{\ast}$ ，根据该相机位姿 $\hat{\mathbf{x}}^{\ast}$ 将地图投影到像素坐标系，计算内点（投影地标与图片特征小于阈值视为内点）集合 $\mathbf{c}^\ast$ 。此外，作者计算相机位姿 $\hat{\mathbf{x}}^{\ast}$ 与初始位姿估计 $\bar{\mathbf{p}}$ 之间的<strong>偏移量</strong>，若该偏移量小于 D，则将内点集合 $\mathbf{c}^\ast$ 并入关联集合 C 中（Algorithm 1：step 6）。</p>
<p>阈值 D 的设置是根据初始位姿估计 $\bar{\mathbf{p}}$ 的<strong>置信度</strong>来决定的，第一帧的初始位姿估计 $\bar{\mathbf{p}}$ 是通过低成本GNSS 接收机获取的，后续的初始位姿估计是结合上一帧的位姿与VO 进行估计的。</p>
<p>最终选取内点数最多的一组C 作为最终的数据关联结果。</p>
<p><img src="/2024/03/01/xiao2020/a1.png" alt="a1" title="algorithm 1"></p>
<h2 id="4-3-Integrating-Frame-to-Frame-Motion"><a href="#4-3-Integrating-Frame-to-Frame-Motion" class="headerlink" title="4.3 Integrating Frame-to-Frame Motion"></a>4.3 Integrating Frame-to-Frame Motion</h2><p>利用ORB-SLAM 算法计算帧间VO ，作为帧间运动约束实现对位姿估计的优化。</p>
<p><img src="/2024/03/01/xiao2020/fig5.png" alt="fig5" title="figure 5"></p>
<p>设定：帧 $c_i$ 较第一帧图像 $C_0$ 的位姿变换是 $\mathbf{R}_{c_i}^{C_0},\mathbf{t}_{c_i}^{C_0}$ ；由于单目相机的<strong>尺度不确定性</strong>，$\mathbf{t}_{c_i}^{C_0}$ 较真正的平移向量存在一个尺度因子 $s$ 的差距；相机坐标系与世界坐标系之间的转换关系为  $\mathbf{R}^{w}_{C_0},\mathbf{t}^{w}_{C_0}$ ；则相应的帧 $c_i$ 在世界坐标系下的位姿为：</p>
<p><img src="/2024/03/01/xiao2020/f18.png" alt="f18" title="formula 18"></p>
<p><img src="/2024/03/01/xiao2020/f19.png" alt="f19" title="formula 19"></p>
<p>由于语义分割消耗时间较久，故地图匹配的位姿结果通常与当前帧的时间不同，假定中间的<strong>延迟为M 帧</strong>。即假设当前时间为t，则距离最近的地图匹配位姿为 t-M 帧的结果，设定帧 $c_i$ 根据<strong>地图匹配</strong>求得的位姿为 $\mathbf{\hat{R}}_{c_i}^{w},\mathbf{\hat{t}}_{c_i}^{w}$ 。设定一个宽度为N 的滑动窗口对位姿进行优化，最终，使用<strong>帧间VO 约束</strong>的基于地图匹配的定位算法优化目标定义为：</p>
<p><img src="/2024/03/01/xiao2020/f20.png" alt="f20" title="formula 20"></p>
<p>由此，可以得到<strong>帧t 对应的位姿</strong>：</p>
<p><img src="/2024/03/01/xiao2020/f21.png" alt="f21" title="formula 21"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 YOLO-SLAM_A semantic SLAM system towards dynamic environment with geometric constraint</title>
    <url>/2024/03/01/yolo-slam/</url>
    <content><![CDATA[<p>Wu, Wenxin, Liang Guo, Hongli Gao, Zhichao You, Yuekai Liu, and Zhiqiang Chen. “YOLO-SLAM: A Semantic SLAM System towards Dynamic Environment with Geometric Constraint.” <em>Neural Computing and Applications</em> 34, no. 8 (April 2022): 6011–26. <a href="https://doi.org/10.1007/s00521-021-06764-3">https://doi.org/10.1007/s00521-021-06764-3</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文提出了一种在<strong>动态环境</strong>中可有效消除移动物体影响的<strong>鲁棒SLAM 系统</strong>——YOLO-SLAM，该系统<strong>紧耦合深度学习</strong>在场景理解的优势和特征点的内在<strong>几何深度信息</strong>来检测动态特征点，本文的主要贡献：</p>
<ol>
<li>设计了一个<strong>轻量级Darknet19-YOLOv3 物体检测网络</strong>，用于获取视觉语义信息；</li>
<li>提出了一种<strong>新的几何约束方法</strong>来识别动态特征点；</li>
<li>基于ORB-SLAM2 构建了YOLO-SLAM 系统，在动态环境中的<strong>准确性</strong>和<strong>鲁棒性</strong>都得到了提高。</li>
</ol>
<span id="more"></span>
<h1 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h1><p>针对动态场景中的SLAM，主要有三种方法来消除动态物体的影响：<strong>几何方法</strong>，<strong>深度学习方法</strong>，以及<strong>两者相结合的方法</strong>。当前学术界更看重两者结合的方法，但是存在一定问题：</p>
<ol>
<li>常用的深度学习网络，如SegNet，Mask-RCNN 很耗时；</li>
<li>多视角几何约束需要多帧图片，无法解决初始化或者前帧图片丢失的情况。</li>
</ol>
<p>针对以上问题，作者提出了一种将深度学习和几何约束<strong>紧耦合</strong>的动态特征点剔除方法：</p>
<ol>
<li>对YOLOv3 网络进行修改，得到一个<strong>轻量级</strong>的物体检测网络，以减少时间消耗；</li>
<li>提出一种新的几何约束方法，可在仅使用<strong>一张图片信息</strong>的情况下实现对动态物体特征点的检测。</li>
</ol>
<h1 id="3-Proposed-method"><a href="#3-Proposed-method" class="headerlink" title="3 Proposed method"></a>3 Proposed method</h1><h2 id="3-1-Framework-of-YOLO-SLAM"><a href="#3-1-Framework-of-YOLO-SLAM" class="headerlink" title="3.1 Framework of YOLO-SLAM"></a>3.1 Framework of YOLO-SLAM</h2><p>YOLO-SLAM 的整体架构如Fig. 1所示。</p>
<p><img src="/2024/03/01/yolo-slam/fig1.png" alt="fig1" title="figure 1"></p>
<p>移除动态特征点的细节过程如Fig. 2所示：</p>
<p><img src="/2024/03/01/yolo-slam/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-2-Lightweight-object-detection"><a href="#3-2-Lightweight-object-detection" class="headerlink" title="3.2 Lightweight object detection"></a>3.2 Lightweight object detection</h2><p>作者将YOLOv3 网络的主干网从Darknet-53 更换为 Darknet-19，可在保持一定精度的同时大幅提高运行速度，两者的运行参数对比如下所示：</p>
<p><img src="/2024/03/01/yolo-slam/t1.png" alt="t1" title="table 1"></p>
<h2 id="3-3-Dynamic-features-screening"><a href="#3-3-Dynamic-features-screening" class="headerlink" title="3.3 Dynamic features screening"></a>3.3 Dynamic features screening</h2><p>RGB-D 相机获取的深度图如Fig. 5所示，可根据<strong>深度差异</strong>来辨别物体的轮廓，在此基础上，利用<strong>depth-RANSAC</strong> 方法来识别出bbox 内的静态特征点。</p>
<p><img src="/2024/03/01/yolo-slam/fig5.png" alt="fig5" title="figure 5"></p>
<p>该方法基于<strong>三个假设</strong>：</p>
<ol>
<li>人是最可能移动的物体，且占据了bbox 的大部分空间；</li>
<li>位于人上的特征点在深度值上差异很小；</li>
<li>bbox 内，人和其他物体的深度差异较明显。</li>
</ol>
<p>基于以上三个假设的depth-RANSAC 算法如下所示。</p>
<p><strong>注：</strong>文章在对迭代次数 k 进行求解时，关于概率计算问题<strong>出现了错误</strong>。</p>
<p>文章将内点所占比例表示为 $w$ ，每次用N 个点计算RANSAC 模型时，在所选的N 个点中至少存在一个外点的概率为 $1-w^N$ ；若连续进行k 次迭代，则 $(1-w^N)^k$ 表示的是<strong>k 次迭代中，每次都至少存在一个外点</strong>，而文中错误地将 $(1-w^N)^k$ 表示为k 次连续迭代中至少存在一个外点。</p>
<p>在此错误前提下，作者使用 P 表示在 k 次迭代中，采样点<strong>全部都是内点</strong>，文中的求解为 $1-P = (1-w^N)^k$ ；但此处 P 代表的含义应是在 k 次迭代中，至少有一次迭代的采样点全部是内点。若要实现作者对P 的含义，则应为 $P = (w^N)^k$ 。</p>
<p><img src="/2024/03/01/yolo-slam/a1.png" alt="a1" title="algorithm 1"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Fusing Semantic Segmentation and Object Detection for Visual SLAM in Dynamic Scenes</title>
    <url>/2024/02/27/yu2021/</url>
    <content><![CDATA[<p>Yu, Peilin, Chi Guo, yang Liu, and Huyin Zhang. “Fusing Semantic Segmentation and Object Detection for Visual SLAM in Dynamic Scenes.” In <em>Proceedings of the 27th ACM Symposium on Virtual Reality Software and Technology</em>, 1–7. VRST ’21. New York, NY, USA: Association for Computing Machinery, 2021. <a href="https://doi.org/10.1145/3489849.3489882">https://doi.org/10.1145/3489849.3489882</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文针对SLAM 算法环境中存在的动态物体，结合<strong>物体检测</strong>和<strong>语义分割</strong>来获取<strong>潜在动态物体</strong>的先验轮廓，在此基础上应用<strong>几何约束</strong>实现对动态特征点的剔除。</p>
<p>本文做出的贡献：</p>
<ol>
<li>提出一种<strong>自适应机制</strong>，使得系统可根据不同的环境选择使用语义分割还是物体检测；</li>
<li>提出一种<strong>静态点恢复技术</strong>，以减少可用静态点的损失，并使用光流和对极约束来检查物体的状态；</li>
<li>在公开数据集TUM 上进行评估，在<strong>高动态环境</strong>下实现了良好的表现。</li>
</ol>
<span id="more"></span>
<h1 id="3-Our-Method"><a href="#3-Our-Method" class="headerlink" title="3 Our Method"></a>3 Our Method</h1><p>本文方法的架构如Fig. 1所示，利用<strong>自适应机制</strong>来切换物体检测分支和语义分割分支，然后结合<strong>几何约束</strong>技术实现对动态特征点的剔除。</p>
<p><img src="/2024/02/27/yu2021/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-1-Extract-Semantic-Information"><a href="#3-1-Extract-Semantic-Information" class="headerlink" title="3.1 Extract Semantic Information"></a>3.1 Extract Semantic Information</h2><p>在<strong>自适应机制模块</strong>中，作者通过多次实验得到一个<strong>阈值</strong>，来决定是否有必要使用语义分割进行处理：</p>
<ul>
<li>如果得分超过这个阈值，说明动态物体占据了当前视野的较大部分。此时，需要进一步使用语义分割来<strong>尽可能地保留静态特征点</strong>（Fig. 2）；</li>
<li>如果得分低于这个阈值，说明动态物体占据了当前视野地较小部分。此时，仅利用物体检测技术即可（Fig. 3）。</li>
</ul>
<p><img src="/2024/02/27/yu2021/fig2.png" alt="fig2" title="figure 2"></p>
<p><img src="/2024/02/27/yu2021/fig3.png" alt="fig3" title="figure 3"></p>
<p>得分的计算方程：</p>
<p><img src="/2024/02/27/yu2021/f1.png" alt="f1" title="formula 1"></p>
<p>其中，$f(u,v)$ 表示若像素被判定属于潜在动态物体则为1，否则为0；$P_I$ 表示所有像素数量。即，上式表示判定为动态物体像素占总体像素的比例。</p>
<h2 id="3-2-Remove-Dynamic-Features"><a href="#3-2-Remove-Dynamic-Features" class="headerlink" title="3.2 Remove Dynamic Features"></a>3.2 Remove Dynamic Features</h2><p>使用以下策略对动态特征点进行准确剔除：</p>
<h3 id="3-2-1-Dynamic-check"><a href="#3-2-1-Dynamic-check" class="headerlink" title="3.2.1 Dynamic check"></a><strong>3.2.1 Dynamic check</strong></h3><p>使用DS-SLAM 中的<strong>移动一致性检测</strong>来判断是否属于动态特征点：潜在动态物体内部的动态特征点超过一定阈值，则判定该物体是动态物体，并移除属于该物体的所有特征点。该策略同时应用于两个分支中。</p>
<h3 id="3-2-2-Static-point-recovery"><a href="#3-2-2-Static-point-recovery" class="headerlink" title="3.2.2 Static point recovery"></a><strong>3.2.2 Static point recovery</strong></h3><p>作者提出一个静态点恢复技术 static point recovery (SPR) 恢复bbox 内的部分静态点来增强系统的鲁棒性。对于物体检测网络处理过的图片，作者将潜在动态物体bbox 外部的点视为静态点，记为第一部分；bbox 内部的点视为潜在动态点，记为第二部分。SPR 技术是针对第二部分的特征点进行恢复操作的，具体操作步骤如下所示：</p>
<p>首先，利用第一部分（即静态特征点）中的所有特征点计算两两之间线段的距离，在相邻帧之间构建 line segment constraint，如Fig. 4所示，</p>
<p><img src="/2024/02/27/yu2021/fig4.png" alt="fig4" title="figure 4"></p>
<p><img src="/2024/02/27/yu2021/f2.png" alt="f2" title="formula 2"></p>
<p>所谓的 line segment constraint，作者认为<strong>静态特征点对</strong>之间的距离在相邻帧之间变化不大，则统计第一部分所有点对在相邻帧之间的距离变化值，用其平均值作为阈值 $\phi_t$ 。</p>
<p><img src="/2024/02/27/yu2021/f5.png" alt="f5" title="formula 5"></p>
<p>然后，利用所有静态点和bbox 内的潜在动态点 $k$ 进行组合，计算潜在动态点 $k$ 的得分：</p>
<p><img src="/2024/02/27/yu2021/f6.png" alt="f6" title="formula 6"></p>
<p>作者判定，若得分 $s_{k, t}$ 超过组合数量的一半，则判定潜在动态点 $k$ 为真实动态点并剔除，否则保留为静态特征点。SPR 效果如Fig. 5所示。</p>
<p><img src="/2024/02/27/yu2021/fig5.png" alt="fig5" title="figure 5"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Semantic SLAM Based on Object Detection and Improved Octomap</title>
    <url>/2024/01/30/zhang2018/</url>
    <content><![CDATA[<p>Zhang, Liang, Leqi Wei, Peiyi Shen, Wei Wei, Guangming Zhu, and Juan Song. “Semantic SLAM Based on Object Detection and Improved Octomap.” <em>IEEE Access</em> 6 (2018): 75545–59. <a href="https://doi.org/10.1109/ACCESS.2018.2873617">https://doi.org/10.1109/ACCESS.2018.2873617</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>基于ORB-SLAM2，利用YOLO 实现<strong>物体检测</strong>，剔除掉<strong>先验动态物体</strong>上的特征点，提高精度；并建立<strong>物体级语义八叉树地图</strong>，且优化了制图的速度。</p>
<p>本文的贡献：</p>
<ul>
<li>本系统可以检测80-200 种物体种类，而现有的语义制图系统只能检测不超过20个种类；</li>
<li>本系统不需要先验3D 模型就可以实现对环境中物体的3D 建模；</li>
<li>本系统利用的是物体级模型信息，而不是像素级的。</li>
</ul>
<span id="more"></span>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3 System Overview"></a>3 System Overview</h1><h2 id="3-1-SLAM-Analysis"><a href="#3-1-SLAM-Analysis" class="headerlink" title="3.1 SLAM Analysis"></a>3.1 SLAM Analysis</h2><p>大多数基于特征的SLAM 系统基于一个<strong>强假设</strong>：移动物体上的特征点数量要远小于静态物体特征点数量。</p>
<p><strong>八叉树地图Octomap</strong> 不仅可以存储RGB 和位置信息，还可以保存<strong>语义信息</strong>，且利用<strong>概率模型</strong>来构建更为精准的地图。</p>
<h2 id="3-2-ORB-SLAM-Analysis"><a href="#3-2-ORB-SLAM-Analysis" class="headerlink" title="3.2 ORB-SLAM Analysis"></a>3.2 ORB-SLAM Analysis</h2><p>ORB-SLAM2 的架构如Fig. 1所示，包含三个并行处理的线程：</p>
<ul>
<li>Tracking：负责实时定位每一帧图片对应的<strong>位姿</strong>，并决定哪些图片作为<strong>关键帧</strong>。与前一帧进行特征匹配并利用BA 进行位姿优化，如果跟踪失败，利用Bag of Word 进行全局<strong>重定位</strong>，将地图中的特征点进行<strong>重投影</strong>并进行位姿优化。</li>
<li>Local Mapping：在获取一个新的关键帧之后，该线程对新地图点进行<strong>三角化</strong>，利用BA 对关键帧和地图点进行<strong>位姿优化</strong>，并对冗余的关键帧和低质量的地图点进行剔除。</li>
<li>Loop Closing：对关键帧进行<strong>回环检测</strong>，若检测成功，计算相似度转换作为环路<strong>累积漂移</strong>，然后进行对齐、融合与位姿优化。</li>
</ul>
<p><img src="/2024/01/30/zhang2018/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-3-Overview-of-Semantic-SLAM-system"><a href="#3-3-Overview-of-Semantic-SLAM-system" class="headerlink" title="3.3 Overview of Semantic SLAM system"></a>3.3 Overview of Semantic SLAM system</h2><p>本文提出的语义SLAM 系统架构如Fig. 2所示，本系统是基于ORB-SLAM2 构建的，ORB-SLAM2 负责相机定位，并利用每一个RGB-D 图片帧进行制图。</p>
<p>tracking 线程使用<strong>关键帧进行跟踪</strong>，来减小移动物体的影响；local mapping 线程添加<strong>少量</strong>的关键帧来创造语义信息，因为语义信息的提取满足实时的要求。</p>
<p>本系统使用YOLO 网络对关键帧进行物体检测，并使用<strong>CRF</strong> (Conditional Random Field) 进行<strong>物体正则化</strong>来对YOLO 检测出的物体置信度进行<strong>矫正优化</strong>，该过程使用MS-COCO 数据集的统计数据计算得到的物体间<strong>约束</strong>。</p>
<p><img src="/2024/01/30/zhang2018/fig2.png" alt="fig2" title="figure 2"></p>
<p>当获取了每个物体的<strong>准确标签</strong>后，利用滤波器对特征进行筛选，并利用投影的点云来创建<strong>临时的物体模型</strong>；在此基础上，利用<strong>数据关联</strong>决定创建新的模型或是与地图中现有的模型进行融合。</p>
<p>最后，Map Generation 利用存储在物体中的点云生成八叉树地图，并使用多线程和Fast Line Rasterization 算法进行加速。</p>
<h2 id="3-4-Relationship-between-Keyframes-and-Objects"><a href="#3-4-Relationship-between-Keyframes-and-Objects" class="headerlink" title="3.4 Relationship between Keyframes and Objects"></a>3.4 Relationship between Keyframes and Objects</h2><p>借鉴ORB-SLAM2 系统中<strong>关键帧和地图点</strong>之间的关系来创建本系统<strong>关键帧和物体</strong>之间的联系。</p>
<p>本系统中，每个<strong>物体 $O_i$</strong> 需包含：</p>
<ul>
<li>物体中点云的坐标；</li>
<li>固定数量的类别标签，以及通过递归贝叶斯更新recursive Bayesian update 计算的置信度；</li>
<li>观测到该物体的关键帧；</li>
<li>物体中点云的Kd-tree 结构（用于快速查找）；</li>
<li>物体所属的类别标签；</li>
<li>物体被观测的数量。</li>
</ul>
<p>每个<strong>关键帧 $K_i$</strong> 需包含：</p>
<ul>
<li>用于物体检测的相应RGB 图片；</li>
<li>用于产生点云的相应深度图片；</li>
<li>本关键帧中观测到的物体。</li>
</ul>
<h1 id="4-Semantic-Mapping"><a href="#4-Semantic-Mapping" class="headerlink" title="4 Semantic Mapping"></a>4 Semantic Mapping</h1><h2 id="4-1-Improved-SLAM"><a href="#4-1-Improved-SLAM" class="headerlink" title="4.1 Improved SLAM"></a>4.1 Improved SLAM</h2><p>ORB-SLAM2 中，tracking 线程的步骤：</p>
<ol>
<li>提取<strong>ORB 特征</strong>；</li>
<li>ORB 特征与<strong>reference 帧</strong>进行<strong>特征匹配</strong>，初步计算相机位姿并返回匹配地图点（通过搜索<strong>相关的关键帧</strong>来获取）的数量；</li>
<li>利用匹配地图点进行<strong>位姿优化</strong>；</li>
<li>决定哪些帧作为<strong>关键帧</strong>。</li>
</ol>
<p>为了减小动态物体的影响，对跟踪线程的第二步进行更改：由之前的reference 帧改为与<strong>关键帧进行特征匹配</strong>，这是因为<strong>旧的关键帧不包含动态物体的特征点</strong>。</p>
<p>跟踪线程的第三步改为：与第二步的结果比较<strong>匹配内点数量</strong>，来判断当前帧是否跟踪失败。</p>
<h2 id="4-2-Object-Detection"><a href="#4-2-Object-Detection" class="headerlink" title="4.2 Object Detection"></a>4.2 Object Detection</h2><p>利用在COCO 数据集上训练的Tiny YOLO 网络进行物体检测。</p>
<h2 id="4-3-Object-Regularization"><a href="#4-3-Object-Regularization" class="headerlink" title="4.3 Object Regularization"></a>4.3 Object Regularization</h2><p>常规的物体检测等网络没有考虑<strong>上下文信息（场景信息）</strong>，使用CRF 可为语义提取过程添加上下文信息约束，CRF 擅长对分类器的<strong>类别得分</strong>与图片的<strong>局部信息</strong>进行建模，可以视为一个<strong>最大后验概率问题</strong>。定义<strong>unary potentials</strong> 来对像素或图像块的所属类别进行概率建模，定义<strong>pairwise potentials</strong> 来对像素间或图像块间的关系进行建模。</p>
<p>作者构建了一个<strong>基于物体的概率稠密CRF 算法</strong>，较基于像素的方法大大减少了计算复杂度，相应的Gibbs 能量方程如下所示：</p>
<p><img src="/2024/01/30/zhang2018/f1-2.png" alt="f1" title="f1-2"></p>
<p>与文章(Runz 和 Agapito, 2017)相似，$x$ 表示类别标签；$i, j$ 取值1 到 $k$ ，其中 $k$ 表示地图中物体的数量；$Z$ 是归一化参数。本物体级 CRF 的目标是最小化 $E(x)$ ，相应的unary potentials 和 pairwise potentials 表示为：</p>
<p><img src="/2024/01/30/zhang2018/f3-4.png" alt="f3" title="f3-4"></p>
<p>其中，$\mu$ 函数被称为<strong>标签兼容性函数</strong>，用来描述相邻位置两个不同标签同时出现的可能性；$f_{i, j}$ 是第 i 个物体和第 j 个物体的约束。</p>
<p><img src="/2024/01/30/zhang2018/f6.png" alt="f6" title="formula 6"></p>
<p>其中，$p_{i, j}$ 表示两个物体出现在同一视野中的概率，通过对COCO 数据集进行统计分析获取，如Fig. 4所示，对角线数字表示该物体在数据集中出现的次数。</p>
<p><img src="/2024/01/30/zhang2018/fig4.png" alt="fig4" title="figure 4"></p>
<p>由此，本系统实现了同时利用YOLO 和 CRF 实现对物体分类的<strong>置信度</strong>确定。</p>
<h2 id="4-4-Temporary-Objects-Generation"><a href="#4-4-Temporary-Objects-Generation" class="headerlink" title="4.4 Temporary Objects Generation"></a>4.4 Temporary Objects Generation</h2><p>在确定物体的类别标签后，使用feature filter 根据物体类别实现对特征点和地图点的筛选。作者将属于<strong>先验动态物体</strong>的ORB 特征、地图点、DBoW 特征进行剔除，只保留先验静态物体的特征。</p>
<p><img src="/2024/01/30/zhang2018/fig5.png" alt="fig5" title="figure 5"></p>
<p>在动态物体特征剔除之后，生成包含物体尺寸、类别、置信度的得分以及点云的<strong>临时物体模型</strong>，并对噪声点进行剔除。</p>
<p><img src="/2024/01/30/zhang2018/fig6.png" alt="fig6" title="figure 6"></p>
<h2 id="4-5-Data-Association"><a href="#4-5-Data-Association" class="headerlink" title="4.5 Data Association"></a>4.5 Data Association</h2><p>本系统中的DA 是用来判定检测物体是新观测到的还是地图中已存在的。</p>
<p>首先，为每个临时物体模型寻找<strong>候选匹配模型</strong>。可轻易找出与当前关键帧相关联的历史关键帧，以此确定候选匹配模型，该过程如Fig. 7所示。</p>
<p><img src="/2024/01/30/zhang2018/fig7.png" alt="fig7" title="figure 7"></p>
<p>然后，在候选匹配模型中选取<strong>最相似的模型</strong>。本过程在临时模型和候选模型的点云间进行nearest neighbor search ，计算匹配点对之间的<strong>欧氏距离</strong>；该过程利用<strong>k-d 树</strong>来加速匹配过程；选取匹配数量最多且超过一定阈值的候选匹配模型作为成功匹配的模型，若没有成功匹配，则视为<strong>新模型</strong>添加进地图中。</p>
<h2 id="4-6-Object-Model-Update"><a href="#4-6-Object-Model-Update" class="headerlink" title="4.6 Object Model Update"></a>4.6 Object Model Update</h2><p>若模型成功匹配，则<strong>点云</strong>与<strong>类别置信度</strong>需要进行融合。利用recursive Bayesian update 来更新相应的概率分布：</p>
<p><img src="/2024/01/30/zhang2018/f9.png" alt="f9" title="formula 9"></p>
<h2 id="4-7-Map-Generation"><a href="#4-7-Map-Generation" class="headerlink" title="4.7 Map Generation"></a>4.7 Map Generation</h2><p><img src="/2024/01/30/zhang2018/fig9.png" alt="fig9" title="figure 9"></p>
<p>在制图中，移动物体以及测距误差会造成很多误差，八叉树地图使用概率模型来解决该问题。八叉树地图的每个叶子节点存储被占用或空闲的概率，当新的3D 点被插入时会更新概率：</p>
<p><img src="/2024/01/30/zhang2018/f10-11.png" alt="f10-11" title="f10-11"></p>
<p>其中，$n$ 表示叶子节点；$z_t$ 表示观测；$P(n|z_t)$ 表示在给定观测 $z_t$ 时体素 $n$ 被占用的概率。</p>
<p>此外，除了占用概率，体素中还会存储<strong>固定数量的类别标签</strong>以及<strong>置信度得分</strong>。</p>
<p>八叉树地图绘制过程中会消耗大量的时间来计算空的体素，作者利用优化来加速该过程，如Fig. 10所示。</p>
<p><img src="/2024/01/30/zhang2018/fig10.png" alt="fig10" title="figure 10"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Semantic SLAM for mobile robots in dynamic environments based on visual camera sensors</title>
    <url>/2024/03/01/zhang2023a/</url>
    <content><![CDATA[<p>Zhang, Qi, and Changdi Li. “Semantic SLAM for Mobile Robots in Dynamic Environments Based on Visual Camera Sensors.” <em>Measurement Science and Technology</em> 34, no. 8 (May 2023): 085202. <a href="https://doi.org/10.1088/1361-6501/acd1a4">https://doi.org/10.1088/1361-6501/acd1a4</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ul>
<li>提出一个可应用于室内外场景的<strong>实时动态语义vSLAM 算法</strong>，可应用于各种相机传感器，并不依赖深度信息；</li>
<li>受益于<strong>更多的检测类别</strong>，提出一个<strong>动态点剔除算法</strong>；</li>
<li>提出了一种几乎不受噪声点影响的<strong>移动bbox判断算法</strong>，该算法以所有物体均可能是动态的为假设，使用不同的方法来检测处于不同姿势的人或物体的运动状态。</li>
</ul>
<span id="more"></span>
<h1 id="3-System-overview"><a href="#3-System-overview" class="headerlink" title="3 System overview"></a>3 System overview</h1><p>本系统结合<strong>语义和几何信息</strong>来检测动态特征点：</p>
<ol>
<li>首先，利用DNN 获取物体的<strong>bbox</strong>，同时使用<strong>运动一致性检验</strong>来获取图片中的外点；</li>
<li>然后，结合两种信息来高效检测、剔除动态特征点。</li>
</ol>
<p>本系统是基于ORB-SLAM3 来搭建的，系统的语义模块参考CrowSLAM，几何模块参考DS-SLAM。</p>
<p><img src="/2024/03/01/zhang2023a/fig2.png" alt="fig2" title="figure 2"></p>
<h2 id="3-1-Semantic-Module"><a href="#3-1-Semantic-Module" class="headerlink" title="3.1 Semantic Module"></a>3.1 Semantic Module</h2><p>本系统使用YOLOX 算法，在高精度的同时保证高速计算，并使用GPU 加速方法——TensorRT。与其他SLAM 系统不同的是，本系统认为在复杂环境中<strong>所有物体都有可能是运动的</strong>，为了识别出尽可能多的物体，作者使用COCO 数据集（80个种类）对YOLOX 进行训练；然后，本系统结合一个<strong>自适应阈值算法</strong>来对<strong>每一个bbox</strong> 进行<strong>运动一致性检验</strong>，来确定该物体是否是动态的。</p>
<h2 id="3-2-Geometry-Module"><a href="#3-2-Geometry-Module" class="headerlink" title="3.2 Geometry Module"></a>3.2 Geometry Module</h2><p>本系统采用DS-SLAM(Yu 等, 2018) 的<strong>运动一致性检验方法</strong>，实现对动态特征点的检测。</p>
<h2 id="3-3-Judging-moving-boxes"><a href="#3-3-Judging-moving-boxes" class="headerlink" title="3.3 Judging moving boxes"></a>3.3 Judging moving boxes</h2><p>本系统结合语义信息和几何信息来判定bbox 是否是动态的，</p>
<p><img src="/2024/03/01/zhang2023a/fig3.png" alt="fig3" title="figure 3"></p>
<p>bbox 运动状态的判定步骤如下：</p>
<ol>
<li>首先，使用独特的方法来处理人类的bbox——NHBC 算法来区分人类是坐着还是站着的，这里使用<strong>bbox 的长宽比</strong>进行确认，坐着的人的长宽比一般小于3；作者认为坐着的人只有上半身是动态的，而站着的人全身都是动态的。</li>
<li>然后，人类和其他物体的判断阈值需<strong>分别决定</strong>，因为人类与其他物体的<strong>运动方式差异较大</strong>，人类通常都是局部运动，如Fig. 4（a）所示，动态人类上只有几个特征点是动态的，所以对于人类bbox，只要其中有一个动态特征点，则视该bbox 为动态的；而其他物体则不尽然，需要bbox 内有足够多的动态特征点才能判定该bbox 是动态的；且部分物体检测到特征点少，部分物体检测到特征点多，因此，设定一个固定的阈值是不合适的。据此，作者提出了<strong>ATA (Adaptive Threshold Adjustment)</strong>，即根据bbox 内外点所占的比例来确定该bbox 是否是动态的，因为false-negative 对SLAM 影响较大，所以作者设置了一个较低的阈值——40%，对于人类，则设置为一个动态特征点。</li>
</ol>
<p><img src="/2024/03/01/zhang2023a/fig4.png" alt="fig4" title="figure 4"></p>
<p>bbox 运动状态检测算法的过程如算法一所示：</p>
<p><img src="/2024/03/01/zhang2023a/a1.png" alt="a1" title="algorithm 1"></p>
<h2 id="3-4-Culling-dynamic-points"><a href="#3-4-Culling-dynamic-points" class="headerlink" title="3.4 Culling dynamic points"></a>3.4 Culling dynamic points</h2><p>由于bbox 的模糊性，作者选择仅对<strong>同时属于动态bbox 内且在静态bbox 外的点</strong>进行剔除。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Dynamic SLAM</category>
      </categories>
      <tags>
        <tag>Dynamic SLAM</tag>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Visual-Based Semantic SLAM with Landmarks for Large-Scale Outdoor Environment</title>
    <url>/2024/02/28/zhao2019/</url>
    <content><![CDATA[<p>Zhao, Zirui, Yijun Mao, Yan Ding, Pengju Ren, and Nanning Zheng. “Visual-Based Semantic SLAM with Landmarks for Large-Scale Outdoor Environment.” In <em>2019 2nd China Symposium on Cognitive Computing and Hybrid Intelligence (CCHI)</em>, 149–54, 2019. <a href="https://doi.org/10.1109/CCHI.2019.8901910">https://doi.org/10.1109/CCHI.2019.8901910</a>.</p>
<h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>本文的贡献：</p>
<ul>
<li>通过将视觉SLAM 地图和语义分割信息进行融合，来构建大型户外环境的<strong>语义3D 地图</strong>；</li>
<li>扩充KITTI 数据集以包含<strong>GPS 信息</strong>，以及从<strong>Google Map 上获取的相关地标标签</strong>；</li>
<li>提出一个基于语义地图将真实世界的地标和点云地图联系起来，以构建一个<strong>拓扑地图</strong>的方法。</li>
</ul>
<span id="more"></span>
<h1 id="3-Approach"><a href="#3-Approach" class="headerlink" title="3 Approach"></a>3 Approach</h1><h2 id="3-1-System-overview"><a href="#3-1-System-overview" class="headerlink" title="3.1 System overview"></a>3.1 System overview</h2><p>系统框架如Fig. 1所示：</p>
<p><img src="/2024/02/28/zhao2019/fig1.png" alt="fig1" title="figure 1"></p>
<h2 id="3-2-Semantic-mapping"><a href="#3-2-Semantic-mapping" class="headerlink" title="3.2 Semantic mapping"></a>3.2 Semantic mapping</h2><p>作者使用PSPNet-101 模型对图片进行语义分割，利用TensorRT 进行实施<strong>推理加速</strong>。</p>
<p>利用ORB-SLAM2 进行<strong>3D 重建</strong>和<strong>轨迹估计</strong>。</p>
<p>利用<strong>贝叶斯更新准则</strong>来为每个地图点的<strong>语义标签概率分布</strong>进行更新。</p>
<p>3D 点云坐标与像素坐标的转换关系：</p>
<p><img src="/2024/02/28/zhao2019/f1.png" alt="f1" title="formula 1"></p>
<p>在特征点被投影至相机坐标系后，可得到每个特征点在19个类别标签上的<strong>概率分布</strong>：</p>
<p><img src="/2024/02/28/zhao2019/f3.png" alt="f3" title="formula 3"></p>
<p>其中，$L_m(x_m, y_m, z_m)$ 表示地图点 $(x_m, y_m, z_m)$ 的类别标签；$F_s$ 表示语义分割后当前帧图片像素在每个类别标签上的概率分布。此外，由于同一特征点可在多帧图像中被观测到，因此需要进行<strong>多观测数据融合</strong>操作，此处利用<strong>贝叶斯更新</strong>：</p>
<p><img src="/2024/02/28/zhao2019/f4.png" alt="f4" title="formula 4"></p>
<p>其中，Z 是归一化常数；$l_k^m$ 表示地图点 m 在帧 k 上的标签；$p(l_l^m | F_{1:k}, P_{1:k})$ 表示从第一帧到 k 帧的<strong>累积概率分布</strong>。</p>
<p>最终，每个地图点的标签通过选取最大概率值来确定：</p>
<p><img src="/2024/02/28/zhao2019/f6.png" alt="f6" title="formula 6"></p>
<p>在实时融合过程中，每个地图点会包含一个语义标签以及一个语义概率分布。</p>
<h2 id="3-3-GPS-fusion"><a href="#3-3-GPS-fusion" class="headerlink" title="3.3 GPS fusion"></a>3.3 GPS fusion</h2><p>为了将<strong>建筑地标</strong>和<strong>点云</strong>在像素级别上联系起来以产生<strong>语义点云</strong>，作者将建筑地标的WGS84 坐标转换至点云坐标系下。作者发现从Goole Map API 获取的WGS84 坐标系经纬度不适合直接转换，因此，首先将其转换至<strong>Cartesian 坐标系</strong>下，单位是米；然后利用现有的方法将其与<strong>点云坐标系</strong>进行对齐。设$P_A$ 为Cartesian 坐标下的点云，其中心为 $centroid_A$ ；$P_B$ 为位姿坐标系下的点云，其中心为 $centroid_B$ ，由于两者的尺度不一致，还需进行尺度转换。旋转矩阵与平移矩阵的计算如下所示：</p>
<p><img src="/2024/02/28/zhao2019/f7.png" alt="f7" title="formula 7"></p>
<p><img src="/2024/02/28/zhao2019/f9.png" alt="f9" title="formula 9"></p>
<p>其中，$\lambda$ 为<strong>尺度因子</strong>，计算方式如下：</p>
<p><img src="/2024/02/28/zhao2019/f11.png" alt="f11" title="formula 11"></p>
<h2 id="3-4-Post-process"><a href="#3-4-Post-process" class="headerlink" title="3.4 Post process"></a>3.4 Post process</h2><p>利用地标的GPS 信息和语义标签，可以将地标级数据和3D 重建结果融合起来，可方便用于任务导向的导航问题。作者使用<strong>基于模糊数学的方法</strong> fuzzy-mathematics-based method 进行地标数据融合：不关注地标位置的准确性，而是关注<strong>地标位置的归属分布</strong>，这点是模仿人类感知习惯的。作者尝试基于<strong>高斯概率分布</strong>来评估位置归属性，如果一个位置接近某个地标，那么根据高斯分布该位置的归属性会更高；<strong>归属性</strong>定义如下所示：</p>
<p><img src="/2024/02/28/zhao2019/f13.png" alt="f13" title="formula 13"></p>
<p>其中，$(x_l, y_l)$ 表示地标位置；$\sigma$ 表示高斯分布的标准差。</p>
<p>拓扑地图如Fig. 3所示，其只包含地标之间的<strong>可抵达关系</strong>以及它们的<strong>几何关系</strong>，拓扑地图中只有节点和边，适合全局路径规划。</p>
<p><img src="/2024/02/28/zhao2019/fig3.png" alt="fig3" title="figure 3"></p>
<h1 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h1><p>现有的工作主要聚焦于准确性或者实时性能，但这对于<strong>机器人整体感知层面</strong>的提升较小；作者利用带有GPS 信息的KITTI 数据集进行<strong>地标语义融合</strong>以及<strong>拓扑语义制图</strong>，结合Google Map API，可构建包含<strong>真实名字与位置信息的地标</strong>，使得离线人机语言交互、任务导向的导航或者地标级的定位成为可能。</p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>论文记录 Visual Mapping and Localization System Based on Compact Instance-Level Road Markings With Spatial Uncertainty</title>
    <url>/2024/03/06/zhou2022/</url>
    <content><![CDATA[<p>Zhou, Yuxuan, Xingxing Li, Shengyu Li, and Xuanbin Wang. “Visual Mapping and Localization System Based on Compact Instance-Level Road Markings With Spatial Uncertainty.” <em>IEEE Robotics and Automation Letters</em> 7, no. 4 (October 2022): 10802–9. <a href="https://doi.org/10.1109/LRA.2022.3196470">https://doi.org/10.1109/LRA.2022.3196470</a>.</p>
<h1 id="3-System-Overview"><a href="#3-System-Overview" class="headerlink" title="3 System Overview"></a>3 System Overview</h1><p>Fig .1是本系统的基本框架，在本系统中汽车被假设为拥有基本的基于GNSS 的全局定位能力，以及基于轮式里程计或视觉-惯性里程计（VIO）的局部导航能力。对于基于视觉的环境感知，制图与定位阶段<strong>共享产生语义道路标志的pipeline</strong>。对于位姿估计，一个<strong>统一的位姿图优化框架</strong>被用于制图和定位阶段，将全局定位、局部导航以及地图匹配（限于终端用户）信息结合在一起实现鲁棒准确的定位。</p>
<span id="more"></span>
<p><img src="/2024/03/06/zhou2022/fig1.png" alt="fig1" title="figure 1"></p>
<h1 id="4-Semantic-Road-Marking-Extraction"><a href="#4-Semantic-Road-Marking-Extraction" class="headerlink" title="4 Semantic Road Marking Extraction"></a>4 Semantic Road Marking Extraction</h1><p>作者采用一个在Appoloscape 上预训练的语义分割模型，并在自采数据（在武汉市采集的227张图片）上进行细调，采用了以下分割种类：车道线（虚线）、车道线（实线）、指引标志（箭头或菱形）、斑马线、停止线及其他。值得注意的是，道路标志的语义分割可以被集成到车辆系统的感知模块内，不必将其视为一个独立模块。</p>
<p>进行语义分割之后，使用IPM 模型来恢复路标的公制几何信息，流程如Fig. 2所示，经过转换，可以<strong>将原始图片中的道路像素转换到相机坐标系中的3D点或带有公制几何信息的IPM 图片中的点</strong>。</p>
<p><img src="/2024/03/06/zhou2022/fig2.png" alt="fig2" title="figure 2"></p>
<h1 id="5-Modelling-the-Instance-level-Road-Markings"><a href="#5-Modelling-the-Instance-level-Road-Markings" class="headerlink" title="5 Modelling the Instance-level Road Markings"></a>5 Modelling the Instance-level Road Markings</h1><h2 id="5-1-Parameterization-of-the-Road-Markings"><a href="#5-1-Parameterization-of-the-Road-Markings" class="headerlink" title="5.1 Parameterization of the Road Markings"></a>5.1 Parameterization of the Road Markings</h2><p>在第4章提到的分割种类中，虽然斑马线形式非常独特，但是由于计算复杂度的问题，其不被用于制图和定位过程。对于其他种类，作者将其分为两种类型：</p>
<ol>
<li>块状patch-like 路标：车道线（虚线），指引标志（箭头、菱形）；</li>
<li>线状路标：车道线（实线），停止线。</li>
</ol>
<p>对于块状路标，经过像素聚类pixel clustering之后，语义点云的<strong>中心、主方向以及特征值eigenvalue</strong> 通过奇异值分解（SVD）计算得到。</p>
<ul>
<li>中心被视为路标的参考位置；</li>
<li>主方向表示路标的方向，可以用于数据关联data association，以及估计汽车的朝向；</li>
<li>特征值是以一种高压缩方式表征路标的形状，类似于bounding box，但是由于其考虑点云中的每一个像素，所以对于边缘噪声更不敏感。</li>
</ul>
<p>对于线状路标，它们被参数化一条包含有任意数量线段的线带：</p>
<blockquote>
<p>after pixel-level clustering, the point cloud of the road marking is fitted using a cubic polynomial along the principal direction.</p>
</blockquote>
<p><strong>采样点</strong>使用特定的间隔（如0.5m）从曲线上获取，线段直接表征着线状路标的局部朝向以及形状，可被用来进行数据关联。</p>
<p><img src="/2024/03/06/zhou2022/fig3.png" alt="fig3" title="figure 3"></p>
<p>使用上述的参数化之后，原始点云数据即可被舍弃，本参数化方法以损失一些细节局部几何信息为代价，实现了更为紧凑的参数化，而且<strong>使得进一步的复杂处理成为可能，</strong>如考虑地图中元素的概率属性或者地图匹配过程中的高效迭代。</p>
<h2 id="5-2-Modelling-the-Uncertainty"><a href="#5-2-Modelling-the-Uncertainty" class="headerlink" title="5.2 Modelling the Uncertainty"></a>5.2 Modelling the Uncertainty</h2><p>由于相机模型的透视本质，其不能像LiDAR一样直接获取环境的准确测距信息；通过IPM 转换可获取周围道路元素的3D 几何信息，但是<strong>其准确性对于像素误差、道路不规则以及汽车的姿势改变较为敏感</strong>。因此，为了概率制图与定位，进而在复杂环境中获取足够的精确度，需要对IPM 生成元素的空间不确定性进行研究。</p>
<h3 id="5-2-1-Pixel-error"><a href="#5-2-1-Pixel-error" class="headerlink" title="5.2.1 Pixel error"></a>5.2.1 Pixel error</h3><p>像素误差可以由相机模型和语义分割误差造成。</p>
<h3 id="5-2-2-Pitch-angle-error"><a href="#5-2-2-Pitch-angle-error" class="headerlink" title="5.2.2 Pitch angle error"></a>5.2.2 Pitch angle error</h3><p>在IPM 转换中，作者做了<strong>虚拟相机坐标系与地面平行的假设</strong>，但由于道路不规则以及车辆的加速刹车等原因，汽车的俯仰角相对于地面会有改变，导致上述假设失效，造成相应的误差。</p>
<h3 id="5-2-3-Road-height-error"><a href="#5-2-3-Road-height-error" class="headerlink" title="5.2.3 Road height error"></a>5.2.3 Road height error</h3><p>作者假设的虚拟相机坐标系距离地面的高度可能存在误差。</p>
<p>本文中，上述误差因子被假设服从0均值的高斯分布。在给定0.1°的俯仰角误差，0.05m的高度误差以及2像素误差时，IPM 中点的横向与纵向不确定度如Fig .4所示，可以发现：</p>
<ul>
<li>感知距离越近，不确定度越小；</li>
<li>纵向不确定度要远大于横向不确定度，已经达到分米级，不可被忽略。</li>
</ul>
<p><img src="/2024/03/06/zhou2022/fig4.png" alt="fig4" title="figure 4"></p>
<p>当把道路元素添加至全局地图中后，汽车位姿估计的不确定度也需要被考虑。</p>
<h2 id="5-3-Dealing-With-Complex-Road-Conditions"><a href="#5-3-Dealing-With-Complex-Road-Conditions" class="headerlink" title="5.3 Dealing With Complex Road Conditions"></a>5.3 Dealing With Complex Road Conditions</h2><p>只要上述提到的误差因子较小且服从相关假设，IPM 转换得到的路标概率属性可以通过<strong>不确定模型</strong>得到处理。然而，现实世界中经常出现与上述假设不符的情况（如Fig. 5所示），导致IPM 产生的系统误差可能会超过米级，不再适用于本文提出的不确定模型，因此需要采取具体措施来解决这些情况。</p>
<p><img src="/2024/03/06/zhou2022/fig5.png" alt="fig5" title="figure 5"></p>
<h2 id="5-4-Data-Association"><a href="#5-4-Data-Association" class="headerlink" title="5.4 Data Association"></a>5.4 Data Association</h2><p>尽管路标几何信息的大部分细节已经被舍弃了，但受益于路标的规则离散分布，仍然可以进行有效的数据关联。一般将路标的语义类别、参考位置、朝向以及特征值用于数据关联。</p>
<p>在定位阶段，将感知到的路标投影至地图坐标系来寻找最近的匹配，由于作者对路标进行了<strong>最小的参数化</strong>，<strong>方便进行高效的重复搜索</strong>来保持多个路标观测的最好一致性。</p>
<h1 id="6-Pose-Graph-Optimization"><a href="#6-Pose-Graph-Optimization" class="headerlink" title="6 Pose Graph Optimization"></a>6 Pose Graph Optimization</h1><p>本架构在制图与定位阶段都使用了位姿图优化，如Fig. 7所示</p>
<p><img src="/2024/03/06/zhou2022/fig7.png" alt="fig7" title="figure 7"></p>
<h2 id="Patch-toPatch-Factor"><a href="#Patch-toPatch-Factor" class="headerlink" title="Patch-toPatch Factor"></a>Patch-toPatch Factor</h2><p>将相机感知到的块状路标与全局地图中的元素进行联系，构建一个简单的<strong>点对点约束</strong>：</p>
<p><img src="/2024/03/06/zhou2022/f10.png" alt="f10" title="formula 10"></p>
<p>式中，$\tilde{p}_{p_j}^{b_k}$ 表示利用IPM 映射产生的路标观测位置， $p_{p_j}^{b_k}$  表示地图中元素在汽车坐标系中的位置。</p>
<h2 id="Line-to-Line-Factor"><a href="#Line-to-Line-Factor" class="headerlink" title="Line-to-Line Factor"></a>Line-to-Line Factor</h2><p>对于相机感知到的线状路标，其中的<strong>采样点与地图中对应线段的距离</strong>被作为构建因子：</p>
<p><img src="/2024/03/06/zhou2022/f11.png" alt="f11" title="formula 11"></p>
<p>式中，$\tilde{p}_{l_j, m}^{b_k}$ 表示IPM 产生的线状路标中一个采样点的观测位置， $p_{l_j, z}^{b_k}, p_{l_j, z+1}^{b_k}$  表示线状地图元素的对应线段在汽车坐标系中的两个顶点。</p>
<p>添加完Fig. 7所示的不同因子之后，即可构建位姿图优化问题来获取汽车位姿的最优估计。值得注意的是，作者在当前系统的部署过程中，为了简化制图过程，地图匹配（patch-to-patch，line-to-line）因子没有被考虑。在制图阶段，需要维护一个<strong>全局的位姿图</strong>；而对于定位阶段，只需要维持一个<strong>几秒的窗口来保证高效性</strong>。</p>
<p>在地图辅助定位中，<strong>路标的不确定性用来决定地图匹配因子的权重</strong>，但是地图中的元素不会进行相应的更新，因为终端用户无权进行地图更新。</p>
<h1 id="7-Experiment"><a href="#7-Experiment" class="headerlink" title="7 Experiment"></a>7 Experiment</h1><p>作者在10月13日采集两组数据序列（A1，A2），进行融合制图，采用RTK/INS（战术级INS）的平滑解作为真值；在11月28日采集了用来进行评估的数据序列（U1），如Fig. 9所示，值得注意的是，U1数据序列的采集汽车方向与A1、A2相反。</p>
<p><img src="/2024/03/06/zhou2022/fig9.png" alt="fig9" title="figure 9"></p>
<h2 id="7-1-On-Vehicle-Mapping"><a href="#7-1-On-Vehicle-Mapping" class="headerlink" title="7.1 On-Vehicle Mapping"></a>7.1 On-Vehicle Mapping</h2><p>在制图过程中，参数化的路标被添加进全局地图中，并基于空间不确定性进行聚集与融合，如Fig. 10所示。</p>
<p><img src="/2024/03/06/zhou2022/fig10.png" alt="fig10" title="figure 10"></p>
<h2 id="7-2-Multi-Source-Map-Merging"><a href="#7-2-Multi-Source-Map-Merging" class="headerlink" title="7.2 Multi-Source Map Merging"></a>7.2 Multi-Source Map Merging</h2><p>在数据序列A1与A2分别进行on-vehicle mapping之后，两个地图会进行融合，进而得到一个一致且相对完整的道路地图，如Fig. 11所示。</p>
<p><img src="/2024/03/06/zhou2022/fig11.png" alt="fig11" title="figure 11"></p>
<h2 id="7-3-Map-Aided-Localization"><a href="#7-3-Map-Aided-Localization" class="headerlink" title="7.3 Map-Aided Localization"></a>7.3 Map-Aided Localization</h2><p>终端汽车中的车载GNSS 利用standard point positioning (SPP) 来实现米级的全局定位，图片与IMU 数据用来进行基于VIO 的局部导航，结合生成的道路地图进行匹配定位，最终利用上述的所有信息进行位姿图优化得到最终的位姿估计。值得注意的是，本架构没有使用边缘化marginalization策略。</p>
<p>考虑到路标的模糊属性，汽车需要观测到足够多的可区分信息才能实现成功的重定位，如Fig. 13所示，</p>
<ol>
<li>由于指示路标（蓝色）更稀疏、更具有区分性，所以它们在地图匹配的第一阶段被使用，以保证在非常有限个候选匹配点对中进行高效匹配；</li>
<li>在经过迭代汽车位姿更为精准后，使用更多的观测路标进行地图匹配，以检查地图匹配的一致性并提高定位精度。</li>
</ol>
<p>本方法与使用稠密点云和iterative closest point (ICP) 方法相比更高效、更灵活，此外，本方法良好的建模空间不确定度有潜力实现更高的定位精度。</p>
<p><img src="/2024/03/06/zhou2022/fig13.png" alt="fig13" title="figure 13"></p>
]]></content>
      <categories>
        <category>论文记录</category>
        <category>SLAM</category>
        <category>Map Match</category>
      </categories>
      <tags>
        <tag>SLAM</tag>
        <tag>Semantic</tag>
        <tag>Map Match</tag>
      </tags>
  </entry>
  <entry>
    <title>zsh、Oh-My-Zsh及相关设置</title>
    <url>/2024/01/26/zsh-setup/</url>
    <content><![CDATA[<p>注：<strong>本文主要参考</strong><a href="https://www.kwchang0831.dev/dev-env/ubuntu/oh-my-zsh">该文章</a>。</p>
<h1 id="1-安装并设置zsh"><a href="#1-安装并设置zsh" class="headerlink" title="1 安装并设置zsh"></a>1 安装并设置zsh</h1><p>安装命令：</p>
<span id="more"></span>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt install zsh</span><br></pre></td></tr></table></figure>
<p>设置zsh为默认shell：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">chsh -s $(<span class="built_in">which</span> zsh)</span><br></pre></td></tr></table></figure>
<h1 id="2-安装Oh-My-Zsh"><a href="#2-安装Oh-My-Zsh" class="headerlink" title="2 安装Oh My Zsh"></a>2 安装Oh My Zsh</h1><p>安装命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sh -c <span class="string">&quot;<span class="subst">$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)</span>&quot;</span></span><br></pre></td></tr></table></figure>
<h1 id="3-安装主题PowerLevel10k"><a href="#3-安装主题PowerLevel10k" class="headerlink" title="3 安装主题PowerLevel10k"></a>3 安装主题PowerLevel10k</h1><p>安装命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/romkatv/powerlevel10k.git <span class="variable">$ZSH_CUSTOM</span>/themes/powerlevel10k</span><br></pre></td></tr></table></figure>
<p>进入.zshrc设置主题：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ZSH_THEME=<span class="string">&quot;powerlevel10k/powerlevel10k&quot;</span></span><br></pre></td></tr></table></figure>
<h1 id="4-安装插件"><a href="#4-安装插件" class="headerlink" title="4 安装插件"></a>4 安装插件</h1><h2 id="4-1-日常插件"><a href="#4-1-日常插件" class="headerlink" title="4.1 日常插件"></a>4.1 日常插件</h2><p>zsh-autosuggestions:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/zsh-users/zsh-autosuggestions <span class="variable">$&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;</span>/plugins/zsh-autosuggestions</span><br></pre></td></tr></table></figure>
<p>zsh-syntax-highlighting:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/zsh-users/zsh-syntax-highlighting.git <span class="variable">$&#123;ZSH_CUSTOM:-~/.oh-my-zsh/custom&#125;</span>/plugins/zsh-syntax-highlighting</span><br></pre></td></tr></table></figure>
<p>设置要启动的插件（Plugins）：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">plugins=(</span><br><span class="line">         git</span><br><span class="line">         zsh-autosuggestions </span><br><span class="line">         zsh-syntax-highlighting</span><br><span class="line">         extract</span><br><span class="line">         sudo</span><br><span class="line">         tmux</span><br><span class="line"></span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<p>值得注意的是，除了zsh-autosuggestions 与zsh-syntax-highlighting 需要单独安装外，上述的其他插件均是内嵌的，可直接添加至配置文件中直接使用。</p>
<h2 id="4-2-Tmux设置"><a href="#4-2-Tmux设置" class="headerlink" title="4.2 Tmux设置"></a>4.2 Tmux设置</h2><p><strong>该部分主要参考</strong><a href="https://louiszhai.github.io/2017/09/30/tmux/#导读">文章</a>。</p>
<p>安装命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install tmux</span><br></pre></td></tr></table></figure>
<p>tmux 的个性化设置需在主目录编辑.tmux.conf 文件，本人的文件设置如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span> -g prefix C-z</span><br><span class="line">unbind C-b</span><br><span class="line"><span class="built_in">bind</span> C-a send-prefix</span><br><span class="line"></span><br><span class="line"><span class="built_in">bind</span> r source-file ~/.tmux.conf \; display-message <span class="string">&quot;Config reloaded..&quot;</span></span><br><span class="line"></span><br><span class="line">unbind <span class="string">&#x27;&quot;&#x27;</span></span><br><span class="line"><span class="built_in">bind</span> - splitw -v -c <span class="string">&#x27;#&#123;pane_current_path&#125;&#x27;</span></span><br><span class="line">unbind %</span><br><span class="line"><span class="built_in">bind</span> | splitw -h -c <span class="string">&#x27;#&#123;pane_current_path&#125;&#x27;</span></span><br><span class="line"></span><br><span class="line">set-option -g mouse on</span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -g base-index 0</span><br><span class="line"><span class="built_in">set</span> -g pane-base-index 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># setw -g utf8 on</span></span><br><span class="line"><span class="built_in">set</span> -g status-interval 1</span><br><span class="line"><span class="built_in">set</span> -g status-justify left</span><br><span class="line">setw -g monitor-activity on</span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -g status-bg black</span><br><span class="line"><span class="built_in">set</span> -g status-fg yellow</span><br><span class="line"><span class="built_in">set</span> -g status-left <span class="string">&quot;#[bg=#FF661D] ❐ #S &quot;</span></span><br><span class="line"><span class="built_in">set</span> -g status-right <span class="string">&quot;%H:%M:%S %d-%b&quot;</span></span><br><span class="line"><span class="built_in">set</span> -g status-left-length 300</span><br><span class="line"><span class="built_in">set</span> -g status-right-length 500</span><br><span class="line"><span class="built_in">set</span> -wg window-status-current-format <span class="string">&quot; #I:#W#F &quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -g message-style <span class="string">&quot;bg=#202529, fg=#91A8BA&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> -g default-terminal <span class="string">&quot;screen-256color&quot;</span></span><br></pre></td></tr></table></figure>
<p>其他使用说明可参考上述链接。在oh my zsh 插件中添加了 tmux 后，可以使用如下快捷键：</p>
<p><img src="/2024/01/26/zsh-setup/hotkey.png" alt="hotkey" title="快捷键设置"></p>
]]></content>
      <categories>
        <category>开发环境搭建</category>
      </categories>
      <tags>
        <tag>Ubuntu</tag>
        <tag>zsh</tag>
      </tags>
  </entry>
</search>
